:orphan:

===========================
About MongoDB Documentation
===========================

.. default-domain:: mongodb

`The MongoDB Manual <http://docs.mongodb.org/manual/#>`_ contains
comprehensive documentation on the MongoDB :term:`document`-oriented
database management system. This page describes the manual's licensing,
editions, and versions, and describes how to make a change request and
how to contribute to the manual.

For more information on MongoDB, see
`MongoDB: A Document Oriented Database <http://www.mongodb.org/about/>`_.
To download MongoDB, see the
`downloads page <http://www.mongodb.org/downloads>`_.

License
-------

This manual is licensed under a Creative Commons
"`Attribution-NonCommercial-ShareAlike 3.0 Unported
<http://creativecommons.org/licenses/by-nc-sa/3.0/>`_"
(i.e. "CC-BY-NC-SA") license.

The MongoDB Manual is copyright |copy| 2011-|year| MongoDB, Inc.

Editions
--------

In addition to the `MongoDB Manual <http://docs.mongodb.org/manual/#>`_, you can
also access this content in the following editions:

- :hardlink:`ePub Format <MongoDB-manual.epub>`

- :hardlink:`Single HTML Page <single/>`

- :hardlink:`PDF Format <MongoDB-manual.pdf>` (without reference.)

- :hardlink:`HTML tar.gz <manual.tar.gz>`

You also can access PDF files that contain subsets of the MongoDB Manual:

- :hardlink:`MongoDB Reference Manual <MongoDB-reference-manual.pdf>`

- :hardlink:`MongoDB CRUD Operations <MongoDB-crud-guide.pdf>`

- :hardlink:`Data Models for MongoDB <MongoDB-data-models-guide.pdf>`

- :hardlink:`MongoDB Data Aggregation <MongoDB-aggregation-guide.pdf>`

- :hardlink:`Replication and MongoDB <MongoDB-replication-guide.pdf>`

- :hardlink:`Sharding and MongoDB <MongoDB-sharding-guide.pdf>`

- :hardlink:`MongoDB Administration <MongoDB-administration-guide.pdf>`

- :hardlink:`MongoDB Security <MongoDB-security-guide.pdf>`

MongoDB Reference documentation is also available as part of `dash
<http://kapeli.com/dash>`_. You can also access the :hardlink:`MongoDB
Man Pages <manpages.tar.gz>` which are also distributed with the
official MongoDB Packages.

Version and Revisions
---------------------

This version of the manual reflects version |version| of MongoDB.

See the `MongoDB Documentation Project Page <http://docs.mongodb.org>`_
for an overview of all editions and output formats of the MongoDB
Manual. You can see the full revision history and track ongoing
improvements and additions for all versions of the manual from its `GitHub
repository <https://github.com/mongodb/docs>`_.

This edition reflects "|branch|" branch of the documentation
as of the "|commit|" revision. This branch is explicitly accessible
via "|hardlink|" and you can always reference the commit of the
current manual in the :hardlink:`release.txt` file.

The most up-to-date, current, and stable version of the manual is
always available at "http://docs.mongodb.org/manual/".

Report an Issue or Make a Change Request
----------------------------------------

To report an issue with this manual or to make a change request, file
a ticket at the
`MongoDB DOCS Project on Jira <https://jira.mongodb.org/browse/DOCS>`_.

.. _meta-contributing:

Contribute to the Documentation
-------------------------------

.. toctree::
   :hidden:

   /meta/translation

The entire documentation source for this manual is available in the
`mongodb/docs repository <https://github.com/mongodb/docs>`_,
which is one of the
`MongoDB project repositories on GitHub <http://github.com/mongodb>`_.

To contribute to the documentation, you can open a
`GitHub account <https://github.com/>`_, fork the
`mongodb/docs repository <https://github.com/mongodb/docs>`_,
make a change, and issue a pull request.

In order for the documentation team to accept your change, you must
complete the
`MongoDB Contributor Agreement <http://www.mongodb.com/contributor>`_.

You can clone the repository by issuing the following command at your
system shell:

.. code-block:: sh

   git clone git://github.com/mongodb/docs.git

About the Documentation Process
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The MongoDB Manual uses `Sphinx <http://sphinx-doc.org//>`_, a
sophisticated documentation engine built upon `Python Docutils
<http://docutils.sourceforge.net/>`_. The original `reStructured Text
<http://docutils.sourceforge.net/rst.html>`_ files, as well as all
necessary Sphinx extensions and build tools, are available in the same
repository as the documentation.

For more information on the MongoDB documentation process, see:

.. toctree::
   :maxdepth: 1

   /meta/style-guide
   /meta/practices
   /meta/organization
   /meta/build

If you have any questions, please feel free to open a :issue:`Jira Case
<DOCS>`.

.. include:: /includes/hash.rst
===================================
Backup and Restore Sharded Clusters
===================================

.. default-domain:: mongodb

The following tutorials describe backup and restoration for sharded clusters:

.. include:: /includes/toc/dfn-list-administration-backup-sharded-clusters.rst

.. include:: /includes/toc/administration-backup-sharded-clusters.rst
===================
Backup and Recovery
===================

.. default-domain:: mongodb

The following tutorials describe backup and restoration for a
:program:`mongod` instance:

.. include:: /includes/toc/dfn-list-administration-backup-and-recovery.rst

.. include:: /includes/toc/administration-backup-and-recovery.rst
.. _configuration-file:

===============================
Run-time Database Configuration
===============================

.. default-domain:: mongodb

The :doc:`command line </reference/program/mongod>` and :doc:`configuration
file </reference/configuration-options>` interfaces provide MongoDB
administrators with a large number of options and settings for
controlling the operation of the database system. This document
provides an overview of common configurations and examples of
best-practice configurations for common use cases.

While both interfaces provide access to the same collection of options
and settings, this document primarily uses the configuration file
interface. If you run MongoDB using a control script or installed from
a package for your operating system, you likely already have a
configuration file located at ``/etc/mongodb.conf``. Confirm this by
checking the contents of the ``/etc/init.d/mongod`` or
``/etc/rc.d/mongod`` script to ensure that the :term:`control scripts
<control script>` start the :program:`mongod` with the appropriate
configuration file (see below.)

To start a MongoDB instance using this configuration issue a command in
the following form:

.. code-block:: sh

   mongod --config /etc/mongodb.conf
   mongod -f /etc/mongodb.conf

Modify the values in the ``/etc/mongodb.conf`` file on your system to
control the configuration of your database instance.

.. _base-config:

Configure the Database
----------------------

Consider the following basic configuration:

.. code-block:: cfg

   fork = true
   bind_ip = 127.0.0.1
   port = 27017
   quiet = true
   dbpath = /srv/mongodb
   logpath = /var/log/mongodb/mongod.log
   logappend = true
   journal = true

For most standalone servers, this is a sufficient base
configuration. It makes several assumptions, but consider the
following explanation:

- :setting:`fork` is ``true``, which enables a
  :term:`daemon` mode for :program:`mongod`, which detaches (i.e. "forks")
  the MongoDB from the current session and allows you to run the
  database as a conventional server.

- :setting:`bind_ip` is ``127.0.0.1``, which forces the
  server to only listen for requests on the localhost IP. Only bind to
  secure interfaces that the application-level systems can access with
  access control provided by system network filtering
  (i.e. ":term:`firewall`").

  .. |mongodb-package| replace:: :program:`mongod`

  .. include:: /includes/note-deb-and-rpm-default-to-localhost.rst

- :setting:`~net.port` is ``27017``, which is the default
  MongoDB port for database instances. MongoDB can bind to any
  port. You can also filter access based on port using network
  filtering tools.

  .. note::

     UNIX-like systems require superuser privileges to attach processes
     to ports lower than 1024.

- :setting:`~systemLog.quiet` is ``true``. This disables all but
  the most critical entries in output/log file. In normal operation
  this is the preferable operation to avoid log noise. In diagnostic
  or testing situations, set this value to ``false``. Use
  :dbcommand:`setParameter` to modify this setting during
  run time.

- :setting:`~storage.dbPath` is ``/srv/mongodb``, which
  specifies where MongoDB will store its data files. ``/srv/mongodb``
  and ``/var/lib/mongodb`` are popular locations. The user account
  that :program:`mongod` runs under will need read and write access to this
  directory.

- :setting:`systemLog.path` is ``/var/log/mongodb/mongod.log``
  which is where :program:`mongod` will write its output. If you do not set
  this value, :program:`mongod` writes all output to standard output
  (e.g. ``stdout``.)

- :setting:`logappend` is ``true``, which ensures that
  :program:`mongod` does not overwrite an existing log file
  following the server start operation.

- :setting:`storage.journal.enabled` is ``true``, which enables :term:`journaling <journal>`.
  Journaling ensures single instance write-durability. 64-bit builds
  of :program:`mongod` enable journaling by default. Thus, this
  setting may be redundant.

Given the default configuration, some of these values may be
redundant. However, in many situations explicitly stating the
configuration increases overall system intelligibility.

.. _configuration-security:

Security Considerations
-----------------------

The following collection of configuration options are useful for
limiting access to a :program:`mongod` instance. Consider the
following:

.. code-block:: cfg

   bind_ip = 127.0.0.1,10.8.0.10,192.168.4.24
   nounixsocket = true
   auth = true

Consider the following explanation for these configuration decisions:

- ":setting:`bind_ip`" has three values: ``127.0.0.1``, the localhost
  interface; ``10.8.0.10``, a private IP address typically used for
  local networks and VPN interfaces; and ``192.168.4.24``, a private
  network interface typically used for local networks.

  Because production MongoDB instances need to be accessible from
  multiple database servers, it is important to bind MongoDB to
  multiple interfaces that are accessible from your application
  servers. At the same time it's important to limit these interfaces
  to interfaces controlled and protected at the network layer.

- ":setting:`nounixsocket`" to ``true`` disables the
  UNIX Socket, which is otherwise enabled by default. This limits
  access on the local system. This is desirable when running MongoDB
  on systems with shared access, but in most situations has minimal impact.

- ":setting:`~security.authentication`" is ``true`` enables the authentication
  system within MongoDB. If enabled you will need to log in by
  connecting over the ``localhost`` interface for the first time to
  create user credentials.

.. seealso:: :doc:`/core/security`

Replication and Sharding Configuration
--------------------------------------

Replication Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~

:term:`Replica set` configuration is straightforward, and only
requires that the :setting:`~replication.replSetName` have a value that is consistent
among all members of the set. Consider the following:

.. code-block:: cfg

   replSet = set0

Use descriptive names for sets. Once configured use the
:program:`mongo` shell to add hosts to the replica set.

.. seealso:: :ref:`Replica set reconfiguration
   <replica-set-reconfiguration-usage>`.

To enable authentication for the :term:`replica set`, add the
following option:

.. code-block:: cfg

   keyFile = /srv/mongodb/keyfile

.. versionadded:: 1.8 for replica sets, and 1.9.1 for sharded replica sets.

   Setting :setting:`~security.keyFile` enables authentication and specifies a key
   file for the replica set member use to when authenticating to each
   other. The content of the key file is arbitrary, but must be the same
   on all members of the :term:`replica set` and :program:`mongos`
   instances that connect to the set. The keyfile must be less than one
   kilobyte in size and may only contain characters in the base64 set and
   the file must not have group or "world" permissions on UNIX systems.

.. seealso:: The :ref:`Replica set Reconfiguration
   <replica-set-reconfiguration-usage>` section for information regarding
   the process for changing replica set during operation.

   Additionally, consider the :ref:`Replica Set Security
   <replica-set-security>` section for information on configuring
   authentication with replica sets.

   Finally, see the :doc:`/replication` document for more information
   on replication in MongoDB and replica set configuration in general.

Sharding Configuration
~~~~~~~~~~~~~~~~~~~~~~

Sharding requires a number of :program:`mongod` instances with
different configurations. The config servers store the cluster's
metadata, while the cluster distributes data among one or more shard
servers.

.. note::

   :term:`Config servers <config database>` are not :term:`replica
   sets <replica set>`.

To set up one or three "config server" instances as :ref:`normal
<base-config>` :program:`mongod` instances, and then add the following
configuration option:

.. code-block:: cfg

   configsvr = true

   bind_ip = 10.8.0.12
   port = 27001

This creates a config server running on the private IP address
``10.8.0.12`` on port ``27001``. Make sure that there are no port
conflicts, and that your config server is accessible from all of your
:program:`mongos` and :program:`mongod` instances.

To set up shards, configure two or more :program:`mongod` instance
using your :ref:`base configuration <base-config>`, adding the
:setting:`shardsvr` setting:

.. code-block:: cfg

   shardsvr = true

Finally, to establish the cluster, configure at least one
:program:`mongos` process with the following settings:

.. code-block:: cfg

   configdb = 10.8.0.12:27001
   chunkSize = 64

You can specify multiple :setting:`~sharding.configDB` instances by specifying
hostnames and ports in the form of a comma separated list. In general,
avoid modifying the :setting:`~sharding.chunkSize` from the default value of 64,
[#chunksize]_ and *should* ensure this setting is consistent among all
:program:`mongos` instances.

.. [#chunksize] :term:`Chunk` size is 64 megabytes by default, which
   provides the ideal balance between the most even distribution of
   data, for which smaller chunk sizes are best, and minimizing chunk
   migration, for which larger chunk sizes are optimal.

.. seealso:: The :doc:`/sharding` section of the manual for more
   information on sharding and cluster configuration.

Run Multiple Database Instances on the Same System
--------------------------------------------------

In many cases running multiple instances of :program:`mongod` on a
single system is not recommended. On some types of deployments
[#multimongod]_ and for testing purposes you may need to run more than
one :program:`mongod` on a single system.

In these cases, use a :ref:`base configuration <base-config>` for each
instance, but consider the following configuration values:

.. code-block:: cfg

   dbpath = /srv/mongodb/db0/
   pidfilepath = /srv/mongodb/db0.pid

The :setting:`~storage.dbPath` value controls the location of the
:program:`mongod` instance's data directory. Ensure that each database
has a distinct and well labeled data directory. The
:setting:`pidfilepath` controls where :program:`mongod` process
places it's :term:`process id <pid>` file. As this tracks the specific
:program:`mongod` file, it is crucial that file be unique and well
labeled to make it easy to start and stop these processes.

Create additional :term:`control scripts <control script>` and/or
adjust your existing MongoDB configuration and control script as
needed to control these processes.

.. [#multimongod] Single-tenant systems with :term:`SSD` or other high
   performance disks may provide acceptable performance levels for
   multiple :program:`mongod` instances. Additionally, you may find that
   multiple databases with small working sets may function acceptably
   on a single system.

Diagnostic Configurations
-------------------------

The following configuration options control various :program:`mongod`
behaviors for diagnostic purposes. The following settings have default
values that tuned for general production purposes:

.. code-block:: cfg

   slowms = 50
   profile = 3
   verbose = true
   objcheck = true
   cpu = true

Use the :ref:`base configuration <base-config>` and add these options
if you are experiencing some unknown issue or performance problem as
needed:

- :setting:`~operationProfiling.slowOpThresholdMs` configures the threshold for the :term:`database
  profiler` to consider a query "slow." The default value is 100
  milliseconds. Set a lower value if the database profiler does not
  return useful results. See :doc:`/administration/optimization`
  for more information on optimizing operations in MongoDB.

- :setting:`profile` sets the :term:`database profiler`
  level. The profiler is not active by default because of the possible
  impact on the profiler itself on performance. Unless this setting
  has a value, queries are not profiled.

- :setting:`verbose` controls the amount of logging output that
  :program:`mongod` write to the log. Only use this option if you are
  experiencing an issue that is not reflected in the normal logging
  level.

- :setting:`objcheck` forces :program:`mongod` to validate all
  requests from clients upon receipt. Use this option to ensure that
  invalid requests are not causing errors, particularly when running a
  database with untrusted clients. This option may affect database
  performance.

- :setting:`cpu` forces :program:`mongod` to report the percentage of
  the last interval spent in :term:`write lock`. The interval is
  typically 4 seconds, and each output line in the log includes both
  the actual interval since the last report and the percentage of
  time spent in write lock.
===============
Data Management
===============

.. default-domain:: mongodb

These document introduce data management practices and strategies for
MongoDB deployments, including strategies for managing multi-data
center deployments, managing larger file stores, and data lifecycle
tools.

.. include:: /includes/toc/dfn-list-administration-data-management.rst

.. include:: /includes/toc/administration-data-management.rst
========================
Index Creation Tutorials
========================

.. default-domain:: mongodb

Instructions for creating and configuring indexes in MongoDB and
building indexes on replica sets and sharded clusters.

.. include:: /includes/toc/dfn-list-indexes-tutorial-creation.rst

.. include:: /includes/toc/indexes-tutorial-creation.rst
==========================
Geospatial Index Tutorials
==========================

.. default-domain:: mongodb

Instructions for creating and querying ``2d``, ``2dsphere``, and
haystack indexes.

.. include:: /includes/toc/dfn-list-indexes-tutorial-geo.rst

.. include:: /includes/toc/indexes-tutorial-geo.rst
==========================
Index Management Tutorials
==========================

.. default-domain:: mongodb

Instructions for managing indexes and assessing index performance and
use.

.. include:: /includes/toc/dfn-list-indexes-tutorial-management.rst

.. include:: /includes/toc/indexes-tutorial-management.rst
=====================
Text Search Tutorials
=====================

.. default-domain:: mongodb

Instructions for enabling MongoDB's text search feature, and for
building and configuring text indexes.

.. include:: /includes/toc/dfn-list-indexes-tutorial-text.rst

.. include:: /includes/toc/indexes-tutorial-text.rst
===================
Indexing Tutorials
===================

.. default-domain:: mongodb

Indexes allow MongoDB to process and fulfill queries quickly by creating
small and efficient representations of the documents in a collection.

The documents in this section outline specific tasks related to
building and maintaining indexes for data in MongoDB collections and
discusses strategies and practical approaches. For a conceptual
overview of MongoDB indexing, see the :doc:`/core/indexes` document.

.. include:: /includes/toc/dfn-list-indexes-tutorials-landing.rst

.. include:: /includes/toc/indexes-tutorials-landing.rst
==========================
Install MongoDB Enterprise
==========================

.. default-domain:: mongodb

These documents provide instructions to install MongoDB Enterprise for
Linux and Windows Systems.

.. include:: /includes/toc/dfn-list-installation-enterprise.rst

.. include:: /includes/toc/installation-enterprise.rst
================
Install on Linux
================

.. default-domain:: mongodb

These documents provide instructions to install MongoDB for various
Linux systems.

Recommended
-----------

For easy installation, MongoDB provides packages for popular
Linux distributions. The following guides detail the installation
process for these systems:

.. include:: /includes/toc/dfn-list-spec-installation-linux-packages.rst

For systems without supported packages, refer to the Manual
Installation tutorial.

Manual Installation
-------------------

Although packages are the preferred installation method, for Linux systems
without supported packages, see the following guide:

.. include:: /includes/toc/dfn-list-spec-installation-linux-other.rst

.. include:: /includes/toc/installation-linux.rst
========================================
Configuration, Maintenance, and Analysis
========================================

.. default-domain:: mongodb

The following tutorials describe routine management operations,
including configuration and performance analysis:

.. include:: /includes/toc/dfn-list-administration-routine.rst

.. include:: /includes/toc/administration-routine.rst
======================
Monitoring for MongoDB
======================

.. default-domain:: mongodb

Monitoring is a critical component of all database administration. A
firm grasp of MongoDB's reporting will allow you to assess the state
of your database and maintain your deployment without crisis.
Additionally, a sense of MongoDB's normal operational parameters will
allow you to diagnose before they escalate to failures.

This document presents an overview of the available monitoring utilities
and the reporting statistics
available in MongoDB. It also introduces diagnostic strategies
and suggestions for monitoring replica sets and
sharded clusters.

.. note::

   `MongoDB Management Service (MMS)
   <https://mms.mongodb.com/?pk_campaign=mongodb-org&pk_kwd=monitoring>`_
   is a hosted monitoring service which collects and aggregates data
   to provide insight into the performance and operation of MongoDB
   deployments. See the `MMS documentation
   <http://mms.mongodb.com/help/>`_ for more information.

Monitoring Strategies
---------------------

There are three methods for collecting data about the state of a
running MongoDB instance:

- First, there is a set of utilities distributed with MongoDB that
  provides real-time reporting of database activities.

- Second, :doc:`database commands </reference/command>` return
  statistics regarding the current database state with greater
  fidelity.

- Third, `MMS Monitoring Service
  <https://mms.mongodb.com/?pk_campaign=mongodb-org&pk_kwd=monitoring>`_
  collects data from running MongoDB deployments and provides
  visualization and alerts based on that data. MMS is a free service
  provided by MongoDB.

Each strategy can help answer different questions and is useful in
different contexts. These methods are complementary.

MongoDB Reporting Tools
-----------------------

This section provides an overview of the reporting methods distributed
with MongoDB. It also offers examples of the kinds of questions that
each method is best suited to help you address.

Utilities
~~~~~~~~~

The MongoDB distribution includes a number of utilities that quickly
return statistics about instances' performance and activity. Typically,
these are most useful for diagnosing issues and assessing normal
operation.

``mongostat``
`````````````

:program:`mongostat` captures and returns the counts of database
operations by type (e.g. insert, query, update, delete, etc.). These
counts report on the load distribution on the server.

Use :program:`mongostat` to understand the distribution of operation types
and to inform capacity planning. See the :doc:`mongostat manual
</reference/program/mongostat>` for details.

``mongotop``
````````````

:program:`mongotop` tracks and reports the current read and write
activity of a MongoDB instance, and reports these statistics on a per
collection basis.

Use :program:`mongotop` to check if your database activity and use
match your expectations. See the :doc:`mongotop manual
</reference/program/mongotop>` for details.

.. _rest-interface:

REST Interface
``````````````

MongoDB provides a simple REST interface that can be useful for configuring
monitoring and alert scripts, and for other administrative tasks.

To enable, configure :program:`mongod` to use :term:`REST`, either by
starting :program:`mongod` with the :setting:`--rest <REST>` option,
or by setting the :setting:`net.http.RESTInterfaceEnabled` setting to ``true`` in a
:doc:`configuration file </reference/configuration-options/>`.

For more information on using the REST Interface see, the
:ecosystem:`Simple REST Interface </tools/http-interfaces>`
documentation.

.. _http-console:

HTTP Console
````````````

MongoDB provides a web interface that exposes diagnostic
and monitoring information in a simple web page. The web interface is
accessible at ``localhost:<port>``, where the
``<port>`` number is **1000** more than the :program:`mongod` port .

For example, if a locally running :program:`mongod` is using the
default port ``27017``, access the HTTP console at
``http://localhost:28017``.

Commands
~~~~~~~~

MongoDB includes a number of commands that report on the state of the
database.

These data may provide a finer level of granularity than the utilities
discussed above. Consider using their output in scripts and programs to
develop custom alerts, or to modify the behavior of your application in
response to the activity of your instance. The :method:`db.currentOp`
method is another useful tool for identifying the database instance's
in-progress operations.

``serverStatus``
````````````````

The :dbcommand:`serverStatus` command, or :method:`db.serverStatus()`
from the shell, returns a general overview of the status of the
database, detailing disk usage, memory use, connection, journaling,
and index access. The command returns quickly and does not impact
MongoDB performance.

:dbcommand:`serverStatus` outputs an account of the state of a MongoDB
instance. This command is rarely run directly. In most cases, the data
is more meaningful when aggregated, as one would see with monitoring
tools including `MMS <http://mms.mongodb.com>`_ .
Nevertheless, all administrators should be familiar with the data
provided by :dbcommand:`serverStatus`.

``dbStats``
```````````

The :dbcommand:`dbStats` command, or :method:`db.stats()` from the shell,
returns a document that addresses storage use and data volumes. The
:dbcommand:`dbStats` reflect the amount of
storage used, the quantity of data contained in the database, and
object, collection, and index counters.

Use this data to monitor the state and storage capacity
of a specific database. This output also allows you to compare
use between databases and to determine the average
:term:`document` size in a database.

``collStats``
`````````````

The :dbcommand:`collStats` provides
statistics that resemble :dbcommand:`dbStats` on the collection level,
including a count of the objects in the collection, the size of
the collection, the amount of disk space used by the collection, and
information about its indexes.

``replSetGetStatus``
````````````````````

The :dbcommand:`replSetGetStatus` command (:method:`rs.status()` from
the shell) returns an overview of your replica set's status. The :doc:`replSetGetStatus
</reference/command/replSetGetStatus>` document details the
state and configuration of the replica set and statistics about its members.

Use this data to ensure that replication is properly configured,
and to check the connections between the current host and the other members
of the replica set.

Third Party Tools
~~~~~~~~~~~~~~~~~

A number of third party monitoring tools have support for MongoDB,
either directly, or through their own plugins.

Self Hosted Monitoring Tools
````````````````````````````

These are monitoring tools that you must install, configure and maintain
on your own servers. Most are open source.

.. list-table::
   :header-rows: 1

   * - **Tool**

     - **Plugin**

     - **Description**

   * - `Ganglia <http://sourceforge.net/apps/trac/ganglia/wiki>`_

     - `mongodb-ganglia <https://github.com/quiiver/mongodb-ganglia>`_

     - Python script to report operations per second, memory usage,
       btree statistics, master/slave status and current connections.

   * - Ganglia

     - `gmond_python_modules <https://github.com/ganglia/gmond_python_modules>`_

     - Parses output from the :dbcommand:`serverStatus` and
       :dbcommand:`replSetGetStatus` commands.

   * - `Motop <https://github.com/tart/motop>`_
     - *None*

     - Realtime monitoring tool for MongoDB servers. Shows
       current operations ordered by durations every second.

   * - `mtop <https://github.com/beaufour/mtop>`_

     - *None*

     - A top like tool.

   * - `Munin <http://munin-monitoring.org/>`_

     - `mongo-munin <https://github.com/erh/mongo-munin>`_

     - Retrieves server statistics.

   * - Munin

     - `mongomon <https://github.com/pcdummy/mongomon>`_

     - Retrieves collection statistics (sizes, index sizes, and each
       (configured) collection count for one DB).

   * - Munin

     - `munin-plugins Ubuntu PPA
       <https://launchpad.net/~chris-lea/+archive/munin-plugins>`_

     - Some additional munin plugins not in the main distribution.

   * - `Nagios <http://www.nagios.org/>`_

     - `nagios-plugin-mongodb
       <https://github.com/mzupan/nagios-plugin-mongodb>`_

     - A simple Nagios check script, written in Python.

   * - `Zabbix <http://www.zabbix.com/>`_

     - `mikoomi-mongodb <https://code.google.com/p/mikoomi/wiki/03>`_

     - Monitors availability, resource utilization, health,
       performance and other important metrics.

Also consider `dex <https://github.com/mongolab/dex>`_, an index and
query analyzing tool for MongoDB that compares MongoDB log files and
indexes to make indexing recommendations.

As part of `MongoDB Enterprise <http://www.mongodb.com/products/mongodb-enterprise>`_,
you can run `MMS On-Prem <http://mms.mongodb.com>`_,
which offers the features of MMS in a package that runs within your
infrastructure.

Hosted (SaaS) Monitoring Tools
``````````````````````````````

These are monitoring tools provided as a hosted service, usually through
a paid subscription.

.. list-table::
   :header-rows: 1

   * - **Name**

     - **Notes**

   * - `MongoDB Management Service <https://mms.mongodb.com/?pk_campaign=mongodb-org&pk_kwd=monitoring>`_

     - MMS is a cloud-based suite of services for managing MongoDB
       deployments. MMS provides monitoring and backup functionality.

   * - `Scout <http://scoutapp.com>`_

     - Several plugins, including `MongoDB Monitoring
       <https://scoutapp.com/plugin_urls/391-mongodb-monitoring>`_,
       `MongoDB Slow Queries
       <http://scoutapp.com/plugin_urls/291-mongodb-slow-queries>`_,
       and `MongoDB Replica Set Monitoring
       <http://scoutapp.com/plugin_urls/2251-mongodb-replica-set-monitoring>`_.

   * - `Server Density <http://www.serverdensity.com>`_

     - `Dashboard for MongoDB
       <http://www.serverdensity.com/mongodb-monitoring/>`_, MongoDB
       specific alerts, replication failover timeline and iPhone, iPad
       and Android mobile apps.

.. _stdout:
.. _standard-output:
.. _monitoring-standard-loggging:

Process Logging
---------------

During normal operation, :program:`mongod` and :program:`mongos`
instances report a live account of all server activity and operations
to either
standard output or a log file. The following runtime settings
control these options.

- :setting:`~systemLog.quiet`. Limits the amount of information written to the
  log or output.

- :setting:`~systemLog.verbosity`. Increases the amount of information written to
  the log or output.

- :setting:`systemLog.path`. Enables logging to a file, rather than the standard
  output. You must specify the full path to the log file when adjusting
  this setting.

- :setting:`logappend`. Adds information to a log
  file instead of overwriting the file.

.. note::

   You can specify these configuration operations as the command line
   arguments to :doc:`mongod </reference/program/mongod>` or :doc:`mongos
   </reference/program/mongos>`

   For example:

   .. code-block:: javascript

      mongod -v --logpath /var/log/mongodb/server1.log --logappend

   Starts a :program:`mongod` instance in :setting:`verbose
   <systemLog.verbosity>` mode, appending data to the log file at
   ``/var/log/mongodb/server1.log/``.

The following :term:`database commands <database command>` also
affect logging:

- :dbcommand:`getLog`. Displays recent messages from the
  :program:`mongod` process log.

- :dbcommand:`logRotate`. Rotates the log files for :program:`mongod`
  processes only. See :doc:`/tutorial/rotate-log-files`.

Diagnosing Performance Issues
-----------------------------

Degraded performance in MongoDB
is typically a function of the relationship between the
quantity of data stored in the database, the amount of system RAM, the
number of connections to the database, and the amount of time the
database spends in a locked state.

In some cases performance issues may be transient and related to
traffic load, data access patterns, or the availability of hardware on
the host system for virtualized environments. Some users also
experience performance limitations as a result of inadequate or
inappropriate indexing strategies, or as a consequence of poor schema
design patterns. In other situations, performance issues may indicate
that the database may be operating at capacity and that it is time to
add additional capacity to the database.

The following are some causes of degraded performance in MongoDB.

Locks
~~~~~

MongoDB uses a locking system to ensure data set consistency. However, if
certain operations are long-running, or a queue forms, performance
will slow as requests and operations wait for the lock. Lock-related
slowdowns can be intermittent. To see if the lock has been affecting
your performance, look to the data in the
:ref:`globalLock` section of the :dbcommand:`serverStatus` output. If
:data:`globalLock.currentQueue.total
<serverStatus.globalLock.currentQueue.total>` is consistently high,
then there is a chance that a large number of requests are waiting for
a lock. This indicates a possible concurrency issue that may be affecting
performance.

If :data:`globalLock.totalTime <serverStatus.globalLock.totalTime>` is
high relative to :data:`~serverStatus.uptime`, the database has
existed in a lock state for a significant amount of time. If
:data:`globalLock.ratio <serverStatus.globalLock.ratio>` is also high,
MongoDB has likely been processing a large number of long running
queries. Long queries are often the result of a number of factors:
ineffective use of indexes, non-optimal schema design, poor query
structure, system architecture issues, or insufficient RAM resulting
in :ref:`page faults <administration-monitoring-page-faults>` and disk
reads.

Memory Usage
~~~~~~~~~~~~

MongoDB uses memory mapped files to store data. Given a data
set of sufficient size, the MongoDB process will allocate all
available memory on the system for its use.
While this is part of the design, and affords MongoDB superior
performance, the memory mapped files make it difficult to determine if
the amount of RAM is sufficient for the data set.

The :ref:`memory usage statuses <memory-status>` metrics of the
:dbcommand:`serverStatus` output can provide insight into MongoDB's
memory use. Check the resident memory use
(i.e. :data:`mem.resident <serverStatus.mem.resident>`): if this
exceeds the amount of system memory *and* there is a significant amount
of data on disk that isn't in RAM, you may have exceeded the capacity
of your system.

You should also check the amount of mapped memory (i.e. :data:`mem.mapped
<serverStatus.mem.mapped>`.) If this value is greater than the amount
of system memory, some operations will require disk access :term:`page
faults <page fault>` to read data from virtual memory and negatively
affect performance.

.. _administration-monitoring-page-faults:

Page Faults
~~~~~~~~~~~

A page fault occurs when MongoDB requires data
not located in physical memory, and must read from virtual memory. To
check for page faults, see the :data:`extra_info.page_faults
<serverStatus.extra_info.page_faults>` value in the
:dbcommand:`serverStatus` output. This data is only available on
Linux systems.

.. we should reverify the previous statement as the info is appearing
   on OS X as well

A single page fault completes quickly and is not problematic. However, in
aggregate, large volumes of page faults typically indicate that MongoDB
is reading too much data from disk. In many situations, MongoDB's
read locks will "yield" after a page fault to allow other processes to
read and avoid blocking while waiting for the next page to read into
memory. This approach improves concurrency, and also improves overall
throughput in high volume systems.

Increasing the amount of RAM accessible to MongoDB may
help reduce the number of page faults. If this is not possible, you
may want to consider deploying a :term:`sharded cluster` and/or
adding :term:`shards <shard>` to your deployment to
distribute load among :program:`mongod` instances.

Number of Connections
~~~~~~~~~~~~~~~~~~~~~

In some cases, the number of connections between the application layer
(i.e. clients) and the database can overwhelm the ability of the
server to handle requests. This can produce performance
irregularities. The following fields in the :dbcommand:`serverStatus`
document can provide insight:

- :data:`globalLock.activeClients
  <serverStatus.globalLock.activeClients>` contains a counter of the
  total number of clients with active operations in progress or
  queued.

- :data:`~serverStatus.connections` is a container for the following
  two fields:

  - :data:`~serverStatus.connections.current` the total number of
    current clients that connect to the database instance.

  - :data:`~serverStatus.connections.available` the total number of
    unused collections available for new clients.

If requests are high because there are numerous concurrent application
requests, the database may have trouble keeping up with demand. If
this is the case, then you will need to increase the capacity of your
deployment. For read-heavy applications increase the size of your
:term:`replica set` and distribute read operations to
:term:`secondary` members. For write heavy applications, deploy
:term:`sharding` and add one or more :term:`shards <shard>` to a
:term:`sharded cluster` to distribute load among :program:`mongod`
instances.

Spikes in the number of connections can also be the result of
application or driver errors. All of the officially supported MongoDB
drivers implement connection pooling, which allows clients to use and
reuse connections more efficiently. Extremely high numbers of
connections, particularly without corresponding workload is often
indicative of a driver or other configuration error.

Unless constrained by system-wide limits MongoDB has no limit on
incoming connections. You can modify system limits
using the ``ulimit`` command, or by editing your system's
``/etc/sysctl`` file. See :doc:`/reference/ulimit` for more
information.

.. _database-profiling:

Database Profiling
~~~~~~~~~~~~~~~~~~

MongoDB's "Profiler" is a database profiling system that can help identify
inefficient queries and operations.

The following profiling levels are available:

.. list-table::
   :header-rows: 1

   * - **Level**

     - **Setting**

   * - 0

     - Off. No profiling

   * - 1

     - On. Only includes *"slow"* operations

   * - 2

     - On. Includes *all* operations

Enable the profiler by setting the
:dbcommand:`profile` value using the following command in the
:program:`mongo` shell:

.. code-block:: javascript

   db.setProfilingLevel(1)

The :setting:`~operationProfiling.slowOpThresholdMs` setting defines what constitutes a "slow"
operation. To set the threshold above which the profiler considers
operations "slow" (and thus, included in the level ``1`` profiling
data), you can configure :setting:`~operationProfiling.slowOpThresholdMs` at runtime as an argument to
the :method:`db.setProfilingLevel()` operation.

.. see:: The documentation of :method:`db.setProfilingLevel()` for more
   information about this command.

By default, :program:`mongod` records all "slow" queries to its
:setting:`log <logpath>`, as defined by :setting:`~operationProfiling.slowOpThresholdMs`. Unlike log data, the data in
``system.profile`` does not persist between :program:`mongod`
restarts.

.. note::

   Because the database profiler can negatively impact
   performance, only enable profiling for strategic intervals and as
   minimally as possible on production systems.

   You may enable profiling on a per-:program:`mongod` basis. This
   setting will not propagate across a :term:`replica set` or
   :term:`sharded cluster`.

You can view the output of the profiler in the ``system.profile``
collection of your database by issuing the ``show profile`` command in
the :program:`mongo` shell, or with the following operation:

.. code-block:: javascript

   db.system.profile.find( { millis : { $gt : 100 } } )

This returns all operations that lasted longer than 100 milliseconds.
Ensure that the value specified here (``100``, in this example) is above the
:setting:`~operationProfiling.slowOpThresholdMs` threshold.

.. seealso:: :doc:`/administration/optimization` addresses strategies
   that may improve the performance of your database queries and
   operations.

.. _replica-set-monitoring:

Replication and Monitoring
--------------------------

Beyond the basic monitoring requirements for any MongoDB instance, for
replica sets, administrators must monitor *replication
lag*. "Replication lag" refers to the amount of time that it takes to
copy (i.e. replicate) a write operation on the :term:`primary` to a
:term:`secondary`. Some small delay period may be acceptable, but two
significant problems emerge as replication lag grows:

- First, operations that occurred during the period of lag are not
  replicated to one or more secondaries. If you're using replication
  to ensure data persistence, exceptionally long delays may impact the
  integrity of your data set.

- Second, if the replication lag exceeds the length of the operation
  log (:term:`oplog`) then MongoDB will have to perform an initial
  sync on the secondary, copying all data from the :term:`primary` and
  rebuilding all indexes. This is uncommon under normal circumstances,
  but if you configure the oplog to be smaller than the default,
  the issue can arise.

  .. note::

     The size of the oplog is only configurable during the first
     run using the :option:`--oplogSize <mongod --oplogSize>` argument to
     the :program:`mongod` command, or preferably, the
     :setting:`~replication.oplogSizeMB` setting
     in the MongoDB configuration file. If you do not specify this on the
     command line before running with the :option:`--replSet <mongod --replSet>`
     option, :program:`mongod` will create a default sized oplog.

     By default, the oplog is 5 percent of total available disk space
     on 64-bit systems. For more information about changing the oplog
     size, see the :doc:`/tutorial/change-oplog-size`

For causes of replication lag, see :ref:`Replication Lag
<replica-set-replication-lag>`.

Replication issues are most often the result of network connectivity
issues between members, or the result of a :term:`primary` that does not
have the resources to support application and replication traffic. To
check the status of a replica, use the :dbcommand:`replSetGetStatus` or
the following helper in the shell:

.. code-block:: javascript

   rs.status()

The :doc:`/reference/command/replSetGetStatus` document provides a more in-depth
overview view of this output. In general, watch the value of
:data:`~replSetGetStatus.members.optimeDate`, and pay particular attention
to the time difference between the :term:`primary` and the
:term:`secondary` members.

Sharding and Monitoring
-----------------------

In most cases, the components of :term:`sharded clusters <sharded cluster>`
benefit from the same monitoring and analysis as all other MongoDB
instances. In addition, clusters require further monitoring to ensure
that data is effectively distributed among nodes and that sharding
operations are functioning appropriately.

.. seealso:: See the :doc:`/core/sharding` documentation for more
   information.

Config Servers
~~~~~~~~~~~~~~

The :term:`config database` maintains a map identifying which
documents are on which shards. The cluster updates this map as
:term:`chunks <chunk>` move between shards. When a configuration
server becomes inaccessible, certain sharding operations become
unavailable, such as moving chunks and starting :program:`mongos`
instances. However, clusters remain accessible from already-running
:program:`mongos` instances.

Because inaccessible configuration servers can seriously impact
the availability of a sharded cluster, you should monitor your
configuration servers to ensure that the cluster remains well
balanced and that :program:`mongos` instances can restart.

`MMS Monitoring <http://mms.mongodb.com>`_ monitors config servers and
can create notifications if a config server becomes inaccessible.

Balancing and Chunk Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The most effective :term:`sharded cluster` deployments evenly balance
:term:`chunks <chunk>` among the shards. To facilitate this, MongoDB
has a background :term:`balancer` process that distributes data to ensure that
chunks are always optimally distributed among the :term:`shards <shard>`.

Issue the :method:`db.printShardingStatus()` or :method:`sh.status()`
command to the :program:`mongos` by way of the :program:`mongo`
shell. This returns an overview of the entire cluster including the
database name, and a list of the chunks.

Stale Locks
~~~~~~~~~~~

In nearly every case, all locks used by the balancer are automatically
released when they become stale. However, because any long lasting
lock can block future balancing, it's important to ensure that all
locks are legitimate. To check the lock status of the database,
connect to a :program:`mongos` instance using the :program:`mongo`
shell. Issue the following command sequence to switch to the
``config`` database and display all outstanding locks on the shard database:

.. code-block:: javascript

   use config
   db.locks.find()

For active deployments, the above query can provide insights.
The balancing process, which originates on a randomly selected
:program:`mongos`, takes a special "balancer" lock that prevents other
balancing activity from transpiring. Use the following command, also
to the ``config`` database, to check the status of the "balancer"
lock.

.. code-block:: javascript

   db.locks.find( { _id : "balancer" } )

If this lock exists, make sure that the balancer process is actively
using this lock.
===================================
Optimization Strategies for MongoDB
===================================

.. default-domain:: mongodb

There are many factors that can affect database performance and
responsiveness including index use, query structure, data models and
application design, as well as operational factors such as architecture
and system configuration.

This section describes techniques for optimizing application
performance with MongoDB.

.. include:: /includes/toc/dfn-list-administration-optimization.rst

.. include:: /includes/toc/administration-optimization.rst
================
Production Notes
================

.. default-domain:: mongodb

This page details system configurations that affect MongoDB,
especially in production.

.. note::
   `MongoDB Management Service (MMS) <http://mms.mongodb.com>`_ is a hosted monitoring service
   which collects and aggregates diagnostic data to provide insight into
   the performance and operation of MongoDB deployments. See the
   `MMS Website <http://mms.mongodb.com/>`_ and the
   `MMS documentation <http://mms.mongodb.com/help/>`_ for more
   information.

Packages
--------

MongoDB
~~~~~~~

Be sure you have the latest stable release.
All releases are available on the `Downloads
<http://www.mongodb.org/downloads>`_ page. This is a good place to
verify what is current, even if you then choose to install via a
package manager.

Always use 64-bit builds for production. The 32-bit build MongoDB
offers for test and development environments is not suitable for
production deployments as it can store no more than 2GB of data.
See the :ref:`32-bit limitations
<faq-32-bit-limitations>` for more information.

32-bit builds exist to support use on development machines.

Operating Systems
~~~~~~~~~~~~~~~~~

MongoDB distributions are currently available for Mac OS X, Linux,
Windows Server 2008 R2 64bit, Windows 7 (32 bit and 64 bit), Windows
Vista, and Solaris platforms.

.. include:: /includes/note-minimum-glibc.rst

Concurrency
-----------

In earlier versions of MongoDB, all write operations contended for a
single readers-writer lock on the MongoDB instance. As of version 2.2,
each database has a readers-writer lock that allows concurrent reads
access to a database, but gives exclusive access to a single write
operation per database. See the :doc:`Concurrency </faq/concurrency/>`
page for more information.

.. In future, Concurrency should perhaps go with a section detailing bulk
   inserts & write throughput.

Journaling
----------

MongoDB uses *write ahead logging* to an on-disk :term:`journal` to
guarantee that MongoDB is able to quickly recover the :doc:`write
operations </core/write-operations>` following a crash or
other serious failure.

In order to ensure that :program:`mongod` will be able to recover its
data files and keep the data files in a valid state following a crash,
leave journaling enabled. See :doc:`Journaling </core/journaling/>` for
more information.

Networking
----------

Use Trusted Networking Environments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Always run MongoDB in a *trusted environment*, with network rules that
prevent access from *all* unknown machines, systems, and networks. As
with any sensitive system dependent on network access, your MongoDB
deployment should only be accessible to specific systems that require
access, such as application servers, monitoring services, and other MongoDB
components.

.. note::
   By default, :setting:`~security.authentication` is not enabled and :program:`mongod`
   assumes a trusted environment. You can enable :doc:`security/auth
   </core/security/>` mode if you need it.

See documents in the :doc:`/security` section for additional
information, specifically:

- :ref:`security-port-numbers`
- :ref:`security-firewalls`
- :doc:`/tutorial/configure-linux-iptables-firewall`
- :doc:`/tutorial/configure-windows-netsh-firewall`

For Windows users, consider the `Windows Server Technet Article on TCP
Configuration <http://technet.microsoft.com/en-us/library/dd349797.aspx>`_
when deploying MongoDB on Windows.

Connection Pools
~~~~~~~~~~~~~~~~

To avoid overloading the connection resources of a single
:program:`mongod` or :program:`mongos` instance, ensure that clients
maintain reasonable connection pool sizes.

.. TODO: explain how to maintain reasonable pool sizes

The :dbcommand:`connPoolStats` database
command returns information regarding the number of open connections
to the current database for :program:`mongos` instances and
:program:`mongod` instances in sharded clusters.

Hardware Considerations
-----------------------

MongoDB is designed specifically with commodity hardware in mind and
has few hardware requirements or limitations. MongoDB's core components
run on little-endian hardware, primarily x86/x86_64 processors. Client
libraries (i.e. drivers) can run on big or little endian systems.

Hardware Requirements and Limitations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The hardware for the most effective MongoDB deployments have the
following properties:

Allocate Sufficient RAM and CPU
```````````````````````````````

As with all software, more RAM and a faster CPU clock speed are
important for performance.

In general, databases are not CPU bound. As such, increasing the
number of cores can help, but does not provide significant marginal
return.

Use Solid State Disks (SSDs)
````````````````````````````

MongoDB has good results and a good price-performance ratio with SATA
SSD (Solid State Disk).

Use SSD if available and economical. Spinning disks can be performant, but
SSDs' capacity for random I/O operations works well with the update
model of :program:`mongod`.

Commodity (SATA) spinning drives are often a good option, as the
random I/O performance increase with more expensive spinning drives is not that
dramatic (only on the order of 2x). Using SSDs or increasing RAM may
be more effective in increasing I/O throughput.

Avoid Remote File Systems
`````````````````````````

- Remote file storage can create performance problems in MongoDB. See
  :ref:`production-nfs` for more information about storage and MongoDB.

.. _production-numa:

MongoDB and NUMA Hardware
~~~~~~~~~~~~~~~~~~~~~~~~~

.. important:: The discussion of NUMA in this section only applies to
   Linux, and therefore does not affect deployments where :program:`mongod`
   instances run other UNIX-like systems or on Windows.

Running MongoDB on a system with Non-Uniform Access Memory (NUMA) can
cause a number of operational problems, including slow performance for
periods of time or high system process usage.

When running MongoDB on NUMA hardware, you should disable NUMA for
MongoDB and instead set an interleave memory policy.

.. note::

   MongoDB version 2.0 and greater checks these settings on start up
   when deployed on a Linux-based system, and prints a warning if the
   system is NUMA-based.

To disable NUMA for MongoDB and set an interleave memory policy, use
the ``numactl`` command and start :program:`mongod` in the following
manner:

.. code-block:: sh

   numactl --interleave=all /usr/bin/local/mongod

Then, disable *zone reclaim* in the ``proc`` settings using the following
command:

.. code-block:: sh

   echo 0 > /proc/sys/vm/zone_reclaim_mode

To fully disable NUMA, you must perform both operations. For more
information, see the `Documentation for /proc/sys/vm/*
<http://www.kernel.org/doc/Documentation/sysctl/vm.txt>`_.

See the `The MySQL "swap insanity" problem and the effects of NUMA
<http://jcole.us/blog/archives/2010/09/28/mysql-swap-insanity-and-the
-nu ma-architecture/>`_ post, which describes the effects of NUMA on
databases. This blog post addresses the impact of NUMA for MySQL,
but the issues for MongoDB are similar. The post introduces NUMA and
its goals, and illustrates how these goals are not compatible with
production databases.

Disk and Storage Systems
~~~~~~~~~~~~~~~~~~~~~~~~

Swap
````

Assign swap space for your systems. Allocating swap space can avoid issues
with memory contention and can prevent the OOM Killer on Linux systems
from killing :program:`mongod`.

The method :program:`mongod` uses to map memory files to memory ensures
that the operating system will never store MongoDB data in swap space.

RAID
````

Most MongoDB deployments should use disks backed by RAID-10.

RAID-5 and RAID-6 do not typically provide sufficient performance to
support a MongoDB deployment.

Avoid RAID-0 with MongoDB deployments. While RAID-0 provides good write
performance, it also provides limited availability and can lead to
reduced performance on read operations, particularly when using
Amazon's EBS volumes.

.. _production-nfs:

Remote Filesystems
``````````````````

The Network File System protocol (NFS) is not recommended for use with
MongoDB as some versions perform poorly.

Performance problems arise when both
the data files and the journal files are hosted on NFS. You may
experience better performance if you place the journal on local or
``iscsi`` volumes. If you must use NFS, add the following NFS options
to your ``/etc/fstab`` file: ``bg``, ``nolock``, and ``noatime``.

Separate Components onto Different Storage Devices
``````````````````````````````````````````````````

For improved performance, consider separating your database's data,
journal, and logs onto different storage devices, based on your application's
access and write pattern.

.. note::
   This will affect your ability to create snapshot-style backups of
   your data, since the files will be on different devices and volumes.

Architecture
------------

Write Concern
~~~~~~~~~~~~~

.. include:: /includes/introduction-write-concern.rst

See :doc:`/core/write-concern` for more information about choosing
an appropriate write concern level for your deployment.

Replica Sets
~~~~~~~~~~~~

See :doc:`/core/replica-set-architectures` for an overview of
architectural considerations for replica set deployments.

Sharded Clusters
~~~~~~~~~~~~~~~~

See :doc:`/core/sharded-cluster-architectures-production` for an
overview of recommended sharded cluster architectures for production
deployments.

Platforms
---------

MongoDB on Linux
~~~~~~~~~~~~~~~~

.. important:: The following discussion only applies to
   Linux, and therefore does not affect deployments where :program:`mongod`
   instances run other UNIX-like systems or on Windows.

Kernel and File Systems
```````````````````````

When running MongoDB in production on Linux, it is recommended that you use
Linux kernel version 2.6.36 or later.

MongoDB preallocates its database files before using them and
often creates large files. As such, you should
use the Ext4 and XFS file systems:

- In general, if you use the Ext4 file system, use at least version ``2.6.23`` of the
  Linux Kernel.

- In general, if you use the XFS file system, use at least version ``2.6.25`` of the
  Linux Kernel.

- Some Linux distributions require different versions of the kernel to
  support using ext4 and/or xfs:

  .. include:: /includes/table/linux-kernel-version-production.rst

.. important:: MongoDB requires a filesystem that supports ``fsync()``
   *on directories*. For example, HGFS and Virtual Box's shared
   folders do *not* support this operation.

Recommended Configuration
`````````````````````````

- Turn off ``atime`` for the storage volume containing the :term:`database
  files <dbpath>`.

- Set the file descriptor limit, ``-n``, and the user process limit
  (ulimit), ``-u``, above 20,000,
  according to the suggestions in the :doc:`/reference/ulimit`. A low
  ulimit will affect MongoDB when under heavy use and can produce
  errors and lead to failed connections to MongoDB
  processes and loss of service.

- Disable ``transparent huge pages`` as MongoDB performs
  better with normal (4096 bytes) virtual memory pages.

- Disable NUMA in your BIOS. If that is not possible see :ref:`MongoDB on NUMA Hardware <production-numa>`.

- Ensure that readahead settings for the block devices that store the
  database files are appropriate. For random access use patterns, set low
  readahead values. A readahead of 32 (16kb) often works well.

- Use the Network Time Protocol (NTP) to synchronize time among
  your hosts. This is especially important in sharded clusters.

.. _readahead:

.. TODO update with updated readahead documentation when DOCS-164 closes


.. _production-virtualization:

MongoDB on Virtual Environments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The section describes considerations when running MongoDB in some of the
more common virtual environments.

EC2
```

MongoDB is compatible with EC2 and requires no configuration changes
specific to the environment.

You may alternately choose to obtain a set of
Amazon Machine Images (AMI) that bundle together MongoDB and Amazon's
Provisioned IOPS storage volumes. Provisioned IOPS can greatly increase
MongoDB's performance and ease of use. For more information, see
`this blog post
<http://www.mongodb.com/blog/post/provisioned-iops-aws-marketplace-significantly-boosts-mongodb-performance-ease-use>`_.

VMWare
``````

MongoDB is compatible with VMWare. As some users have
run into issues with VMWare's memory overcommit feature,
disabling the feature is recommended.

It is possible to clone a virtual machine running MongoDB.
You might use this function to
spin up a new virtual host to add as a member of a replica
set. If you clone a VM with journaling enabled, the clone snapshot will
be valid. If not using journaling, first stop :program:`mongod`,
then clone the VM, and finally, restart :program:`mongod`.

OpenVZ
``````

Some users have had issues when running MongoDB on some older version
of OpenVZ due to its handling of virtual memory, as with VMWare.

This issue seems to have been resolved in the more recent versions of
OpenVZ.


Performance Monitoring
----------------------

iostat
~~~~~~

On Linux, use the ``iostat`` command to check if disk I/O is a bottleneck
for your database. Specify a number of seconds when running iostat to
avoid displaying stats covering the time since server boot.

For example, the following command will display extended statistics and
the time for each displayed report, with traffic in MB/s, at one second
intervals:

.. code-block:: bash

   iostat -xmt 1

Key fields from ``iostat``:

- ``%util``: this is the most useful field for a quick check, it
  indicates what percent of the time the device/drive is in use.

- ``avgrq-sz``: average request size. Smaller number for this value
  reflect more random IO operations.

bwm-ng
~~~~~~

`bwm-ng <http://www.gropp.org/?id=projects&sub=bwm-ng>`_ is a
command-line tool for monitoring network use. If you suspect a
network-based bottleneck, you may use ``bwm-ng`` to begin your
diagnostic process.

Backups
-------

To make backups of your MongoDB database, please refer to
:doc:`/core/backups`.
================================
Replica Set Deployment Tutorials
================================

.. default-domain:: mongodb

The following tutorials provide information in deploying replica sets.

.. seealso:: :doc:`/administration/security-deployment` for additional
   related tutorials.

.. include:: /includes/toc/dfn-list-replica-set-deployment.rst

.. include:: /includes/toc/replica-set-deployment.rst
=================================
Replica Set Maintenance Tutorials
=================================

.. default-domain:: mongodb

The following tutorials provide information in maintaining existing
replica sets.

.. include:: /includes/toc/dfn-list-replica-set-maintenance.rst

.. include:: /includes/toc/replica-set-maintenance.rst
==============================
Member Configuration Tutorials
==============================

.. default-domain:: mongodb

The following tutorials provide information in configuring replica set
members to support specific operations, such as to provide dedicated
backups, to support reporting, or to act as a cold standby.

.. include:: /includes/toc/dfn-list-replica-set-member-configuration.rst

.. include:: /includes/toc/replica-set-member-configuration.rst
=====================
Replica Set Tutorials
=====================

.. default-domain:: mongodb

The administration of :term:`replica sets <replica set>` includes the
initial deployment of the set, adding and removing members to a set,
and configuring the operational parameters and properties of the
set. Administrators generally need not intervene in failover or
replication processes as MongoDB automates these functions. In the
exceptional situations that require manual interventions, the
tutorials in these sections describe processes such as resyncing a
member. The tutorials in this section form the basis for all replica
set administration.

.. include:: /includes/toc/dfn-list-spec-replica-set-tutorials-landing.rst

.. include:: /includes/toc/replica-set-tutorials-landing.rst
=================
MongoDB Scripting
=================

.. default-domain:: mongodb

The :program:`mongo` shell is an interactive JavaScript shell for
MongoDB, and is part of all `MongoDB distributions
<http://www.mongodb.org/downloads>`_. This section provides an
introduction to the shell, and outlines key functions, operations, and
use of the :program:`mongo` shell. Also consider :doc:`/faq/mongo` and
the :doc:`shell method </reference/method>` and other relevant
:doc:`reference material </reference>`.

.. note:: Most examples in the :doc:`MongoDB Manual </contents>` use
   the :program:`mongo` shell; however, many :doc:`drivers
   </applications/drivers>` provide similar interfaces to MongoDB.

.. include:: /includes/toc/dfn-list-administration-core-scripting.rst

.. include:: /includes/toc/administration-core-scripting.rst
========================
Access Control Tutorials
========================

.. default-domain:: mongodb

The following tutorials provide instructions for MongoDB''s
authentication and authorization related features.

.. include:: /includes/toc/dfn-list-security-tutorials-access-control.rst

.. include:: /includes/toc/security-tutorials-access-control.rst
==================
Security Checklist
==================

.. default-domain:: mongodb

This documents provides a list of security measures that you should implement to
protect your MongoDB installation.

Require Authentication
----------------------

Enable MongoDB authentication and specify the authentication mechanism.
You can use the MongoDB authentication mechanism or an existing external
framework. Authentication requires that all clients and servers provide
valid credentials before they can connect to the system. In clustered
deployments, enable authentication for each MongoDB server.

See :doc:`/core/authentication`, :doc:`/tutorial/enable-authentication`,
and :doc:`/tutorial/enable-authentication-in-sharded-cluster`.

Configure Role-Based Access Control
-----------------------------------

Create roles that define the exact access a set of users needs. Follow a
principle of least privilege. Then create users and assign them only the
roles they need to perform their operations. A user can be a person or a
client application.

Create a user administrator first, then create additional users. Create a
unique MongoDB user for each person and application that accesses the
system.

See :doc:`/core/authorization`, :doc:`/tutorial/define-roles`,
:doc:`/tutorial/add-user-administrator`, and
:doc:`/tutorial/add-user-to-database`.

Encrypt Communication
---------------------

Configure MongoDB to use SSL for all incoming and outgoing
connections. Use SSL to encrypt communication between
:program:`mongod` and :program:`mongos` components of a MongoDB
client, as well as between all applications and MongoDB.

See :doc:`/tutorial/configure-ssl`.

Limit Network Exposure
----------------------

Ensure that MongoDB runs in a trusted network environment and limit
the interfaces on which MongoDB instances listen for incoming
connections. Allow only trusted clients to access the network
interfaces and ports on which MongoDB instances are available.

See the :setting:`bind_ip` setting, and see
:doc:`/tutorial/configure-linux-iptables-firewall` and
:doc:`/tutorial/configure-windows-netsh-firewall`.

Audit System Activity
---------------------

Track access and changes to database configurations and data.
`MongoDB Enterprise <http://www.mongodb.com/products/mongodb-enterprise>`_
includes a system auditing facility that can record
system events (e.g. user operations, connection events) on a
MongoDB instance. These audit records permit forensic analysis and
allow administrators to verify proper controls.

See :doc:`/core/auditing` and :doc:`/tutorial/configure-auditing`.

Encrypt and Protect Data
------------------------

Encrypt MongoDB data on each host using file-system, device, or physical
encryption. Protect MongoDB data using file-system permissions. MongoDB
data includes data files, configuration files, auditing logs, and key
files.

Run MongoDB with a Dedicated User
---------------------------------

Run MongoDB processes with a dedicated operating system user account.
Ensure that the account has permissions to access data but no unnecessary
permissions.

See :doc:`/installation` for more information on running MongoDB.

Run MongoDB with Secure Configuration Options
---------------------------------------------

MongoDB supports the execution of JavaScript code for certain server-side
operations: :dbcommand:`mapReduce`, :dbcommand:`group`, :dbcommand:`eval`,
and :query:`$where`. If you do not use these operations, disable
server-side scripting by setting :setting:`noscripting` to ``true``.

Use only the MongoDB wire protocol on production deployments. Do **not**
enable the following, all of which enable the web server interface:
:setting:`httpinterface`, :setting:`jsonp`, and :setting:`net.http.RESTInterfaceEnabled`. Leave
these *disabled*, unless required for backwards compatibility.

Keep input validation enabled. MongoDB enables input validation by default
through the :setting:`objcheck` setting. This ensures that all
documents stored by the :program:`mongod` instance are valid :term:`BSON`.
=============================
Security Deployment Tutorials
=============================

.. default-domain:: mongodb

The following tutorials provide information in deploying MongoDB using
authentication and authorization.

.. include:: /includes/toc/dfn-list-security-tutorials-deployment.rst

.. include:: /includes/toc/security-tutorials-deployment.rst
==========================
Network Security Tutorials
==========================

.. default-domain:: mongodb

The following tutorials provide information on handling network
security for MongoDB.

.. include:: /includes/toc/dfn-list-security-tutorials-network.rst

.. include:: /includes/toc/security-tutorials-network.rst
==================================
User and Role Management Tutorials
==================================

.. default-domain:: mongodb

The following tutorials provide instructions on how to enable
authentication and limit access for users with privilege roles.

.. include:: /includes/toc/dfn-list-security-tutorials-user-role-management.rst

.. include:: /includes/toc/security-tutorials-user-role-management.rst
==================
Security Tutorials
==================

.. default-domain:: mongodb

The following tutorials provide instructions for enabling and using
the security features available in MongoDB.

.. include:: /includes/toc/dfn-list-spec-security-tutorials-landing.rst

.. include:: /includes/toc/security-tutorials-landing.rst
===============================
Sharded Cluster Data Management
===============================

.. default-domain:: mongodb

The following documents provide information in managing data in sharded clusters.

.. include:: /includes/toc/dfn-list-sharded-cluster-data.rst

.. include:: /includes/toc/sharded-cluster-data.rst
====================================
Sharded Cluster Deployment Tutorials
====================================

.. default-domain:: mongodb

The following tutorials provide information on deploying sharded clusters.

.. include:: /includes/toc/dfn-list-sharded-cluster-deployment.rst

.. seealso:: :doc:`/tutorial/enable-authentication-in-sharded-cluster`

.. include:: /includes/toc/sharded-cluster-deployment.rst
=====================================
Sharded Cluster Maintenance Tutorials
=====================================

.. default-domain:: mongodb

The following tutorials provide information in maintaining sharded clusters.

.. include:: /includes/toc/dfn-list-sharded-cluster-maintenance.rst

.. include:: /includes/toc/sharded-cluster-maintenance.rst

.. seealso:: :doc:`/administration/backup-sharded-clusters`
.. index:: sharded clusters
.. _sharding-sharded-cluster:

=========================
Sharded Cluster Tutorials
=========================

.. default-domain:: mongodb

The following tutorials provide instructions for administering
:term:`sharded clusters <sharded cluster>`. For a higher-level
overview, see :doc:`/sharding`.

.. include:: /includes/toc/dfn-list-spec-sharded-cluster-tutorials-landing.rst

.. include:: /includes/toc/sharded-cluster-tutorials-landing.rst
======================
Operational Strategies
======================

.. default-domain:: mongodb

These documents address higher level strategies for common
administrative tasks and requirements with respect to MongoDB
deployments.

.. include:: /includes/toc/dfn-list-administration-core-strategy.rst

.. include:: /includes/toc/administration-core-strategy.rst
========================
Administration Tutorials
========================

.. default-domain:: mongodb

The administration tutorials provide specific step-by-step
instructions for performing common MongoDB setup, maintenance, and
configuration operations.

.. include:: /includes/toc/dfn-list-spec-administration-tutorials-landing.rst

.. include:: /includes/toc/administration-tutorials-landing.rst

.. seealso:: The MongoDB Manual contains administrative documentation
   and tutorials though out several sections. See
   :doc:`/administration/replica-sets` and
   :doc:`/administration/sharded-clusters` for additional tutorials and
   information.
==============
Administration
==============

.. default-domain:: mongodb

.. admin-introduction-start

The administration documentation addresses the ongoing operation and
maintenance of MongoDB instances and deployments. This documentation
includes both high level overviews of these concerns as well as
tutorials that cover specific procedures and processes for operating
MongoDB.

.. admin-introduction-end

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`MongoDB
   Administration <MongoDB-administration-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-administration-landing.rst

.. seealso:: The MongoDB Manual contains administrative documentation
   and tutorials though out several sections. See
   :doc:`/administration/replica-sets` and
   :doc:`/administration/sharded-clusters` for additional tutorials and
   information.

.. include:: /includes/toc/administration-landing.rst
.. _aggregation-framework:

===========
Aggregation
===========

.. default-domain:: mongodb

Aggregations operations process data records and return computed
results. Aggregation operations group values from multiple documents
together, and can perform a variety of operations on the grouped data
to return a single result. MongoDB provides three ways to perform
aggregation: the :doc:`aggregation pipeline
</core/aggregation-pipeline>`, the :doc:`map-reduce function
</core/map-reduce>`, and :doc:`single purpose aggregation methods and
commands </core/single-purpose-aggregation>`.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`MongoDB
   Aggregation and Data Processing <MongoDB-aggregation-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-aggregation-landing.rst

.. include:: /includes/toc/aggregation-landing.rst
====================
Aggregation Examples
====================

.. default-domain:: mongodb

This document provides the practical examples that display the
capabilities of :doc:`aggregation </core/aggregation>`.

.. include:: /includes/toc/dfn-list-aggregation-examples.rst

.. include:: /includes/toc/aggregation-examples.rst
======================
MongoDB CRUD Tutorials
======================

.. default-domain:: mongodb

The following tutorials provide instructions for querying and modifying
data. For a higher-level overview of these operations, see
:doc:`/crud`.


.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-crud-tutorials.rst

.. include:: /includes/toc/crud-tutorials.rst
===================================
Model Specific Application Contexts
===================================

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-data-models-applications.rst

.. include:: /includes/toc/data-models-applications.rst
=====================================
Model Relationships Between Documents
=====================================

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-data-models-relationships.rst

.. include:: /includes/toc/data-models-relationships.rst
=====================
Model Tree Structures
=====================

.. default-domain:: mongodb

MongoDB allows various ways to use tree data structures to model large
hierarchical or nested data relationships.

.. include:: /images/data-model-tree.rst

.. include:: /includes/toc/dfn-list-data-models-tree-structures.rst

.. include:: /includes/toc/data-models-tree-structures.rst
.. _data-modeling-patterns:
.. _data-modeling-examples:

================================
Data Model Examples and Patterns
================================

.. default-domain:: mongodb

.. /tutorial/model-tree-structures.txt is just a composite page that
   includes all the tree structure pages for easy overview.

The following documents provide overviews of various data modeling
patterns and common schema design considerations:

.. include:: /includes/toc/dfn-list-spec-data-model-examples-landing.rst

.. include:: /includes/toc/data-model-examples-landing.rst
============
Design Notes
============

.. default-domain:: mongodb

This page details features of MongoDB that may be important to bear in
mind when designing your applications.

Schema Considerations
---------------------

Dynamic Schema
~~~~~~~~~~~~~~

Data in MongoDB has a *dynamic schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. This
facilitates iterative development and polymorphism. Nevertheless,
collections often hold documents with highly homogeneous
structures. See :doc:`/core/data-models` for more information.

Some operational considerations include:

- the exact set of collections to be used;

- the indexes to be used: with the exception of the ``_id`` index, all
  indexes must be created explicitly;

- shard key declarations: choosing a good shard key is very important
  as the shard key cannot be changed once set.

Avoid importing unmodified data
directly from a relational database. In general, you will want to "roll
up" certain data into richer documents that take advantage of MongoDB's
support for sub-documents and nested arrays.

Case Sensitive Strings
~~~~~~~~~~~~~~~~~~~~~~

MongoDB strings are case sensitive. So a search for ``"joe"`` will not
find ``"Joe"``.

Consider:

- storing data in a normalized case format, or

- using regular expressions ending with ``/i``, and/or

- using :doc:`$toLower </reference/operator/aggregation/toLower/>` or
  :doc:`$toUpper </reference/operator/aggregation/toUpper/>` in the
  :doc:`aggregation framework </core/aggregation/>`.

Type Sensitive Fields
~~~~~~~~~~~~~~~~~~~~~

MongoDB data is stored in the :meta-driver:`BSON </legacy/bson/>`
format, a binary encoded serialization of JSON-like documents.  BSON
encodes additional type information. See `bsonspec.org
<http://bsonspec.org/#/specification>`_ for more information.

Consider the following document which has a field ``x`` with the
*string* value ``"123"``:

.. code-block:: javascript

   { x : "123" }

Then the following query which looks for a *number* value ``123`` will
**not** return that document:

.. code-block:: javascript

   db.mycollection.find( { x : 123 } )

General Considerations
----------------------

By Default, Updates Affect **one** Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To update multiple documents that meet your query criteria, set the
:method:`update` ``multi`` option to ``true`` or ``1``.
See: :ref:`Update Multiple Documents <update-multiple-documents>`.

Prior to MongoDB 2.2, you would specify the ``upsert`` and ``multi``
options in the :method:`update` method as positional boolean
options. See: the :method:`update` method reference documentation.

BSON Document Size Limit
~~~~~~~~~~~~~~~~~~~~~~~~

The :limit:`BSON Document Size` limit is currently
set at 16MB per document. If you require larger documents, use :doc:`GridFS
</core/gridfs/>`.

No Fully Generalized Transactions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB does not have :doc:`fully generalized transactions
</tutorial/isolate-sequence-of-operations/>`. If you model your data
using rich documents that closely resemble your application's
objects, each logical object will be in one MongoDB document. MongoDB
allows you to modify a document in a single atomic operation. These
kinds of data modification pattern covers most common uses of
transactions in other systems.

Replica Set Considerations
--------------------------

Use an Odd Number of Replica Set Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:doc:`Replica sets </replication/>` perform consensus elections. To
ensure that elections will proceed successfully, either use an odd
number of members, typically three, or else use an :term:`arbiter` to ensure an
odd number of votes.

Keep Replica Set Members Up-to-Date
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB replica sets support :doc:`automatic failover
</core/replica-set-high-availability>`. It is important for
your secondaries to be up-to-date. There are various strategies for
assessing consistency:

1. Use monitoring tools to alert you to lag events. See
   :doc:`/administration/monitoring` for a detailed discussion of
   MongoDB's monitoring options.

#. Specify appropriate write concern.

#. If your application requires *manual* fail over,
   you can configure your secondaries
   as :ref:`priority 0 <replica-set-secondary-only-members>`.
   Priority 0 secondaries require manual action for a failover.
   This may be practical for a small replica set, but large deployments
   should fail over automatically.

.. seealso:: :ref:`replica set rollbacks <replica-set-rollback>`.

Sharding Considerations
-----------------------

- Pick your shard keys carefully. You cannot choose a new shard
  key for a collection that is already sharded.

- Shard key values are immutable.

- When enabling sharding on an *existing collection*, MongoDB imposes
  a maximum size on those collections to ensure that it is possible to
  create chunks. For a detailed explanation of this limit, see:
  :limit:`<sharding-existing-collection-data-size>`.

  To shard
  large amounts of data, create a new empty sharded collection, and
  ingest the data from the source collection using an application
  level import operation.

- Unique indexes are not enforced across shards except for the shard
  key itself. See :doc:`/tutorial/enforce-unique-keys-for-sharded-collections`.

- Consider :doc:`pre-splitting </administration/sharded-clusters>` a
  sharded collection before a massive bulk import.
.. _drivers:
.. _driver:

====================================
MongoDB Drivers and Client Libraries
====================================

An application communicates with MongoDB by way of a client library,
called a :ecosystem:`driver </drivers>`, that handles all interaction with the
database in a language appropriate to the application.

Drivers
-------

See the following pages for more information about the MongoDB
:ecosystem:`drivers </drivers>`:

- JavaScript (:ecosystem:`Language Center </drivers/javascript>`, :api:`docs <js/current>`)
- Python (:ecosystem:`Language Center </drivers/python>`, :api:`docs <python/current>`)
- Ruby (:ecosystem:`Language Center </drivers/ruby>`, :api:`docs <ruby/current>`)
- PHP (:ecosystem:`Language Center </drivers/php>`, `docs <http://php.net/mongo/>`_)
- Perl (:ecosystem:`Language Center </drivers/perl>`, :api:`docs <perl/current/>`)
- Java (:ecosystem:`Language Center </drivers/java>`, :api:`docs <java/current>`)
- Scala (:ecosystem:`Language Center </drivers/scala>`, :api:`docs <scala/casbah/current/>`)
- C# (:ecosystem:`Language Center </drivers/csharp>`, :api:`docs <csharp/current/>`)
- C (:ecosystem:`Language Center </drivers/c>`, :api:`docs <c/current/>`)
- C++ (:ecosystem:`Language Center </drivers/cpp>`, :api:`docs <cplusplus/current/>`)
- Haskell (`Language Center <http://hackage.haskell.org/package/mongoDB>`_, :api:`docs <haskell/mongodb>`)
- Erlang (:ecosystem:`Language Center </drivers/erlang>`, :api:`docs <erlang/mongodb>`)

.. _drivers-version-numbers:

Driver Version Numbers
----------------------

Driver version numbers use `semantic versioning <http://semver.org/>`_
or "**major.minor.patch**" versioning system. The first number is the
major version, the second the minor version, and the third indicates a
patch.

.. example:: Driver version numbers.

   If your driver has a version number of ``2.9.1``, ``2`` is the major
   version, ``9`` is minor, and ``1`` is the patch.

The numbering scheme for drivers differs from the scheme for the
MongoDB server. For more information on server versioning, see
:ref:`release-version-numbers`.
==============================
Geospatial Indexes and Queries
==============================

.. default-domain:: mongodb

MongoDB offers a number of indexes and query mechanisms to handle
geospatial information. This section introduces MongoDB's geospatial
features. For complete examples of geospatial queries in MongoDB, see
:doc:`/administration/indexes-geo`.

Surfaces
--------

Before storing your location data and writing queries, you must decide
the type of surface to use to perform calculations. The type you choose
affects how you store data, what type of index to build, and the syntax
of your queries.

MongoDB offers two surface types:

Spherical
~~~~~~~~~

To calculate geometry over an Earth-like sphere, store your
location data on a spherical surface and use :doc:`2dsphere
</core/2dsphere>` index.

Store your location data as GeoJSON objects with this
coordinate-axis order: **longitude, latitude**. The coordinate
reference system for GeoJSON uses the :term:`WGS84` datum.

Flat
~~~~

To calculate distances on a Euclidean plane, store your location data
as legacy coordinate pairs and use a :doc:`2d </core/2d>` index.

.. _geo-overview-location-data:

Location Data
-------------

If you choose spherical surface calculations, you store location data
as either:

GeoJSON Objects
~~~~~~~~~~~~~~~

Queries on :term:`GeoJSON` objects always calculate on a sphere. The
default coordinate reference system for GeoJSON uses the :term:`WGS84`
datum.

.. versionadded:: 2.4
   Support for GeoJSON storage and queries is new in version
   2.4. Prior to version 2.4, all geospatial data used coordinate
   pairs.

MongoDB supports the following GeoJSON objects:

- Point

- LineString

- Polygon

Legacy Coordinate Pairs
~~~~~~~~~~~~~~~~~~~~~~~

MongoDB supports spherical surface calculations on :term:`legacy
coordinate pairs` using a ``2dsphere`` index by converting the data to
the GeoJSON Point type.

If you choose flat surface calculations, and use a ``2d`` index you
can store data only as :term:`legacy coordinate pairs`.

Query Operations
----------------

MongoDB's geospatial query operators let you query for:

Inclusion
~~~~~~~~~

MongoDB can query for locations contained entirely within a
specified polygon. Inclusion queries use the :query:`$geoWithin`
operator.

Both ``2d`` and ``2dsphere`` indexes can support inclusion
queries. MongoDB does not require an index for inclusion queries after
2.2.3; however, these indexes will improve query performance.

Intersection
~~~~~~~~~~~~

MongoDB can query for locations that intersect with a specified
geometry. These queries apply only to data on a spherical
surface. These queries use the :query:`$geoIntersects` operator.

Only ``2dsphere`` indexes support intersection.

Proximity
~~~~~~~~~

MongoDB can query for the points nearest to another
point. Proximity queries use the :query:`$near` operator. The
:query:`$near` operator requires a ``2d`` or ``2dsphere`` index.

.. _index-feature-geospatial:

Geospatial Indexes
------------------

MongoDB provides the following geospatial index types to support the
geospatial queries.

``2dsphere``
~~~~~~~~~~~~

:doc:`2dsphere </core/2dsphere>` indexes support:

- Calculations on a sphere

- GeoJSON objects and include backwards compatibility for legacy
  coordinate pairs.

- A compound index with scalar index fields (i.e. ascending or
  descending) as a prefix or suffix of the ``2dsphere`` index field

.. versionadded:: 2.4
   ``2dsphere`` indexes are not available before version 2.4.

.. seealso:: :doc:`/tutorial/query-a-2dsphere-index`

``2d``
~~~~~~

:doc:`2d </core/2d>` indexes support:

- Calculations using flat geometry

- Legacy coordinate pairs (i.e., geospatial points on a flat
  coordinate system)

- A compound index with only one additional field, as a suffix of the
  ``2d`` index field

.. seealso:: :doc:`/tutorial/query-a-2d-index`

Geospatial Indexes and Sharding
-------------------------------

You *cannot* use a geospatial index as the :term:`shard key` index.

You can create and maintain a geospatial index on
a sharded collection if using fields other than shard key.

Queries using :query:`$near` are not supported for sharded
collections. Use :dbcommand:`geoNear` instead. You also can query for
geospatial data using :query:`$geoWithin`.

Additional Resources
--------------------

The following pages provide complete documentation for geospatial
indexes and queries:

.. include:: /includes/toc/dfn-list-indexes-concepts-geo.rst

.. include:: /includes/toc/indexes-concepts-geo.rst

.. seealso:: :ref:`geospatial-query-compatibility-chart`
===================
Indexing Strategies
===================

.. default-domain:: mongodb

The best indexes for your application must take a number
of factors into account, including the kinds of queries you expect,
the ratio of reads to writes, and the amount of free memory on your
system.

When developing your indexing strategy you should have a deep
understanding of your application's queries. Before you build indexes,
map out the types of queries you will run so that you can build
indexes that reference those fields. Indexes come with a performance
cost, but are more than worth the cost for frequent queries on large
data set. Consider the relative frequency of each query in the
application and whether the query justifies an index.

The best overall strategy for designing indexes is to profile a
variety of index configurations with data sets similar to the ones
you'll be running in production to see which configurations perform
best.Inspect the current indexes created for your collections to
ensure they are supporting your current and planned queries. If an
index is no longer used, drop the index.

MongoDB can only use *one* index to support any given
operation. However, each clause of an :query:`$or` query may use a
different index.

The following documents introduce indexing strategies:

.. include:: /includes/toc/dfn-list-indexes-tutorial-strategies.rst

.. include:: /includes/toc/indexes-tutorial-strategies.rst

For a conceptual introduction to indexes in MongoDB see
:doc:`/core/indexes`.
====================================
Replica Set Read and Write Semantics
====================================

.. default-domain:: mongodb

From the perspective of a client application, whether a MongoDB
instance is running as a single server (i.e. "standalone") or a
:term:`replica set` is transparent.

By default, in MongoDB, read operations to a replica set return results
from the :doc:`primary </core/replica-set-primary>` and are
:term:`consistent <strict consistency>` with the last write operation.

Users may configure :term:`read preference` on a per-connection basis
to prefer that the read operations return on the :term:`secondary`
members. If clients configure the :term:`read preference` to permit
secondary reads, read operations cannot return from :term:`secondary`
members that have not replicated more recent updates or
operations. When reading from a secondary, a query may return data that
reflects a previous state.

This behavior is sometimes characterized as :term:`eventual
consistency` because the secondary member's state will *eventually*
reflect the primary's state and MongoDB cannot guarantee :term:`strict
consistency` for read operations from secondary members.

To guarantee consistency for reads from secondary members, you can
configure the :term:`client` and :term:`driver` to ensure that write
operations succeed on all members before completing successfully. See
:doc:`/core/write-concern` for more information. Additionally, such
configuration can help prevent :doc:`/core/replica-set-rollbacks`
during a failover.

.. note::

   :term:`Sharded clusters <sharded cluster>` where the shards are also
   replica sets provide the same operational semantics with
   regards to write and read operations.

.. include:: /includes/toc/dfn-list-replica-set-read-write-semantics.rst

.. include:: /includes/toc/replica-set-read-write-semantics.rst
=======================
MongoDB Manual Contents
=======================

See :doc:`/about` for more information about the MongoDB Documentation
project, this Manual and additional editions of this text.

.. toctree::
   :titlesonly:

   Installation </installation>
   /crud
   /data-modeling
   /administration
   /security
   /aggregation
   /indexes
   /replication
   /sharding
   /faq
   /reference
   /release-notes
   Drivers </applications/drivers>
==============
``2d`` Indexes
==============

.. default-domain:: mongodb

Use a ``2d`` index for data stored as points on a two-dimensional plane. The
``2d`` index is intended for legacy coordinate pairs used in MongoDB 2.2
and earlier.

Use a ``2d`` index if:

- your database has legacy location data from MongoDB 2.2 or earlier, *and*

- you do not intend to store any location data as :term:`GeoJSON` objects.

See the :doc:`/reference/operator/query-geospatial` for the query
operators that support geospatial queries.

Considerations
--------------

.. |first-geo-index| replace:: ``2d``
.. |second-geo-index| replace:: :doc:`2dsphere </core/2dsphere>`
.. include:: /includes/fact-limitation-one-geo-index-per-collection.rst

Do not use a ``2d`` index if your location data includes GeoJSON objects. To
index on both legacy coordinate pairs *and* GeoJSON objects, use a
:doc:`2dsphere </core/2dsphere>` index.

You cannot use a ``2d`` index as a shard key when sharding a
collection. However, you can create and maintain a geospatial index on
a sharded collection by using a different field as the shard key.

Behavior
--------

The ``2d`` index supports calculations on a flat, Euclidean plane. The
``2d`` index also supports *distance-only* calculations on a sphere,
but for *geometric* calculations (e.g. :query:`$geoWithin`) on a
sphere, store data as GeoJSON objects and use the ``2dsphere`` index
type.

A ``2d`` index can reference two fields. The first must be the location
field. A ``2d`` compound index constructs queries that select first on
the location field, and then filters those results by the additional
criteria. A compound ``2d`` index can cover queries.

.. _geospatial-indexes-store-grid-coordinates:

Points on a 2D Plane
--------------------

.. default-domain:: mongodb

To store location data as legacy coordinate pairs, use an array or an
embedded document. When possible, use the array format:

.. code-block:: javascript

   loc : [ <longitude> , <latitude> ]

Consider the embedded document form:

.. code-block:: javascript

   loc : { lng : <longitude> , lat : <latitude> }

Arrays are preferred as certain languages do not guarantee associative
map ordering.

For all points, if you use longitude and latitude,
store coordinates in **longitude, latitude** order.
====================
``2dsphere`` Indexes
====================

.. default-domain:: mongodb

.. versionadded:: 2.4

A ``2dsphere`` index supports queries that calculate geometries on an
earth-like sphere. The index supports data stored as both
:term:`GeoJSON` objects and as legacy coordinate pairs. The index
supports legacy coordinate pairs by converting the data to the GeoJSON
``Point`` type.

The ``2dsphere`` index supports all MongoDB geospatial queries: queries
for inclusion, intersection and proximity.

A :ref:`compound <index-type-compound>` ``2dsphere`` index can reference
multiple location and non-location fields within a collection’s
documents. You can arrange the fields in any order.

The default datum for an earth-like sphere in MongoDB 2.4 is :term:`WGS84`.
Coordinate-axis order is **longitude, latitude**.

See the :doc:`/reference/operator/query-geospatial` for the query
operators that support geospatial queries.

.. _2dsphere-v2:

``2dsphere`` Version 2
----------------------

.. versionchanged:: 2.6

MongoDB 2.6 introduces a version 2 of ``2dsphere`` indexes. Version 2
is the default version of ``2dsphere`` indexes created in MongoDB 2.6.
To create a ``2dsphere`` index as a version 1, include the option ``{
"2dsphereIndexVersion": 1 }`` when creating the index.

Version 2 adds support for additional GeoJSON object:
:ref:`geojson-multipoint`, :ref:`geojson-multilinestring`,
:ref:`geojson-multilinestring`, :ref:`geojson-multipolygon`, and
:ref:`geojson-geometrycollection`.

Version 2 ``2dsphere`` indexes are sparse by default and ignores the
:doc:`sparse: true </core/index-sparse>` option. If a document lacks a
``2dsphere`` index field (or the field is a ``null`` or an empty
array), MongoDB does not add an entry for the document to the
``2dsphere`` index. For inserts, MongoDB inserts the document but does
not add to the ``2dsphere`` index.

Version 1 ``2dsphere`` indexes are not sparse by default and will
reject documents with ``null`` location fields.

Considerations
--------------

.. |first-geo-index| replace:: ``2dsphere``
.. |second-geo-index| replace:: :doc:`2d </core/2d>`
.. include:: /includes/fact-limitation-one-geo-index-per-collection.rst

You cannot use a ``2dsphere`` index as a shard key when sharding a
collection. However, you can create and maintain a geospatial index on
a sharded collection by using a different field as the shard key.

.. _geospatial-indexes-store-geojson:

GeoJSON Objects
---------------

MongoDB supports the following GeoJSON objects:

- :ref:`geojson-point`

- :ref:`geojson-linestring`

- :ref:`geojson-polygon`

- :ref:`geojson-multipoint`

- :ref:`geojson-multilinestring`

- :ref:`geojson-multipolygon`

- :ref:`geojson-geometrycollection`

The :ref:`geojson-multipoint`, :ref:`geojson-multilinestring`,
:ref:`geojson-multilinestring`, :ref:`geojson-multipolygon`, and
:ref:`geojson-geometrycollection` require ``2dsphere`` index version 2.

In order to index GeoJSON data, you must store the data in a location
field that you name. The location field contains a subdocument with a
``type`` field specifying the GeoJSON object type and a ``coordinates``
field specifying the object's coordinates. Always store coordinates in
``longitude, latitude`` order.

Use the following syntax:

.. code-block:: javascript

   { <location field> : { type : "<GeoJSON type>" ,
                          coordinates : <coordinates>
   } }

.. _geojson-point:

``Point``
~~~~~~~~~

.. versionadded:: 2.4

The following example stores a GeoJSON :term:`Point`:

.. code-block:: javascript

   { loc : { type : "Point" ,
             coordinates : [ 40, 5 ]
   } }

.. _geojson-linestring:

``LineString``
~~~~~~~~~~~~~~

.. versionadded:: 2.4

The following example stores a GeoJSON :term:`LineString`:

.. code-block:: javascript

   { loc : { type : "LineString" ,
             coordinates : [ [ 40 , 5 ] , [ 41 , 6 ] ]
   } }

.. _geojson-polygon:

``Polygon``
~~~~~~~~~~~

.. versionadded:: 2.4

:term:`Polygons <Polygon>` consist of an array of GeoJSON
``LinearRing`` coordinate arrays. These ``LinearRings`` are closed
``LineStrings``. Closed ``LineStrings`` have at least four coordinate
pairs and specify the same position as the first and last coordinates.

The following example stores a GeoJSON ``Polygon`` with an exterior
ring and no interior rings (or holes). Note the first and last
coordinate pair with the ``[ 0 , 0 ]`` coordinate:

.. code-block:: javascript

   { loc :
      { type : "Polygon" ,
        coordinates : [ [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0 ] ] ]
   } }

For Polygons with multiple rings:

- The first described ring must be the exterior ring.

- The exterior ring cannot self-intersect.

- Any interior ring must be entirely contained by the outer ring.

- Interior rings cannot intersect or overlap each other. Interior
  rings can share an edge.

The following document represents a polygon with an interior ring as
GeoJSON:

.. code-block:: javascript

   { loc :
      { type : "Polygon" ,
        coordinates : [ [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0 ] ],
                        [ [ 2 , 2 ] , [ 3 , 3 ] , [ 4 , 2 ] , [ 2 , 2 ] ] ]
   } }

.. include:: /images/index-2dsphere-polygon-with-ring.rst

.. _geojson-multipoint:

``MultiPoint``
~~~~~~~~~~~~~~

.. versionadded:: 2.6
   Requires ``2dsphere`` index version 2.

The following example stores coordinates of GeoJSON type `MultiPoint
<http://geojson.org/geojson-spec.html#id5>`_:

.. code-block:: javascript

   {
     loc: { "type": "MultiPoint",
            "coordinates": [
                              [ -73.9580, 40.8003 ],
                              [ -73.9498, 40.7968 ],
                              [ -73.9737, 40.7648 ],
                              [ -73.9814, 40.7681 ]
                           ]
          }
   }

.. _geojson-multilinestring:

``MultiLineString``
~~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.6
   Requires ``2dsphere`` index version 2.

The following example stores coordinates of GeoJSON type
`MultiLineString <http://geojson.org/geojson-spec.html#id6>`_:

.. code-block:: javascript

   {
     loc: { "type": "MultiLineString",
            "coordinates": [
                              [ [ -73.96943, 40.78519 ], [ -73.96082, 40.78095 ] ],
                              [ [ -73.96415, 40.79229 ], [ -73.95544, 40.78854 ] ],
                              [ [ -73.97162, 40.78205 ], [ -73.96374, 40.77715 ] ],
                              [ [ -73.97880, 40.77247 ], [ -73.97036, 40.76811 ] ]
                           ]
          }
   }

.. _geojson-multipolygon:

``MultiPolygon``
~~~~~~~~~~~~~~~~

.. versionadded:: 2.6
   Requires ``2dsphere`` index version 2.

The following example stores coordinates of GeoJSON type `MultiPolygon
<http://geojson.org/geojson-spec.html#id7>`_:

.. code-block:: javascript

   {
     loc: { "type": "MultiPolygon",
            "coordinates": [
                             [ [ [  -73.958, 40.8003 ], [ -73.9498, 40.7968 ], [ -73.9737, 40.7648 ], [ -73.9814, 40.7681 ], [  -73.958, 40.8003 ] ] ],
                             [ [ [  -73.958, 40.8003 ], [ -73.9498, 40.7968 ], [ -73.9737, 40.7648 ], [  -73.958, 40.8003 ] ] ]
                           ]
          }
   }

.. _geojson-geometrycollection:

``GeometryCollection``
~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.6
   Requires ``2dsphere`` index version 2.

The following example stores coordinates of GeoJSON type
`GeometryCollection
<http://geojson.org/geojson-spec.html#geometrycollection>`_:

.. code-block:: javascript

   {
     loc: { "type": "GeometryCollection",
            "geometries": [
                            { "type": "MultiPoint",
                                      "coordinates": [
                                                       [ -73.9580, 40.8003 ],
                                                       [ -73.9498, 40.7968 ],
                                                       [ -73.9737, 40.7648 ],
                                                       [ -73.9814, 40.7681 ]
                                                     ]
                            },
                            { "type": "MultiLineString",
                                      "coordinates": [
                                                       [ [ -73.96943, 40.78519 ], [ -73.96082, 40.78095 ] ],
                                                       [ [ -73.96415, 40.79229 ], [ -73.95544, 40.78854 ] ],
                                                       [ [ -73.97162, 40.78205 ], [ -73.96374, 40.77715 ] ],
                                                       [ [ -73.97880, 40.77247 ], [ -73.97036, 40.76811 ] ]
                                                     ]
                            }
                          ]
          }
    }
=======================
Administration Concepts
=======================

.. default-domain:: mongodb

The core administration documents address strategies and practices
used in the operation of MongoDB systems and deployments.

.. include:: /includes/toc/dfn-list-spec-administration-concepts.rst

.. include:: /includes/toc/administration-core-landing.rst
========================
Aggregation Introduction
========================

.. default-domain:: mongodb

*Aggregations* are operations that process data records and return
computed results. MongoDB provides a rich set of aggregation
operations that examine and perform calculations on the data sets.
Running data aggregation on the :program:`mongod` instance simplifies
application code and limits resource requirements.

Like queries, aggregation operations in MongoDB use :term:`collections
<collection>` of documents as an input and return results in the form
of one or more documents.

Aggregation Modalities
----------------------

Aggregation Pipelines
~~~~~~~~~~~~~~~~~~~~~

MongoDB 2.2 introduced a new :doc:`aggregation framework
</core/aggregation-pipeline>`, modeled on the concept of data
processing pipelines. Documents enter a multi-stage pipeline that
transforms the documents into an aggregated result.

The most basic pipeline stages provide *filters* that operate like
queries and *document transformations* that modify the form
of the output document.

Other pipeline operations provide tools for grouping and sorting
documents by specific field or fields as well as tools for aggregating
the contents of arrays, including arrays of documents. In addition,
pipeline stages can use :ref:`operators
<aggregation-expression-operators>` for tasks such as calculating the
average or concatenating a string.

The pipeline provides efficient data aggregation using native
operations within MongoDB, and is the preferred method for data
aggregation in MongoDB.

.. include:: /images/aggregation-pipeline.rst

Map-Reduce
~~~~~~~~~~

MongoDB also provides :doc:`map-reduce </core/map-reduce>` operations
to perform aggregation. In general, map-reduce operations have two
phases: a *map* stage that processes each document and *emits* one or
more objects for each input document, and *reduce* phase that combines
the output of the map operation. Optionally, map-reduce can have a
*finalize* stage to make final modifications to the result. Like other
aggregation operations, map-reduce can specify a query condition to
select the input documents as well as sort and limit the results.

Map-reduce uses custom JavaScript functions to perform the map and
reduce operations, as well as the optional *finalize* operation. While
the custom JavaScript provide great flexibility compared to the
aggregation pipeline, in general, map-reduce is less efficient and more
complex than the aggregation pipeline.

Additionally, map-reduce operations can have output sets that
exceed the 16 megabyte output limitation of the aggregation pipeline.

.. note:: Starting in MongoDB 2.4, certain :program:`mongo` shell
   functions and properties are inaccessible in map-reduce
   operations. MongoDB 2.4 also provides support for multiple
   JavaScript operations to run at the same time. Before MongoDB 2.4,
   JavaScript code executed in a single thread, raising concurrency
   issues for map-reduce.

.. include:: /images/map-reduce.rst

Single Purpose Aggregation Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a number of common :doc:`single purpose aggregation operations
</core/single-purpose-aggregation>`, MongoDB provides special purpose
database commands. These common
aggregation operations are: returning a count of matching documents,
returning the distinct values for a field, and grouping data based on
the values of a field. All of these operations aggregate documents from a
single collection. While these operations provide simple access to
common aggregation processes, they lack the flexibility and
capabilities of the aggregation pipeline and map-reduce.

.. include:: /images/distinct.rst

Additional Features and Behaviors
---------------------------------

Both the aggregation pipeline and map-reduce can operate on a
:doc:`sharded collection </core/sharding-introduction>`. Map-reduce
operations can also output to a sharded collection. See
:doc:`/core/aggregation-pipeline-sharded-collections` and
:doc:`/core/map-reduce-sharded-collections` for details.

The aggregation pipeline can use indexes to improve its performance
during some of its stages. In addition, the aggregation pipeline has an
internal optimization phase. See
:ref:`aggregation-pipeline-operators-and-performance` and
:doc:`/core/aggregation-pipeline-optimization` for details.

For a feature comparison of the aggregation pipeline,
map-reduce, and the special group functionality, see
:doc:`/reference/aggregation-commands-comparison`.
=====================
Aggregation Mechanics
=====================

.. default-domain:: mongodb

This section describes behaviors and limitations for the various
aggregation modalities.

.. include:: /includes/toc/dfn-list-aggregation-mechanics.rst

.. include:: /includes/toc/aggregation-mechanics.rst
===========================
Aggregation Pipeline Limits
===========================

.. default-domain:: mongodb

Aggregation operations with the :dbcommand:`aggregate` command have the
following limitations.

Type Restrictions
-----------------

.. include:: /includes/fact-aggregation-types.rst

.. versionchanged:: 2.4
   Removed restriction on ``Binary`` type data. In MongoDB 2.2, the pipeline
   could not operate on ``Binary`` type data.

Result Size Restrictions
------------------------

If the :dbcommand:`aggregate` command returns a single document that
contains the complete result set, the command will produce an error if
the result set exceeds the :limit:`BSON Document Size` limit, which is
currently 16 megabytes. To manage result sets that exceed this limit,
the :dbcommand:`aggregate` command can return result sets of *any size*
if the command return a cursor or store the results to a collection.

.. versionchanged:: 2.6

   The :dbcommand:`aggregate` command can return results as a cursor or
   store the results in a collection, which are not subject to the size
   limit. The :method:`db.collection.aggregate()` returns a cursor and
   can return result sets of any size.

.. _agg-memory-restrictions:

Memory Restrictions
-------------------

.. include:: /includes/fact-agg-memory-limit.rst
=================================
Aggregation Pipeline Optimization
=================================

.. default-domain:: mongodb

.. versionchanged:: 2.4

Aggregation pipeline operations have an optimization phase which
attempts to rearrange the pipeline for improved performance.

.. _aggregation-pipeline-sequence-optimization:

Pipeline Sequence Optimization
------------------------------

``$sort`` + ``$skip`` + ``$limit`` Sequence Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you have a sequence with :pipeline:`$sort` followed by a
:pipeline:`$skip` followed by a :pipeline:`$limit`, an
optimization occurs that moves the :pipeline:`$limit` operator before
the :pipeline:`$skip` operator. For example, if the pipeline consists of
the following stages:

.. code-block:: javascript

   { $sort: { age : -1 } },
   { $skip: 10 },
   { $limit: 5 }

During the optimization phase, the optimizer transforms the sequence to
the following:

.. code-block:: javascript

   { $sort: { age : -1 } },
   { $limit: 15 }
   { $skip: 10 }

.. note::

   The :pipeline:`$limit` value has increased to the sum of the
   initial value and the :pipeline:`$skip` value.

The optimized sequence now has :pipeline:`$sort` immediately preceding
the :pipeline:`$limit`. See :pipeline:`$sort` for information on the
behavior of the :pipeline:`$sort` operation when it immediately
precedes :pipeline:`$limit`.

``$limit`` + ``$skip`` + ``$limit`` + ``$skip`` Sequence Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When you have a continuous sequence of a :pipeline:`$limit` pipeline
stage followed by a :pipeline:`$skip` pipeline stage, the optimization
phase attempts to arrange the pipeline stages to combine the limits
and skips. For example, if the pipeline consists of the following
stages:

.. code-block:: javascript

   { $limit: 100 },
   { $skip: 5 },
   { $limit: 10},
   { $skip: 2 }

During the intermediate step, the optimizer reverses the position of
the :pipeline:`$skip` followed by a :pipeline:`$limit` to
:pipeline:`$limit` followed by the :pipeline:`$skip`.

.. code-block:: javascript

   { $limit: 100 },
   { $limit: 15},
   { $skip: 5 },
   { $skip: 2 }

The :pipeline:`$limit` value has increased to the sum of the
initial value and the :pipeline:`$skip` value. Then, for the final
:pipeline:`$limit` value, the optimizer selects the minimum between
the adjacent :pipeline:`$limit` values. For the final
:pipeline:`$skip` value, the optimizer adds the adjacent
:pipeline:`$skip` values, to transform the sequence to the
following:

.. code-block:: javascript

   { $limit: 15 },
   { $skip: 7 }

.. _aggregation-pipeline-projection-optimization:

Projection Optimization
-----------------------

The aggregation pipeline can determine if it requires only a subset of
the fields in the documents to obtain the results. If so, the pipeline
will only use those required fields, reducing the amount of data
passing through the pipeline.
.. _aggregation-pipeline-sharded-collection:

============================================
Aggregation Pipeline and Sharded Collections
============================================

.. default-domain:: mongodb

The aggregation pipeline supports operations on :term:`sharded
<sharded cluster>` collections. This section describes behaviors
specific to the :ref:`aggregation pipeline <aggregation-pipeline>` and
sharded collections.

Behavior
--------

.. versionchanged:: 2.6

When operating on a sharded collection, the aggregation pipeline is
split into two parts. The first pipeline runs on each shard, or if an
early :pipeline:`$match` can exclude shards through the use of the
shard key in the predicate, the pipeline runs on only the relevant
shards.

The second pipeline consists of the remaining pipeline stages and runs
on the :ref:`primary shard <primary-shard>`. The primary shard merges
the cursors from the other shards and runs the second pipeline on these
results. The primary shard forwards
the final results to the :program:`mongos`. In previous versions, the
second pipeline would run on the :program:`mongos`.
[#agg-pipeline-upgrade]_

When splitting the aggregation pipeline into two parts, the pipeline is
split to ensure that the shards perform as many stages as possible. To
retrieve information on the division, use the :method:`explain
<db.collection.aggregate()>` option for the
:method:`db.collection.aggregate()` method.

.. [#agg-pipeline-upgrade]
   Until all shards upgrade to v2.6, the second pipeline runs on the
   :program:`mongos` if any shards are still running v2.4.
====================
Aggregation Pipeline
====================

.. versionadded:: 2.2

.. default-domain:: mongodb

The aggregation pipeline is a framework for data aggregation modeled
on the concept of data processing pipelines. Documents enter a
multi-stage pipeline that transforms the documents into an aggregated
results.

The aggregation pipeline provides an alternative to :term:`map-reduce`
and may be the preferred solution for many aggregation tasks where the
complexity of map-reduce may be unwarranted.

.. include:: /images/aggregation-pipeline.rst

Aggregation pipeline have some limitations on value types and result
size. See :doc:`/core/aggregation-pipeline-limits` for details on
limits and restrictions on the aggregation pipeline.

.. _aggregation-pipeline:

Pipeline
--------

Conceptually, documents from a collection travel through an
aggregation pipeline, which transforms these objects as they pass
through. For those familiar with UNIX-like shells (e.g. bash), the
concept is analogous to the pipe (i.e. ``|``).

The MongoDB aggregation pipeline starts with the documents of a
collection and streams the documents from one :ref:`pipeline operator
<aggregation-pipeline-operator-reference>` to the next to process the
documents. Each operator in the pipeline transforms the documents as they
pass through the pipeline. Pipeline operators do not need to produce
one output document for every input document. Operators may generate
new documents or filter out documents. Pipeline operators can be
repeated in the pipeline.

.. include:: /includes/fact-agg-helper-returns-cursor.rst

See :ref:`aggregation-pipeline-operator-reference` for the list of
pipeline operators that define the stages.

For example usage of the aggregation pipeline, consider
:doc:`/tutorial/aggregation-with-user-preference-data` and
:doc:`/tutorial/aggregation-zip-code-data-set`, as well as the
:dbcommand:`aggregate` command and the
:method:`db.collection.aggregate()` method reference pages.

.. _aggregation-expressions:

Pipeline Expressions
--------------------

Each pipeline operator takes a pipeline expression as its operand.
Pipeline expressions specify the transformation to apply to the input
documents. Expressions have a :term:`document` structure and can
contain fields, values, and :ref:`operators
<aggregation-expression-operators>`.

Pipeline expressions can only operate on the current document in the
pipeline and cannot refer to data from other documents: expression
operations provide in-memory transformation of documents.

Generally, expressions are stateless and are only evaluated when seen
by the aggregation process with one exception: :term:`accumulator`
expressions. The accumulator expressions, used with the
:pipeline:`$group` pipeline operator, maintain their state (e.g.
totals, maximums, minimums, and related data) as documents progress
through the pipeline.

For the expression operators, see
:ref:`aggregation-expression-operators`.

.. _aggregation-optimize-performance:

Aggregation Pipeline Behavior
-----------------------------

In MongoDB, the :dbcommand:`aggregate` command operates on a single
collection, logically passing the *entire* collection into the
aggregation pipeline. To optimize the operation, wherever possible, use
the following strategies to avoid scanning the entire collection.

.. _aggregation-pipeline-operators-and-performance:

Pipeline Operators and Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :pipeline:`$match`, :pipeline:`$sort`, :pipeline:`$limit`, and
:pipeline:`$skip` pipeline operators can take advantage of an index
when they occur at the **beginning** of the pipeline **before** any of
the following aggregation operators: :pipeline:`$project`,
:pipeline:`$unwind`, and :pipeline:`$group`.

.. versionadded:: 2.4
   The :pipeline:`$geoNear` pipeline operator takes advantage of a
   geospatial index. When using :pipeline:`$geoNear`, the
   :pipeline:`$geoNear` pipeline operation must appear as the first
   stage in an aggregation pipeline.

For unsharded collections, when the aggregation pipeline only needs to
access the indexed fields to fulfill its operations, an index can
:ref:`cover <read-operations-covered-query>` the pipeline.

.. example::

   Consider the following index on the ``orders`` collection:

   .. code-block:: javascript

      { status: 1, amount: 1, cust_id: 1 }

   This index can cover the following aggregation pipeline operation
   because MongoDB does not need to inspect the data outside of the
   index to fulfill the operation:

   .. code-block:: javascript

      db.orders.aggregate([
                            { $match: { status: "A" } },
                            { $group: { _id: "$cust_id", total: { $sum: "$amount" } } },
                            { $sort: { total: -1 } }
                         ])

Early Filtering
~~~~~~~~~~~~~~~

If your aggregation operation requires only a subset of the data in a
collection, use the :pipeline:`$match`, :pipeline:`$limit`, and
:pipeline:`$skip` stages to restrict the documents that enter at the
beginning of the pipeline. When placed at the beginning of a pipeline,
:pipeline:`$match` operations use suitable indexes to scan only
the matching documents in a collection.

Placing a :pipeline:`$match` pipeline stage followed by a
:pipeline:`$sort` stage at the start of the pipeline is logically
equivalent to a single query with a sort and can use an index. When
possible, place :pipeline:`$match` operators at the beginning of the
pipeline.

Additional Features
~~~~~~~~~~~~~~~~~~~

The aggregation pipeline has an internal optimization phase that
provides improved performance for certain sequences of operators. For
details, see :ref:`aggregation-pipeline-sequence-optimization`.

The aggregation pipeline supports operations on sharded collections.
See :ref:`aggregation-pipeline-sharded-collection`.
====================
Aggregation Concepts
====================

.. default-domain:: mongodb

MongoDB provides the three approaches to aggregation,
each with its own strengths and purposes for a given situation. This
section describes these approaches and also describes behaviors and
limitations specific to each approach. See also the :doc:`chart
</reference/aggregation-commands-comparison>` that compares the
approaches.

.. include:: /includes/toc/dfn-list-aggregation-core.rst

.. include:: /includes/toc/aggregation-core.rst
.. _auditing:

========
Auditing
========

.. default-domain:: mongodb

.. versionadded:: 2.6

MongoDB Enterprise includes an auditing capability for
:program:`mongod` and :program:`mongos` instances. The auditing
facility allows administrators and users to track system activity for
deployments with multiple users and applications. The auditing facility
can write audit events to the console, the :term:`syslog`, a JSON file,
or a BSON file. For details on the audit log messages, see
:doc:`/reference/audit-message`.

Audit Events and Filter
-----------------------

The auditing system can record the following operations:

- schema (DDL),
- replica set,
- authentication and authorization, and
- general operations.

See :ref:`audit-action-details-results` for the specific actions
recorded.

By default, the auditing system records all these operations; however,
you can configure the :option:`--auditFilter` option to restrict the
events captured.

See :doc:`/tutorial/configure-auditing` to enable and configure
auditing for MongoDB Enterprise. To set up filters, see
:ref:`audit-filter`.

Audit Guarantee
---------------

The auditing system writes every audit event [#filter]_ to an
in-memory buffer of audit events. MongoDB writes this buffer to disk
periodically. For events collected from any single connection, the
events have a total order: if MongoDB writes one event to disk, the
system guarantees that it has written all prior events for that
connection to disk.

If an audit event entry corresponds to an operation that affects the
durable state of the database, such as a modification to data, MongoDB
will always write the audit event to disk *before* writing to the
:term:`journal` for that entry.

That is, before adding an operation to the journal, MongoDB writes all
audit events on the connection that triggered the operation, up to and
including the entry for the operation.

These auditing guarantees require that MongoDB runs with
the :setting:`journaling <storage.journal.enabled>` enabled.

.. warning:: MongoDB may lose events **if** the server terminates
   before it commits the events to the audit log. The client may
   receive confirmation of the event before MongoDB commits to the
   audit log. For example, while auditing an aggregation operation, the
   server might crash after returning the result but before the audit
   log flushes.

.. [#filter] Audit configuration can include a :ref:`filter
   <audit-filter>` to limit events to audit.
.. _authentication:

==============
Authentication
==============

.. default-domain:: mongodb

Authentication is the process of verifying the identity of a
client. When enabled, MongoDB requires all clients to provide
credentials to access MongoDB databases. By default, MongoDB does not
require authentication.

MongoDB supports a number of authentication mechanisms, or methods
clients can use to validate their identity. These mechanisms allow
MongoDB to integrate into existing authentication systems that your
environments may use. MongoDB's default authentication method is a
challenge and response mechanism. MongoDB also supports x509
certificate authentication, LDAP proxy authentication, and Kerberos
authentication.

With authentication, MongoDB requires authentication for all clients,
including connections between all MongoDB components in a deployment.
See :ref:`inter-process-auth-key-file` for more information.

Authentication is distinct from :ref:`authorization <authorization>`,
which determines the client's access to resources and operations.

Authentication Mechanisms and Credential Storage
------------------------------------------------

MongoDB supports multiple authentication mechanisms to fit into
existing deployments and use existing authentication infrastructure.
To declare a specific authentication mechanism use the
:parameter:`authenticationMechanisms` parameter. For details, see
:doc:`/tutorial/enable-authentication`.

MongoDB represents authentication credentials differently depending on
authentication mechanism. This section addresses all available methods
and describes how each method stores user credentials.

.. _replica-set-security:

``MONGODB-CR`` Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``MONGODB-CR`` is a challenge-response mechanism that authenticates users
through passwords. ``MONGODB-CR`` applies by default when you enable
authentication in MongoDB without setting a mechanism in the
:parameter:`authenticationMechanisms` parameter. You can also explicitly
apply ``MONGODB-CR`` by setting it as the value of
:parameter:`authenticationMechanisms`.

When you enable ``MONGODB-CR`` authentication using the
:setting:`~security.authentication` setting, MongoDB uses the credentials stored in the
``admin`` database's :data:`system.users <admin.system.users>` collection.

When you enable ``MONGODB-CR`` authentication using the :setting:`~security.keyFile`
setting, you must store the key file on each :program:`mongod` or
:program:`mongos` instance. MongoDB uses the key file as stored on each
instance. See :doc:`/tutorial/generate-key-file` for instructions on
generating a key file.

.. _security-auth-x509:

x.509 Certificate Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.6

MongoDB supports x.509 certificate authentication for use with a secure
:doc:`SSL connection </tutorial/configure-ssl>`.

Instead of usernames and passwords, clients can use certificates to
authenticate to servers.

Instead of key files, MongoDB instances can use certificates to
authenticate to sharded clusters and replica sets.

When used to verify membership in a sharded cluster or replica set, an
x.509 certificate must have certain properties, as described in
:ref:`x509-internal-authentication`.

For more information, see :doc:`/tutorial/configure-x509`.

.. _security-auth-kerberos:

Kerberos Authentication
~~~~~~~~~~~~~~~~~~~~~~~

`MongoDB Enterprise <http://www.mongodb.com/products/mongodb-enterprise>`_
supports authentication using a Kerberos service. Kerberos is an industry
standard authentication protocol for large client/server system.

To use MongoDB with Kerberos, you must have a properly configured Kerberos
deployment and the ability to generate valid *keytab* files.

When you use MongoDB with Kerberos, you must store a *keytab* file on each
:program:`mongod` and :program:`mongos` instance. Transmit keytab files
only over secure channels.

You must create corresponding user credentials in the :data:`system.users
<admin.system.users>` collection on the ``admin`` database.

For more information, see
:doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`.

.. _security-auth-ldap:

LDAP Proxy Authority Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`MongoDB Enterprise <http://www.mongodb.com/products/mongodb-enterprise>`_
supports proxy authentication through a Lightweight Directory Access
Protocol (LDAP) service. See
:doc:`/tutorial/configure-ldap-sasl-authentication`.

.. include:: /includes/admonition-mongodb-enterprise-windows-ldap.rst

MongoDB does **not** support LDAP authentication in mixed sharded cluster
deployments that contain both version 2.4 and version 2.6 shards. To
upgrade to 2.6, see :doc:`/release-notes/2.6-upgrade` for upgrade
instructions.

Authentication Options
----------------------

Clients can authenticate using the Challenge Response,
:ref:`x.509 <security-auth-x509>`, :ref:`LDAP Proxy
<security-auth-ldap>` and :ref:`Kerberos
<security-auth-kerberos>` methods.

MongoDB can use the :setting:`~security.keyFile` and :ref:`x.509
<security-auth-x509>` methods to authenticate members of a single
MongoDB deployment to each other.

Authentication Behavior
-----------------------

.. _localhost-exception:

Localhost Exception
~~~~~~~~~~~~~~~~~~~

The localhost exception allows you to enable authentication before
creating the first user in the system. When active, the localhost
exception allows all connections from the localhost interface to have
full access to that instance. The exception applies only when there
are no user documents in the ``admin`` database of a MongoDB instance.

If you use the localhost exception when deploying a new MongoDB
system, the first user created must be an administrator who has
privileges to create other users, such as a user with the
:authrole:`userAdmin` or :authrole:`userAdminAnyDatabase` role. See
:doc:`/tutorial/enable-authentication` and
:doc:`/tutorial/add-user-administrator` for more information.

In the case of a sharded cluster, the localhost exception applies to the
cluster as a whole when no user exists in the cluster's ``admin``
database, which exists on the config servers and clients access via
:program:`mongos` instances. The localhost exception applies separately on
each shard according to whether a user exists in the shard's ``admin``
database.

To prevent unauthorized access to a cluster's shards, you must either
create an administrator on each shard or disable the localhost exception.
To disable the localhost exception, use :setting:`setParameter` to set the
:parameter:`enableLocalhostAuthBypass` parameter to ``0`` during startup.

.. _client-authentication:

Client Authentication
~~~~~~~~~~~~~~~~~~~~~

Each client connection should authenticate as exactly one user. If a
client authenticates to a database as one user and later authenticates on
the same database as a different user, the second authentication
invalidates the first. Clients may be authenticated to multiple databases
at the same time.

MongoDB stores all user information, including credentials and
:doc:`authorization </core/authorization>` information, for a MongoDB
instance in the :data:`system.users <admin.system.users>` collection
in the ``admin`` database.

See :doc:`/tutorial/authenticate-as-client` for more information.

.. _inter-process-auth-key-file:

Authentication Between MongoDB Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:term:`Replica sets <replica set>` and :term:`sharded clusters <sharded
cluster>` require authentication between MongoDB instances. The default
mechanism for authentication between MongoDB instances is the
:setting:`~security.keyFile` setting. The key file serves as a shared password. The
content of the key file is arbitrary but must be the same on all
:program:`mongod` and :program:`mongos` instances that connect to each
other.

Always run replica sets and sharded clusters in a trusted networking
environment. Ensure that the network permits only trusted traffic to reach
each :program:`mongod` and :program:`mongos` instance.

Use your environment's firewall and network routing to ensure that traffic
*only* from clients and other members can reach your :program:`mongod` and
:program:`mongos` instances. If needed, use virtual private networks
(VPNs) to ensure secure connections over wide area networks (WANs).

Always ensure that:

- Your network configuration will allow every member of the replica set or
  sharded cluster to contact every other member.

- If you use MongoDB's authentication system to limit access to your
  infrastructure, ensure that you configure a :setting:`~security.keyFile` on all
  members to permit authentication.

.. index:: sharding; localhost
.. _sharding-localhost:
.. _sharding-security:

Authentication on Sharded Clusters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In a sharded cluster, you can authenticate to the cluster as a whole, to a
specific database on the cluster, or to a given shard. This section
describes how to authenticate to each and where the credentials for
authenticating to each are stored.

To authenticate to a sharded cluster, connect and authenticate
to the :program:`mongos` instance. The credentials all users of a
sharded cluster reside on the ``admin`` databases of the :ref:`config
servers <sharding-config-server>`.

.. versionchanged:: 2.6
   Previously, the credentials for authenticating to a
   database on a cluster resided on the :program:`mongod` instance that is the
   :ref:`primary shard <primary-shard>` for that database.

To perform maintenance operations that require direct connections to
specific shards in a sharded cluster,
(e.g. :dbcommand:`cleanupOrphaned`, :dbcommand:`compact`,
:method:`rs.reconfig()`) you must create *shard local*
administrative users for each shard. The credentials for these users
reside in the ``admin`` database of the shard.

For additional information, see
:doc:`/tutorial/enable-authentication-in-sharded-cluster`.
.. _authorization:

=============
Authorization
=============

.. default-domain:: mongodb

MongoDB employs Role-Based Access Control (RBAC) to govern access to a
MongoDB system. A user is granted one or more :ref:`roles <roles>` that
determine the user's access to database resources and operations. Outside
of role assignments, the user has no access to the system.

MongoDB provides :doc:`built-in roles </reference/built-in-roles>`, each
with a dedicated purpose for a common use case. Examples include the
:authrole:`read`, :authrole:`readWrite`, :authrole:`dbAdmin`, and
:authrole:`root` roles.

Administrators also can create new roles and privileges to cater to
operational needs. Administrators can assign privileges scoped as
granularly as the collection level.

When granted a role, a user receives all the privileges of that role. A
user can have several roles concurrently, in which case the user receives
the union of all the privileges of the respective roles.

.. _roles:

Roles
-----

A role consists of privileges that pair resources with allowed operations.
Each privilege is defined directly in the role or inherited from another
role.

A role's privileges apply to the database where the role is created. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>`.

A user assigned a role receives all the privileges of that role. The user can
have multiple roles and can have different roles on different databases.

Roles always grant privileges and never limit access. For example, if a user
has both :authrole:`read` *and* :authrole:`readWriteAnyDatabase` roles on a
database, the greater access prevails.

.. _privileges:

Privileges
~~~~~~~~~~

A privilege consists of a specified resource and the actions permitted on the
resource.

A privilege :doc:`resource </reference/resource-document>` is either a
database, collection, set of collections, or the cluster. If the cluster, the
affiliated actions affect the state of the system rather than a specific
database or collection.

An :doc:`action </reference/privilege-actions>` is a command or method the
user is allowed to perform on the resource. A resource can have multiple
allowed actions. For available actions see
:doc:`/reference/privilege-actions`.

For example, a privilege that includes the :authaction:`update` action
allows a user to modify existing documents on the resource. To
additionally grant the user permission to create documents on the
resource, the administrator would add the :authaction:`insert` action to
the privilege.

For privilege syntax, see :data:`admin.system.roles.privileges`.

.. _inheritance:

Inherited Privileges
~~~~~~~~~~~~~~~~~~~~

A role can include one or more existing roles in its definition, in which case
the role inherits all the privileges of the included roles.

A role can inherit privileges from other roles in its database. A role created
on the ``admin`` database can inherit privileges from roles in any database.

.. _user-defined-roles:

User-Defined Roles
~~~~~~~~~~~~~~~~~~

.. versionadded:: 2.6

User administrators can create custom roles to ensure collection-level and
command-level granularity and to adhere to the policy of :term:`least
privilege`. Administrators create and edit roles using the :ref:`role
management commands <role-management-commands>`.

MongoDB scopes a user-defined role to the database in which it is created and
uniquely identifies the role by the pairing of its name and its database.
MongoDB stores the roles in the ``admin`` database's :doc:`system.roles
</reference/system-roles-collection>` collection. Do not access this
collection directly but instead use the :ref:`role management commands
<role-management-commands>` to view and edit custom roles.

Role Assignment to Users
------------------------

User administrators create the users that access the system's databases.
MongoDB's :ref:`user management commands <user-management-commands>` let
administrators create users and assign them roles.

MongoDB scopes a user to the database in which the user is created. MongoDB
stores all user definitions in the ``admin`` database, no matter which
database the user is scoped to. MongoDB stores users in the ``admin``
database's :doc:`system.users collection
</reference/system-users-collection>`. Do not access this collection directly
but instead use the :ref:`user management commands
<user-management-commands>`.

The first role assigned in a database should be either :authrole:`userAdmin`
or :authrole:`userAdminAnyDatabase`. This user can then create all other users
in the system. See :doc:`/tutorial/add-user-administrator`.

See Also
--------

:doc:`/reference/built-in-roles`

:doc:`/reference/resource-document`

:doc:`/reference/privilege-actions`

:doc:`/tutorial/add-user-administrator`

:doc:`/tutorial/add-user-to-database`
======================
MongoDB Backup Methods
======================

.. default-domain:: mongodb

When deploying MongoDB in production, you should have a strategy for
capturing and restoring backups in the case of data loss
events. MongoDB provides backup methods to support different
requirements and configurations:

- :ref:`backup-with-mms`
- :ref:`backup-with-file-copies`
- :ref:`backup-with-mongodump`

Backup Methods
--------------

.. _backup-with-mms:

Backups with the MongoDB Management Service (MMS)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The `MongoDB Management Service
<https://mms.10gen.com/?pk_campaign=MongoDB-Org&pk_kwd=Backup-Docs>`_
supports backup and restore for MongoDB deployments.

MMS continually backs up MongoDB replica sets and sharded
systems by reading the oplog data from your MongoDB cluster.

MMS Backup offers point in time recovery of MongoDB replica sets and a
consistent snapshot of sharded systems.

MMS achieves point in time recovery by storing oplog data so that it
can create a restore for any moment in time in the last 24 hours for a
particular replica set.

For sharded systems, MMS does not provide restores for arbitrary moments in
time. MMS does provide periodic consistent snapshots of the entire
sharded cluster. Sharded cluster snapshots are difficult to achieve
with other MongoDB backup methods.

To restore a MongoDB cluster from an MMS Backup snapshot, you download
a compressed archive of your MongoDB data files and distribute those
files before restarting the :program:`mongod` processes.

To get started with MMS Backup `sign up for MMS
<http://mms.mongodb.com>`_, and consider the
complete documentation of MMS see the  :mms:`MMS Manual
</>`.

.. _backup-with-file-copies:

Backup by Copying Underlying Data Files
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can create a backup by copying MongoDB's underlying data
files.

If the volume where MongoDB stores data files supports point in time
snapshots, you can use these snapshots to create backups of a MongoDB
system at an exact moment in time.

File systems snapshots are an operating system volume manager feature,
and are not specific to MongoDB. The mechanics of snapshots depend on
the underlying storage system. For example, if you use
Amazon’s EBS storage system for EC2 supports snapshots. On
Linux the LVM manager can create a snapshot.

To get a correct snapshot of a running :program:`mongod` process, you
must have journaling enabled and the journal must reside on the same
logical volume as the other MongoDB data files. Without journaling
enabled, there is no guarantee that the snapshot
will be consistent or valid.

To get a consistent snapshot of a sharded system, you must disable the
balancer and capture a snapshot from every shard and a config server at
approximately the same moment in time.

If your storage system does not support snapshots, you can copy the
files directly using ``cp``, ``rsync``, or a similar tool. Since
copying multiple files is not an atomic operation, you must stop all
writes to the :program:`mongod` before copying the files. Otherwise, you will
copy the files in an invalid state.

Backups produced by copying the underlying data do not support point
in time recovery for replica sets and are difficult to manage for
larger sharded clusters. Additionally, these backups are larger
because they include the indexes and duplicate underlying storage
padding and fragmentation. :program:`mongodump` by contrast create
smaller backups.

For more information, see the
:doc:`/tutorial/backup-with-filesystem-snapshots` and
:doc:`/tutorial/backup-sharded-cluster-with-filesystem-snapshots` for
complete instructions on using LVM to create snapshots. Also see
:ecosystem:`Back up and Restore Processes for MongoDB on Amazon EC2
</tutorial/backup-and-restore-mongodb-on-amazon-ec2>`.

.. _backup-with-mongodump:

Backup with ``mongodump``
~~~~~~~~~~~~~~~~~~~~~~~~~

The :program:`mongodump` tool reads data from a MongoDB database and
creates high fidelity BSON files. The :program:`mongorestore`
tool can populate a MongoDB database with the data from these BSON
files. These tools are simple and efficient for backing up small
MongoDB deployments, but are not ideal for capturing backups of larger
systems.

:program:`mongodump` and :program:`mongorestore` can operate against a
running :program:`mongod` process, and can manipulate the underlying
data files directly. By default, :program:`mongodump` does not
capture the contents of the :doc:`local database </reference/local-database>`.

:program:`mongodump` only captures the documents in the database. The
resulting backup is space efficient, but :program:`mongorestore` or
:program:`mongod` must rebuild the indexes after restoring data.

When connected to a MongoDB instance, :program:`mongodump` can
adversely affect :program:`mongod` performance. If your data is larger
than system memory, the queries will push the working set out of
memory.

To mitigate the impact of :program:`mongodump` on the performance of
the replica set, use :program:`mongodump` to capture backups from a
:doc:`secondary </core/replica-set-secondary>` member of a replica set.
Alternatively, you can shut down a secondary and use
:program:`mongodump` with the data files directly. If you shut down a
secondary to capture data with :program:`mongodump` ensure that the
operation can complete before its oplog becomes too stale to continue
replicating.

For replica sets, :program:`mongodump` also supports a point in time
feature with the :option:`--oplog <mongodump --oplog>`
option. Applications may continue modifying data while
:program:`mongodump` captures the output. To restore a point in time
backup created with :option:`--oplog <mongodump --oplog>`, use
:program:`mongorestore` with the :option:`--oplogReplay
<mongorestore --oplogReplay>` option.

If applications modify data while :program:`mongodump` is creating a
backup, :program:`mongodump` will compete for resources with
those applications.

See :doc:`/tutorial/backup-with-mongodump`,
:doc:`/tutorial/backup-small-sharded-cluster-with-mongodump`,
and :doc:`/tutorial/backup-sharded-cluster-with-database-dumps`
for more information.

Further Reading
---------------

.. include:: /includes/toc/dfn-list-administration-backup-and-recovery.rst
.. _write-operations-bulk-insert:

=======================
Bulk Inserts in MongoDB
=======================

.. default-domain:: mongodb

In some situations you may need to insert or ingest a large amount of
data into a MongoDB database. These *bulk inserts* have some
special considerations that are different from other write
operations.

.. TODO: section on general write operation considerations?

Use the ``insert()`` Method
---------------------------

The :method:`insert() <db.collection.insert()>` method, when passed an
array of documents, performs a bulk insert, and inserts each document
atomically. Bulk inserts can significantly increase performance by
amortizing :ref:`write concern <write-operations-write-concern>` costs.

.. versionadded:: 2.2
   :method:`insert() <db.collection.insert()>` in the :program:`mongo`
   shell gained support for bulk inserts in version 2.2.

In the :doc:`drivers </applications/drivers>`, you can configure write
concern for batches rather than on a per-document level.

Drivers have a ``ContinueOnError`` option in their insert operation, so
that the bulk operation will continue to insert remaining documents in a
batch even if an insert fails.

.. note::

   If multiple errors occur during a bulk insert, clients only receive
   the last error generated.

.. seealso::

   :doc:`Driver documentation </applications/drivers>` for details
   on performing bulk inserts in your application. Also see
   :doc:`/core/import-export`.

Bulk Inserts on Sharded Clusters
--------------------------------

While ``ContinueOnError`` is optional on unsharded clusters, all bulk
operations to a :term:`sharded collection <sharded cluster>` run with
``ContinueOnError``, which cannot be disabled.

Large bulk insert operations, including initial data inserts or routine
data import, can affect :term:`sharded cluster` performance. For
bulk inserts, consider the following strategies:

Pre-Split the Collection
~~~~~~~~~~~~~~~~~~~~~~~~

If the sharded collection is empty, then the collection has only
one initial :term:`chunk`, which resides on a single shard.
MongoDB must then take time to receive data, create splits, and
distribute the split chunks to the available shards. To avoid this
performance cost, you can pre-split the collection, as described in
:doc:`/tutorial/split-chunks-in-sharded-cluster`.

Insert to Multiple ``mongos``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To parallelize import processes, send insert operations to more than
one :program:`mongos` instance. Pre-split empty collections first as
described in :doc:`/tutorial/split-chunks-in-sharded-cluster`.

Avoid Monotonic Throttling
~~~~~~~~~~~~~~~~~~~~~~~~~~

If your shard key increases monotonically during an insert, then all
inserted data goes to the last chunk in the collection, which will
always end up on a single shard. Therefore, the insert capacity of the
cluster will never exceed the insert capacity of that single shard.

If your insert volume is larger than what a single shard can process,
and if you cannot avoid a monotonically increasing shard key, then
consider the following modifications to your application:

- Reverse the binary bits of the shard key. This preserves the
  information and avoids correlating insertion order with increasing
  sequence of values.

- Swap the first and last 16-bit words to "shuffle" the inserts.

.. example:: The following example, in C++, swaps the leading and
   trailing 16-bit word of :term:`BSON` :term:`ObjectIds <ObjectId>`
   generated so that they are no longer monotonically increasing.

   .. code-block:: cpp

      using namespace mongo;
      OID make_an_id() {
        OID x = OID::gen();
        const unsigned char *p = x.getData();
        swap( (unsigned short&) p[0], (unsigned short&) p[10] );
        return x;
      }

      void foo() {
        // create an object
        BSONObj o = BSON( "_id" << make_an_id() << "x" << 3 << "name" << "jane" );
        // now we may insert o into a sharded collection
      }

.. seealso:: :ref:`sharding-shard-key` for information
   on choosing a sharded key. Also see :ref:`Shard Key
   Internals <sharding-internals-shard-keys>` (in particular,
   :ref:`sharding-internals-operations-and-reliability`).
==================
Capped Collections
==================

.. default-domain:: mongodb

:term:`Capped collections <capped collection>` are fixed-size
collections that support high-throughput operations that insert,
retrieve, and delete documents based on insertion order. Capped
collections work in a way similar to circular buffers: once a
collection fills its allocated space, it makes room for new documents
by overwriting the oldest documents in the collection.

See :method:`~db.createCollection()` or :dbcommand:`createCollection`
for more information on creating capped collections.

Capped collections have the following behaviors:

- Capped collections guarantee preservation of the insertion order. As
  a result, queries do not need an index to return documents in
  insertion order. Without this indexing overhead, they can
  support higher insertion throughput.

- Capped collections guarantee that insertion order is identical to the
  order on disk (:term:`natural order`) and do so by prohibiting updates
  that increase document size. Capped collections only allow updates
  that fit the original document size, which ensures a document does not
  change its location on disk.

- Capped collections automatically remove the oldest documents in the
  collection without requiring scripts or explicit remove operations.

For example, the :term:`oplog.rs <oplog>` collection that stores a log
of the operations in a :term:`replica set` uses a capped
collection. Consider the following potential use cases for capped
collections:

- Store log information generated by high-volume systems. Inserting
  documents in a capped collection without an index is close to the
  speed of writing log information directly to a file
  system. Furthermore, the built-in *first-in-first-out* property
  maintains the order of events, while managing storage use.

- Cache small amounts of data in a capped collections. Since caches
  are read rather than write heavy, you would either need to ensure
  that this collection *always* remains in the working set (i.e. in
  RAM) *or* accept some write penalty for the required index or
  indexes.

.. _capped-collections-recommendations-and-restrictions:

Recommendations and Restrictions
--------------------------------

- You *can* update documents in a collection after inserting
  them. *However,* these updates **cannot** cause the documents to
  grow. If the update operation causes the document to grow beyond
  their original size, the update operation will fail.

  If you plan to update documents in a capped collection, create an
  index so that these update operations do not require a table scan.

- You cannot delete documents from a capped collection. To remove all
  records from a capped collection, use the 'emptycapped' command. To
  remove the collection entirely, use the :method:`~db.collection.drop()`
  method.

- You cannot shard a capped collection.

- Capped collections created after 2.2 have an ``_id`` field and an
  index on the ``_id`` field by default. Capped collections created
  before 2.2 do not have an index on the ``_id`` field by default. If
  you are using capped collections with replication prior to 2.2, you
  should explicitly create an index on the ``_id`` field.

  .. warning::

     If you have a capped collection in a :term:`replica set` outside
     of the ``local`` database, before 2.2, you should create a
     unique index on ``_id``. Ensure uniqueness using the ``unique:
     true`` option to the :method:`~db.collection.ensureIndex()`
     method or by using an :term:`ObjectId` for the ``_id`` field.
     Alternately, you can use the ``autoIndexId`` option to
     :dbcommand:`create` when creating the capped collection, as in the
     :ref:`capped-collections-options` procedure.

- Use natural ordering to retrieve the most recently inserted elements
  from the collection efficiently. This is (somewhat) analogous to
  tail on a log file.

- The aggregation pipeline operator :pipeline:`$out` cannot write
  results to a capped collection.

Procedures
----------

Create a Capped Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~

You must create capped collections explicitly using the
:method:`~db.createCollection()` method, which is a helper in the
:program:`mongo` shell for the :dbcommand:`create` command. When
creating a capped collection you must specify the maximum size of the
collection in bytes, which MongoDB will pre-allocate for the collection.
The size of the capped collection includes a small amount of space for
internal overhead.

.. code-block:: javascript

   db.createCollection( "log", { capped: true, size: 100000 } )

Additionally, you may also specify a maximum number of documents for the
collection using the ``max`` field as in the following document:

.. code-block:: javascript

   db.createCollection("log", { capped : true, size : 5242880, max : 5000 } )

.. important:: The ``size`` argument is *always* required, even when
   you specify ``max`` number of documents. MongoDB will remove older
   documents if a collection reaches the maximum size limit before it
   reaches the maximum document count.

.. see:: :method:`~db.createCollection()` and  :dbcommand:`create`.

.. _capped-collections-options:

Query a Capped Collection
~~~~~~~~~~~~~~~~~~~~~~~~~

If you perform a :method:`~db.collection.find()` on a capped collection
with no ordering specified, MongoDB guarantees that the ordering of
results is the same as the insertion order.

To retrieve documents in reverse insertion order, issue
:method:`~db.collection.find()` along with the :method:`~cursor.sort()`
method with the :operator:`$natural` parameter set to ``-1``, as shown
in the following example:

.. code-block:: javascript

   db.cappedCollection.find().sort( { $natural: -1 } )

Check if a Collection is Capped
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the :method:`~db.collection.isCapped()` method to determine if a
collection is capped, as follows:

.. code-block:: javascript

   db.collection.isCapped()

Convert a Collection to Capped
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can convert a non-capped collection to a capped collection with
the :dbcommand:`convertToCapped` command:

.. code-block:: javascript

   db.runCommand({"convertToCapped": "mycoll", size: 100000});

The ``size`` parameter specifies the size of the capped collection in
bytes.

.. include:: /includes/warning-blocking-global.rst

.. versionchanged:: 2.2
   Before 2.2, capped collections did not have an index on ``_id``
   unless you specified ``autoIndexId`` to the :dbcommand:`create`,
   after 2.2 this became the default.

Automatically Remove Data After a Specified Period of Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For additional flexibility when expiring data, consider MongoDB's
:term:`TTL` indexes, as described in
:doc:`/tutorial/expire-data`. These indexes allow you to expire and
remove data from normal collections using a special type, based on the
value of a date-typed field and a TTL value for the index.

:doc:`TTL Collections </tutorial/expire-data>` are not compatible with
capped collections.

Tailable Cursor
~~~~~~~~~~~~~~~

You can use a tailable cursor with capped collections. Similar to the
Unix ``tail -f`` command, the tailable cursor "tails" the end of a
capped collection. As new documents are inserted into the capped
collection, you can use the tailable cursor to continue retrieving
documents.

See :doc:`/tutorial/create-tailable-cursor` for information on creating
a tailable cursor.
=========================
MongoDB CRUD Introduction
=========================

.. default-domain:: mongodb

MongoDB stores data in the form of *documents*, which are JSON-like
field and value pairs. Documents are analogous to structures in
programming languages that associate keys with values, where keys may
hold other pairs of keys and values (e.g. dictionaries, hashes, maps,
and associative arrays). Formally, MongoDB documents are :term:`BSON`
documents, which is a binary representation of :term:`JSON` with
additional type information. For more information, see
:doc:`/core/document`.

.. include:: /images/crud-annotated-document.rst

MongoDB stores all documents in :term:`collections <collection>`. A
collection is a group of related documents that have a set of shared
common indexes. Collections are analogous to a table in relational
databases.

.. include:: /images/crud-annotated-collection.rst

Database Operations
-------------------

Query
~~~~~

In MongoDB a query targets a specific collection of documents. Queries
specify criteria, or conditions, that identify the documents that
MongoDB returns to the clients. A query may include a *projection* that
specifies the fields from the matching documents to return. You can
optionally modify queries to impose limits, skips, and sort orders.

In the following diagram, the query process specifies a query criteria
and a sort modifier:

.. include:: /images/crud-query-stages.rst

See :doc:`/core/read-operations-introduction` for more information.

Data Modification
~~~~~~~~~~~~~~~~~

Data modification refers to operations that create, update, or delete
data. In MongoDB, these operations modify the data of a single
:term:`collection`. For the update and delete operations, you can
specify the criteria to select the documents to update or remove.

In the following diagram, the insert operation adds a new document to
the ``users`` collection.

.. include:: /images/crud-insert-stages.rst

See :doc:`/core/write-operations-introduction` for more
information.

Related Features
----------------

:doc:`/indexes`
~~~~~~~~~~~~~~~

To enhance the performance of common queries and updates, MongoDB
has full support for secondary indexes. These indexes allow
applications to store a *view* of a portion of the collection in an
efficient data structure. Most indexes store an ordered
representation of all values of a field or a group of
fields. Indexes may also :ref:`enforce uniqueness
<index-type-unique>`, store objects in a :doc:`geospatial
representation </applications/geospatial-indexes>`, and facilitate
:doc:`text search </core/index-text>`.

:doc:`/core/read-preference`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For replica sets and sharded clusters with replica set components,
applications specify :ref:`read preferences
<replica-set-read-preference>`. A read preference determines how
the client direct read operations to the set.

:doc:`/core/write-concern`
~~~~~~~~~~~~~~~~~~~~~~~~~~

Applications can also control the behavior of write operations using
:ref:`write concern <write-concern>`. Particularly useful for
deployments with replica sets, the write concern semantics allow
clients to specify the assurance that MongoDB provides when reporting
on the success of a write operation.

:doc:`/aggregation`
~~~~~~~~~~~~~~~~~~~

In addition to the basic queries, MongoDB provides several data
aggregation features. For example, MongoDB can return counts of the
number of documents that match a query, or return the number of
distinct values for a field, or process a collection of documents using
a versatile stage-based data processing pipeline or map-reduce
operations.
=====================
MongoDB CRUD Concepts
=====================

.. default-domain:: mongodb

The :doc:`/core/read-operations` and :doc:`/core/write-operations`
documents introduce the behavior and operations of read and write
operations for MongoDB deployments.

.. include:: /includes/toc/dfn-list-spec-crud-core-landing.rst

.. include:: /includes/toc/crud-core-landing.rst
.. _read-operations-cursors:

=======
Cursors
=======

.. default-domain:: mongodb

In the :program:`mongo` shell, the primary method for the read
operation is the :method:`db.collection.find()` method. This method
queries a collection and returns a :term:`cursor` to the returning
documents.

To access the documents, you need to iterate the cursor. However, in
the :program:`mongo` shell, if the returned cursor is not assigned to a
variable using the ``var`` keyword, then the cursor is automatically
iterated up to 20 times [#set-shell-batch-size]_ to print up to the
first 20 documents in the results.

For example, in the :program:`mongo` shell, the following read
operation queries the ``inventory`` collection for documents that have
``type`` equal to ``'food'`` and automatically print up to the first 20
matching documents:

.. code-block:: javascript

   db.inventory.find( { type: 'food' } );

To manually iterate the cursor to access the documents, see
:doc:`/tutorial/iterate-a-cursor`.

.. include:: /includes/footnote-set-shell-batch-size.rst

.. _cursor-behaviors:

Cursor Behaviors
----------------

Closure of Inactive Cursors
~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, the server will automatically close the cursor after 10
minutes of inactivity or if client has exhausted the cursor. To
override this behavior, you can specify the ``noTimeout``
:meta-driver:`wire protocol flag </legacy/mongodb-wire-protocol>` in
your query; however, you should either close the cursor manually or
exhaust the cursor. In the :program:`mongo` shell, you can set the
``noTimeout`` flag:

.. code-block:: javascript

   var myCursor = db.inventory.find().addOption(DBQuery.Option.noTimeout);

See your :doc:`driver </applications/drivers>` documentation for
information on setting the ``noTimeout`` flag. For the :program:`mongo`
shell, see :method:`cursor.addOption()` for a complete list of
available cursor flags.

Cursor Isolation
~~~~~~~~~~~~~~~~

Because the cursor is not isolated during its lifetime, intervening
write operations on a document may result in a cursor that returns a
document more than once if that document has changed. To handle this
situation, see the information on :ref:`snapshot mode
<faq-developers-isolate-cursors>`.

Cursor Batches
~~~~~~~~~~~~~~

The MongoDB server returns the query results in batches. Batch size
will not exceed the :ref:`maximum BSON document size
<limit-bson-document-size>`. For most queries, the *first* batch
returns 101 documents or just enough documents to exceed 1 megabyte.
Subsequent batch size is 4 megabytes. To override the default size of
the batch, see :method:`~cursor.batchSize()` and
:method:`~cursor.limit()`.

For queries that include a sort operation *without* an index, the
server must load all the documents in memory to perform the sort and
will return all documents in the first batch.

As you iterate through the cursor and reach the end of the returned
batch, if there are more results, :method:`cursor.next()` will perform
a :data:`getmore operation <currentOp.op>` to retrieve the next batch.
To see how many documents remain in the batch as you iterate the
cursor, you can use the :method:`~cursor.objsLeftInBatch()` method, as
in the following example:

.. code-block:: javascript

   var myCursor = db.inventory.find();

   var myFirstDocument = myCursor.hasNext() ? myCursor.next() : null;

   myCursor.objsLeftInBatch();

Cursor Information
------------------

You can use the command :dbcommand:`cursorInfo` to retrieve the
following information on cursors:

- total number of open cursors

- size of the client cursors in current use

- number of timed out cursors since the last server restart

Consider the following example:

.. code-block:: javascript

   db.runCommand( { cursorInfo: 1 } )

The result from the command returns the following document:

.. code-block:: javascript

   {
     "totalOpen" : <number>,
     "clientCursors_size" : <number>,
     "timedOut" : <number>,
     "ok" : 1
   }
.. _data-modeling-decisions:

=================
Data Model Design
=================

.. default-domain:: mongodb

Effective data models support your application needs. The key
consideration for the structure of your documents is the decision to
:ref:`embed <data-modeling-embedding>` or to :ref:`use references
<data-modeling-referencing>`.

.. _data-modeling-embedding:

Embedded Data Models
--------------------

With MongoDB, you may embed related data in a single structure or
document. These schema are generally known as "denormalized" models,
and take advantage of MongoDB's rich documents. Consider the following
diagram:

.. include:: /images/data-model-denormalized.rst

Embedded data models allow applications to store related pieces of
information in the same database record. As a result, applications may
need to issue fewer queries and updates to complete common operations.

In general, use embedded data models when:

- you have "contains" relationships between entities. See
  :ref:`data-modeling-example-one-to-one`.

- you have one-to-many relationships between entities. In these
  relationships the "many" or child documents always appear with or
  are viewed in the context of the "one" or parent documents.  See
  :ref:`data-modeling-example-one-to-many`.

In general, embedding provides better performance for read operations,
as well as the ability to request and retrieve related data in a single
database operation. Embedded data models make it possible to update
related data in a single atomic write operation.

However, embedding related data in documents may lead to situations
where documents grow after creation. Document growth can impact write
performance and lead to data fragmentation. See
:ref:`data-model-document-growth` for details. Furthermore, documents
in MongoDB must be smaller than the :limit:`maximum BSON document size
<BSON Document Size>`. For bulk binary data, consider :doc:`GridFS
</core/gridfs>`.

To interact with embedded documents, use :term:`dot notation` to "reach
into" embedded documents. See :ref:`query for data in arrays
<read-operations-arrays>` and :ref:`query data in sub-documents
<read-operations-subdocuments>` for more examples on accessing data in
arrays and embedded documents.

.. _data-modeling-referencing:

Normalized Data Models
----------------------

Normalized data models describe relationships using :doc:`references
</reference/database-references>` between documents.

.. include:: /images/data-model-normalized.rst

In general, use normalized data models:

- when embedding would result in duplication of data but would not
  provide sufficient read performance advantages to outweigh the
  implications of the duplication.

- to represent more complex many-to-many relationships.

- to model large hierarchical data sets.

References provides more flexibility than embedding. However,
client-side applications must issue follow-up queries to resolve the
references. In other words, normalized data models can require more
roundtrips to the server.

See :ref:`data-modeling-publisher-and-books` for an example of
referencing. For examples of various tree models using references, see
:doc:`/applications/data-models-tree-structures`.
===================================
Operational Factors and Data Models
===================================

.. default-domain:: mongodb

Modeling application data for MongoDB depends on both the data itself,
as well as the characteristics of MongoDB itself. For example,
different data models may allow applications to use more efficient
queries, increase the throughput of insert and update operations, or
distribute activity to a sharded cluster more effectively.

These factors are *operational* or address requirements that arise
outside of the application but impact the performance of MongoDB based
applications. When developing a data model, analyze all of your
application's :doc:`read operations </core/read-operations>` and
:doc:`write operations </core/write-operations>` in conjunction with
the following considerations.

.. _data-model-document-growth:

Document Growth
---------------

Some updates to documents can increase the size of documents.
These updates include pushing elements to an array (i.e.
:update:`$push`) and adding new fields to a document. If the document
size exceeds the allocated space for that document, MongoDB will
relocate the document on disk. Relocating documents takes longer than
*in place updates* and can lead to fragmented storage. Although
MongoDB automatically :ref:`adds padding to document allocations
<record-allocation-stratgies>` to minimize the likelihood of relocation, data
models should avoid document growth when possible.

For instance, if your applications require updates that will cause
document growth, you may want to refactor your data model to use
references between data in distinct documents rather than a
denormalized data model.

MongoDB adaptively adjusts the amount of automatic padding to reduce
occurrences of relocation. You may also use a *pre-allocation*
strategy to explicitly avoid document growth. Refer to the
:ecosystem:`Pre-Aggregated Reports Use Case
</use-cases/pre-aggregated-reports>` for an example of the
*pre-allocation* approach to handling document growth.

See :doc:`/core/storage` for more information on MongoDB's storage
model and record allocation strategies.

.. _data-model-atomicity:
.. _data-modeling-atomicity:

Atomicity
---------

In MongoDB, operations are atomic at the :term:`document` level. No
**single** write operation can change more than one document.
Operations that modify more than a single document in a collection
still operate on one document at a time. [#record-atomicity]_ Ensure
that your application stores all fields with atomic dependency
requirements in the same document. If the application can tolerate
non-atomic updates for two pieces of data, you can store these data in
separate documents.

A data model that embeds related data in a single document
facilitates these kinds of atomic operations. For data models that
store references between related pieces of data, the application must
issue separate read and write operations to retrieve and modify these
related pieces of data.

See :ref:`data-modeling-atomic-operation` for an example data model
that provides atomic updates for a single document.

.. [#record-atomicity] Document-level atomic operations include all
   operations within a single MongoDB document record: operations that
   affect multiple sub-documents within that single record are still
   atomic.

Sharding
--------

MongoDB uses :term:`sharding` to provide horizontal scaling. These
clusters support deployments with large data sets and high-throughput
operations. Sharding allows users to :term:`partition` a
:term:`collection` within a database to distribute the collection's
documents across a number of :program:`mongod` instances or
:term:`shards <shard>`.

To distribute data and application traffic in a sharded collection,
MongoDB uses the :ref:`shard key <shard-key>`. Selecting the proper
:ref:`shard key <shard-key>` has significant implications for
performance, and can enable or prevent query isolation and increased
write capacity. It is important to consider carefully the field or
fields to use as the shard key.

See :doc:`/core/sharding-introduction` and
:doc:`/core/sharding-shard-key` for more information.

Indexes
-------

Use indexes to improve performance for common queries. Build indexes on
fields that appear often in queries and for all operations that return
sorted results. MongoDB automatically creates a unique index on the
``_id`` field.

As you create indexes, consider the following behaviors of indexes:

- Each index requires at least 8KB of data space.

- Adding an index has some negative performance impact for write
  operations. For collections with high write-to-read ratio, indexes
  are expensive since each insert must also update any indexes.

- Collections with high read-to-write ratio often benefit from
  additional indexes. Indexes do not affect un-indexed read operations.

- When active, each index consumes disk space and memory. This usage
  can be significant and should be tracked for capacity planning,
  especially for concerns over working set size.

See :doc:`/applications/indexes` for more information on indexes as
well as :doc:`/tutorial/analyze-query-plan/`. Additionally, the MongoDB
:doc:`database profiler </tutorial/manage-the-database-profiler>` may
help identify inefficient queries.

.. _data-model-large-number-of-collections:

Large Number of Collections
---------------------------

In certain situations, you might choose to store related information in
several collections rather than in a single collection.

Consider a sample collection ``logs`` that stores log documents for
various environment and applications. The ``logs`` collection contains
documents of the following form:

.. code-block:: javascript

   { log: "dev", ts: ..., info: ... }
   { log: "debug", ts: ..., info: ...}

If the total number of documents is low, you may group documents into
collection by type. For logs, consider maintaining distinct log
collections, such as ``logs.dev`` and ``logs.debug``. The ``logs.dev``
collection would contain only the documents related to the dev
environment.

Generally, having a large number of collections has no significant
performance penalty and results in very good performance. Distinct
collections are very important for high-throughput batch processing.

When using models that have a large number of collections, consider
the following behaviors:

- Each collection has a certain minimum overhead of a few kilobytes.

- Each index, including the index on ``_id``, requires at least 8KB of
  data space.

- For each :term:`database`, a single namespace file (i.e.
  ``<database>.ns``) stores all meta-data for that database, and each
  index and collection has its own entry in the namespace file. MongoDB
  places :limit:`limits on the size of namespace files
  <Size of Namespace File>`.

- MongoDB has :limit:`limits on the number of namespaces
  <Number of Namespaces>`. You may wish to know the current number of
  namespaces in order to determine how many additional namespaces the
  database can support. To get the current number of namespaces, run
  the following in the :program:`mongo` shell:

  .. code-block:: javascript

     db.system.namespaces.count()

  .. todo make a tutorial called "how to change size of namespace file"

  The limit on the number of namespaces depend on the ``<database>.ns``
  size. The namespace file defaults to 16 MB.

  To change the size of the *new* namespace file, start the server with
  the option :option:`--nssize \<new size MB\> <--nssize>`. For
  existing databases, after starting up the server with
  :option:`--nssize`, run the :method:`db.repairDatabase()` command
  from the :program:`mongo` shell. For impacts and considerations on
  running :method:`db.repairDatabase()`, see
  :dbcommand:`repairDatabase`.

Data Lifecycle Management
-------------------------

Data modeling decisions should take data lifecycle management into
consideration.

The :doc:`Time to Live or TTL feature </tutorial/expire-data>` of
collections expires documents after a period of time. Consider using
the TTL feature if your application requires some data to persist in
the database for a limited period of time.

Additionally, if your application only uses recently inserted
documents, consider :doc:`/core/capped-collections`. Capped collections
provide *first-in-first-out* (FIFO) management of inserted documents
and efficiently support operations that insert and read documents based
on insertion order.
==========================
Data Modeling Introduction
==========================

.. default-domain:: mongodb

Data in MongoDB has a *flexible schema*. Unlike SQL databases, where
you must determine and declare a table's schema before inserting data,
MongoDB's :term:`collections <collection>` do not enforce
:term:`document` structure. This flexibility facilitates the mapping of
documents to an entity or an object. Each document can match the data
fields of the represented entity, even if the data has substantial
variation. In practice, however, the documents in a collection share a
similar structure.

The key challenge in data modeling is balancing the needs of the
application, the performance characteristics of the database engine, and
the data retrieval patterns. When designing data models, always
consider the application usage of the data (i.e. queries, updates, and
processing of the data) as well as the inherent structure of the data
itself.

Document Structure
------------------

The key decision in designing data models for MongoDB applications
revolves around the structure of documents and how the application
represents relationships between data. There are two tools that allow
applications to represent these relationships: *references* and
*embedded documents*.

References
~~~~~~~~~~

References store the relationships between data by including
links or *references* from one document to another. Applications can
resolve these :doc:`references </reference/database-references>` to
access the related data. Broadly, these are *normalized* data models.

.. include:: /images/data-model-normalized.rst

See :ref:`data-modeling-referencing` for the strengths and weaknesses of
using references.

Embedded Data
~~~~~~~~~~~~~

Embedded documents capture relationships between data by storing
related data in a single document structure. MongoDB documents make it
possible to embed document structures as sub-documents in a field or
array within a document. These *denormalized* data models allow
applications to retrieve and manipulate related data in a single
database operation.

.. include:: /images/data-model-denormalized.rst

See :ref:`data-modeling-embedding` for the strengths and weaknesses of
embedding sub-documents.

Atomicity of Write Operations
-----------------------------

In MongoDB, write operations are atomic at the :term:`document` level,
and no single write operation can atomically affect more than one
document or more than one collection. A denormalized data model with
embedded data combines all related data for a represented entity in a
single document. This facilitates atomic write operations since a
single write operation can insert or update the data for an entity.
Normalizing the data would split the data across multiple collections
and would require multiple write operations that are not atomic
collectively.

However, schemas that facilitate atomic writes may limit ways that
applications can use the data or may limit ways to modify applications.
The :ref:`Atomicity Considerations <data-model-atomicity>`
documentation describes the challenge of designing a schema that
balances flexibility and atomicity.

Document Growth
---------------

Some updates, such as pushing elements to an array or adding new
fields, increase a :term:`document's <document>` size. If the document
size exceeds the allocated space for that document, MongoDB relocates
the document on disk. The growth consideration can affect the decision
to normalize or denormalize data. See :ref:`Document Growth
Considerations <data-model-document-growth>` for more about planning
for and managing document growth in MongoDB.

Data Use and Performance
------------------------

When designing a data model, consider how applications will use your
database. For instance, if your application only uses recently
inserted documents, consider using :doc:`/core/capped-collections`. Or
if your application needs are mainly read operations to a collection,
adding indexes to support common queries can improve performance.

See :doc:`/core/data-model-operations` for more information on these
and other operational considerations that affect data model designs.
======================
Data Modeling Concepts
======================

.. default-domain:: mongodb

When constructing a data model for your MongoDB collection, there are
various options you can choose from, each of which has its strengths
and weaknesses. The following sections guide you through key design
decisions and detail various considerations for choosing the best data
model for your application needs.

For a general introduction to data modeling in MongoDB, see the
:doc:`Data Modeling Introduction
</core/data-modeling-introduction>`. For example data models, see
:doc:`Data Modeling Examples and Patterns
</applications/data-models>`.

.. include:: /includes/toc/dfn-list-data-modeling-concepts.rst

.. include:: /includes/toc/data-modeling-concepts.rst
.. index:: read operation; architecture
.. _read-operations-architecture:

===================
Distributed Queries
===================

.. default-domain:: mongodb

.. might make sense to break this in half and move it to
   replication/sharding and cross reference here?

Read Operations to Sharded Clusters
-----------------------------------

:term:`Sharded clusters <sharded cluster>` allow you to partition a data
set among a cluster of :program:`mongod` instances in a way that is
nearly transparent to the application. For an overview of sharded
clusters, see the :doc:`/sharding` section of this manual.

For a sharded cluster, applications issue operations to one of the
:program:`mongos` instances associated with the cluster.

.. include:: /images/sharded-cluster.rst

Read operations on sharded clusters are most efficient when directed to
a specific shard. Queries to sharded collections should include the
collection's :ref:`shard key <sharding-shard-key>`. When a query
includes a shard key, the :program:`mongos` can use cluster metadata
from the :ref:`config database <sharding-config-server>` to route the
queries to shards.

.. include:: /images/sharded-cluster-targeted-query.rst

If a query does not include the shard key, the :program:`mongos` must
direct the query to *all* shards in the cluster. These *scatter
gather* queries can be inefficient. On larger clusters, scatter gather
queries are unfeasible for routine operations.

.. include:: /images/sharded-cluster-scatter-gather-query.rst

For more information on read operations in sharded clusters, see the
:doc:`/core/sharded-cluster-query-router` and :ref:`sharding-shard-key`
sections.

.. index:: read operation; connection pooling
.. index:: connection pooling; read operations
.. _read-operations-connection-pooling:

Read Operations to Replica Sets
-------------------------------

:term:`Replica sets <replica set>` use :term:`read preferences <read
preference>` to determine where and how to route read operations to
members of the replica set. By default, MongoDB always reads data from
a replica set's :term:`primary`. You can modify that behavior by
changing the :ref:`read preference mode
<replica-set-read-preference-modes>`.

You can configure the :ref:`read preference mode
<replica-set-read-preference-modes>` on a per-connection or
per-operation basis to allow reads from :term:`secondaries
<secondary>` to:

- reduce latency in multi-data-center deployments,

- improve read throughput by distributing high read-volumes (relative
  to write volume),

- for backup operations, and/or

- to allow reads during :ref:`failover <replica-set-failover>`
  situations.

.. include:: /images/replica-set-read-preference.rst

Read operations from secondary members of replica sets are not
guaranteed to reflect the current state of the primary, and the state
of secondaries will trail the primary by some amount of time. Often,
applications don't rely on this kind of strict consistency, but
application developers should always consider the needs of their
application before setting read preference.

For more information on read preference or on the read preference
modes, see :doc:`/core/read-preference` and
:ref:`replica-set-read-preference-modes`.
============================
Distributed Write Operations
============================

.. default-domain:: mongodb

.. _write-operations-sharded-clusters:

Write Operations on Sharded Clusters
------------------------------------

For sharded collections in a :term:`sharded cluster`, the
:program:`mongos` directs write operations from applications to the
shards that are responsible for the specific *portion* of the data
set. The :program:`mongos` uses the cluster metadata from the
:ref:`config database <sharding-config-server>` to route the write
operation to the appropriate shards.

.. include:: /images/sharded-cluster.rst

MongoDB partitions data in a sharded collection into *ranges* based on
the values of the :term:`shard key`. Then, MongoDB distributes these
chunks to shards. The shard key determines the distribution of chunks to
shards. This can affect the performance of write operations in the
cluster.

.. include:: /images/sharding-range-based.rst

.. important:: Update operations that affect a *single* document
   **must** include the :term:`shard key` or the ``_id``
   field. Updates that affect multiple documents are more efficient in
   some situations if they have the :term:`shard key`, but can be
   broadcast to all shards.

If the value of the shard key increases or decreases with every
insert, all insert operations target a single shard. As a result, the
capacity of a single shard becomes the limit for the insert capacity
of the sharded cluster.

For more information, see :doc:`/administration/sharded-clusters` and
:ref:`write-operations-bulk-insert`.

.. _write-operations-replica-sets:

Write Operations on Replica Sets
--------------------------------

In :term:`replica sets <replica set>`, all write operations go to the
set's :term:`primary`, which applies the write operation then records
the operations on the primary's operation log or :term:`oplog`. The
oplog is a reproducible sequence of operations to the data
set. :term:`Secondary` members of the set are continuously replicating the
oplog and applying the operations to themselves in an asynchronous
process.

.. include:: /images/replica-set-read-write-operations-primary.rst

Large volumes of write operations, particularly bulk operations, may
create situations where the secondary members have difficulty applying
the replicating operations from the primary at a sufficient rate: this
can cause the secondary's state to fall behind that of the primary. Secondaries
that are significantly behind the primary present problems for normal
operation of the replica set, particularly :ref:`failover
<replica-set-failover-administration>` in the form of :ref:`rollbacks
<replica-set-rollback>` as well as general :doc:`read consistency
</applications/replication>`.

To help avoid this issue, you can customize the :ref:`write concern
<write-operations-write-concern>` to return confirmation of the write
operation to another member [#write-concern-throttling]_ of the replica
set every 100 or 1,000 operations. This provides an opportunity for
secondaries to catch up with the primary. Write concern can slow the
overall progress of write operations but ensure that the secondaries
can maintain a largely current state with respect to the primary.

.. include:: /images/crud-write-concern-w2.rst

For more information on replica sets and write operations, see
:ref:`replica-set-write-concern`, :ref:`replica-set-oplog-sizing`, and
:doc:`/tutorial/change-oplog-size`.

.. [#write-concern-throttling] Calling :dbcommand:`getLastError`
   intermittently with a ``w`` value of ``2`` or ``majority`` will
   slow the throughput of write traffic; however, this practice will
   allow the secondaries to remain current with the state of the
   primary.

   .. include:: /includes/fact-master-slave-majority.rst
=========
Documents
=========

.. default-domain:: mongodb

MongoDB stores all data in documents, which are JSON-style data
structures composed of field-and-value pairs:

.. code-block:: javascript

   { "item": "pencil", "qty": 500, "type": "no.2" }

Most user-accessible data structures in MongoDB are documents,
including:

- All database records.

- :doc:`Query selectors </core/read-operations>`, which define what records to
  select for read, update, and delete operations.

- :doc:`Update definitions </core/write-operations>`, which define what fields
  to modify during an update.

- :doc:`Index specifications </core/indexes>`, which define what
  fields to index.

- Data output by MongoDB for reporting and configuration, such as the
  output of the :dbcommand:`serverStatus` and the :ref:`replica set
  configuration document <replica-set-configuration-document>`.

Document Format
---------------

MongoDB stores documents on disk in the :term:`BSON` serialization
format. BSON is a binary representation of :term:`JSON` documents,
though it contains more data types than JSON. For the BSON spec, see
`bsonspec.org <http://bsonspec.org/>`_. See also
:doc:`/reference/bson-types`.

The :program:`mongo` JavaScript shell and the :doc:`MongoDB language
drivers </applications/drivers>` translate between BSON and the
language-specific document representation.

.. _document-structure:

Document Structure
------------------

MongoDB documents are composed of field-and-value pairs and have the
following structure:

.. code-block:: javascript

   {
      field1: value1,
      field2: value2,
      field3: value3,
      ...
      fieldN: valueN
   }

The value of a field can be any of the BSON :doc:`data types
</reference/bson-types>`, including other documents, arrays, and arrays
of documents. The following document contains values of varying types:

.. code-block:: javascript

   var mydoc = {
                  _id: ObjectId("5099803df3f4948bd2f98391"),
                  name: { first: "Alan", last: "Turing" },
                  birth: new Date('Jun 23, 1912'),
                  death: new Date('Jun 07, 1954'),
                  contribs: [ "Turing machine", "Turing test", "Turingery" ],
                  views : NumberLong(1250000)
               }

The above fields have the following data types:

- ``_id`` holds an *ObjectId*.

- ``name`` holds a *subdocument* that contains the fields ``first`` and
  ``last``.

- ``birth`` and ``death`` hold values of the *Date* type.

- ``contribs`` holds an *array of strings*.

- ``views`` holds a value of the *NumberLong* type.

Field Names
-----------

Field names are strings.

.. include:: /includes/fact-document-field-name-restrictions.rst

BSON documents may have more than one field with the same name.
Most :doc:`MongoDB interfaces </applications/drivers>`, however, represent MongoDB
with a structure (e.g. a hash table) that does not support duplicate
field names. If you need to manipulate documents that have more than one
field with the same name, see the :doc:`driver documentation
</applications/drivers>` for your driver.

Some documents created by internal MongoDB processes may have duplicate
fields, but *no* MongoDB process will *ever* add duplicate fields to an
existing user document.

Field Value Limit
-----------------

For :doc:`indexed collections </core/indexes-introduction>`, the values
for the indexed fields have a
:limit:`Maximum Index Key Length <Index Key>` limit. See
:limit:`Maximum Index Key Length <Index Key>` for details.

Document Limitations
--------------------

Documents have the following attributes:

Document Size Limit
~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-document-max-size.rst

Document Field Order
~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-update-field-order.rst
   :start-after: order-of-document-fields

.. _document-id-field:

The ``_id`` Field
-----------------

The ``_id`` field has the following behavior and constraints:

- By default, MongoDB creates a unique index on the ``_id`` field
  during the creation of a collection.

- The ``_id`` field is always the first field in the documents. If the
  server receives a document that does not have the ``_id`` field
  first, then the server will move the field to the beginning.

- The ``_id`` field may contain values of any :doc:`BSON data type
  </reference/bson-types>`, other than an array.

  .. warning:: To ensure functioning replication, do not store values
     that are of the BSON regular expression type in the ``_id``
     field.

     .. See :issue:`SERVER-9562` for more information.

The following are common options for storing values for ``_id``:

- Use an :doc:`ObjectId </reference/object-id>`.

- Use a natural unique identifier, if available. This saves space and
  avoids an additional index.

- Generate an auto-incrementing number. See
  :doc:`/tutorial/create-an-auto-incrementing-field`.

- Generate a UUID in your application code. For a more efficient
  storage of the UUID values in the collection and in the ``_id``
  index, store the UUID as a value of the BSON ``BinData`` type.

  .. include:: /includes/fact-bindata-storage-optimization.rst

- Use your driver's BSON UUID facility to generate UUIDs. Be aware
  that driver implementations may implement UUID serialization and
  deserialization logic differently, which may not be fully compatible
  with other drivers. See your :api:`driver documentation <>` for
  information concerning UUID interoperability.

.. include:: /includes/note-insert-id-field.rst

.. _document-dot-notation:

Dot Notation
------------

.. include:: /includes/fact-dot-notation.rst

.. seealso::

   - :ref:`read-operations-subdocuments` for dot notation examples
     with subdocuments.

   - :ref:`read-operations-arrays` for dot notation examples with
     arrays.
.. _index-geohaystack-index:

================
Haystack Indexes
================

.. default-domain:: mongodb

A haystack index is a special index that is optimized to return results
over small areas. Haystack indexes improve performance on queries that
use flat geometry.

For queries that use spherical geometry, a **2dsphere index is a better
option** than a haystack index. :doc:`2dsphere indexes
</core/2dsphere>` allow field reordering; haystack indexes require the
first field to be the location field. Also, haystack indexes are only
usable via commands and so always return all results at once.

Haystack indexes create "buckets" of documents from the same geographic
area in order to improve performance for queries limited to that area.
Each bucket in a haystack index contains all the documents within a
specified proximity to a given longitude and latitude.

To create a geohaystacks index, see
:doc:`/tutorial/build-a-geohaystack-index`. For information and example
on querying a haystack index, see
:doc:`/tutorial/query-a-geohaystack-index`.
======================
``2d`` Index Internals
======================

.. default-domain:: mongodb

This document provides a more in-depth explanation of the internals of MongoDB's
``2d`` geospatial indexes. This material is not necessary for normal operations
or application development but may be useful for troubleshooting and for
further understanding.

.. _geospatial-indexes-geohash:

Calculation of Geohash Values for ``2d`` Indexes
------------------------------------------------

When you create a geospatial index on :term:`legacy coordinate pairs
<legacy coordinate pairs>`, MongoDB computes :term:`geohash` values
for the coordinate pairs within the specified :ref:`location range
<geospatial-indexes-range>` and then indexes the geohash values.

To calculate a geohash value, recursively divide a two-dimensional map into
quadrants. Then assign each quadrant a two-bit value. For example, a
two-bit representation of four quadrants would be:

.. code-block:: javascript

   01  11

   00  10

These two-bit values (``00``, ``01``, ``10``, and ``11``) represent each
of the quadrants and all points within each quadrant. For a geohash with
two bits of resolution, all points in the bottom left quadrant would
have a geohash of ``00``. The top left quadrant would have the geohash
of ``01``. The bottom right and top right would have a geohash of ``10``
and ``11``, respectively.

To provide additional precision, continue dividing each quadrant into
sub-quadrants. Each sub-quadrant would have the geohash value of the
containing quadrant concatenated with the value of the sub-quadrant. The
geohash for the upper-right quadrant is ``11``, and the geohash for the
sub-quadrants would be (clockwise from the top left): ``1101``,
``1111``, ``1110``, and ``1100``, respectively.

.. Commented out -- per Jesse's feedback, users don't control this. To
   calculate a more precise geohash, continue dividing the sub-quadrant
   and concatenate the two-bit identifier for each division. The more
   "bits" in the hash identifier for a given point, the smaller possible
   area that the hash can describe and the higher the resolution of the
   geospatial index.

.. _geospatial-indexes-multi-location:

Multi-location Documents for ``2d`` Indexes
-------------------------------------------

.. versionadded:: 2.0
   Support for multiple locations in a document.

While ``2d`` geospatial indexes do not support more than one set of
coordinates in a document, you can use a :ref:`multi-key index
<index-type-multi-key>` to index multiple coordinate pairs in
a single document. In the simplest example you may have a field (e.g.
``locs``) that holds an array of coordinates, as in the following
example:

.. code-block:: javascript

   { _id : ObjectId(...),
     locs : [ [ 55.5 , 42.3 ] ,
              [ -74 , 44.74 ] ,
              { lng : 55.5 , lat : 42.3 } ]
   }

The values of the array may be either arrays, as in ``[ 55.5, 42.3 ]``,
or embedded documents, as in ``{ lng : 55.5 , lat : 42.3 }``.

You could then create a geospatial index on the ``locs`` field, as in
the following:

.. code-block:: javascript

   db.places.ensureIndex( { "locs": "2d" } )

You may also model the location data as a field inside of a
sub-document. In this case, the document would contain a field
(e.g. ``addresses``) that holds an array of documents where each
document has a field (e.g. ``loc:``) that holds location
coordinates. For example:

.. code-block:: javascript

   { _id : ObjectId(...),
     name : "...",
     addresses : [ {
                    context : "home" ,
                    loc : [ 55.5, 42.3 ]
                   } ,
                   {
                    context : "home",
                    loc : [ -74 , 44.74 ]
                   }
                 ]
   }

You could then create the geospatial index on the ``addresses.loc`` field as
in the following example:

.. code-block:: javascript

   db.records.ensureIndex( { "addresses.loc": "2d" } )

To include the location field with the distance field in multi-location
document queries, specify ``includeLocs: true`` in the
:dbcommand:`geoNear` command.
.. index:: GridFS

======
GridFS
======

.. default-domain:: mongodb

:term:`GridFS` is a specification for storing and retrieving files
that exceed the :term:`BSON`\-document :ref:`size limit
<limit-bson-document-size>` of 16MB.

Instead of storing a file in a single document, GridFS divides a file
into parts, or chunks, [#chunk-disambiguation]_ and stores each of
those chunks as a separate document. By default GridFS limits chunk
size to 255k. GridFS uses two collections to store files. One
collection stores the file chunks, and the other stores file metadata.

When you query a GridFS store for a file, the driver or client will
reassemble the chunks as needed. You can perform range queries on
files stored through GridFS.  You also can access information from
arbitrary sections of files, which allows you to "skip" into the
middle of a video or audio file.

GridFS is useful not only for storing files that exceed 16MB but also
for storing any files for which you want access without having to load
the entire file into memory. For more information on the indications
of GridFS, see :ref:`faq-developers-when-to-use-gridfs`.

.. [#chunk-disambiguation] The use of the term *chunks* in the context
   of GridFS is not related to the use of the term *chunks* in
   the context of sharding.

.. versionchanged:: 2.4.10
   The default chunk size changed from 256k to 255k.

.. index:: GridFS; initialize
.. _gridfs-implement:

Implement GridFS
----------------

To store and retrieve files using :term:`GridFS`, use either of the following:

- A MongoDB driver. See the :doc:`drivers</applications/drivers>`
  documentation for information on using GridFS with your driver.

- The :program:`mongofiles` command-line tool in the :program:`mongo`
  shell. See :doc:`/reference/program/mongofiles`.

GridFS Collections
------------------

:term:`GridFS` stores files in two collections:

- ``chunks`` stores the binary chunks. For details, see
  :ref:`gridfs-chunks-collection`.

- ``files`` stores the file's metadata. For details, see
  :ref:`gridfs-files-collection`.

GridFS places the collections in a common bucket by prefixing each
with the bucket name. By default, GridFS uses two collections with
names prefixed by ``fs`` bucket:

- ``fs.files``
- ``fs.chunks``

You can choose a different bucket name than ``fs``, and create
multiple buckets in a single database.

Each document in the ``chunks`` collection represents a distinct chunk
of a file as represented in the GridFS store. Each chunk is identified
by its unique :term:`ObjectId` stored in its ``_id`` field.

For descriptions of all fields in the ``chunks`` and ``files``
collections, see :doc:`/reference/gridfs`.

.. index:: GridFS; index
.. _gridfs-index:

GridFS Index
------------

:term:`GridFS` uses a :term:`unique <unique index>`, :term:`compound
<compound index>` index on the ``chunks`` collection for the
``files_id`` and ``n`` fields. The ``files_id`` field contains the
``_id`` of the chunk's "parent" document. The ``n`` field contains the
sequence number of the chunk. GridFS numbers all chunks, starting with
0. For descriptions of the documents and fields in the ``chunks``
collection, see :doc:`/reference/gridfs`.

The GridFS index allows efficient retrieval of chunks using the
``files_id`` and ``n`` values, as shown in the following example:

.. code-block:: javascript

   cursor = db.fs.chunks.find({files_id: myFileID}).sort({n:1});

See the relevant :doc:`driver </applications/drivers>` documentation
for the specific behavior of your GridFS application. If your driver
does not create this index, issue the following operation using the
:program:`mongo` shell:

.. code-block:: javascript

   db.fs.chunks.ensureIndex( { files_id: 1, n: 1 }, { unique: true } );

Example Interface
-----------------

The following is an example of the GridFS interface in Java. The example
is for demonstration purposes only. For API specifics, see the relevant
:doc:`driver </applications/drivers>` documentation.

By default, the interface must support the default GridFS bucket, named
``fs``, as in the following:

.. code-block:: java

   // returns default GridFS bucket (i.e. "fs" collection)
   GridFS myFS = new GridFS(myDatabase);

   // saves the file to "fs" GridFS bucket
   myFS.createFile(new File("/tmp/largething.mpg"));

Optionally, interfaces may support other additional GridFS buckets as
in the following example:

.. code-block:: java

   // returns GridFS bucket named "contracts"
   GridFS myContracts = new GridFS(myDatabase, "contracts");

   // retrieve GridFS object "smithco"
   GridFSDBFile file = myContracts.findOne("smithco");

   // saves the GridFS file to the file system
   file.writeTo(new File("/tmp/smithco.pdf"));
==============================
Import and Export MongoDB Data
==============================

.. default-domain:: mongodb

.. TODO incorporate review/feedback from audit of page after 2.6 to do
   a bit of rewrite on this page.

This document provides an overview of the import and export programs
included in the MongoDB distribution. These tools are useful when you
want to backup or export a portion of your data without capturing the
state of the entire database, or for simple data ingestion cases. For
more complex data migration tasks, you may want to write your own
import and export scripts using a client :term:`driver` to interact
with the database itself. For disaster recovery protection and routine
database backup operation, use full :doc:`database instance backups
</core/backups>`.

.. warning::

   Because these tools primarily operate by interacting with a running
   :program:`mongod` instance, they can impact the performance of your
   running database.

   Not only do these processes create traffic for a running
   database instance, they also force the database to read all data
   through memory. When MongoDB reads infrequently used data, it can
   supplant more frequently accessed data, causing a deterioration in
   performance for the database's regular workload.

.. seealso:: :doc:`/core/backups` or :mms:`MMS Backup Manual </backup>`
   for more information on backing up MongoDB instances. Additionally,
   consider the following references for the MongoDB import/export
   tools:

   - :doc:`/reference/program/mongoimport`
   - :doc:`/reference/program/mongoexport`
   - :doc:`/reference/program/mongorestore`
   - :doc:`/reference/program/mongodump`

Data Import, Export, and Backup Operations
---------------------------------------------

For resilient and non-disruptive backups,
use a file system or block-level disk snapshot function, such as the
methods described in the :doc:`/core/backups` document. The
tools and operations discussed provide functionality that is useful in
the context of providing some kinds of backups.

In contrast, use import and export tools to backup a small subset of
your data or to move data to or from a third party system. These backups may
capture a small crucial set of data or a frequently modified section of
data for extra insurance, or for ease of access.

.. include:: /includes/warning-type-fidelity-loss.rst

No matter how you decide to import or export your data, consider the
following guidelines:

- Label files so that you can identify the contents of the export or
  backup as well as the point in time the export/backup reflect.

- Do not create or apply exports if the backup process itself will
  have an adverse effect on a production system.

- Make sure that they reflect a consistent data state. Export or backup
  processes can impact data integrity (i.e. type fidelity) and
  consistency if updates continue during the backup process.

- Test backups and exports by restoring and importing to ensure that
  the backups are useful.

Human Intelligible Import/Export Formats
----------------------------------------

This section describes a process to import/export a collection to a
file in a :term:`JSON` or :term:`CSV` format.

The examples in this section use the MongoDB tools
:doc:`/reference/program/mongoimport` and
:doc:`/reference/program/mongoexport`. These tools may also be useful
for importing data into a MongoDB database from third party
applications.

If you want to simply copy a database or collection from one instance
to another, consider using the :dbcommand:`copydb`, :dbcommand:`clone`,
or :dbcommand:`cloneCollection` commands, which may be more suited to
this task. The :program:`mongo` shell provides the
:method:`db.copyDatabase()` method.

Collection Export with :program:`mongoexport`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/warning-type-fidelity-loss.rst

With the :program:`mongoexport` utility you can create a backup
file. In the most simple invocation, the command takes the following
form:

.. code-block:: sh

   mongoexport --collection collection --out collection.json

This will export all documents in the collection named
``collection`` into the file ``collection.json``. Without the
output specification (i.e. ":option:`--out collection.json
<mongoexport --out>`"), :program:`mongoexport` writes output to
standard output (i.e. "stdout"). You can further narrow the results by
supplying a query filter using the ":option:`--query <mongoexport --query>`"
and limit results to a single database using the
":option:`--db <mongoexport --db>`" option. For instance:

.. code-block:: sh

   mongoexport --db sales --collection contacts --query '{"field": 1}'

This command returns all documents in the ``sales`` database's
``contacts`` collection, with a field named ``field`` with a value
of ``1``. Enclose the query in single quotes (e.g. ``'``) to ensure
that it does not interact with your shell environment. The resulting
documents will return on standard output.

By default, :program:`mongoexport` returns one :term:`JSON document`
per MongoDB document. Specify the
":option:`--jsonArray <mongoexport --jsonArray>`" argument to return
the export as a single :term:`JSON` array. Use the ":option:`--csv
<mongoexport --csv>`" file to return the result in CSV (comma
separated values) format.

If your :program:`mongod` instance is not running, you can use the
":option:`--dbpath <mongoexport --dbpath>`" option to specify the
location to your MongoDB instance's database files. See the following
example:

.. code-block:: sh

   mongoexport --db sales --collection contacts --dbpath /srv/MongoDB/

This reads the data files directly. This locks the data directory to
prevent conflicting writes. The :program:`mongod` process must *not* be
running or attached to these data files when you run :program:`mongoexport`
in this configuration.

The ":option:`--host <mongoexport --host>`" and ":option:`--port
<mongoexport --port>`" options allow you to specify a non-local host
to connect to capture the export. Consider the following example:

.. code-block:: sh

   mongoexport --host mongodb1.example.net --port 37017 --username user --password pass --collection contacts --out mdb1-examplenet.json

On any :program:`mongoexport` command you may, as above specify username and
password credentials as above.

Collection Import with :program:`mongoimport`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/warning-type-fidelity-loss.rst

To restore a backup taken with :program:`mongoexport`. Most of the
arguments to :program:`mongoexport` also exist for
:program:`mongoimport`. Consider the following command:

.. code-block:: sh

   mongoimport --collection collection --file collection.json

This imports the contents of the file ``collection.json`` into the
collection named ``collection``. If you do not specify a file with
the ":option:`--file <mongoimport --file>`" option,
:program:`mongoimport` accepts input over standard input
(e.g. "stdin.")

If you specify the ":option:`--upsert <mongoimport --upsert>`" option,
all of :program:`mongoimport` operations will attempt to update
existing documents in the database and insert other documents. This
option will cause some performance impact depending on your
configuration.

You can specify the database option :option:`--db <mongoimport --db>`
to import these documents to a particular database. If your
MongoDB instance is not running, use the ":option:`--dbpath
<mongoimport --dbpath>`" option to specify the location of your
MongoDB instance's database files. Consider using the
":option:`--journal <mongoimport --journal>`" option to ensure that
:program:`mongoimport` records its operations in the journal. The
``mongod`` process must *not* be running or attached to these data
files when you run :program:`mongoimport` in this configuration.

Use the ":option:`--ignoreBlanks <mongoimport --ignoreBlanks>`" option
to ignore blank fields. For :term:`CSV` and :term:`TSV` imports, this
option provides the desired functionality in most cases: it avoids
inserting blank fields in MongoDB documents.
.. index:: index; compound
.. index:: compound index
.. _index-type-compound:

================
Compound Indexes
================

.. default-domain:: mongodb

MongoDB supports *compound indexes*, where a single index structure
holds references to multiple fields [#compound-index-field-limit]_
within a collection's documents. The following diagram illustrates an
example of a compound index on two fields:

.. include:: /images/index-compound-key.rst

.. [#compound-index-field-limit]
   MongoDB imposes a :limit:`limit of 31 fields for any compound index
   <Number of Indexed Fields in a Compound Index>`.

Compound indexes can support queries that match on multiple fields.

.. example::

   Consider a collection named
   ``products`` that holds documents that resemble the following
   document:

   .. code-block:: javascript

      {
       "_id": ObjectId(...),
       "item": "Banana",
       "category": ["food", "produce", "grocery"],
       "location": "4th Street Store",
       "stock": 4,
       "type": "cases",
       "arrival": Date(...)
      }

   If applications query on the ``item`` field as well as query on both
   the ``item`` field and the ``stock`` field, you can specify a single
   compound index to support both of these queries:

   .. code-block:: javascript

      db.products.ensureIndex( { "item": 1, "stock": 1 } )

.. important:: You may not create compound indexes that have
   ``hashed`` index fields. You will receive an error if you attempt to
   create a compound index that includes :doc:`a hashed index
   </core/index-hashed>`.

The order of the fields in a compound index is very important. In the
previous example, the index will contain references to documents sorted
first by the values of the ``item`` field and, within each value of the
``item`` field, sorted by values of the ``stock`` field. See
:ref:`index-ascending-and-descending` for more information.

In addition to supporting queries that match on all the index fields,
compound indexes can support queries that match on the prefix of the
index fields. For details, see :ref:`compound-index-prefix`.

.. index:: index; sort order
.. _index-ascending-and-descending:

Sort Order
----------

Indexes store references to fields in either ascending (``1``) or
descending (``-1``) sort order. For single-field indexes, the sort
order of keys doesn't matter because MongoDB can traverse the index in
either direction. However, for :ref:`compound indexes
<index-type-compound>`, sort order can matter in determining whether
the index can support a sort operation.

Consider a collection ``events`` that contains documents with the
fields ``username`` and ``date``. Applications can issue queries that
return results sorted first by ascending ``username`` values and then
by descending (i.e. more recent to last) ``date`` values, such as:

.. code-block:: javascript

   db.events.find().sort( { username: 1, date: -1 } )

or queries that return results sorted first by descending ``username``
values and then by ascending ``date`` values, such as:

.. code-block:: javascript

   db.events.find().sort( { username: -1, date: 1 } )

The following index can support both these sort operations:

.. code-block:: javascript

   db.events.ensureIndex( { "username" : 1, "date" : -1 } )

However, the above index **cannot** support sorting by ascending
``username`` values and then by ascending ``date`` values, such as the
following:

.. code-block:: javascript

   db.events.find().sort( { username: 1, date: 1 } )

.. _compound-index-prefix:

Prefixes
--------

Compound indexes support queries on any prefix of the index
fields. Index prefixes are the beginning subset of indexed fields. For
example, given the index ``{ a: 1, b: 1, c: 1 }``, both ``{ a: 1 }``
and ``{ a: 1, b: 1 }`` are prefixes of the index.

If you have a collection that has a compound index on ``{ a: 1, b:
1 }``, as well as an index that consists of the prefix of that index,
i.e. ``{ a: 1 }``, assuming none of the index has a sparse or unique
constraints, then you can drop the ``{ a: 1 }`` index. MongoDB will be
able to use the compound index in all of situations that it would have
used the ``{ a: 1 }`` index.

For example, given the following index:

.. code-block:: javascript

   { "item": 1, "location": 1, "stock": 1 }

MongoDB **can** use this index to support queries that include:

- the ``item`` field,
- the ``item`` field *and* the ``location`` field,
- the ``item`` field *and* the ``location`` field *and* the
  ``stock`` field, or
- only the ``item`` *and* ``stock`` fields; however, this index
  would be less efficient than an index on only ``item`` and
  ``stock``.

MongoDB **cannot** use this index to support queries that include:

- only the ``location`` field,
- only the ``stock`` field, or
- only the ``location`` *and* ``stock`` fields.

Index Intersection
------------------

.. include:: /includes/fact-index-intersection-vs-compound-indexes.rst
.. index:: index; options
.. _index-creation-operations:
.. _index-operations:

==============
Index Creation
==============

.. default-domain:: mongodb

MongoDB provides several options that *only* affect the creation of
the index. Specify these options in a document as the second argument
to the :method:`db.collection.ensureIndex()` method. This section
describes the uses of these creation options and their behavior.

.. related::

   Some options that you can specify to
   :method:`~db.collection.ensureIndex()` options control the
   :doc:`properties of the index </core/index-properties>`, which are
   *not* index creation options. For example, the :doc:`unique
   </core/index-unique>` option affects the behavior of the index
   after creation.

   For a detailed description of MongoDB's index types, see
   :doc:`/core/index-types` and :doc:`/core/index-properties` for
   related documentation.

.. index:: index; background creation
.. _index-creation-background:

Background Construction
-----------------------

By default, creating an index blocks all other operations on a
database. When building an index on a collection, the database that
holds the collection is unavailable for read or write operations until
the index build completes. Any operation that requires a read or write
lock on all databases (e.g. :command:`listDatabases`) will wait for the
foreground index build to complete.

For potentially long running index building operations, consider the
``background`` operation so that the MongoDB database remains available
during the index building operation. For example, to create an index in
the background of the ``zipcode`` field of the ``people`` collection,
issue the following:

.. code-block:: javascript

   db.people.ensureIndex( { zipcode: 1}, {background: true} )

By default, ``background`` is ``false`` for building MongoDB indexes.

You can combine the background option with other options, as in the
following:

.. code-block:: javascript

   db.people.ensureIndex( { zipcode: 1}, {background: true, sparse: true } )

Behavior
~~~~~~~~

As of MongoDB version 2.4, a :program:`mongod` instance can build more
than one index in the background concurrently.

.. versionchanged:: 2.4
   Before 2.4, a :program:`mongod` instance could only build one
   background index per database at a time.

.. versionchanged:: 2.2
   Before 2.2, a single :program:`mongod` instance could only build
   one index at a time.

Background indexing operations run in the background so that other database
operations can run while creating the index. However, the :program:`mongo`
shell session or connection where you are creating
the index *will* block until the index build is complete. To continue
issuing commands to the database, open another
connection or :program:`mongo` instance.

Queries will not use partially-built indexes: the index will only be
usable once the index build is complete.

.. note::

   If MongoDB is building an index in the background, you cannot
   perform other administrative operations involving that collection,
   including running :dbcommand:`repairDatabase`, dropping the
   collection (i.e. :method:`db.collection.drop()`), and running
   :dbcommand:`compact`. These operations will return an error during
   background index builds.

Performance
~~~~~~~~~~~

The background index operation uses an incremental approach that is
slower than the normal "foreground" index builds. If the index is
larger than the available RAM, then the incremental process can take
*much* longer than the foreground build.

If your application
includes :method:`ensureIndex() <db.collection.ensureIndex()>`
operations, and an index *doesn't* exist for other operational
concerns, building the index can have a severe impact on the
performance of the database.

To avoid performance issues, make sure that your application checks
for the indexes at start up using the
:method:`~db.collection.getIndexes()` method or the :api:`equivalent
method for your driver <>` and terminates if the proper indexes do not
exist. Always build indexes in production instances using separate
application code, during designated maintenance windows.

.. _index-creation-building-indexes-on-secondaries:

Building Indexes on Secondaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.6
   Secondary members can now build indexes in the
   background. Previously all index builds on secondaries were in the
   foreground.

Background index operations on a :term:`replica set`
:term:`secondaries <secondary>` begin after the :term:`primary`
completes building the index. If MongoDB builds an index in the
background on the primary, the secondaries will then build that index
in the background.

To build large indexes on secondaries the best approach is to
restart one secondary at a time in :term:`standalone` mode and build
the index. After building the index, restart as a member of the
replica set, allow it to catch up with the other members of the set,
and then build the index on the next secondary. When all the
secondaries have the new index, step down the primary, restart it as a
standalone, and build the index on the former primary.

The amount of time required to build the index on a secondary must be
within the window of the :term:`oplog`, so that the secondary can
catch up with the primary.

Indexes on secondary members in "recovering" mode are always built in
the foreground to allow them to catch up as soon as possible.

See :ref:`index-building-replica-sets` for a complete procedure for
building indexes on secondaries.

.. index:: index; duplicates
.. index:: index; drop duplicates
.. _index-creation-duplicate-dropping:

Drop Duplicates
---------------

MongoDB cannot create a :ref:`unique index <index-type-unique>` on a
field that has duplicate values. To force the creation of a unique
index, you can specify the ``dropDups`` option, which will only index
the first occurrence of a value for the key, and delete all subsequent
values.

.. important::

   As in all unique indexes, if a document does not have the indexed
   field, MongoDB will include it in the index with a "null" value.

   If subsequent fields *do not* have the indexed field, and you have
   set ``{dropDups: true}``, MongoDB will remove these documents from
   the collection when creating the index. If you combine ``dropDups``
   with the :ref:`sparse <index-type-sparse>` option, this index will
   only include documents in the index that have the value, and the
   documents without the field will remain in the database.

To create a unique index that drops duplicates on the ``username``
field of the ``accounts`` collection, use a command in the following form:

.. code-block:: javascript

   db.accounts.ensureIndex( { username: 1 }, { unique: true, dropDups: true } )

.. warning::

   Specifying ``{ dropDups: true }`` will delete data from your
   database. Use with extreme caution.

By default, ``dropDups`` is ``false``.

.. index:: index; name
.. _index-names:

Index Names
-----------

.. default-domain:: mongodb

The default name for an index is the concatenation of the indexed keys
and each key's direction in the index, 1 or -1.

.. example:: Issue the following command to create an index on ``item``
   and ``quantity``:

   .. code-block:: javascript

      db.products.ensureIndex( { item: 1, quantity: -1 } )

   The resulting index is named: ``item_1_quantity_-1``.

Optionally, you can specify a name for an index instead of using the
default name.

.. example:: Issue the following command to create an index on ``item``
   and ``quantity`` and specify ``inventory`` as the index name:

   .. code-block:: javascript

      db.products.ensureIndex( { item: 1, quantity: -1 } , { name: "inventory" } )

   The resulting index has the name ``inventory``.

To view the name of an index, use the :method:`getIndexes()
<db.collection.getIndexes()>` method.
.. index:: index; hashed
.. _index-type-hashed:

============
Hashed Index
============

.. default-domain:: mongodb

.. versionadded:: 2.4

Hashed indexes maintain entries with hashes of the values of the
indexed field. The hashing function collapses sub-documents and
computes the hash for the entire value but does not support multi-key
(i.e. arrays) indexes.

Hashed indexes support :doc:`sharding </core/sharding-introduction>` a
collection using a :ref:`hashed shard key <sharding-hashed-sharding>`.
Using a hashed shard key to shard a collection ensures a more even
distribution of data. See
:doc:`/tutorial/shard-collection-with-a-hashed-shard-key` for more
details.

MongoDB can use the ``hashed`` index to support equality queries, but
``hashed`` indexes do not support range queries.

You may not create compound indexes that have ``hashed`` index fields
or specify a unique constraint on a ``hashed`` index; however, you can
create both a ``hashed`` index and an ascending/descending (i.e.
non-hashed) index on the same field: MongoDB will use the scalar index
for range queries.

.. _hashed-index-warning:

.. include:: /includes/warning-hashed-index-floating-point.rst

Create a ``hashed`` index using an operation that resembles the
following:

.. code-block:: javascript

   db.active.ensureIndex( { a: "hashed" } )

This operation creates a hashed index for the ``active`` collection on
the ``a`` field.

.. COMMENTED OUT the footnote.. Used to belong to a point item that was
   removed 6 months ago.
   .. [#hash-size] The hash stored in the ``hashed`` index is 64 bits of the
      128 bit ``md5`` hash.
==================
Index Intersection
==================

.. default-domain:: mongodb

.. versionadded:: 2.6

MongoDB can use the intersection of multiple indexes to fulfill
queries. [#previous-versions]_ In general, each index intersection
involves two indexes; however, MongoDB can employ multiple/nested index
intersections to resolve a query.

To illustrate index intersection, consider a collection ``orders`` that
has the following indexes:

.. code-block:: javascript

   { qty: 1 }
   { item: 1 }

MongoDB can use the intersection of the two indexes to support
the following query:

.. code-block:: javascript

   db.orders.find( { item: "abc123", qty: { $gt: 15 } } )

For query plans that use index intersection, the
:method:`~cursor.explain()` returns the value ``Complex Plan`` in the
``cursor`` field.

.. [#previous-versions] In previous versions, MongoDB could use only a
   single index to fulfill most queries. The exception to this is
   queries with :query:`$or` clauses, which could use a single index
   for each :query:`$or` clause.

Index Prefix Intersection
~~~~~~~~~~~~~~~~~~~~~~~~~

With index intersection, MongoDB can use an intersection of either the
entire index or the index prefix. An index prefix is a subset of a
compound index, consisting of one or more keys starting from the
beginning of the index.

Consider a collection ``orders`` with the following indexes:

.. code-block:: javascript

   { qty: 1 }
   { status: 1, ord_date: -1 }

To fulfill the following query which specifies a condition on both the
``qty`` field and the ``status`` field, MongoDB can use the
intersection of the two indexes:

.. code-block:: javascript

   db.orders.find( { qty: { $gt: 10 } , status: "A" } )

.. _index-intersection-compound-indexes:

Index Intersection and Compound Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Index intersection does not eliminate the need for creating
:doc:`compound indexes </core/index-compound>`. However, because both
the list order (i.e. the order in which the keys are listed in the
index) and the sort order (i.e. ascending or descending), matter in
:doc:`compound indexes </core/index-compound>`, a compound index may
not support a query condition that does not include the :ref:`index
prefix keys <compound-index-prefix>` or that specifies a different sort
order.

For example, if a collection ``orders`` has the following compound
index, with the ``status`` field listed before the ``ord_date`` field:

.. code-block:: javascript

   { status: 1, ord_date: -1 }

The compound index can support the following queries:

.. code-block:: javascript

   db.orders.find( { status: { $in: ["A", "P" ] } } )
   db.orders.find(
      {
        ord_date: { $gt: new Date("2014-02-01") },
        status: {$in:[ "P", "A" ] }
      }
   )

But not the following two queries:

.. code-block:: javascript

   db.orders.find( { ord_date: { $gt: new Date("2014-02-01") } } )
   db.orders.find( { } ).sort( { ord_date: 1 } )

However, if the collection has two separate indexes:

.. code-block:: javascript

   { status: 1 }
   { ord_date: -1 }

The two indexes can, either individually or through index intersection,
support all four aforementioned queries.

The choice between creating compound indexes that support your queries
or relying on index intersection depends on the specifics of your
system.

.. seealso:: :doc:`compound indexes </core/index-compound>`,
   :ref:`compound-key-indexes`

Index Intersection and Sort
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Index intersection does not apply when the :method:`~cursor.sort()`
operation requires an index completely separate from the query
predicate.

For example, the ``orders`` collection has the following indexes:

.. code-block:: javascript

   { qty: 1 }
   { status: 1, ord_date: -1 }
   { status: 1 }
   { ord_date: -1 }

MongoDB cannot use index intersection for the following query with sort:

.. code-block:: javascript

   db.orders.find( { qty: { $gt: 10 } } ).sort( { status: 1 } )

That is, MongoDB does not use the ``{ qty: 1 }`` index for the query,
and the separate ``{ status: 1 }`` or the ``{ status: 1, ord_date: -1
}`` index for the sort.

However, MongoDB can use index intersection for the following query
with sort since the index ``{ status: 1, ord_date: -1 }`` can fulfill
part of the query predicate.

.. code-block:: javascript

   db.orders.find( { qty: { $gt: 10 } , status: "A" } ).sort( { ord_date: -1 } )
.. index:: index; multikey
.. _index-type-multi-key:
.. _index-type-multikey:

================
Multikey Indexes
================

.. default-domain:: mongodb

To index a field that holds an array value, MongoDB adds index items
for each item in the array. These *multikey* indexes allow MongoDB to
return documents from queries using the value of an array. MongoDB
automatically determines whether to create a multikey index if the
indexed field contains an array value; you do not need to explicitly
specify the multikey type.

Consider the following illustration of a multikey index:

.. include:: /images/index-multikey.rst

.. TODO in the following paragraph, is 'however' the correct word for here?
   and while we're at it, should we clarify the first half of the sentence
   as well?

Multikey indexes support all operations supported by other MongoDB
indexes; however, applications may use multikey indexes to select
documents based on ranges of values for the value of an array. Multikey
indexes support arrays that hold both values (e.g. strings, numbers)
*and* nested documents.

Limitations
-----------

Interactions between Compound and Multikey Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

While you can create multikey :ref:`compound indexes
<index-type-compound>`, at most one field in a compound index may hold
an array. For example, given an index on ``{ a: 1, b: 1 }``, the
following documents are permissible:

.. code-block:: javascript

   {a: [1, 2], b: 1}

   {a: 1, b: [1, 2]}

However, the following document is impermissible, and MongoDB
cannot insert such a document into a collection with the ``{a: 1,
b: 1 }`` index:

.. code-block:: javascript

   {a: [1, 2], b: [1, 2]}

If you attempt to insert a such a document, MongoDB will reject the
insertion, and produce an error that says ``cannot index parallel
arrays``. MongoDB does not index parallel arrays because they
require the index to include each value in the Cartesian product of
the compound keys, which could quickly result in incredibly large
and difficult to maintain indexes.

Shard Keys
~~~~~~~~~~

.. important:: The index of a shard key **cannot** be a multi-key index.

Hashed Indexes
~~~~~~~~~~~~~~

``hashed`` indexes are not compatible with multi-key indexes.

To compute the hash for a ``hashed`` index, MongoDB collapses
sub-documents and computes the hash for the entire value. For fields
that hold arrays or sub-documents, you cannot use the index to support
queries that introspect the sub-document.

Examples
--------

Index Basic Arrays
~~~~~~~~~~~~~~~~~~

Given the following document:

.. code-block:: javascript

   {
     "_id" : ObjectId("..."),
     "name" : "Warm Weather",
     "author" : "Steve",
     "tags" : [ "weather", "hot", "record", "april" ]
   }

Then an index on the ``tags`` field, ``{ tags: 1 }``, would be a
multikey index and would include these four separate entries for that
document:

- ``"weather"``,

- ``"hot"``,

- ``"record"``, and

- ``"april"``.

Queries could use the multikey index to return queries for any of
the above values.

.. TODO the following example should be revamped a bit. It's a strange example.

Index Arrays with Embedded Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can create multikey indexes on fields in objects embedded in
arrays, as in the following example:

Consider a ``feedback`` collection with documents in the following
form:

.. code-block:: javascript

   {
    "_id": ObjectId(...),
    "title": "Grocery Quality",
    "comments": [
       { author_id: ObjectId(...),
         date: Date(...),
         text: "Please expand the cheddar selection." },
       { author_id: ObjectId(...),
         date: Date(...),
         text: "Please expand the mustard selection." },
       { author_id: ObjectId(...),
         date: Date(...),
         text: "Please expand the olive selection." }
    ]
   }

An index on the ``comments.text`` field would be a multikey index
and would add items to the index for all embedded documents in
the array.

With the index ``{ "comments.text": 1 }`` on the ``feedback`` collection,
consider the following query:

.. code-block:: javascript

   db.feedback.find( { "comments.text": "Please expand the olive selection." } )

The query would select the documents in the collection that contain
the following embedded document in the ``comments`` array:

.. code-block:: javascript

   { author_id: ObjectId(...),
     date: Date(...),
     text: "Please expand the olive selection." }
================
Index Properties
================

.. default-domain:: mongodb

In addition to the numerous :doc:`index types </core/index-types>`
MongoDB supports, indexes can also have various properties. The
following documents detail the index properties that you can select
when building an index.

.. include:: /includes/toc/dfn-list-indexes-concepts-properties.rst

.. include:: /includes/toc/indexes-concepts-properties.rst
====================
Single Field Indexes
====================

.. default-domain:: mongodb

MongoDB provides complete support for indexes on any field in a
:term:`collection` of :term:`documents <document>`. By default, all collections
have an index on the :ref:`_id field <index-type-id>`, and
applications and users may add additional indexes to support important
queries and operations.

MongoDB supports indexes that contain either a single field *or*
multiple fields depending on the operations that this index-type supports. This
document describes indexes that contain a single field. Consider the
following illustration of a single field index.

.. include:: /images/index-ascending.rst

.. seealso:: :doc:`/core/index-compound` for information about indexes
   that include multiple fields, and :doc:`/core/indexes-introduction`
   for a higher level introduction to indexing in MongoDB.

Example
-------

Given the following document in the ``friends`` collection:

.. code-block:: javascript

   { "_id" : ObjectId(...),
     "name" : "Alice"
     "age" : 27
   }

The following command creates an index on the ``name`` field:

.. code-block:: sh

   db.friends.ensureIndex( { "name" : 1 } )

Cases
-----

.. index:: _id index
.. index:: _id
.. index:: index; _id
.. index:: index types; primary key
.. _index-type-id:

``_id`` Field Index
~~~~~~~~~~~~~~~~~~~

MongoDB creates the ``_id`` index, which is an ascending :ref:`unique
index <index-type-unique>` on the ``_id`` field, for all collections when
the collection is created. You cannot remove the index on the ``_id``
field.

Think of the ``_id`` field as the :term:`primary key` for a collection.
Every document *must* have a unique ``_id`` field. You may store any
unique value in the ``_id`` field. The default value of ``_id`` is an
:term:`ObjectId` which is generated when the client inserts the document. An
:term:`ObjectId` is a 12-byte unique identifier suitable for use
as the value of an ``_id`` field.

.. note::

   In :term:`sharded clusters <sharded cluster>`, if you do *not* use
   the ``_id`` field as the :term:`shard key`, then your application
   **must** ensure the uniqueness of the values in the ``_id`` field
   to prevent errors.  This is most-often done by using a standard
   auto-generated :term:`ObjectId`.

   Before version 2.2, :term:`capped collections <capped collection>`
   did not have an ``_id`` field. In version 2.2 and newer, capped
   collections do have an ``_id`` field, except those in the ``local``
   :term:`database`. See :ref:`Capped Collections Recommendations
   and Restrictions <capped-collections-recommendations-and-restrictions>`
   for more information.

.. index:: index; embedded fields
.. _index-embedded-fields:

Indexes on Embedded Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. default-domain:: mongodb

You can create indexes on fields embedded in sub-documents, just as you
can index top-level fields in documents. Indexes on embedded fields
differ from :ref:`indexes on sub-documents <index-sub-documents>`,
which include the full content up to the maximum :limit:`index size
<Index Key>` of the sub-document in the index. Instead, indexes on
embedded fields allow you to use a "dot notation," to introspect into
sub-documents.

Consider a collection named ``people`` that holds documents that resemble
the following example document:

.. code-block:: javascript

   {"_id": ObjectId(...)
    "name": "John Doe"
    "address": {
           "street": "Main",
           "zipcode": "53511",
           "state": "WI"
           }
   }

You can create an index on the ``address.zipcode`` field, using the
following specification:

.. code-block:: javascript

   db.people.ensureIndex( { "address.zipcode": 1 } )

.. index:: index; subdocuments
.. _index-subdocuments:
.. _index-sub-documents:
.. _index-subdocument:
.. _index-sub-document:

Indexes on Subdocuments
~~~~~~~~~~~~~~~~~~~~~~~~

.. default-domain:: mongodb

You can also create indexes on subdocuments.

For example, the ``factories`` collection contains documents that
contain a ``metro`` field, such as:

.. code-block:: javascript

   {
     _id: ObjectId(...),
     metro: {
              city: "New York",
              state: "NY"
            },
     name: "Giant Factory"
   }

The ``metro`` field is a subdocument, containing the embedded fields
``city`` and ``state``. The following command creates an index on the ``metro``
field as a whole:

.. code-block:: javascript

   db.factories.ensureIndex( { metro: 1 } )

The following query can use the index on the ``metro`` field:

.. code-block:: javascript

   db.factories.find( { metro: { city: "New York", state: "NY" } } )

This query returns the above document. When performing equality matches
on subdocuments, field order matters and the subdocuments must match
exactly. For example, the following query does not match the above
document:

.. code-block:: javascript

   db.factories.find( { metro: { state: "NY", city: "New York" } } )

See :ref:`query-subdocuments` for more information regarding querying
on subdocuments.
.. index:: index; sparse
.. _index-type-sparse:

==============
Sparse Indexes
==============

.. default-domain:: mongodb

Sparse indexes only contain entries for documents that have the indexed
field, even if the index field contains a null value. The index skips
over any document that is missing the indexed field. The index is
"sparse" because it does not include all documents of a collection. By
contrast, non-sparse indexes contain all documents in a collection,
storing null values for those documents that do not contain the indexed
field.

The following example in the :program:`mongo` shell creates a sparse
index on the ``xmpp_id`` field of the ``addresses`` collection:

.. code-block:: javascript

   db.addresses.ensureIndex( { "xmpp_id": 1 }, { sparse: true } )

By default, ``sparse`` is ``false`` on MongoDB indexes.

.. versionchanged:: 2.6

   If a sparse index results in an incomplete result set for queries
   and sort operations, MongoDB will not use that index unless a
   :method:`~cursor.hint()` explicitly specifies the index. For
   example, the query ``{ x: { $exists: false } }`` will not use a
   sparse index on the ``x`` field unless explicitly hinted. See
   :ref:`sparse-index-incomplete-results` for an example that details
   the behavior.

   For :ref:`2dsphere indexes (version 2) <2dsphere-v2>`, MongoDB
   ignores the ``sparse`` flag.

.. note::

   Do not confuse sparse indexes in MongoDB with `block-level`_
   indexes in other databases. Think of them as dense indexes with a
   specific filter.

   .. _`block-level`: http://en.wikipedia.org/wiki/Database_index#Sparse_index

.. tip::

   You can specify a *sparse* and :ref:`unique index
   <index-type-unique>`, that rejects documents that have duplicate
   values for a field, but allows multiple documents that omit that
   key.

Examples
--------

Create a Sparse Index On A Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``scores`` that contains the following documents:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e32fb408eea0eec2647"), "userid" : "newbie" }
   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }
   { "_id" : ObjectId("523b6e6ffb408eea0eec2649"), "userid" : "nina", "score" : 90 }

The collection has a sparse index on the field ``score``:

.. code-block:: javascript

    db.scores.ensureIndex( { score: 1 } , { sparse: true } )

Then, the following query on the ``scores`` collection uses the sparse
index to return the documents that have the ``score`` field less than
(:query:`$lt`) ``90``:

.. code-block:: javascript

   db.scores.find( { score: { $lt: 90 } } )

Because the document for the userid ``"newbie"`` does not contain the
``score`` field and thus does not meet the query criteria, the query
can use the sparse index to return the results:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }

.. _sparse-index-incomplete-results:

Sparse Index On A Collection Cannot Return Complete Results
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``scores`` that contains the following documents:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e32fb408eea0eec2647"), "userid" : "newbie" }
   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }
   { "_id" : ObjectId("523b6e6ffb408eea0eec2649"), "userid" : "nina", "score" : 90 }

The collection has a sparse index on the field ``score``:

.. code-block:: javascript

    db.scores.ensureIndex( { score: 1 } , { sparse: true } )

Because the document for the userid ``"newbie"`` does not contain the
``score`` field, the sparse index does not contain an entry for that
document.

Consider the following query to return **all** documents in the ``scores``
collection, sorted by the ``score`` field:

.. code-block:: javascript

   db.scores.find().sort( { score: -1 } )

Even though the sort is by the indexed field, MongoDB will **not**
select the sparse index to fulfill the query in order to return
complete results:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e6ffb408eea0eec2649"), "userid" : "nina", "score" : 90 }
   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }
   { "_id" : ObjectId("523b6e32fb408eea0eec2647"), "userid" : "newbie" }

To use the sparse index, explicitly specify the index with
:method:`~db.cursor.hint()`:

.. code-block:: javascript

   db.scores.find().sort( { score: -1 } ).hint( { score: 1 } )

The use of the index results in the return of only those documents with
the ``score`` field:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e6ffb408eea0eec2649"), "userid" : "nina", "score" : 90 }
   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }

.. seealso:: :method:`~cursor.explain()` and :doc:`/tutorial/analyze-query-plan`

Sparse Index with Unique Constraint
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``scores`` that contains the following documents:

.. code-block:: javascript

   { "_id" : ObjectId("523b6e32fb408eea0eec2647"), "userid" : "newbie" }
   { "_id" : ObjectId("523b6e61fb408eea0eec2648"), "userid" : "abby", "score" : 82 }
   { "_id" : ObjectId("523b6e6ffb408eea0eec2649"), "userid" : "nina", "score" : 90 }

You could create an index with a :ref:`unique constraint
<index-type-unique>` and sparse filter on the ``score`` field using
the following operation:

.. code-block:: javascript

    db.scores.ensureIndex( { score: 1 } , { sparse: true, unique: true } )

This index *would permit* the insertion of documents that had unique
values for the ``score`` field *or* did not include a ``score`` field.
Consider the following :doc:`insert operation
</tutorial/insert-documents>`:

.. code-block:: javascript

   db.scores.insert( { "userid": "AAAAAAA", "score": 43 } )
   db.scores.insert( { "userid": "BBBBBBB", "score": 34 } )
   db.scores.insert( { "userid": "CCCCCCC" } )
   db.scores.insert( { "userid": "DDDDDDD" } )

However, the index *would not permit* the addition of the following
documents since documents already exists with ``score`` value of ``82``
and ``90``:

.. code-block:: javascript

   db.scores.insert( { "userid": "AAAAAAA", "score": 82 } )
   db.scores.insert( { "userid": "BBBBBBB", "score": 90 } )
.. _index-feature-text:

============
Text Indexes
============

.. default-domain:: mongodb

.. versionadded:: 2.4

MongoDB provides ``text`` indexes to support text search of string
content in documents of a collection.

``text`` indexes can include any field whose value is a string or an
array of string elements. To perform queries that access the ``text``
index, use the :query:`$text` query operator.

.. versionchanged:: 2.6

   MongoDB enables the text search feature by
   default. In MongoDB 2.4, you need to enable the text search
   feature manually to create ``text`` indexes and perform :ref:`text search
   <index-text-text-search>`.

.. _create-text-index:

Create Text Index
-----------------

To create a ``text`` index, use the
:method:`db.collection.ensureIndex()` method. To index a field that
contains a string or an array of string elements, include the field and
specify the string literal ``"text"`` in the index document, as in the
following example:

.. code-block:: javascript

   db.reviews.ensureIndex( { comments: "text" } )

.. include:: /includes/fact-text-index-limit-one.rst

For examples of creating ``text`` indexes on multiple fields, see
:doc:`/tutorial/create-text-index-on-multiple-fields`.

Supported Languages and Stop Words
----------------------------------

MongoDB supports text search for various languages. ``text`` indexes
drop language-specific stop words (e.g. in English, “the,” “an,” “a,”
“and,” etc.) and uses simple language-specific suffix stemming. For a
list of the supported languages, see :ref:`text-search-languages`.

If the index language is English, ``text`` indexes are case-insensitive
for non-diacritics; i.e. case insensitive for ``[A-z]``.

To specify a language for the ``text`` index, see
:doc:`/tutorial/specify-language-for-text-index`

Restrictions
------------

Text Search and Hints
~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-hint-text-query-restriction.rst

Compound Index
~~~~~~~~~~~~~~

A :doc:`compound index </core/index-compound>` can include a ``text``
index key in combination with ascending/descending index keys. However,
these compound indexes have the following restrictions:

.. include:: /includes/fact-compound-index-with-text-restrictions.rst

See :doc:`/tutorial/limit-number-of-items-scanned-for-text-search`.

.. _text-index-storage-requirements:

Storage Requirements and Performance Costs
------------------------------------------

``text`` indexes have the following storage requirements and
performance costs:

- ``text`` indexes change the space allocation method for all future
  record allocations in a collection to :collflag:`usePowerOf2Sizes`.

- ``text`` indexes can be large. They contain one index entry for each
  unique post-stemmed word in each indexed field for each document
  inserted.

- Building a ``text`` index is very similar to building a large
  multi-key index and will take longer than building a simple ordered
  (scalar) index on the same data.

- When building a large ``text`` index on an existing collection,
  ensure that you have a sufficiently high limit on open file
  descriptors. See the :doc:`recommended settings </reference/ulimit>`.

- ``text`` indexes will impact insertion throughput because MongoDB
  must add an index entry for each unique post-stemmed word in each
  indexed field of each new source document.

- Additionally, ``text`` indexes do not store phrases or information
  about the proximity of words in the documents. As a result, phrase
  queries will run much more effectively when the entire collection
  fits in RAM.

.. _index-text-text-search:

Text Search
-----------

Text search supports the search of string content in documents of a
collection. MongoDB provides the :query:`$text` operator to perform
text search in queries and in :doc:`aggregation pipelines
</tutorial/text-search-in-aggregation>`.

The text search process:

- tokenizes and stems the search term(s) during both the index creation
  and the text command execution.

- assigns a score to each document that contains the search term in the
  indexed fields. The score determines the relevance of a document to a
  given search query.

The :query:`$text` operator can search for words and phrases. The query
matches on the complete stemmed words. For example, if a document field
contains the word ``blueberry``, a search on the term ``blue`` will not
match the document. However, a search on either ``blueberry`` or
``blueberries`` will match.

For information and examples on various text search patterns, see the
:query:`$text` query operator. For examples of text search in
aggregation pipeline, see :doc:`/tutorial/text-search-in-aggregation`.
.. index:: index; TTL index
.. index:: TTL index
.. _index-feature-ttl:

===========
TTL Indexes
===========

.. default-domain:: mongodb

TTL indexes are special indexes that MongoDB can use to automatically
remove documents from a collection after a certain amount of
time. This is ideal for some types of information like machine
generated event data, logs, and session information that only need to
persist in a database for a limited amount of time.

Considerations
--------------

TTL indexes have the following limitations:

- :ref:`Compound indexes <index-type-compound>` are *not* supported.

- The indexed field **must** be a date :term:`type <bson types>`.

- If the field holds an array, and there are multiple date-typed data
  in the index, the document will expire when the *lowest*
  (i.e. earliest) matches the expiration threshold.

.. include:: /includes/fact-ttl-collection-background-timing.rst

In all other respects, TTL indexes are normal indexes,
and if appropriate, MongoDB can use these
indexes to fulfill arbitrary queries.

Additional Information
----------------------

:doc:`/tutorial/expire-data`
.. index:: index types
.. _index-types:

===========
Index Types
===========

.. default-domain:: mongodb

MongoDB provides a number of different index types. You can create
indexes on any field or embedded field within a document or
sub-document. You can create :doc:`single field indexes
</core/index-single>` or :doc:`compound indexes
</core/index-compound>`. MongoDB also supports indexes of arrays,
called :ref:`multi-key indexes <index-type-multi-key>`, as well as
supports :doc:`indexes on geospatial data
</applications/geospatial-indexes>`. For a list of the supported index
types, see :ref:`index-type-list`.

In general, you should create indexes that support your common and
user-facing queries. Having these indexes will ensure that MongoDB
scans the smallest possible number of documents.

In the :program:`mongo` shell, you can create an index by calling the
:method:`ensureIndex() <db.collection.ensureIndex()>` method. For
more detailed instructions about building indexes, see the
:doc:`Indexing Tutorials </administration/indexes>` page.

Behavior of Indexes
-------------------

All indexes in MongoDB are :term:`B-tree` indexes, which can
efficiently support equality matches and range queries. The index
stores items internally in order sorted by the value of the index
field. The ordering of index entries supports efficient range-based
operations and allows MongoDB to return sorted results using the order
of documents in the index.

Ordering of Indexes
~~~~~~~~~~~~~~~~~~~

MongoDB indexes may be ascending, (i.e. ``1``) or descending
(i.e. ``-1``) in their ordering. Nevertheless, MongoDB may also
traverse the index in either directions. As a result, for
single-field indexes, ascending and descending indexes are
interchangeable. This is not the case for compound indexes: in
compound indexes, the direction of the sort order can have a
greater impact on the results.

See :ref:`index-ascending-and-descending` for more information on the
impact of index order on results in compound indexes.

Index Intersection
~~~~~~~~~~~~~~~~~~

MongoDB can use the intersection of indexes to fulfill queries with
compound conditions. See :doc:`/core/index-intersection` for details.

Limits
~~~~~~

Certain restrictions apply to indexes, such as the length of the index
keys or the number of indexes per collection. See :ref:`Index
Limitations <index-limitations>` for details.

.. _index-type-list:

Index Type Documentation
------------------------

.. include:: /includes/toc/dfn-list-indexes-concepts-types.rst

.. include:: /includes/toc/indexes-concepts-types.rst
.. index:: index; unique
.. _index-type-unique:

==============
Unique Indexes
==============

.. default-domain:: mongodb

A unique index causes MongoDB to reject all documents that contain a
duplicate value for the indexed field. To create a unique index on the
``user_id`` field of the ``members`` collection, use the following
operation in the :program:`mongo` shell:

.. code-block:: javascript

   db.addresses.ensureIndex( { "user_id": 1 }, { unique: true } )

By default, ``unique`` is ``false`` on MongoDB indexes.

If you use the unique constraint on a :ref:`compound index
<index-type-compound>`, then MongoDB will enforce uniqueness on the
*combination* of values rather than the individual value for any or
all values of the key.

If a document does not have a value for the indexed field in a unique
index, the index will store a null value for this document. Because of
the unique constraint, MongoDB will only permit one document that lacks
the indexed field. If there is more than one document without a value
for the indexed field or is missing the indexed field, the index build
will fail with a duplicate key error.

You can combine the unique constraint with the :ref:`sparse index
<index-type-sparse>` to filter these null values from the unique index
and avoid the error.

You may not specify a unique constraint on a :ref:`hashed
index <index-type-hashed>`.
.. index:: index; overview
.. _index-overview-synopsis:

==================
Index Introduction
==================

.. default-domain:: mongodb

Indexes support the efficient execution of queries in MongoDB. Without
indexes MongoDB must scan every document in a collection to select
those documents that match the query statement. These *collection
scans* are inefficient because they require :program:`mongod` to
process a larger volume of data than an index for each operation.

Indexes are special data structures [#b-tree]_ that store a small
portion of the collection's data set in an easy to traverse form. The
index stores the value of a specific field or set of fields, ordered by
the value of the field.

Fundamentally, indexes in MongoDB are similar to indexes in other
database systems. MongoDB defines indexes at the :term:`collection`
level and supports indexes on any field or sub-field of the documents
in a MongoDB collection.

If an appropriate index exists for a query, MongoDB can use the index
to limit the number of documents it must inspect. In some cases,
MongoDB can use the data from the index to determine which documents
match a query. The following diagram illustrates a query that selects
documents using an index.

.. include:: /images/index-with-query.rst

Consider the documentation of the :ref:`query optimizer
<read-operations-query-optimization>` for more information on the
relationship between queries and indexes.

.. tip:: Create indexes to support common and user-facing
   queries. Having these indexes will ensure that MongoDB only scans
   the smallest possible number of documents.

Indexes can also optimize the performance of other operations in
specific situations:

**Sorted Results**

MongoDB can use indexes to return documents sorted by the index key
directly from the index without requiring an additional sort phase.

.. include:: /images/index-for-sort.rst

.. _covered-queries:

**Covered Results**

When the query criteria and the :term:`projection` of a query include
*only* the indexed fields, MongoDB will return results directly from
the index *without* scanning any documents or bringing documents into
memory. These covered queries can be *very* efficient. Indexes can also
cover :doc:`aggregation pipeline operations
</core/aggregation-pipeline>`.

.. include:: /images/index-for-covered-query.rst

.. [#b-tree] MongoDB indexes use a B-tree data structure.

Index Types
-----------

MongoDB provides an umber of different index types to support specific
types of data and queries.

Default ``_id``
~~~~~~~~~~~~~~~

All MongoDB collections have an index on the ``_id`` field that exists
by default. If applications do not specify a value for ``_id`` the
driver or the :program:`mongod` will create an ``_id`` field with an
:term:`ObjectId` value.

The ``_id`` index is *unique*, and prevents clients from inserting two
documents with the same value for the ``_id`` field.

Single Field
~~~~~~~~~~~~

In addition to the MongoDB-defined ``_id`` index, MongoDB supports
user-defined indexes on a :doc:`single field of a document
</core/index-single>`. Consider the following illustration of a
single-field index:

.. include:: /images/index-ascending.rst

Compound Index
~~~~~~~~~~~~~~

MongoDB *also* supports user-defined indexes on multiple fields. These
:doc:`compound indexes </core/index-compound>` behave like single-field
indexes; *however*, the query can select documents based on additional
fields. The order of fields listed in a compound index has
significance. For instance, if a compound index consists of ``{ userid:
1, score: -1 }``, the index sorts first by ``userid`` and then, within
each ``userid`` value, sort by ``score``. Consider the following
illustration of this compound index:

.. include:: /images/index-compound-key.rst

Multikey Index
~~~~~~~~~~~~~~

MongoDB uses :doc:`multikey indexes </core/index-multikey>` to index
the content stored in arrays. If you index a field that holds an array
value, MongoDB creates separate index entries for *every* element of
the array. These :doc:`multikey indexes </core/index-multikey>` allow
queries to select documents that contain arrays by matching on element
or elements of the arrays. MongoDB automatically determines whether to
create a multikey index if the indexed field contains an array value;
you do not need to explicitly specify the multikey type.

Consider the following illustration of a multikey index:

.. include:: /images/index-multikey.rst

Geospatial Index
~~~~~~~~~~~~~~~~

To support efficient queries of geospatial coordinate data, MongoDB
provides two special indexes: :doc:`2d indexes </core/2d>` that uses
planar geometry when returning results and :doc:`2sphere indexes
</core/2dsphere>` that use spherical geometry to return results.

See :doc:`/core/geospatial-indexes` for a high level introduction to
geospatial indexes.

Text Indexes
~~~~~~~~~~~~

MongoDB provides a *beta* ``text`` index type that supports searching
for string content in a collection. These text indexes do not store
language-specific *stop* words (e.g. "the", "a", "or") and *stem* the
words in a collection to only store root words.

See :doc:`/core/index-text` for more information on text indexes and
search.

Hashed Indexes
~~~~~~~~~~~~~~

To support :ref:`hash based sharding <sharding-hashed-sharding>`,
MongoDB provides a :doc:`hashed index </core/index-hashed>` type,
which indexes the hash of the value of a field. These indexes have a
more random distribution of values along their range, but *only*
support equality matches and cannot support range-based queries.

Index Properties
----------------

Unique Indexes
~~~~~~~~~~~~~~

The :doc:`unique </core/index-unique>` property for an index causes
MongoDB to reject duplicate values for the indexed field.  To create a
:doc:`unique index </core/index-unique>` on a field that already has
duplicate values, see :ref:`index-creation-duplicate-dropping` for
index creation options. Other than the unique constraint, unique
indexes are functionally interchangeable with other MongoDB
indexes.

Sparse Indexes
~~~~~~~~~~~~~~

The :doc:`sparse </core/index-sparse>` property of an index ensures
that the index only contain entries for documents that have the indexed
field. The index skips documents that *do not* have the indexed field.

You can combine the sparse index option with the unique index option
to reject documents that have duplicate values for a field but ignore
documents that do not have the indexed key.

Index Intersection
------------------

.. versionadded:: 2.6

MongoDB can use the :doc:`intersection of indexes
</core/index-intersection>` to fulfill queries. For queries that
specify compound query conditions, if one index can fulfill a part of a
query condition, and another index can fulfill another part of the
query condition, then MongoDB can use the intersection of the two
indexes to fulfill the query. Whether the use of a compound index or
the use of an index intersection is more efficient depends on the
particular query and the system.

For details on index intersection, see :doc:`/core/index-intersection`.
==============
Index Concepts
==============

.. default-domain:: mongodb

These documents describe and provide examples of the types,
configuration options, and behavior of indexes in MongoDB. For an over
view of indexing, see :doc:`Index Introduction
</core/indexes-introduction>`. For operational instructions, see
:doc:`Indexing Tutorials </administration/indexes>`. The :doc:`Indexing
Reference </reference/indexes>` documents the commands and operations
specific to index construction, maintenance, and querying in MongoDB,
including index types and creation options.

.. include:: /includes/toc/dfn-list-spec-indexes-concepts-landing.rst

.. include:: /includes/toc/indexes-concepts-landing.rst
:orphan:

=======================
Introduction to MongoDB
=======================

.. default-domain:: mongodb

Welcome to MongoDB. This document provides a brief introduction to MongoDB
and some key concepts. See the :doc:`installation guides </installation>` for
information on downloading and installing MongoDB.

.. This document provides a brief introduction to MongoDB and some key
   concepts and walks you through the process of installing and using
   MongoDB.

What is MongoDB
---------------

MongoDB is an open-source document database that provides high
performance, high availability, and automatic scaling.

Document Database
~~~~~~~~~~~~~~~~~

A record in MongoDB is a document, which is a data structure composed
of field and value pairs. MongoDB documents are similar to JSON
objects. The values of fields may include other documents, arrays,
and arrays of documents.

.. include:: /images/crud-annotated-document.rst

The advantages of using documents are:

- Documents (i.e. objects) correspond to native data types in
  many programming languages.

- Embedded documents and arrays reduce need for expensive joins.

- Dynamic schema supports fluent polymorphism.

Key Features
~~~~~~~~~~~~

High Performance
````````````````

MongoDB provides high performance data persistence. In particular,

- Support for embedded data models reduces I/O activity on database
  system.

- Indexes support faster queries and can include keys from embedded
  documents and arrays.

High Availability
`````````````````

To provide high availability, MongoDB's replication facility, called
replica sets, provide:

- *automatic* failover.

- data redundancy.

A :ref:`replica set <replication-introduction>` is a group of
MongoDB servers that maintain the same data set, providing redundancy
and increasing data availability.

Automatic Scaling
`````````````````

MongoDB provides horizontal scalability as part of its *core*
functionality.

- Automatic :ref:`sharding <sharding-introduction>` distributes
  data across a cluster of machines.

- Replica sets can provide eventually-consistent reads for low-latency
  high throughput deployments.
.. _journaling-internals:

====================
Journaling Mechanics
====================

.. default-domain:: mongodb

When running with journaling, MongoDB stores and applies :doc:`write
operations </core/write-operations>` in memory and in the on-disk journal
before the changes are present in the data files on disk. This document discusses the
implementation and mechanics of journaling in MongoDB systems.  See
:doc:`/tutorial/manage-journaling`
for information on configuring, tuning, and managing journaling.

.. _journaling-journal-files:

Journal Files
-------------

With journaling enabled, MongoDB creates a journal subdirectory within
the directory defined by :setting:`~storage.dbPath`, which is :file:`/data/db`
by default. The journal directory holds journal files, which contain
write-ahead redo logs. The directory also holds a last-sequence-number
file. A clean shutdown removes all the files in the journal directory.
A dirty shutdown (crash) leaves files in the journal directory; these are
used to automatically recover the database to a consistent state when
the mongod process is restarted.

Journal files are append-only files and have file names prefixed with
``j._``. When a journal file holds 1 gigabyte of data, MongoDB creates
a new journal file. Once MongoDB applies all the write operations in a
particular journal file to the database data files, it deletes the file, as
it is no longer needed for recovery purposes. Unless you
write *many* bytes of data per second, the journal directory should
contain only two or three journal files.

You can use the
:setting:`smallfiles` run time option when starting :program:`mongod`
to limit the size of each journal file to 128 megabytes, if you prefer.

To speed the frequent sequential writes that occur to the current
journal file, you can ensure that the journal directory is on a
different filesystem from the database data files.

.. important::

   If you place the journal on a different filesystem from your data
   files you *cannot* use a filesystem snapshot alone to capture valid
   backups of a :setting:`~storage.dbPath` directory. In this case, use
   :method:`~db.fsyncLock()` to ensure that database files are consistent
   before the snapshot and :method:`~db.fsyncUnlock()` once the snapshot
   is complete.

.. note::

   Depending on your filesystem, you might experience a preallocation
   lag the first time you start a :program:`mongod` instance with
   journaling enabled.

   MongoDB may preallocate journal files if the :program:`mongod`
   process determines that it is more efficient to preallocate journal
   files than create new journal files as needed. The amount of time
   required to pre-allocate lag might last several minutes, during
   which you will not be able to connect to the database. This is a
   one-time preallocation and does not occur with future invocations.

To avoid preallocation lag, see :ref:`journaling-avoid-preallocation-lag`.

.. _journaling-storage-views:

Storage Views used in Journaling
--------------------------------

Journaling adds three internal storage views to MongoDB.

The ``shared view`` stores modified data for upload to the MongoDB
data files. The ``shared view`` is the only view with direct access
to the MongoDB data files. When running with journaling, :program:`mongod`
asks the operating system to map your existing on-disk data files to the
``shared view`` virtual memory view. The operating system maps the files but
does not load them. MongoDB later loads data files into the ``shared view`` as
needed.

The ``private view`` stores data for use with :doc:`read operations
</core/read-operations>`.  The ``private view`` is the first place MongoDB applies new :doc:`write operations
</core/write-operations>`. Upon a journal commit, MongoDB copies the changes made
in the ``private view`` to the ``shared view``, where they are then available for
uploading to the database data files.

The journal is an on-disk view that stores new write operations
after MongoDB applies the operation to the ``private view`` but
before applying them to the data files. The journal provides durability.
If the :program:`mongod` instance were to crash without having applied
the writes to the data files, the journal could replay the writes to
the ``shared view`` for eventual upload to the data files.

.. _journaling-record-write-operation:

How Journaling Records Write Operations
---------------------------------------

MongoDB copies the write operations to the journal in batches called
group commits. These "group commits" help
minimize the performance impact of journaling, since a group commit must
block all writers during the commit.  See :setting:`~storage.journal.commitIntervalMs` for
information on the default commit interval.

Journaling stores raw operations that allow MongoDB to reconstruct the
following:

- document insertion/updates
- index modifications
- metadata changes to the namespace files
- creation and dropping of databases and their associated data files

As :doc:`write operations </core/write-operations>` occur, MongoDB
writes the data to the ``private view`` in RAM and then copies the write
operations in batches to the journal. The journal stores the operations
on disk to ensure durability.  Each journal entry describes the bytes the
write operation changed in the data files.

MongoDB next applies the journal's write operations to the ``shared
view``. At this point, the ``shared view`` becomes inconsistent with the
data files.

At default intervals of 60 seconds, MongoDB asks the operating system to
flush the ``shared view`` to disk. This brings the data files up-to-date
with the latest write operations.  The operating system may choose to
flush the ``shared view`` to disk at a higher frequency than 60 seconds,
particularly if the system is low on free memory.

When MongoDB flushes write operations to the data files, MongoDB notes which
journal writes have been flushed.  Once a journal file contains only flushed writes,
it is no longer needed for recovery, and MongoDB either deletes it or recycles it for a new journal file.

As part of journaling, MongoDB routinely asks the operating system to
remap the ``shared view`` to the ``private view``, in order to save physical RAM.
Upon a new remapping, the operating system knows that physical memory pages can be
shared between the ``shared view`` and the ``private view`` mappings.

.. note::

   The interaction between the ``shared view`` and the on-disk
   data files is similar to how MongoDB works *without*
   journaling, which is that MongoDB asks the operating system to flush
   in-memory changes back to the data files every 60 seconds.
=======================
Kerberos Authentication
=======================

.. default-domain:: mongodb

.. versionadded:: 2.4

Overview
--------

MongoDB Enterprise provides support for Kerberos authentication of
MongoDB clients to :program:`mongod` and :program:`mongos`. Kerberos is
an industry standard authentication protocol for large client/server
systems. Kerberos allows MongoDB and applications to take advantage of
existing authentication infrastructure and processes.

Kerberos Components and MongoDB
-------------------------------

Principals
~~~~~~~~~~

In a Kerberos-based system, every participant in the authenticated
communication is known as a "principal", and every principal must have
a unique name.

Principals belong to administrative units called *realms*. For each
realm, the Kerberos Key Distribution Center (KDC) maintains a database
of the realm's principal and the principals' associated "secret keys".

For a client-server authentication, the client requests from the KDC a
"ticket" for access to a specific asset. KDC uses the client's
secret and the server's secret to construct the ticket which allows the
client and server to mutually authenticate each other, while keeping
the secrets hidden.

For the configuration of MongoDB for Kerberos support, two kinds of
principal names are of interest: :ref:`user principals
<kerberos-user-principal>` and :ref:`service principals
<kerberos-service-principal>`.

.. _kerberos-user-principal:

User Principal
``````````````

To authenticate using Kerberos, you must add the Kerberos user
principals to MongoDB to the ``$external`` database. User principal
names have the form:

.. code-block:: none

   <username>@<KERBEROS REALM>

For every user you want to authenticate using Kerberos, you must create
a corresponding user in MongoDB in the ``$external`` database.

For examples of adding a user to MongoDB as well as authenticating as
that user, see
:doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
and
:doc:`/tutorial/control-access-to-mongodb-windows-with-kerberos-authentication`.

.. seealso:: :doc:`/reference/command/nav-user-management` for general
   information regarding creating and managing users in MongoDB.

.. _kerberos-service-principal:

Service Principal
`````````````````

Every MongoDB :program:`mongod` and :program:`mongos` instance (or
:program:`mongod.exe` or :program:`mongos.exe` on Windows) must have an
associated service principal. Service principal names have the form:

.. code-block:: none

   <service>/<fully qualified domain name>@<KERBEROS REALM>

For MongoDB, the ``<service>`` defaults to ``mongodb``. For example, if
``m1.example.com`` is a MongoDB server, and ``example.com`` maintains
the ``EXAMPLE.COM`` Kerberos realm, then ``m1`` should have the service
principal name ``mongodb/m1.example.com@EXAMPLE.COM``.

To specify a different value for ``<service>``, use
:setting:`saslServiceName` during the start up of :program:`mongod` or
:program:`mongos` (or :program:`mongod.exe` or :program:`mongos.exe`).
:program:`mongo` shell or other clients may also specify a different
service principal name using :setting:`saslServiceName`.

Service principal names must be reachable over the network using the
fully qualified domain name (FQDN) part of its service principal name.

By default, Kerberos attempts to identify hosts using the
``/etc/kerb5.conf`` file before using DNS to resolve hosts.

On Windows, if running MongoDB as a service, see
:ref:`assign-service-principal-name`.

.. _keytab-files:

Linux Keytab Files
~~~~~~~~~~~~~~~~~~

Linux systems can store Kerberos authentication keys for a
:ref:`service principal <kerberos-service-principal>` in *keytab*
files. Each Kerberized :program:`mongod` and :program:`mongos` instance
running on Linux must have access to a keytab file containing keys for
its :ref:`service principal <kerberos-service-principal>`.

To keep keytab files secure, use file permissions that restrict access
to only the user that runs the :program:`mongod` or :program:`mongos`
process.

.. _linux-init-mongodb-clients:

Tickets
~~~~~~~

On Linux, MongoDB clients can use Kerberos's ``kinit`` program to
initialize a credential cache for authenticating the user principal to
servers.

Windows Active Directory
~~~~~~~~~~~~~~~~~~~~~~~~

Unlike on Linux systems, :program:`mongod` and :program:`mongos`
instances running on Windows do not require access to keytab files.
Instead, the :program:`mongod` and :program:`mongos` instances read
their server credentials from a credential store specific to the
operating system.

However, from the Windows Active Directory, you can export a keytab
file for use on Linux systems. See `Ktpass
<http://technet.microsoft.com/en-us/library/cc753771.aspx>`_ for more
information.

Authenticate With Kerberos
~~~~~~~~~~~~~~~~~~~~~~~~~~

To configure MongoDB for Kerberos support and authenticate, see
:doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
and
:doc:`/tutorial/control-access-to-mongodb-windows-with-kerberos-authentication`.

Operational Considerations
--------------------------

The HTTP Console
~~~~~~~~~~~~~~~~

The MongoDB :ecosystem:`HTTP Console
</tools/http-interface/#http-console>` interface does not support
Kerberos authentication.

DNS
~~~

Each host that runs a :program:`mongod` or :program:`mongos` instance
must have both ``A`` and ``PTR`` DNS records to provide forward and
reverse lookup.

Without ``A`` and ``PTR`` DNS records, the host cannot resolve the
components of the Kerberos domain or the Key Distribution Center (KDC).

System Time Synchronization
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To successfully authenticate, the system time for each
:program:`mongod` and :program:`mongos` instance must be within
5 minutes of the system time of the other hosts in the Kerberos
infrastructure.

Kerberized MongoDB Environments
-------------------------------

.. _kerberos-and-drivers:

Driver Support
~~~~~~~~~~~~~~

The following MongoDB drivers support Kerberos authentication:

- :ecosystem:`Java </tutorial/authenticate-with-java-driver/>`
- :ecosystem:`C# </tutorial/authenticate-with-csharp-driver/>`
- :ecosystem:`C++ </tutorial/authenticate-with-cpp-driver/>`
- `Python <http://api.mongodb.org/python/current/examples/authentication.html>`_

Use with Additional MongoDB Authentication Mechanism
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Although MongoDB supports the use of Kerberos authentication with other
authentication mechanisms, only add the other mechanisms as necessary.
See the ``Incorporate Additional Authentication Mechanisms`` section in
:doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
and
:doc:`/tutorial/control-access-to-mongodb-windows-with-kerberos-authentication`
for details.
======================
Map Reduce Concurrency
======================

.. default-domain:: mongodb

The map-reduce operation is composed of many tasks, including reads
from the input collection, executions of the ``map`` function,
executions of the ``reduce`` function, writes to a temporary collection
during processing, and writes to the output collection.

During the operation, map-reduce takes the following locks:

- The read phase takes a read lock.  It yields every 100 documents.

- The insert into the temporary collection takes a write lock for a
  single write.

- If the output collection does not exist, the creation of the output
  collection takes a write lock.

- If the output collection exists, then the output actions (i.e.
  ``merge``, ``replace``, ``reduce``) take a write lock.

.. versionchanged:: 2.4
   The V8 JavaScript engine, which became the default in 2.4, allows
   multiple JavaScript operations to execute at the same time. Prior to
   2.4, JavaScript code (i.e. ``map``, ``reduce``, ``finalize``
   functions) executed in a single thread.

.. note::

   The final write lock during post-processing makes the results appear
   atomically. However, output actions ``merge`` and ``reduce`` may
   take minutes to process. For the ``merge`` and ``reduce``, the
   ``nonAtomic`` flag is available. See the
   :method:`db.collection.mapReduce()` reference for more information.
==================================
Map-Reduce and Sharded Collections
==================================

.. default-domain:: mongodb

Map-reduce supports operations on sharded collections, both as an input
and as an output. This section describes the behaviors of
:dbcommand:`mapReduce` specific to sharded collections.

.. _map-reduce-sharded-cluster:

Sharded Collection as Input
---------------------------

When using sharded collection as the input for a map-reduce operation,
:program:`mongos` will automatically dispatch the map-reduce job to
each shard in parallel. There is no special option
required. :program:`mongos` will wait for jobs on all shards to
finish.

Sharded Collection as Output
----------------------------

.. versionchanged:: 2.2

If the ``out`` field for :dbcommand:`mapReduce` has the ``sharded``
value, MongoDB shards the output collection using the ``_id`` field as
the shard key.

To output to a sharded collection:

- If the output collection does not exist, MongoDB creates and shards
  the collection on the ``_id`` field.

- For a new or an empty sharded collection, MongoDB uses the results of
  the first stage of the map-reduce operation to create the initial
  :term:`chunks <chunk>` distributed among the shards.

- :program:`mongos` dispatches, in parallel, a map-reduce
  post-processing job to every shard that owns a chunk. During the
  post-processing, each shard will pull the results
  for its own chunks from the other shards, run the final
  reduce/finalize, and write locally to the output collection.

.. note::

   - During later map-reduce jobs, MongoDB splits chunks as needed.

   - Balancing of chunks for the output collection is automatically
     prevented during post-processing to avoid concurrency issues.

In MongoDB 2.0:

- :program:`mongos` retrieves the results from each shard,
  performs a merge sort to order the results, and proceeds to the reduce/finalize phase as
  needed. :program:`mongos` then writes the result to the output
  collection in sharded mode.

- This model requires only a small amount of memory, even for large data sets.

- Shard chunks are not automatically split during insertion. This
  requires manual intervention until the chunks are granular and
  balanced.

.. important::
   For best results, only use the sharded output options for
   :dbcommand:`mapReduce` in version 2.2 or later.
==========
Map-Reduce
==========

.. default-domain:: mongodb

Map-reduce is a data processing paradigm for condensing large volumes
of data into useful *aggregated* results. For map-reduce operations,
MongoDB provides the :dbcommand:`mapReduce` database command.

Consider the following map-reduce operation:

.. include:: /images/map-reduce.rst

In this map-reduce operation, MongoDB applies the *map* phase to each
input document (i.e. the documents in the collection that match the
query condition). The map function emits key-value pairs. For those
keys that have multiple values, MongoDB applies the *reduce* phase, which
collects and condenses the aggregated data. MongoDB then stores the results
in a collection. Optionally, the output of the reduce function may
pass through a *finalize* function to further condense or process the
results of the aggregation.

All map-reduce functions in MongoDB are JavaScript and run
within the :program:`mongod` process. Map-reduce operations take the
documents of a single :term:`collection` as the *input* and can perform
any arbitrary sorting and limiting before beginning the map stage.
:dbcommand:`mapReduce` can return the results of a map-reduce operation
as a document, or may write the results to collections. The input and
the output collections may be sharded.

.. note::

   For most aggregation operations, the
   :doc:`/core/aggregation-pipeline` provides better performance and
   more coherent interface. However, map-reduce operations provide
   some flexibility that is not presently available in the aggregation
   pipeline.

Map-Reduce JavaScript Functions
-------------------------------

In MongoDB, map-reduce operations use custom JavaScript functions to
*map*, or associate, values to a key. If a key has multiple values
mapped to it, the operation *reduces* the values for the key to a
single object.

The use of custom JavaScript functions provide flexibility to
map-reduce operations. For instance, when processing a document, the
map function can create more than one key and value mapping or no
mapping. Map-reduce operations can also use a custom JavaScript
function to make final modifications to the results at the end of the
map and reduce operation, such as perform additional calculations.

Map-Reduce Behavior
-------------------

In MongoDB, the map-reduce operation can write results to a collection
or return the results inline. If you write map-reduce output to a
collection, you can perform subsequent map-reduce operations on the
same input collection that merge replace, merge, or reduce new results
with previous results. See :dbcommand:`mapReduce` and
:doc:`/tutorial/perform-incremental-map-reduce` for details and
examples.

When returning the results of a map reduce operation *inline*, the
result documents must be within the :limit:`BSON Document Size` limit,
which is currently 16 megabytes. For additional information on limits
and restrictions on map-reduce operations, see the
:doc:`/reference/command/mapReduce` reference page.

MongoDB supports map-reduce operations on :doc:`sharded collections
</core/sharding-introduction>`. Map-reduce operations can also output
the results to a sharded collection. See
:doc:`/core/map-reduce-sharded-collections`.
========================
Master Slave Replication
========================

.. default-domain:: mongodb

.. important:: :doc:`Replica sets </core/replication>` replace
   :term:`master`\-:term:`slave` replication for most use cases. If
   possible, use replica sets rather than master-slave replication for
   all new production deployments. This documentation remains to
   support legacy deployments and for archival purposes only.

In addition to providing all the functionality of master-slave
deployments, replica sets are also more robust for production use.
Master-slave replication preceded replica sets
and made it possible have a large number of non-master (i.e. slave) nodes, as well as
to restrict replicated operations to only a single database; however,
master-slave replication provides less redundancy and does not
automate failover. See :ref:`replica-set-equivalent` for a replica set
configuration that is equivalent to master-slave replication.  If you
wish to convert an existing master-slave deployment to a replica set,
see :ref:`convert-master-slave-to-replica-set`.

Fundamental Operations
----------------------

Initial Deployment
~~~~~~~~~~~~~~~~~~

To configure a :term:`master`\-:term:`slave` deployment, start two
:program:`mongod` instances: one in :setting:`master` mode, and the
other in :setting:`slave` mode.

To start a :program:`mongod` instance in :setting:`master` mode,
invoke :program:`mongod` as follows:

.. code-block:: javascript

   mongod --master --dbpath /data/masterdb/

With the :option:`--master <mongod --master>` option, the
:program:`mongod` will create a :data:`local.oplog.$main` collection,
which the "operation log" that queues operations that the slaves will
apply to replicate operations from the master. The
:option:`--dbpath <mongod --dbpath>` is optional.

To start a :program:`mongod` instance in :setting:`slave` mode,
invoke :program:`mongod` as follows:

.. code-block:: javascript

   mongod --slave --source <masterhostname><:<port>> --dbpath /data/slavedb/

Specify the hostname and port of the master instance to the
:option:`--source <mongod --source>` argument. The
:option:`--dbpath <mongod --dbpath>` is optional.

For :setting:`slave` instances, MongoDB stores data about the source
server in the :data:`local.sources` collection.

Configuration Options for Master-Slave Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As an alternative to specifying the :option:`--source <mongod --source>`
run-time option, can add a document to :data:`local.sources`
specifying the :setting:`master` instance, as in the following
operation in the :program:`mongo` shell:

.. code-block:: javascript
   :linenos:

   use local
   db.sources.find()
   db.sources.insert( { host: <masterhostname> <,only: databasename> } );

In line 1, you switch context to the ``local`` database. In line 2,
the :method:`~db.collection.find()` operation should return no
documents, to ensure that there are no documents in the ``sources``
collection. Finally, line 3 uses :method:`db.collection.insert()` to
insert the source document into the :data:`local.sources`
collection. The model of the :data:`local.sources` document is as
follows:

.. describe:: host

   The host field specifies the :setting:`master`\ :program:`mongod`
   instance, and holds a resolvable hostname, i.e. IP address, or
   a name from a ``host`` file, or preferably a fully qualified domain
   name.

   You can append ``<:port>`` to the host name if the
   :program:`mongod` is not running on the default ``27017`` port.

.. describe:: only

   Optional. Specify a name of a database. When specified, MongoDB
   will only replicate the indicated database.

Operational Considerations for Replication with Master Slave Deployments
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Master instances store operations in an :term:`oplog` which is a
:doc:`capped collection </core/capped-collections>`. As a result, if a
slave falls too far behind the state of the master, it cannot
"catchup" and must re-sync from scratch. Slave may become out of sync
with a master if:

- The slave falls far behind the data updates available from that
  master.

- The slave stops (i.e. shuts down) and restarts later after the
  master has overwritten the relevant operations from the master.

When slaves, are out of sync, replication stops. Administrators must
intervene manually to restart replication. Use the :dbcommand:`resync`
command. Alternatively, the :option:`--autoresync <mongod --autoresync>`
allows a slave to restart replication automatically,
after ten second pause, when the slave falls out of sync with the
master. With :option:`--autoresync <mongod --autoresync>` specified,
the slave will only attempt to re-sync once in a ten minute period.

To prevent these situations you should specify a larger oplog when
you start the :setting:`master` instance, by adding the
:option:`--oplogSize <mongod --oplogSize>` option when starting
:program:`mongod`. If you do not specify
:option:`--oplogSize <mongod --oplogSize>`, :program:`mongod` will
allocate 5% of available disk space on start up to the oplog, with a
minimum of 1GB for 64bit machines and 50MB for 32bit machines.

Run time Master-Slave Configuration
-----------------------------------

MongoDB provides a number of run time configuration options for
:program:`mongod` instances in :term:`master`\-:term:`slave`
deployments. You can specify these options in :doc:`configuration
files </administration/configuration>` or on the command-line. See
documentation of the following:

- For *master* nodes:

  - :setting:`master`

  - :setting:`slave`

- For *slave* nodes:

  - :setting:`source`

  - :setting:`only`

  - :setting:`slaveDelay`

Also consider the :ref:`Master-Slave Replication Command Line Options
<cli-mongod-master-slave>` for related options.

Diagnostics
~~~~~~~~~~~

On a :term:`master` instance, issue the following operation in the
:program:`mongo` shell to return replication status from the
perspective of the master:

.. code-block:: javascript

   rs.printReplicationInfo()

On a :term:`slave` instance, use the following operation in the
:program:`mongo` shell to return the replication status from the
perspective of the slave:

.. code-block:: javascript

   rs.printSlaveReplicationInfo()

Use the :dbcommand:`serverStatus` as in the following operation, to
return status of the replication:

.. code-block:: javascript

   db.serverStatus()

See :ref:`server status repl fields <server-status-repl>` for
documentation of the relevant section of output.

Security
--------

When running with :setting:`~security.authentication` enabled, in
:term:`master`\-:term:`slave` deployments configure a
:setting:`~security.keyFile` so that slave :program:`mongod` instances can
authenticate and communicate with the master :program:`mongod`
instance.

To enable authentication and configure the :setting:`~security.keyFile` add the
following option to your configuration file:

.. code-block:: cfg

   keyFile = /srv/mongodb/keyfile

.. note::

   You may chose to set these run-time configuration options using the
   :option:`--keyFile <mongod --keyFile>` option on the command line.

Setting :setting:`~security.keyFile` enables authentication and specifies a key
file for the :program:`mongod` instances to use when authenticating to
each other. The content of the key file is arbitrary but must be the
same on all members of the deployment can connect to each other.

The key file must be less one kilobyte in size and may only contain
characters in the base64 set. The key file must not have group or "world"
permissions on UNIX systems. Use the following command to use the
OpenSSL package to generate "random" content for use in a key file:

.. code-block:: bash

   openssl rand -base64 741

.. seealso:: :doc:`/security` for more information about security in MongoDB

Ongoing Administration and Operation of Master-Slave Deployments
----------------------------------------------------------------

.. _replica-set-equivalent:

Deploy Master-Slave Equivalent using Replica Sets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want a replication configuration that resembles
:term:`master`\-:term:`slave` replication, using :term:`replica sets
<replica set>` replica sets, consider the following replica
configuration document. In this deployment hosts ``<master>`` and
``<slave>`` [#host-are-hostnames]_ provide replication that is roughly
equivalent to a two-instance master-slave deployment:

.. code-block:: javascript

   {
     _id : 'setName',
     members : [
       { _id : 0, host : "<master>", priority : 1 },
       { _id : 1, host : "<slave>", priority : 0, votes : 0 }
     ]
   }

See :doc:`/reference/replica-configuration` for more information about
replica set configurations.

.. [#host-are-hostnames] In replica set configurations, the
   :data:`~local.system.replset.members[n].host` field must hold a resolvable
   hostname.

.. _convert-master-slave-to-replica-set:

Convert a Master-Slave Deployment to a Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To convert a master-slave deployment to a replica set, restart the
current master as a one-member replica set. Then remove the data
directors from previous secondaries and add them as new secondaries to
the new replica set.

1. To confirm that the current instance is master, run:

   .. code-block:: sh

      db.isMaster()

   This should return a document that resembles the following:

   .. code-block:: sh

      {
              "ismaster" : true,
              "maxBsonObjectSize" : 16777216,
              "maxMessageSizeBytes" : 48000000,
              "localTime" : ISODate("2013-07-08T20:15:13.664Z"),
              "ok" : 1
      }

#. Shut down the :program:`mongod` processes on the master and all
   slave(s), using the following command while connected to each
   instance:

   .. code-block:: sh

      db.adminCommand({shutdown : 1, force : true})

#. Back up your ``/data/db`` directories, in case you need to revert
   to the master-slave deployment.

#. Start the former master with the :option:`--replSet <replSet>`
   option, as in the following:

   .. code-block:: sh

      mongod --replSet <setname>

#. Connect to the :program:`mongod` with the :program:`mongo` shell,
   and initiate the replica set with the following command:

   .. code-block:: sh

      rs.initiate()

   When the command returns, you will have successfully deployed a
   one-member replica set. You can check the status of your replica set
   at any time by running the following command:

   .. code-block:: sh

      rs.status()

You can now follow the :doc:`convert a standalone to a replica set
</tutorial/convert-standalone-to-replica-set>` tutorial to deploy your
replica set, picking up from the :ref:`Expand the Replica Set
<expand-the-replica-set>` section.

Failing over to a Slave (Promotion)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To permanently failover from a unavailable or damaged :term:`master`
(``A`` in the following example) to a :term:`slave` (``B``):

1. Shut down ``A``.

2. Stop :program:`mongod` on ``B``.

3. Back up and move all data files that begin with ``local`` on ``B``
   from the :setting:`~storage.dbPath`.

   .. warning::

      Removing ``local.*`` is irrevocable and cannot be
      undone. Perform this step with extreme caution.

4. Restart :program:`mongod` on ``B`` with the :option:`--master
   <mongod --master>` option.

.. note:: This is a one time operation, and is not reversible. ``A``
   cannot become a slave of ``B`` until it completes a full resync.

Inverting Master and Slave
~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have a :term:`master` (``A``) and a :term:`slave` (``B``) and
you would like to reverse their roles, follow this procedure. The
procedure assumes ``A`` is healthy, up-to-date and available.

If ``A`` is not healthy but the hardware is okay (power outage, server
crash, etc.), skip steps 1 and 2 and in step 8 replace all of ``A``'s
files with ``B``'s files in step 8.

If ``A`` is not healthy and the hardware is not okay, replace ``A`` with
a new machine. Also follow the instructions in the previous paragraph.

To invert the master and slave in a deployment:

1. Halt writes on ``A`` using the :term:`fsync` command.

2. Make sure ``B`` is up to date with the state of ``A``.

3. Shut down ``B``.

4. Back up and move all data files that begin with ``local`` on ``B``
   from the :setting:`~storage.dbPath` to remove the existing
   ``local.sources`` data.

   .. warning::

      Removing ``local.*`` is irrevocable and cannot be
      undone. Perform this step with extreme caution.

5. Start ``B`` with the :option:`--master <mongod --master>` option.

6. Do a write on ``B``, which primes the :term:`oplog` to provide a new
   sync start point.

7. Shut down ``B``. ``B`` will now have a new set of data files that
   start with ``local``.

8. Shut down ``A`` and replace all files in the :setting:`~storage.dbPath` of
   ``A`` that start with ``local`` with a copy of the files in the
   :setting:`~storage.dbPath` of ``B`` that begin with ``local``.

   Considering compressing the ``local`` files from ``B`` while you
   copy them, as they may be quite large.

9. Start ``B`` with the :option:`--master <mongod --master>` option.

10. Start ``A`` with all the usual slave options, but include
    :option:`fastsync <mongod --fastsync>`.

Creating a Slave from an Existing Master's Disk Image
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you can stop write operations to the :term:`master` for an
indefinite period, you can copy the data files from the master to the
new :term:`slave` and then start the slave with :option:`--fastsync
<mongod --fastsync>`.

.. warning::

   Be careful with :option:`--fastsync <mongod --fastsync>`.  If the
   data on both instances is **not** identical, a discrepancy will exist
   forever.

:setting:`fastsync` is a way to start a slave by starting with an
existing master disk image/backup. This option declares that the
administrator guarantees the image is correct and completely up-to-date
with that of the master. If you have a full and complete copy of data
from a master you can use this option to avoid a full synchronization
upon starting the slave.

Creating a Slave from an Existing Slave's Disk Image
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can just copy the other :term:`slave's <slave>` data file snapshot
without any special options. Only take data snapshots when a
:program:`mongod` process is down or locked using
:method:`db.fsyncLock()`.

Resyncing a Slave that is too Stale to Recover
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:term:`Slaves <slave>` asynchronously apply write operations from the
:term:`master` that the slaves poll from the master's
:term:`oplog`. The oplog is finite in length, and if a slave is too
far behind, a full resync will be necessary. To resync the slave,
connect to a slave using the :program:`mongo` and issue the
:dbcommand:`resync` command:

.. code-block:: javascript

   use admin
   db.runCommand( { resync: 1 } )

This forces a full resync of all data (which will be very slow on a
large database). You can achieve the same effect by stopping
:program:`mongod` on the slave, deleting the entire content of the
:setting:`~storage.dbPath` on the slave, and restarting the :program:`mongod`.

Slave Chaining
~~~~~~~~~~~~~~

:term:`Slaves <slave>` cannot be "chained." They must all connect to the
:term:`master` directly.

If a slave attempts "slave from" another slave you will see the
following line in the :program:`mongod` long of the shell:

.. code-block:: none

   assertion 13051 tailable cursor requested on non capped collection ns:local.oplog.$main

Correcting a Slave's Source
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To change a :term:`slave's <slave>` source, manually modify the
slave's :data:`local.sources` collection.

.. example::

   Consider the following: If you accidentally set an incorrect hostname
   for the slave's :setting:`source`, as in the following example:

   .. code-block:: javascript

      mongod --slave --source prod.mississippi

   You can correct this, by restarting the slave without the
   :option:`--slave <mongod --slave>` and
   :option:`--source <mongod --source>` arguments:

   .. code-block:: javascript

      mongod

   Connect to this :program:`mongod` instance using the
   :program:`mongo` shell and update the :data:`local.sources`
   collection, with the following operation sequence:

   .. code-block:: javascript

      use local

      db.sources.update( { host : "prod.mississippi" },
                         { $set : { host : "prod.mississippi.example.net" } } )

   Restart the slave with the correct command line arguments or with no
   :option:`--source <mongod --source>` option.
   After configuring :data:`local.sources` the
   first time, the :option:`--source <mongod --source>` will have no
   subsequent effect. Therefore, both of the following invocations are
   correct:

   .. code-block:: javascript

      mongod --slave --source prod.mississippi.example.net

   or

   .. code-block:: javascript

      mongod --slave

   The slave now polls data from the correct :term:`master`.
==============================================
Operational Segregation in MongoDB Deployments
==============================================

.. default-domain:: mongodb

Operational Overview
--------------------

MongoDB includes a number of features that allow database administrators
and developers to segregate application operations to MongoDB
deployments by functional or geographical groupings.

.. COMMENT

   We will move the content about data center awareness to a separate
   page with more specific examples about geographical segregation of
   operations.

.. END-COMMENT

This capability provides "data center awareness," which allows
applications to target MongoDB deployments with consideration of the
physical location of the :program:`mongod` instances. MongoDB supports
segmentation of operations across different dimensions, which may
include multiple data centers and geographical regions in multi-data
center deployments, racks, networks, or power circuits in single
data center deployments.

MongoDB also supports segregation of database operations based on
functional or operational parameters, to ensure that certain
:program:`mongod` instances are only used for reporting workloads or
that certain high-frequency portions of a sharded collection only
exist on specific shards.

Specifically, with MongoDB, you can:

- ensure write operations propagate to specific members of a replica
  set, or to specific members of replica sets.

- ensure that specific members of a replica set respond to queries.

- ensure that specific ranges of your :term:`shard key` balance onto and
  reside on specific :term:`shards <shard>`.

- combine the above features in a single distributed deployment, on a
  per-operation (for read and write operations) and collection (for
  chunk distribution in sharded clusters distribution) basis.

For full documentation of these features, see the following
documentation in the MongoDB Manual:

- :doc:`Read Preferences </core/read-preference>`, which controls how drivers
  help applications target read operations to members of a replica set.

- :doc:`Write Concerns </core/write-concern>`, which controls
  how MongoDB ensures that write operations propagate to members of a
  replica set.

- :ref:`Replica Set Tags <replica-set-configuration-tag-sets>`, which
  control how applications create and interact with custom groupings
  of replica set members to create custom application-specific read
  preferences and write concerns.

- :ref:`Tag Aware Sharding <tag-aware-sharding>`, which allows MongoDB
  administrators to define an application-specific balancing policy,
  to control how documents belonging to specific ranges of a shard key
  distribute to shards in the :term:`sharded cluster`.

.. seealso::

   Before adding operational segregation features to your application
   and MongoDB deployment, become familiar with all documentation of
   :doc:`replication </replication>`, and :doc:`sharding </sharding>`.

.. TODO uncomment this section when we can write content for it:

   Examples of Operational Segregation
   -----------------------------------

   Increase Data Locality in Geographically Distributed Cluster
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   Functional Segregation for Reporting and Backups
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   Increase Read Locality for Distributed Applications
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   Ensure Geographical Redundancy for Write Operations
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.. _read-operations-indexing:

==================
Query Optimization
==================

.. default-domain:: mongodb

Indexes improve the efficiency of read operations by reducing the
amount of data that query operations need to process. This simplifies
the work associated with fulfilling queries within MongoDB.

Create an Index to Support Read Operations
------------------------------------------

If your application queries a collection on a particular field or
fields, then an index on the queried field or fields can prevent the
query from scanning the whole collection to find and return the query
results. For more information about indexes, see the :doc:`complete
documentation of indexes in MongoDB</core/indexes>`.

.. example:: An application queries the ``inventory`` collection on the
   ``type`` field. The value of the ``type`` field is user-driven.

   .. code-block:: javascript

      var typeValue = <someUserInput>;
      db.inventory.find( { type: typeValue } );

   To improve the performance of this query, add an ascending, or a
   descending, index to the ``inventory`` collection on the ``type``
   field. [#ensureIndexOrder]_ In the :program:`mongo` shell, you can
   create indexes using the :method:`db.collection.ensureIndex()`
   method:

   .. code-block:: javascript

      db.inventory.ensureIndex( { type: 1 } )

   This index can prevent the above query on ``type`` from scanning the
   whole collection to return the results.

To analyze the performance of the query with an index, see
:doc:`/tutorial/analyze-query-plan`.

In addition to optimizing read operations, indexes can support sort
operations and allow for a more efficient storage utilization. See
:method:`db.collection.ensureIndex()` and
:doc:`/administration/indexes` for more information about index
creation.

.. [#ensureIndexOrder]
   For single-field indexes, the selection between ascending and
   descending order is immaterial. For compound indexes, the selection
   is important. See :ref:`indexing order
   <index-ascending-and-descending>` for more details.

.. _read-operations-query-selectivity:

Query Selectivity
-----------------

Some query operations are not selective. These operations cannot use
indexes effectively or cannot use indexes at all.

The inequality operators :query:`$nin` and :query:`$ne` are not
very selective, as they often match a large portion of the index. As a
result, in most cases, a :query:`$nin` or :query:`$ne` query with
an index may perform no better than a :query:`$nin` or
:query:`$ne` query that must scan all documents in a collection.

Queries that specify regular expressions, with inline JavaScript
regular expressions or :query:`$regex` operator expressions, cannot
use an index with one exception. Queries that specify regular
expression *with anchors* at the beginning of a string *can* use an
index.

.. _read-operations-covered-query:

Covering a Query
----------------

An index :ref:`covers <indexes-covered-queries>` a query, a *covered
query*, when:

- all the fields in the :ref:`query <read-operations-query-document>`
  are part of that index, **and**

- all the fields returned in the documents that match the query are in
  the same index.

For these queries, MongoDB does not need to inspect documents outside
of the index. This is often more efficient than inspecting entire
documents.

.. example::

   Given a collection ``inventory`` with the following index on the
   ``type`` and ``item`` fields:

   .. code-block:: sh

      { type: 1, item: 1 }

   This index will cover the following query on the ``type`` and ``item``
   fields, which returns only the ``item`` field:

   .. code-block:: javascript

      db.inventory.find( { type: "food", item:/^c/ },
                         { item: 1, _id: 0 } )

   However, the index will **not** cover the following query, which
   returns the ``item`` field **and** the ``_id`` field:

   .. code-block:: javascript

      db.inventory.find( { type: "food", item:/^c/ },
                         { item: 1 } )

See :ref:`indexes-covered-queries` for more information on the
behavior and use of covered queries.
.. index:: query optimizer
.. _read-operations-query-optimization:

===========
Query Plans
===========

.. default-domain:: mongodb

.. TODO Consider moving this to the mechanics of the index section

The MongoDB query optimizer processes queries and chooses the most
efficient query plan for a query given the available indexes. The query
system then uses this query plan each time the query runs.

.. include:: /includes/fact-query-optimizer-cache-behavior.rst

The query optimizer occasionally reevaluates query plans as the content
of the collection changes to ensure optimal query plans. You can also
specify which indexes the optimizer evaluates with :ref:`index-filters`.

You can use the :method:`~cursor.explain()` method to view statistics
about the query plan for a given query. This information can help as you
develop :doc:`indexing strategies </applications/indexes>`.

Query Optimization
------------------

To create a new query plan, the query optimizer:

1. runs the query against several candidate indexes in parallel.

#. records the matches in a common results buffer
   or buffers.

   - If the candidate plans include only :term:`ordered query plans
     <ordered query plan>`, there is a single common results buffer.

   - If the candidate plans include only :term:`unordered query plans
     <unordered query plan>`, there is a single common results buffer.

   - If the candidate plans include *both* :term:`ordered query plans
     <ordered query plan>` and :term:`unordered query plans
     <unordered query plan>`, there are two common results buffers, one
     for the ordered plans and the other for the unordered plans.

   If an index returns a result already returned by another index, the
   optimizer skips the duplicate match. In the case of the two buffers,
   both buffers are de-duped.

#. stops the testing of candidate plans and selects an index when one of
   the following events occur:

   - An :term:`unordered query plan` has returned all the matching results; *or*

   - An :term:`ordered query plan` has returned all the matching results; *or*

   - An :term:`ordered query plan` has returned a threshold number of
     matching results:

     - Version 2.0: Threshold is the query batch size. The default
       batch size is 101.

     - Version 2.2: Threshold is 101.

The selected index becomes the index specified in the query plan;
future iterations of this query or queries with the same query
pattern will use this index. Query pattern refers to query select
conditions that differ only in the values, as in the following two
queries with the same query pattern:

.. code-block:: javascript

   db.inventory.find( { type: 'food' } )
   db.inventory.find( { type: 'utensil' } )

Query Plan Revision
-------------------

As collections change over time, the query optimizer deletes the query
plan and re-evaluates after any of the following events:

- The collection receives 1,000 write operations.

- The :dbcommand:`reIndex` rebuilds the index.

- You add or drop an index.

- The :program:`mongod` process restarts.

Cached Query Plan Interface
---------------------------

.. versionadded:: 2.6

MongoDB provides :doc:`/reference/method/js-plan-cache` to view and
modify the cached query plans.

.. _index-filters:

Index Filters
-------------

.. versionadded:: 2.6

Index filters determine which indexes the optimizer evaluates for a
:term:`query shape`. A query shape consists of a combination of query,
sort, and projection specifications. If an index filter exists for a
given query shape, the optimizer only considers those indexes
specified in the filter.

When an index filter exists for the query shape, MongoDB ignores the
:method:`~cursor.hint()`. To see whether MongoDB applied an index
filter for a query, check the :data:`explain.filterSet` field of the
:method:`~cursor.explain()` output.

Index filters only affects which indexes the optimizer evaluates; the
optimizer may still select the collection scan as the winning plan for
a given query shape.

Index filters exist for the duration of the server process and do not
persist after shutdown. MongoDB also provides a command to manually remove
filters.

Because index filters overrides the expected behavior of the optimizer
as well as the :method:`~cursor.hint()` method, use index filters
sparingly.

See :dbcommand:`planCacheListFilters`,
:dbcommand:`planCacheClearFilters`, and :dbcommand:`planCacheSetFilter`.
.. index:: read operations; query
.. _read-operations-query-operations:
.. _read-operations-queries:

========================
Read Operations Overview
========================

.. default-domain:: mongodb

Read operations, or :term:`queries <query>`, retrieve data stored in
the database. In MongoDB, queries select :term:`documents <document>`
from a single :term:`collection`.

Queries specify criteria, or conditions, that identify the documents
that MongoDB returns to the clients. A query may include a *projection*
that specifies the fields from the matching documents to return. The
projection limits the amount of data that MongoDB returns to the client
over the network.

Query Interface
---------------

For query operations, MongoDB provide a :method:`db.collection.find()`
method. The method accepts both the query criteria and projections and
returns a :doc:`cursor </core/cursors>` to the matching documents. You
can optionally modify the query to impose limits, skips, and sort
orders.

The following diagram highlights the components of a MongoDB query
operation:

.. include:: /images/crud-annotated-mongodb-find.rst

The next diagram shows the same query in SQL:

.. include:: /images/crud-annotated-sql-select.rst

.. example::

   .. code-block:: javascript

      db.users.find( { age: { $gt: 18 } }, { name: 1, address: 1 } ).limit(5)

   This query selects the documents in the ``users`` collection that
   match the condition ``age`` is greater than ``18``. To specify the
   greater than condition, query criteria uses the greater than (i.e.
   :query:`$gt`) :ref:`query selection operator <query-selectors>`.
   The query returns at most ``5`` matching documents (or more
   precisely, a cursor to those documents). The matching documents will
   return with only the ``_id``, ``name`` and ``address`` fields. See
   :ref:`projections` for details.

.. see:: :doc:`/reference/sql-comparison` for additional examples
   of MongoDB queries and the corresponding SQL statements.

Query Behavior
--------------

MongoDB queries exhibit the following behavior:

- All queries in MongoDB address a *single* collection.

- You can modify the query to impose :method:`limits <cursor.limit()>`,
  :method:`skips <cursor.skip()>`, and :method:`sort orders
  <cursor.sort()>`.

- The order of documents returned by a query is not defined unless you
  specify a :method:`~cursor.sort()`.

- Operations that :doc:`modify existing documents
  </tutorial/modify-documents>` (i.e. *updates*) use the same query
  syntax as queries to select documents to update.

- In :doc:`aggregation </core/aggregation>` pipeline, the
  :pipeline:`$match` pipeline stage provides access to MongoDB
  queries.

MongoDB provides a :method:`db.collection.findOne()` method as a
special case of :method:`~db.collection.find()` that returns a single
document.

Query Statements
----------------

Consider the following diagram of the query process that specifies a
query criteria and a sort modifier:

.. include:: /images/crud-query-stages.rst

In the diagram, the query selects documents from the ``users``
collection. Using a :doc:`query selection operator
</reference/operator>` to define the conditions for matching documents,
the query selects documents that have ``age`` greater than (i.e.
:query:`$gt`) ``18``. Then the :method:`~cursor.sort()` modifier
sorts the results by ``age`` in ascending order.

For additional examples of queries, see
:doc:`/tutorial/query-documents`.

.. _projections:

Projections
-----------

Queries in MongoDB return all fields in all matching documents by
default. To limit the amount of data that MongoDB sends to
applications, include a :term:`projection` in the queries. By
projecting results with a subset of fields, applications reduce their
network overhead and processing requirements.

Projections, which are the *second* argument to the
:method:`~db.collection.find()` method, may either specify a list of
fields to return *or* list fields to exclude in the result documents.

.. important:: Except for excluding the ``_id`` field in inclusive
   projections, you cannot mix exclusive and inclusive projections.

Consider the following diagram of the query process that specifies a
query criteria and a projection:

.. include:: /images/crud-query-w-projection-stages.rst

In the diagram, the query selects from the ``users`` collection. The
criteria matches the documents that have ``age`` equal to ``18``. Then
the projection specifies that only the ``name`` field should return in
the matching documents.

Projection Examples
~~~~~~~~~~~~~~~~~~~

Exclude One Field From a Result Set
```````````````````````````````````

.. code-block:: javascript

   db.records.find( { "user_id": { $lt: 42} }, { history: 0} )

This query selects a number of documents in the ``records``
collection that match the query ``{ "user_id": { $lt: 42} }``, but
excludes the ``history`` field.

Return Two fields *and* the ``_id`` Field
`````````````````````````````````````````

.. code-block:: javascript

   db.records.find( { "user_id": { $lt: 42} }, { "name": 1, "email": 1} )

This query selects a number of documents in the ``records`` collection
that match the query ``{ "user_id": { $lt: 42} }``, but returns
documents that have the ``_id`` field (implicitly included) as well as
the ``name`` and ``email`` fields.

Return Two Fields *and* Exclude ``_id``
```````````````````````````````````````

.. code-block:: javascript

   db.records.find( { "user_id": { $lt: 42} }, { "_id": 0, "name": 1 , "email": 1 } )

This query selects a number of documents in the ``records``
collection that match the query ``{ "user_id": { $lt: 42} }``, but
only returns the ``name`` and  ``email`` fields.

.. see:: :doc:`/tutorial/project-fields-from-query-results` for more
   examples of queries with projection statements.

Projection Behavior
~~~~~~~~~~~~~~~~~~~

MongoDB projections have the following properties:

- In MongoDB, the ``_id`` field is always included in results unless
  explicitly excluded.

- For fields that contain arrays, MongoDB provides the following
  projection operators: :projection:`$elemMatch`, :projection:`$slice`,
  :projection:`$`.

- For related projection functionality in the :doc:`aggregation
  framework </core/aggregation>` pipeline, use the
  :pipeline:`$project` pipeline stage.
===============
Read Operations
===============

.. default-domain:: mongodb

The following documents describe read operations:

.. include:: /includes/toc/dfn-list-crud-read-operations.rst

.. include:: /includes/toc/crud-read-operations.rst
.. index:: read preference; behavior
.. _replica-set-read-preference-behavior:

=========================
Read Preference Processes
=========================

.. default-domain:: mongodb

.. versionchanged:: 2.2

MongoDB drivers use the following procedures to direct operations to
replica sets and sharded clusters. To determine how to route their
operations, applications periodically update their view of the replica
set's state, identifying which members are up or down, which member is
:term:`primary`, and verifying the latency to each :program:`mongod`
instance.

.. index:: read preference; ping time
.. index:: read preference; nearest
.. index:: read preference; member selection
.. _replica-set-read-preference-behavior-ping-time:
.. _replica-set-read-preference-behavior-nearest:
.. _replica-set-read-preference-behavior-member-selection:

Member Selection
----------------

Clients, by way of their drivers, and :program:`mongos` instances for
sharded clusters, periodically update their view of the replica set's state.

When you select non-:readmode:`primary` read preference, the driver
will determine which member to target using the following process:

#. Assembles a list of suitable members, taking into account member type
   (i.e. secondary, primary, or all members).

#. Excludes members not matching the tag sets, if specified.

#. Determines which suitable member is the closest to the
   client in absolute terms.

#. Builds a list of members that are within a defined ping distance
   (in milliseconds) of the "absolute nearest"
   member.

   Applications can configure the threshold used in this stage. The
   default "acceptable latency" is 15 milliseconds, which you can
   override in the drivers with their own
   ``secondaryAcceptableLatencyMS`` option. For :program:`mongos` you
   can use the :option:`--localThreshold <mongos --localThreshold>` or
   :setting:`localThreshold` runtime options to set this value.

#. Selects a member from these hosts at random. The member receives
   the read operation.

Drivers can then associate the thread or connection with the selected member.
This :ref:`request association
<replica-set-read-preference-behavior-requests>` is configurable by
the application. See your :doc:`driver </applications/drivers>`
documentation about request association configuration and default
behavior.

.. _replica-set-read-preference-behavior-requests:

Request Association
-------------------

.. important:: *Request association* is configurable by the
   application. See your :doc:`driver </applications/drivers>`
   documentation about request association configuration and default
   behavior.

Because :term:`secondary` members of a :term:`replica set` may lag
behind the current :term:`primary` by different amounts, reads for
:term:`secondary` members may reflect data at different points in time.
To prevent sequential reads from jumping around in time, the driver
**can** associate application threads to a specific member of the set
after the first read, thereby preventing reads from other members.
The thread will continue to read from the same member until:

- The application performs a read with a different read preference,

- The thread terminates, or

- The client receives a socket exception, as is the case when there's
  a network error or when the :program:`mongod` closes connections
  during a :term:`failover`.  This triggers a :ref:`retry
  <replica-set-read-preference-behavior-retry>`, which may be
  transparent to the application.

When using request association, if the client detects that the set has
elected a new :term:`primary`, the driver will discard all associations
between threads and members.

.. _replica-set-read-preference-behavior-retry:

Auto-Retry
----------

Connections between MongoDB drivers and :program:`mongod` instances in
a :term:`replica set` must balance two concerns:

#. The client should attempt to prefer current results, and any
   connection should read from the same member of the replica set as
   much as possible.

#. The client should minimize the amount of time that the database is
   inaccessible as the result of a connection issue, networking
   problem, or :term:`failover` in a replica set.

As a result, MongoDB drivers and :program:`mongos`:

- Reuse a connection to specific :program:`mongod` for as long as
  possible after establishing a connection to that instance. This
  connection is *pinned* to this :program:`mongod`.

- Attempt to reconnect to a new member, obeying existing :ref:`read
  preference modes <replica-set-read-preference-modes>`, if the connection
  to :program:`mongod` is lost.

  Reconnections are transparent to the application itself. If
  the connection permits reads from :term:`secondary` members, after
  reconnecting, the application can receive two sequential reads
  returning from different secondaries. Depending on the state of the
  individual secondary member's replication, the documents can reflect
  the state of your database at different moments.

- Return an error *only* after attempting to connect to three members
  of the set that match the :ref:`read preference mode
  <replica-set-read-preference-modes>` and :ref:`tag set
  <replica-set-read-preference-tag-sets>`.  If there are fewer than
  three members of the set, the client will error after connecting to
  all existing members of the set.

  After this error, the driver selects a new member using the
  specified read preference mode. In the absence of a specified read
  preference, the driver uses :readmode:`primary`.

- After detecting a failover situation, [#fn-failover]_ the driver
  attempts to refresh the state of the replica set as quickly as
  possible.

.. [#fn-failover] When a :term:`failover` occurs, all members of the set
   close all client connections that produce a socket error in the
   driver. This behavior prevents or minimizes :term:`rollback`.

.. index:: read preference; sharding
.. index:: read preference; mongos
.. _replica-set-read-preference-behavior-sharding:
.. _replica-set-read-preference-behavior-mongos:

Read Preference in Sharded Clusters
-----------------------------------

.. versionchanged:: 2.2
   Before version 2.2, :program:`mongos` did not support the
   :ref:`read preference mode semantics <replica-set-read-preference-modes>`.

In most :term:`sharded clusters <sharded cluster>`, each shard consists of a :term:`replica
set`. As such, read preferences are also applicable.
With regard to read preference, read operations in a sharded cluster
are identical to unsharded replica sets.

Unlike simple replica sets, in sharded clusters, all interactions with
the shards pass from the clients to the :program:`mongos` instances
that are actually connected to the set members. :program:`mongos` is then
responsible for the application of read preferences, which is
transparent to applications.

There are no configuration changes required for full support of read
preference modes in sharded environments, as long as the
:program:`mongos` is at least version 2.2. All :program:`mongos`
maintain their own connection pool to the replica set members. As a
result:

- A request without a specified preference has
  :readmode:`primary`, the default, unless, the :program:`mongos`
  reuses an existing connection that has a different mode set.

  To prevent confusion, always explicitly set your read preference mode.

- All :readmode:`nearest` and latency calculations reflect the
  connection between the :program:`mongos` and the :program:`mongod`
  instances, not the client and the :program:`mongod` instances.

  This produces the desired result, because all results must pass
  through the :program:`mongos` before returning to the client.
.. index:: read preference
.. index:: slaveOk
.. index:: read preference; background
.. _replica-set-read-preference:
.. _replica-set-read-preference-background:

===============
Read Preference
===============

.. default-domain:: mongodb

.. include:: /includes/introduction-read-preference.rst

.. important:: You must exercise care when specifying read preferences:
   modes other than :readmode:`primary` can *and will* return stale data
   because the secondary queries will not include the most recent write
   operations to the replica set's :term:`primary`.

.. include:: /images/replica-set-read-preference.rst

Use Cases
---------

Indications
~~~~~~~~~~~

The following are common use cases for using non-:readmode:`primary`
read preference modes:

- Running systems operations that do not affect the front-end
  application.

  Issuing reads to secondaries helps distribute load and prevent
  operations from affecting the main workload of the primary. This can
  be a good choice for reporting and analytics workloads, for example.

  .. note::

     Read preferences aren't relevant to direct connections to
     a single :program:`mongod` instance. However, in order to perform
     read operations on a direct connection to a secondary member of a
     replica set, you must set a read preference, such as
     :term:`secondary`.

- Providing local reads for geographically distributed applications.

  If you have application servers in multiple data centers, you may
  consider having a :ref:`geographically distributed replica set
  <replica-set-geographical-distribution>` and using a non primary read
  preference or the :readmode:`nearest`. This allows the client to read
  from the lowest-latency members, rather than always reading from the
  primary.

- Maintaining availability during a failover.

  Use :readmode:`primaryPreferred` if you want an application to
  read from the primary under normal circumstances, but to
  allow stale reads from secondaries in an emergency. This provides a
  "read-only mode" for your application during a failover.

Counter-Indications
~~~~~~~~~~~~~~~~~~~

In general, do not use :readmode:`secondary` and
:readmode:`secondaryPreferred` to provide extra capacity.

Distributing read operations to secondaries can compromise availability
if *any* members of the set are unavailable because the other
members of the set will need to be able to handle all application
requests.

:doc:`Sharding </sharding>` increases read and write capacity by
distributing read and write operations across a group of machines,
and is often a better strategy for adding capacity.

See :doc:`/core/read-preference-mechanics` for more information
about the internal application of read preferences.

Read Preference Modes
---------------------

.. versionadded:: 2.2

.. important::

   All read preference modes except :readmode:`primary` may return
   stale data because :term:`secondaries <secondary>` replicate operations
   from the primary with some delay. Ensure that your application can
   tolerate stale data if you choose to use a non-:readmode:`primary`
   mode.

MongoDB :doc:`drivers </applications/drivers>` support five
read preference modes.

.. include:: /includes/read-preference-modes-table.rst

The syntax for specifying
the read preference mode is :api:`specific to the driver and to the
idioms of the host language <>`.

Read preference modes are also available to clients connecting to a
:term:`sharded cluster` through a :program:`mongos`. The
:program:`mongos` instance obeys specified read preferences when
connecting to the :term:`replica set` that provides each :term:`shard`
in the cluster.

In the :program:`mongo` shell, the :method:`~cursor.readPref()` cursor
method provides access to read preferences.

For more information, see :ref:`read preference background
<replica-set-read-preference-background>` and :ref:`read preference
behavior <replica-set-read-preference-behavior>`. See also the
:api:`documentation for your driver <>`.

.. index:: tag sets
.. index:: read preference; tag sets
.. _replica-set-read-preference-tag-sets:

Tag Sets
--------

Tag sets allow you to target read operations to specific members of a
replica set.

Custom read preferences and write concerns evaluate tags sets in
different ways. Read preferences consider the value of a tag when
selecting a member to read from. Write concerns ignore the value
of a tag to when selecting a member, *except* to consider whether or not
the value is unique.

You can specify tag sets with the following read preference modes:

- :readmode:`primaryPreferred`
- :readmode:`secondary`
- :readmode:`secondaryPreferred`
- :readmode:`nearest`

Tags are not compatible with mode :readmode:`primary` and, in general, only
apply when :ref:`selecting <replica-set-read-preference-behavior-member-selection>`
a :term:`secondary` member of a set for a read operation. However, the
:readmode:`nearest` read mode, when combined with a tag set, selects
the matching member with the lowest network latency. This member may be a
primary or secondary.

All interfaces use the same :ref:`member selection logic
<replica-set-read-preference-behavior-member-selection>` to choose the
member to which to direct read operations, basing the choice on read
preference mode and tag sets.

For information on configuring tag sets, see the
:doc:`/tutorial/configure-replica-set-tag-sets` tutorial.

For more information on how read preference :ref:`modes
<replica-set-read-preference-modes>` interact with tag sets, see the
:doc:`documentation for each read preference mode </reference/read-preference>`.
.. index:: replica set members; arbiters
.. _replica-set-arbiter-configuration:

===================
Replica Set Arbiter
===================

.. default-domain:: mongodb

.. start-content

An arbiter does **not** have a copy of data set and **cannot** become
a primary. Replica sets may have arbiters to add a vote in
:ref:`elections of for primary <replica-set-elections>`.  Arbiters
allow replica sets to have an uneven number of members, without the
overhead of a member that replicates data.

.. important:: Do not run an arbiter on systems that also host the
   primary or the secondary members of the replica set.

Only add an arbiter to sets with even numbers of members. If you add
an arbiter to a set with an odd number of members, the set may suffer
from tied :term:`elections <election>`. To add an arbiter, see
:doc:`/tutorial/add-replica-set-arbiter`.

.. end-content

Example
-------

.. start-content-even-votes-example

For example, in the following replica set, an arbiter allows the set
to have an odd number of votes for elections:

.. include:: /images/replica-set-four-members-add-arbiter.rst

.. end-content-even-votes-example

Security
--------

Authentication
~~~~~~~~~~~~~~

When running with :setting:`~security.authentication`, arbiters exchange credentials with
other members of the set to authenticate. MongoDB encrypts the
authentication process. The MongoDB authentication exchange is
cryptographically secure.

Arbiters, use :setting:`keyfiles <keyfile>` to authenticate to the
replica set.

Communication
~~~~~~~~~~~~~

The only communication between arbiters and other set members are:
votes during elections, heartbeats, and configuration data. These
exchanges are not encrypted.

**However**, if your MongoDB deployment uses SSL, MongoDB will encrypt
*all* communication between replica set members. See
:doc:`/tutorial/configure-ssl` for more information.

As with all MongoDB components, run arbiters on in trusted network
environments.
======================================
Replica Sets with Four or More Members
======================================

.. default-domain:: mongodb

Although the standard replica set configuration has three members you
can deploy larger sets. Add additional members to a set to increase
redundancy or to add capacity for distributing secondary read
operations.

When adding members, ensure that:

- The set has an odd number of voting members. If you have an *even*
  number of voting members, deploy an :ref:`arbiter
  <replica-set-arbiters>` so that the set has an odd number.

  The following replica set needs an arbiter to have an odd number of
  voting members.

  .. include:: /images/replica-set-four-members-add-arbiter.rst

- A replica set can have up to 12 members, [#master-slave]_ but only 7
  voting members. See :ref:`non-voting members
  <replica-set-non-voting-members>` for more information.

  The following 9 member replica set has 7 voting members and 2
  non-voting members.

  .. include:: /images/replica-set-only-seven-voting-members.rst

- Members that cannot become primary in a :term:`failover` have
  :ref:`priority 0 configuration
  <replica-set-secondary-only-members>`.

  For instance, some members that have limited resources or networking
  constraints and should never be able to become primary. Configure
  members that should not become primary to have :ref:`priority 0
  <replica-set-secondary-only-members>`. In following replica set, the
  secondary member in the third data center has a priority of 0:

  .. include:: /images/replica-set-three-data-centers.rst

- A majority of the set's members should be in your applications main
  data center.

.. seealso:: :doc:`/tutorial/deploy-replica-set`,
  :doc:`/tutorial/add-replica-set-arbiter`, and
  :doc:`/tutorial/expand-replica-set`.

.. [#master-slave]
   .. include:: /includes/fact-master-slave-workaround.rst
.. _replica-set-geographical-distribution:

=======================================
Geographically Distributed Replica Sets
=======================================

.. default-domain:: mongodb

Adding members to a replica set in multiple data centers adds
redundancy and provides fault tolerance if one data center is
unavailable. Members in additional data centers should have a
:doc:`priority of 0 </core/replica-set-priority-0-member>` to prevent
them from becoming primary.

For example: the architecture of a geographically distributed replica
set may be:

- One :term:`primary <primary>` in the main data center.

- One :term:`secondary <secondary>` member in the main data center. This
  member can become primary at any time.

- One :doc:`priority 0 </core/replica-set-priority-0-member>` member in
  a second data center. This member cannot become primary.

In the following replica set, the primary and one secondary are in *Data
Center 1*, while *Data Center 2* has a :doc:`priority 0
</core/replica-set-priority-0-member>` secondary that cannot become a
primary.

.. include:: /images/replica-set-three-members-geographically-distributed.rst

If the primary is unavailable, the replica set will elect a new
primary from *Data Center 1*. If the data centers cannot connect to
each other, the member in *Data Center 2* will not become the primary.

If *Data Center 1* becomes unavailable, you can manually recover the
data set from *Data Center 2* with minimal downtime. With sufficient
:ref:`write concern <write-concern>`, there will be no data loss.

To facilitate elections, the main data center should hold a majority
of members. Also ensure that the set has an odd number of members. If
adding a member in another data center results in a set with an even
number of members, deploy an :ref:`arbiter
<replica-set-arbiters>`. For more information on elections, see
:doc:`/core/replica-set-elections`.

.. seealso:: :doc:`/tutorial/deploy-geographically-distributed-replica-set`.
.. _replica-set-three-members:

=========================
Three Member Replica Sets
=========================

.. default-domain:: mongodb

The minimum architecture of a replica set has three members. A three
member replica set can have either three members that hold data, or
two members that hold data and an arbiter.

.. _primary-two-secondary-members:

Primary with Two Secondary Members
----------------------------------

A replica set with three members that store data has:

- One :doc:`primary </core/replica-set-primary>`.

- Two :doc:`secondary </core/replica-set-secondary>` members. Both
  secondaries can become the primary in an :doc:`election
  </core/replica-set-elections>`.

.. include:: /images/replica-set-primary-with-two-secondaries.rst

These deployments provide two complete copies of the data set at all
times in addition to the primary. These replica sets provide
additional fault tolerance and :ref:`high availability
<replica-set-failover>`. If the primary is unavailable, the replica
set elects a secondary to be primary and continues normal
operation. The old primary rejoins the set when available.

.. include:: /images/replica-set-trigger-election.rst

Primary with a Secondary and an Arbiter
---------------------------------------

A three member replica set with a two members that store data has:

- One :doc:`primary </core/replica-set-primary>`.

- One :doc:`secondary </core/replica-set-secondary>` member. The
  secondary can become primary in an :doc:`election
  </core/replica-set-elections>`.

- One :doc:`arbiter </core/replica-set-arbiter>`. The arbiter only
  votes in elections.

.. include:: /images/replica-set-primary-with-secondary-and-arbiter.rst

Since the arbiter does not hold a copy of the data, these deployments
provides only one complete copy of the data. Arbiters require fewer
resources, at the expense of more limited redundancy and fault
tolerance.

However, a deployment with a primary, secondary, and an arbiter
ensures that a replica set remains available if the primary *or* the
secondary is unavailable. If the primary is unavailable, the replica
set will elect the secondary to be primary.

.. include:: /images/replica-set-w-arbiter-trigger-election.rst

.. seealso:: :doc:`/tutorial/deploy-replica-set`.
.. _replica-set-deployment-overview:
.. _replica-set-architecture:

====================================
Replica Set Deployment Architectures
====================================

.. default-domain:: mongodb

The architecture of a :term:`replica set <replica set>` affects the
set's capacity and capability. This document provides strategies for
replica set deployments and describes common architectures.

The standard replica set deployment for production system is a
three-member replica set. These sets provide redundancy and fault
tolerance. Avoid complexity when possible, but let your application
requirements dictate the architecture.

Strategies
----------

Determine the Number of Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Add members in a replica set according to these strategies.

Deploy an Odd Number of Members
```````````````````````````````

An odd number of members ensures that the replica set is always able
to elect a primary. If you have an even number of members, add an
arbiter to get an odd number. :term:`Arbiters <arbiter>` do not store
a copy of the data and require fewer resources. As a result, you may
run an arbiter on an application server or other shared process.

.. _replica-set-architectures-consider-fault-tolerance:

Consider Fault Tolerance
````````````````````````

*Fault tolerance* for a replica set is the number of members that can
become unavailable and still leave enough members in the set to
elect a primary. In other words, it is the difference between the
number of members in the set and the majority needed to elect a
primary. Without a primary, a replica set cannot accept write
operations. Fault tolerance is an effect of replica set size, but the
relationship is not direct. See the following table:

.. list-table::
   :header-rows: 1
   :widths: 15 25 15

   * - Number of Members.

     - Majority Required to Elect a New Primary.

     - Fault Tolerance.

   * - 3

     - 2

     - 1

   * - 4

     - 3

     - 1

   * - 5

     - 3

     - 2

   * - 6

     - 4

     - 2

Adding a member to the replica set does not *always* increase the
fault tolerance. However, in these cases, additional members can
provide support for dedicated functions, such as backups or reporting.

Use Hidden and Delayed Members for Dedicated Functions
``````````````````````````````````````````````````````

Add :ref:`hidden <replica-set-hidden-members>` or :ref:`delayed
<replica-set-delayed-members>` members to support dedicated functions,
such as backup or reporting.

Load Balance on Read-Heavy Deployments
``````````````````````````````````````

In a deployment with *very* high read traffic, you can improve read
throughput by distributing reads to secondary members. As your
deployment grows, add or move members to alternate data centers to
improve redundancy and availability.

Always ensure that the main facility is able to elect a primary.

Add Capacity Ahead of Demand
````````````````````````````

The existing members of a replica set must have spare capacity to
support adding a new member. Always add new members before the
current demand saturates the capacity of the set.

.. _determine-geographic-distribution:

Determine the Distribution of Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Distribute Members Geographically
`````````````````````````````````

To protect your data if your main data center fails, keep at least one
member in an alternate data center. Set these members'
:data:`~local.system.replset.members[n].priority` to 0 to prevent them
from becoming primary.

Keep a Majority of Members in One Location
``````````````````````````````````````````

When a replica set has members in multiple data centers, network
partitions can prevent communication between data centers. To
replicate data, members must be able to communicate to other members.

In an election, members must see each other to create a
majority. To ensure that the replica set members can confirm a
majority and elect a primary, keep a majority of the set’s members in
one location.

Target Operations with Tags
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use :ref:`replica set tags <replica-set-configuration-tag-sets>` to
ensure that operations replicate to specific data centers. Tags also
support targeting read operations to specific machines.

.. seealso:: :doc:`/data-center-awareness` and
   :doc:`/core/operational-segregation`.

Use Journaling to Protect Against Power Failures
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Enable journaling to protect data against service interruptions.
Without journaling MongoDB cannot recover data after unexpected
shutdowns, including power failures and unexpected reboots.

All 64-bit versions of MongoDB after version 2.0 have journaling
enabled by default.

Replica Set Naming
------------------

.. include:: /includes/fact-unique-replica-set-names.rst

Deployment Patterns
-------------------

The following documents describe common replica set deployment
patterns. Other patterns are possible and effective depending on the
the application's requirements. If needed, combine features of each
architecture in your own deployment:

.. include:: /includes/toc/dfn-list-replica-set-architectures.rst

.. include:: /includes/toc/replica-set-architectures.rst
.. index:: replica set members; delayed
.. _replica-set-delayed-configuration:
.. _replica-set-delayed-members:
.. _replica-set-delayed-replication:

===========================
Delayed Replica Set Members
===========================

.. default-domain:: mongodb

Delayed members contain copies of a :term:`replica set's <replica
set>` data set. However, a delayed member's data set reflects an
earlier, or delayed, state of the set. For example, if the current
time is 09:52 and a member has a delay of an hour, the delayed member
has no operation more recent than 08:52.

Because delayed members are a "rolling backup" or a running
"historical" snapshot of the data set, they may help you recover from
various kinds of human error. For example, a delayed member can make
it possible to recover from unsuccessful application upgrades and
operator errors including dropped databases and collections.

Considerations
--------------

Requirements
~~~~~~~~~~~~

Delayed members:

- **Must be** :ref:`priority 0 <replica-set-secondary-only-members>`
  members. Set the priority to 0 to prevent a delayed member from
  becoming primary.

- **Should be** :ref:`hidden <replica-set-hidden-members>`
  members. Always prevent applications from seeing and querying
  delayed members.

- *do* vote in :term:`elections <election>` for primary.

Behavior
~~~~~~~~

Delayed members apply operations from the :term:`oplog` on a delay.
When choosing the amount of delay, consider that the amount of delay:

- must be is equal to or greater than your maintenance windows.

- must be *smaller* than the capacity of the oplog. For more
  information on oplog size, see :ref:`replica-set-oplog-sizing`.

Sharding
~~~~~~~~

In sharded clusters, delayed members have limited utility when the
:term:`balancer` is enabled. Because delayed members replicate chunk
migrations with a delay, the state of delayed members in a sharded
cluster are not useful for recovering to a previous state of the
sharded cluster if any migrations occur during the delay window.

Example
-------

In the following 5-member replica set, the primary and all secondaries
have copies of the data set. One member applies operations with a
delay of 3600 seconds, or an hour. This delayed member is also
*hidden* and is a *priority 0 member*.

.. include:: /images/replica-set-delayed-member.rst

Configuration
-------------

A delayed member has its
:data:`~local.system.replset.members[n].priority` equal to ``0``,
:data:`~local.system.replset.members[n].hidden` equal to ``true``, and
its :data:`~local.system.replset.members[n].slaveDelay` equal to the
number of seconds of delay:

.. code-block:: javascript

   {
      "_id" : <num>,
      "host" : <hostname:port>,
      "priority" : 0,
      "slaveDelay" : <seconds>,
      "hidden" : true
   }

To configure a delayed member, see
:doc:`/tutorial/configure-a-delayed-replica-set-member`.
.. index:: replica set; network partitions
.. index:: replica set; elections
.. index:: replica set; failover
.. _replica-set-election-internals:
.. _replica-set-elections:

=====================
Replica Set Elections
=====================

.. default-domain:: mongodb

:term:`Replica sets <replica set>` use elections to determine which
set member will become :term:`primary`. Elections occur after
initiating a replica set, and also any time the primary becomes
unavailable. The primary is the only member in the set that can accept
write operations. If a primary becomes unavailable, elections allow
the set to recover normal operations without manual
intervention. Elections are part of the :ref:`failover process
<replica-set-failover-administration>`.

.. important:: Elections are essential for independent operation of a
   replica set; however, elections take time to complete. While an
   election is in process, the replica set has no primary and cannot
   accept writes. MongoDB avoids elections unless necessary.

In the following three-member replica set, the primary is
unavailable. The remaining secondaries hold an election to choose a new
primary.

.. include:: /images/replica-set-trigger-election.rst

Factors and Conditions that Affect Elections
--------------------------------------------

Heartbeats
~~~~~~~~~~

Replica set members send heartbeats (pings) to each other every two
seconds. If a heartbeat does not return within 10 seconds, the other
members mark the delinquent member as inaccessible.

Priority Comparisons
~~~~~~~~~~~~~~~~~~~~

The :data:`~local.system.replset.members[n].priority` setting affects
elections. Members will prefer to vote for members with the highest
priority value.

Members with a priority value of ``0`` cannot become primary and do
not seek election. For details, see
:doc:`/core/replica-set-priority-0-member`.

A replica set does *not* hold an election as long as the current primary has the
highest priority value and is within 10 seconds of the latest
:term:`oplog` entry in the set. If a higher-priority member catches up to within 10 seconds of the
latest oplog entry of the current primary, the set holds an election in order to provide the
higher-priority node a chance to become primary.

Optime
~~~~~~

The :data:`optime <replSetGetStatus.members.optime>` is the timestamp of
the last operation that a member applied from the oplog. A replica set
member cannot become primary unless it has the highest (i.e. most
recent) :data:`~replSetGetStatus.members.optime` of any visible member
in the set.

Connections
~~~~~~~~~~~

A replica set member cannot become primary unless it can connect to
a majority of the members in the replica set. For the purposes of
elections, a majority refers to the total number of *votes*, rather
than the total number of members.

If you have a three-member replica set, where every member has one
vote, the set can elect a primary as long as two members can connect
to each other. If two members are unavailable, the remaining
member remains a :term:`secondary` because it cannot connect to a
majority of the set's members. If the remaining member is a
:term:`primary` and two members become unavailable, the primary steps
down and becomes and secondary.

Network Partitions
~~~~~~~~~~~~~~~~~~

Network partitions affect the formation of a majority for an
election. If a primary steps down and neither portion of the replica
set has a majority the set will **not** elect a new primary. The
replica set becomes read-only.

To avoid this situation, place a majority of instances in one data
center and a minority of instances in any other data centers combined.

Election Mechanics
------------------

Election Triggering Events
~~~~~~~~~~~~~~~~~~~~~~~~~~

Replica sets hold an election any time there is no
primary. Specifically, the following:

- the initiation of a new replica set.

- a secondary loses contact with a primary. Secondaries call for
  elections when they cannot see a primary.

- a primary steps down.

.. note::

   :doc:`Priority 0 members </core/replica-set-priority-0-member>`, do
   not trigger elections, even when they cannot connect to the
   primary.

A primary will step down:

- after receiving the :dbcommand:`replSetStepDown`
  command.

- if one of the current secondaries is eligible for election *and*
  has a higher priority.

- if primary cannot contact a majority of the members of the replica
  set.

In some cases, modifying a replica set's configuration will trigger an
election by modifying the set so that the primary must step down.

.. important:: When a primary steps down, it closes all open client
   connections, so that clients don't attempt to write data to a
   secondary. This helps clients maintain an accurate view of the
   replica set and helps prevent :term:`rollbacks <rollback>`.

Participation in Elections
~~~~~~~~~~~~~~~~~~~~~~~~~~

Every replica set member has a *priority* that helps determine its
eligibility to become a :term:`primary`. In an election, the replica
set elects an eligible member with the highest
:data:`~local.system.replset.members[n].priority` value as primary. By
default, all members have a priority of ``1`` and have an equal chance
of becoming primary. In the default, all members also can trigger an
election.

You can set the :data:`~local.system.replset.members[n].priority`
value to weight the election in favor of a particular member or group
of members. For example, if you have a :doc:`geographically
distributed replica set
</core/replica-set-architecture-geographically-distributed>`, you can
adjust priorities so that only members in a specific data center can
become primary.

The first member to receive the majority of votes becomes primary.
By default, all members have a single vote, unless you modify the
:data:`~local.system.replset.members[n].votes` setting. :doc:`Non-voting
members </tutorial/configure-a-non-voting-replica-set-member>` have
:data:`~local.system.replset.members[n].votes` value of ``0``. All
other members have ``1`` vote.

.. include:: /includes/members-used-to-allow-multiple-votes.rst

The :data:`~replSetGetStatus.members.state` of a member also affects
its eligibility to vote. Only members in the following states can
vote: ``PRIMARY``, ``SECONDARY``, ``RECOVERING``, ``ARBITER``, and
``ROLLBACK``.

.. important:: Do not alter the number of votes in a replica set to
   control the outcome of an election. Instead, modify the
   :data:`~local.system.replset.members[n].priority` value.

.. _replica-set-vetos:

Vetoes in Elections
~~~~~~~~~~~~~~~~~~~

All members of a replica set can veto an election, including
:ref:`non-voting members <replica-set-non-voting-members>`. A member
will veto an election:

- If the member seeking an election is not a member of the voter's set.

- If the member seeking an election is not up-to-date with the most
  recent operation accessible in the replica set.

- If the member seeking an election has a lower priority than another member
  in the set that is also eligible for election.

- If a :ref:`priority 0 member
  <replica-set-secondary-only-members>` [#imply-secondary-only]_ is
  the most current member at the time of the election. In this case, another
  eligible member of the set will catch up to the state of this
  secondary member and then attempt to become primary.

- If the current primary has more recent operations
  (i.e. a higher :data:`optime <replSetGetStatus.members.optime>`) than
  the member seeking election, from the perspective of the voting
  member.

- If the current primary has the same or more recent operations
  (i.e. a higher or equal :data:`optime
  <replSetGetStatus.members.optime>`) than the member seeking
  election.

.. [#imply-secondary-only] Remember that :ref:`hidden
   <replica-set-hidden-members>` and :ref:`delayed
   <replica-set-delayed-members>` imply :ref:`priority 0
   <replica-set-secondary-only-members>` configuration.

.. index:: replica set members; non-voting
.. _replica-set-non-voting-configuration:
.. _replica-set-non-voting-members:

Non-Voting Members
------------------

.. default-domain:: mongodb

Non-voting members hold copies of the replica set's data and can
accept read operations from client applications. Non-voting members
do not vote in elections, but **can** :ref:`veto <replica-set-vetos>`
an election and become primary.

Because a replica set can have up to 12 members but only up to seven
voting members, non-voting members allow a replica set to have more
than seven members.

For instance, the following nine-member replica set has seven voting
members and two non-voting members.

.. include:: /images/replica-set-only-seven-voting-members.rst

A non-voting member has a
:data:`~local.system.replset.members[n].votes` setting equal to ``0``
in its member configuration:

.. code-block:: javascript

   {
     "_id" : <num>
     "host" : <hostname:port>,
     "votes" : 0
   }

.. important:: Do **not** alter the number of votes to control which
   members will become primary. Instead, modify the
   :data:`~local.system.replset.members[n].priority` option. *Only*
   alter the number of votes in exceptional cases. For example, to
   permit more than seven members.

When possible, all members should have only one vote. Changing the
number of votes can cause ties, deadlocks, and the wrong members to
become primary.

To configure a non-voting member, see
:doc:`/tutorial/configure-a-non-voting-replica-set-member`.
.. index:: replica set members; hidden
.. _replica-set-hidden-configuration:
.. _replica-set-hidden-members:

==========================
Hidden Replica Set Members
==========================

.. default-domain:: mongodb

A hidden member maintains a copy of the :term:`primary's <primary>`
data set but is **invisible** to client applications. Hidden members
are good for workloads with different usage patterns from the other
members in the :term:`replica set`. Hidden members are always
:ref:`priority 0 members <replica-set-secondary-only-members>` and
**cannot become primary**. The :method:`db.isMaster()` method does not
display hidden members. Hidden members, however, **do vote** in
:ref:`elections <replica-set-elections>`.

In the following five-member replica set, all four secondary members
have copies of the primary's data set, but one of the secondary members
is hidden.

.. include:: /images/replica-set-hidden-member.rst

Behavior
--------

Read Operations
~~~~~~~~~~~~~~~

Clients will not distribute reads with the appropriate :doc:`read
preference </core/read-preference>` to hidden members. As a result, these
members receive no traffic other than basic replication. Use hidden
members for dedicated tasks such as reporting and
backups. :doc:`Delayed members </core/replica-set-delayed-member>`
should be hidden.

In a sharded cluster, :program:`mongos` do not interact with hidden
members.

Voting
~~~~~~

Hidden members *do* vote in replica set elections. If you stop a
hidden member, ensure that the set has an active majority or the
:term:`primary` will step down.

For the purposes of backups, you can avoid stopping a hidden member
with the :method:`db.fsyncLock()` and :method:`db.fsyncUnlock()`
operations to flush all writes and lock the :program:`mongod` instance
for the duration of the backup operation.

Further Reading
---------------

For more information about backing up MongoDB databases,
see :doc:`/core/backups`. To configure a hidden member, see
:doc:`/tutorial/configure-a-hidden-replica-set-member`.
.. index:: pair: replica set; failover
.. _replica-set-failover-administration:
.. _replica-set-failover:
.. _failover:

=============================
Replica Set High Availability
=============================

.. default-domain:: mongodb

:term:`Replica sets <replica set>` provide high availability using
automatic :term:`failover`. Failover allows a :term:`secondary`
members to become :term:`primary` if primary is unavailable. Failover,
in most situations does not require manual intervention.

Replica set members keep the same data set but are otherwise
independent. If the primary becomes unavailable, the replica set holds
an :doc:`election </core/replica-set-elections>` to select a new
primary. In some situations, the failover process may require
a :doc:`rollback </core/replica-set-rollbacks>`. [#rollback-automatic]_

The deployment of a replica set affects the outcome of failover
situations. To support effective failover, ensure that one facility
can elect a primary if needed. Choose the facility that hosts the core
application systems to host the majority of the replica set. Place a
majority of voting members and all the members that can become primary
in this facility. Otherwise, network partitions could prevent the set
from being able to form a majority.

.. [#rollback-automatic] Replica sets remove "rollback" data when
   needed without intervention. Administrators must apply or discard
   rollback data manually.

Failover Processes
------------------

The replica set recovers from the loss of a primary by holding an
election. Consider the following:

.. include:: /includes/toc/dfn-list-replica-set-high-availability.rst

.. include:: /includes/toc/replica-set-high-availability.rst
===================
Replica Set Members
===================

.. default-domain:: mongodb

A *replica set* in MongoDB is a group of :program:`mongod` processes
that provide redundancy and high availability. The members of a
replica set are:

:ref:`replica-set-primary-member`.
   The *primary* receives all write operations.

:ref:`replica-set-secondary-members`.
   Secondaries replicate operations from the primary to maintain an
   identical data set. Secondaries may have additional configurations
   for special usage profiles. For example, secondaries may be
   :ref:`non-voting <replica-set-non-voting-members>` or
   :ref:`priority 0 <replica-set-secondary-only-members>`.

You can also maintain an :ref:`arbiter <replica-set-arbiters>` as part
of a replica set. Arbiters do not keep a copy of the data. However,
arbiters play a role in the elections that select a primary if the
current primary is unavailable.

A replica set can have up to 12 members. [#master-slave]_ However, only
7 members can vote at a time.

The minimum requirements for a replica set are: A :ref:`primary
<replica-set-primary-member>`, a :ref:`secondary
<replica-set-secondary-members>`, and an :ref:`arbiter
<replica-set-arbiters>`. Most deployments, however, will keep three
members that store data: A :ref:`primary <replica-set-primary-member>`
and two :ref:`secondary members <replica-set-secondary-members>`.

.. only:: (website or singlehtml)

   .. _replica-set-primary-member:

   Primary
   -------

   .. include:: /core/replica-set-primary.txt
      :start-after: start-content
      :end-before: start-content-election-example

.. class:: hidden

   .. toctree::
      :titlesonly:

      /core/replica-set-primary

.. only:: (website or singlehtml)

   .. _replica-set-secondary-members:

   Secondaries
   -----------

   .. include:: /core/replica-set-secondary.txt
      :start-after: start-content
      :end-before: start-content-election-example

   .. include:: /core/replica-set-secondary.txt
      :start-after: end-content-election-example
      :end-before: end-content

.. class:: hidden

   .. toctree::
      :titlesonly:

      /core/replica-set-secondary

.. only:: (website or singlehtml)

   .. _replica-set-arbiters:

   Arbiter
   -------

   .. include:: /core/replica-set-arbiter.txt
      :start-after: start-content
      :end-before: end-content

.. class:: hidden

   .. toctree::
      :titlesonly:

      /core/replica-set-arbiter

.. [#master-slave]
   .. include:: /includes/fact-master-slave-workaround.rst
=================
Replica Set Oplog
=================

.. default-domain:: mongodb

The :term:`oplog` (operations log) is a special :term:`capped
collection` that keeps a rolling record of all operations that modify
the data stored in your databases. MongoDB applies database operations
on the :term:`primary` and then records the operations on the
primary's oplog. The :term:`secondary` members then copy and apply
these operations in an asynchronous process. All replica set members
contain a copy of the oplog, allowing them to maintain the current
state of the database.

To facilitate replication, all replica set members send heartbeats
(pings) to all other members. Any member can import oplog entries from
any other member.

Whether applied once or multiple times to the target dataset, each
operation in the oplog produces the same results, i.e. each operation
in the oplog is :term:`idempotent`. For proper replication operations,
entries in the oplog must be idempotent:

- initial sync
- post-rollback catch-up
- sharding chunk migrations

.. _replica-set-oplog-sizing:

Oplog Size
----------

When you start a replica set member for the first time, MongoDB creates
an oplog of a default size. The size depends on the architectural
details of your operating system.

In most cases, the default oplog size is sufficient. For example, if an
oplog is 5% of free disk space and fills up in 24 hours of operations, then
secondaries can stop copying entries from the oplog for up to 24 hours
without becoming too stale to continue replicating. However, most
replica sets have much lower operation volumes, and their oplogs can
hold much higher numbers of operations.

Before :program:`mongod` creates an oplog, you can specify its size
with the :setting:`~replication.oplogSizeMB` option. However, after you have started
a replica set member for the first time, you can only change the size
of the oplog using the :doc:`/tutorial/change-oplog-size` procedure.

..  Actual oplog sizing as of 2012-07-02:
..  32 bit systems = ~48 megabytes
..  64 bit = larger of 5% of disk or ~1 gigabyte
..  64 bit OS X = ~183 megabytes

By default, the size of the oplog is as follows:

- For 64-bit Linux, Solaris, FreeBSD, and Windows systems, MongoDB
  allocates 5% of the available free disk space to the oplog. If this
  amount is smaller than a gigabyte, then MongoDB allocates 1
  gigabyte of space.

- For 64-bit OS X systems, MongoDB allocates 183 megabytes of space to
  the oplog.

- For 32-bit systems, MongoDB allocates about 48 megabytes of space to
  the oplog.

Workloads that Might Require a Larger Oplog Size
------------------------------------------------

If you can predict your replica set's workload to resemble one of the
following patterns, then you might want to create an oplog that is
larger than the default. Conversely, if your application predominantly
performs reads with a minimal amount of write operations, a smaller oplog
may be sufficient.

The following workloads might require a larger oplog size.

Updates to Multiple Documents at Once
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The oplog must translate multi-updates into individual operations in
order to maintain :term:`idempotency <idempotent>`. This can use a great
deal of oplog space without a corresponding increase in data size or disk
use.

Deletions Equal the Same Amount of Data as Inserts
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you delete roughly the same amount of data as you insert, the
database will not grow significantly in disk use, but the size
of the operation log can be quite large.

Significant Number of In-Place Updates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If a significant portion of the workload is in-place updates, the database
records a large number of operations but does not change the quantity of
data on disk.

Oplog Status
------------

To view oplog status, including the size and the time range of
operations, issue the :method:`rs.printReplicationInfo()` method. For
more information on oplog status, see
:ref:`replica-set-troubleshooting-check-oplog-size`.

Under various exceptional situations, updates to a :term:`secondary's
<secondary>` oplog might lag behind the desired performance time.  Use
:method:`db.getReplicationInfo()` from a secondary member and the
:doc:`replication status </reference/method/db.getReplicationInfo>`
output to assess the current state of replication and determine if
there is any unintended replication delay.

See :ref:`Replication Lag <replica-set-replication-lag>` for more
information.
:orphan:

===================
Replica Set Primary
===================

.. start-content

The primary is the only member in the replica set that receives write
operations. MongoDB applies write operations on the :term:`primary` and
then records the operations on the primary's :doc:`oplog
</core/replica-set-oplog>`. :ref:`Secondary
<replica-set-secondary-members>` members replicate this log and apply
the operations to their data sets.

In the following three-member replica set, the primary accepts all
write operations. Then the secondaries replicate the oplog to apply to
their data sets.

.. include:: /images/replica-set-read-write-operations-primary.rst

All members of the replica set can accept read operations. However, by
default, an application directs its read operations to the primary
member. See :doc:`/core/read-preference` for details on changing the
default read behavior.

The replica set can have at most one primary. If the current primary
becomes unavailable, an election determines the new primary. See
:doc:`/core/replica-set-elections` for more details.

.. start-content-election-example

In the following 3-member replica set, the primary becomes unavailable.
This triggers an election which selects one of the remaining
secondaries as the new primary.

.. include:: /images/replica-set-trigger-election.rst
.. _replica-set-secondary-only-members:

==============================
Priority 0 Replica Set Members
==============================

.. default-domain:: mongodb

A *priority 0* member is a secondary that **cannot** become
:term:`primary`. *Priority 0* members cannot *trigger*
:term:`elections <election>`.  Otherwise these members function as
normal secondaries. A *priority 0* member maintains a copy of the data
set, accepts read operations, and votes in elections. Configure a
*priority 0* member to prevent :term:`secondaries <secondary>` from
becoming primary, which is particularly useful in multi-data center
deployments.

In a three-member replica set, in one data center hosts the primary
and a secondary. A second data center hosts one *priority 0* member
that cannot become primary.

.. include:: /images/replica-set-three-members-geographically-distributed.rst

Priority 0 Members as Standbys
------------------------------

A *priority 0* member can function as a standby. In some replica sets,
it might not be possible to add a new member in a reasonable amount of
time. A standby member keeps a current copy of the data to be able to
replace an unavailable member.

In many cases, you need not set standby to *priority 0*. However, in
sets with varied hardware or :ref:`geographic distribution
<replica-set-geographical-distribution>`, a *priority 0* standby
ensures that only qualified members become primary.

A *priority 0* standby may also be valuable for some members of a set
with different hardware or workload profiles. In these cases, deploy a
member with *priority 0* so it can't become primary. Also consider
using an :ref:`hidden member <replica-set-hidden-members>` for this
purpose.

If your set already has seven voting members, also configure the
member as :ref:`non-voting <replica-set-non-voting-members>`.

Priority 0 Members and Failover
-------------------------------

When configuring a *priority 0* member, consider potential failover
patterns, including all possible network partitions. Always ensure
that your main data center contains both a quorum of voting members
and contains members that are eligible to be primary.

Configuration
-------------

To configure a *priority 0* member, see
:doc:`/tutorial/configure-secondary-only-replica-set-member`.
.. index:: rollbacks
   single: replica set; rollbacks
   single: consistency; rollbacks

.. _replica-set-rollbacks:
.. _replica-set-rollback:

=====================================
Rollbacks During Replica Set Failover
=====================================

.. default-domain:: mongodb

A rollback reverts write operations on a former :term:`primary` when the
member rejoins its :term:`replica set` after a :term:`failover`.
A rollback is necessary only if the primary had accepted write
operations that the :term:`secondaries <secondary>` had **not**
successfully replicated before the primary stepped down. When the
primary rejoins the set as a secondary, it reverts, or "rolls back," its
write operations to maintain database consistency with the other
members.

MongoDB attempts to avoid rollbacks, which should be rare. When a
rollback does occur, it is often the result of a network
partition. Secondaries that can not keep up with the throughput of
operations on the former primary, increase the size and impact of the
rollback.

A rollback does *not* occur if the write operations replicate to another
member of the replica set before the primary steps down *and* if that
member remains available and accessible to a majority of the replica
set.

Collect Rollback Data
---------------------

When a rollback does occur, administrators must decide whether to
apply or ignore the rollback data. MongoDB writes the rollback data to
:term:`BSON` files in the ``rollback/`` folder under the database's
:setting:`~storage.dbPath` directory.  The names of rollback files have the
following form:

.. code-block:: none

   <database>.<collection>.<timestamp>.bson

For example:

.. code-block:: none

   records.accounts.2011-05-09T18-10-04.0.bson

Administrators must apply rollback data manually after the member
completes the rollback and returns to secondary status. Use
:doc:`bsondump </reference/program/bsondump>` to read the contents of
the rollback files. Then use :program:`mongorestore` to apply the
changes to the new primary.

Avoid Replica Set Rollbacks
---------------------------

To prevent rollbacks, use :ref:`replica acknowledged write concern
<write-concern-replica-acknowledged>` to guarantee that the write
operations propagate to the members of a replica set.

Rollback Limitations
--------------------

A :program:`mongod` instance will not rollback more than 300
megabytes of data. If your system must rollback more than 300
megabytes, you must manually intervene to recover the data. If
this is the case, the following line will appear in your
:program:`mongod` log:

.. code-block:: none

   [replica set sync] replSet syncThread: 13410 replSet too much data to roll back

In this situation, save the data directly or force the member to perform
an initial sync. To force initial sync, sync from a "current" member of
the set by deleting the content of the :setting:`~storage.dbPath` directory for
the member that requires a larger rollback.

.. seealso:: :doc:`/core/replica-set-high-availability` and
   :doc:`/core/replica-set-elections`.
=============================
Replica Set Secondary Members
=============================

.. default-domain:: mongodb

.. start-content

A secondary maintains a copy of the :term:`primary's <primary>` data
set. To replicate data, a secondary applies operations from the
primary's :doc:`oplog </core/replica-set-oplog>` to its own data set
in an asynchronous process. A replica set can have one or more
secondaries.

The following three-member replica set has two secondary
members. The secondaries replicate the primary's oplog and apply
the operations to their data sets.

.. include:: /images/replica-set-primary-with-two-secondaries.rst

Although clients cannot write data to secondaries, clients can read
data from secondary members. See :doc:`/core/read-preference` for more
information on how clients direct read operations to replica sets.

A secondary can become a primary.
If the current primary becomes unavailable, the replica set
holds an :term:`election` to choose which of the secondaries
becomes the new primary.

.. start-content-election-example

In the following three-member replica set, the primary becomes unavailable.
This triggers an election where one of the remaining
secondaries becomes the new primary.

.. include:: /images/replica-set-trigger-election.rst

.. end-content-election-example

See
:doc:`/core/replica-set-elections` for more details.

You can configure a secondary member for a specific purpose. You can
configure a secondary to:

- Prevent it from becoming a primary in an election, which allows it to
  reside in a secondary data center or to serve as a cold standby. See
  :doc:`/core/replica-set-priority-0-member`.

- Prevent applications from reading from it, which allows it to run applications
  that require separation from normal traffic. See
  :doc:`/core/replica-set-hidden-member`.

- Keep a running "historical" snapshot for use in recovery from
  certain errors, such as unintentionally deleted databases. See
  :doc:`/core/replica-set-delayed-member`.

.. end-content

.. class:: hidden

   .. toctree::
      :titlesonly:

      /core/replica-set-priority-0-member
      /core/replica-set-hidden-member
      /core/replica-set-delayed-member
================================
Replica Set Data Synchronization
================================

.. default-domain:: mongodb

In order to maintain up-to-date copies of the shared data set, members
of a replica set :term:`sync` or replicate data from other
members. MongoDB uses two forms of data synchronization: :ref:`initial
sync <replica-set-initial-sync>` to populate new members with the full
data set, and replication to apply ongoing changes to the entire
data set.

.. _replica-set-initial-sync:

Initial Sync
------------

Initial sync copies all the data from one member of the replica set to
another member. A member uses initial sync when the member has no data,
such as when the member is new, or when the member has data but is
missing a history of the set's replication.

When you perform an initial sync, MongoDB does the following:

#. Clones all databases. To clone, the :program:`mongod` queries every
   collection in each source database and inserts all data into its
   own copies of these collections.  At this time, _id indexes are also built.

#. Applies all changes to the data set. Using the oplog from the
   source, the :program:`mongod` updates its data set to reflect the
   current state of the replica set.

#. Builds all indexes on all collections (except _id indexes, which were already completed).

   When the :program:`mongod` finishes building all index builds, the member
   can transition to a normal state, i.e. :term:`secondary`.

To perform an initial sync, see
:doc:`/tutorial/resync-replica-set-member`.

.. _replica-set-replication:

Replication
-----------

Replica set members replicate data continuously after the initial
sync. This process keeps the members up to date with all changes to
the replica set's data. In most cases, secondaries synchronize from
the primary. Secondaries may automatically change their *sync targets*
if needed based on changes in the ping time and state of other members'
replication.

.. COMMENT: 2013-05-23 > Removed following internal mechanics information.

   In MongoDB 2.0, secondaries will change sync targets only if the
   connection the sync target drops or produces an error.
   Secondaries will stop syncing from a member if
   the connection used to poll oplog entries is unresponsive for 30
   seconds. If a connection times out, the member may select a new
   member to sync from.

   If you have two secondary members in one data center and a primary in
   a second facility, and if you start all three instances at roughly
   the same time (i.e. with no existing data sets or oplog), both
   secondaries will likely sync from the primary, as neither secondary
   has more recent oplog entries.
   If you restart one of the secondaries, then when it rejoins the set
   it will likely begin syncing from the other secondary because of
   proximity.

   If you have a primary in one facility and a secondary in an
   alternate facility, and if you add another secondary to the alternate
   facility, the new secondary will likely sync from the existing
   secondary because it is closer than the primary.

.. END-COMMENT

For a member to sync from another, the
:data:`~local.system.replset.members[n].buildIndexes` setting for both
members must have the same value/
:data:`~local.system.replset.members[n].buildIndexes` must be either
``true`` or ``false`` for both members.

Beginning in version 2.2, secondaries avoid syncing from
:ref:`delayed members <replica-set-delayed-members>` and :ref:`hidden
members <replica-set-hidden-members>`.

Validity and Durability
-----------------------

.. TODO continue

In a replica set, only the primary can accept write operations. Writing
only to the primary provides :term:`strict consistency` among members.

:term:`Journaling <journal>` provides single-instance write durability.
Without journaling, if a MongoDB instance terminates
ungracefully, you must assume that the database is in an invalid state.

.. _replica-set-internals-multi-threaded-replication:

Multithreaded Replication
-------------------------

MongoDB applies write operations in batches using multiple threads to
improve concurrency. MongoDB groups batches by namespace and applies
operations using a group of threads, but always applies the write
operations to a namespace in order.

While applying a batch, MongoDB blocks all reads. As a result,
secondaries can never return data that reflects a state that never
existed on the primary.

Pre-Fetching Indexes to Improve Replication Throughput
------------------------------------------------------

To help improve the performance of applying oplog entries, MongoDB
fetches memory pages that hold affected data and indexes. This
*pre-fetch* stage minimizes the amount of time MongoDB holds the write
lock while applying oplog entries. By default, secondaries will
pre-fetch all :ref:`indexes`.

Optionally, you can disable all pre-fetching or only pre-fetch
the index on the ``_id`` field. See the :setting:`replIndexPrefetch`
setting for more information.
==============================
Write Concern for Replica Sets
==============================

.. default-domain:: mongodb

.. TODO: connect this to the actual write concern documentation and add an intro

MongoDB's built-in :doc:`write concern </core/write-concern>` confirms
the success of write operations to a :term:`replica set's <replica
set>` :term:`primary`. Write concern uses the :dbcommand:`getLastError`
command after write operations to return an object with error
information or confirmation that there are no errors.

From the perspective of a client application, whether a MongoDB
instance is running as a single server (i.e. "standalone") or a
:term:`replica set` is transparent. However, replica sets offer some
configuration options for write and read operations. [#sharded-clusters]_

.. [#sharded-clusters] :term:`Sharded clusters <sharded cluster>` where the
   shards are also replica sets provide the same configuration options
   with regards to write and read operations.

.. TODO: remove footnote

Verify Write Operations
~~~~~~~~~~~~~~~~~~~~~~~

The default write concern confirms write operations only on the
primary.  You can configure write concern to confirm write operations
to additional replica set members as well by issuing the
:dbcommand:`getLastError` command with the ``w`` option.

.. TODO: provide an example of how to do this (i.e. write the code block)

The ``w`` option confirms that write operations have replicated to the
specified number of replica set members, including the primary. You can
either specify a number or specify ``majority``, which ensures the write
propagates to a majority of set members.

.. include:: /images/crud-write-concern-w2.rst

If you specify a ``w`` value greater than the number of members that
hold a copy of the data (i.e., greater than the number of
non-:term:`arbiter` members), the operation blocks until those members
become available. This can cause the operation to block forever. To
specify a timeout threshold for the :dbcommand:`getLastError` operation,
use the ``wtimeout`` argument. A ``wtimeout`` value of ``0``
means that the operation will never time out.

See :ref:`getLastError Examples <gle-examples>` for example
invocations.

Modify Default Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can configure your own "default" :dbcommand:`getLastError`
behavior for a replica set. Use the
:data:`~local.system.replset.settings.getLastErrorDefaults` setting in
the :doc:`replica set configuration
</reference/replica-configuration>`. The following sequence of
commands creates a configuration that waits for the write operation to
complete on a majority of the set members before returning:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.settings = {}
   cfg.settings.getLastErrorDefaults = {w: "majority"}
   rs.reconfig(cfg)

The :data:`~local.system.replset.settings.getLastErrorDefaults`
setting affects only those :dbcommand:`getLastError` commands that
have *no* other arguments.

.. note::

   Use of insufficient write concern can lead to :ref:`rollbacks
   <replica-set-rollbacks>` in the case of :ref:`replica set failover
   <replica-set-failover>`. Always ensure that your operations have
   specified the required write concern for your application.

.. seealso:: :ref:`write-operations-write-concern` and
   :ref:`connections-write-concern`

Custom Write Concerns
~~~~~~~~~~~~~~~~~~~~~

You can use replica set tags to create custom write concerns using the
:data:`~local.system.replset.settings.getLastErrorDefaults` and
:data:`~local.system.replset.settings.getLastErrorModes` replica set
settings.

.. note::

   Custom write concern modes specify the field name and a number of
   *distinct* values for that field. By contrast, read preferences use
   the value of fields in the tag document to direct read operations.

   In some cases, you may be able to use the same tags for read
   preferences and write concerns; however, you may need to create
   additional tags for write concerns depending on the requirements of
   your application.

Single Tag Write Concerns
~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a five member replica set, where each member has one of the
following tag sets:

.. code-block:: javascript

   { "use": "reporting" }
   { "use": "backup" }
   { "use": "application" }
   { "use": "application" }
   { "use": "application" }

You could create a custom write concern mode that will ensure that
applicable write operations will not return until members with two
different values of the ``use`` tag have acknowledged the write
operation. Create the mode with the following sequence of operations
in the :program:`mongo` shell:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.settings = { getLastErrorModes: { use2: { "use": 2 } } }
   rs.reconfig(cfg)

.. these examples need to be better so that they avoid overwriting
   getLastErrorModes upon repetition (i.e. they don't $push documents
   to getLastErrorModes.)

To use this mode pass the string ``use2`` to the ``w`` option of
:dbcommand:`getLastError` as follows:

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "use2" } )

Specific Custom Write Concerns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have a three member replica with the following tag sets:

.. code-block:: javascript

   { "disk": "ssd" }
   { "disk": "san" }
   { "disk": "spinning" }

You cannot specify a custom
:data:`~local.system.replset.settings.getLastErrorModes` value to
ensure that the write propagates to the ``san`` before
returning. However, you may implement this write concern policy by
creating the following additional tags, so that the set resembles the
following:

.. code-block:: javascript

   { "disk": "ssd" }
   { "disk": "san", "disk.san": "san" }
   { "disk": "spinning" }

Then, create a custom
:data:`~local.system.replset.settings.getLastErrorModes` value, as
follows:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.settings = { getLastErrorModes: { san: { "disk.san": 1 } } }
   rs.reconfig(cfg)

.. these examples need to be better so that they avoid overwriting
   getLastErrorModes upon repetition (i.e. they don't $push documents
   to getLastErrorModes.)

To use this mode pass the string ``san`` to the ``w`` option of
:dbcommand:`getLastError` as follows:

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "san" } )

This operation will not return until a replica set member with the tag
``disk.san`` returns.

You may set a custom write concern mode as the default write concern
mode using :data:`~local.system.replset.settings.getLastErrorDefaults`
replica set as in the following setting:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.settings.getLastErrorDefaults = { ssd: 1 }
   rs.reconfig(cfg)

.. seealso:: :ref:`replica-set-configuration-tag-sets` for further
   information about replica set reconfiguration and tag sets.
.. _replication-introduction:

========================
Replication Introduction
========================

.. default-domain:: mongodb

Replication is the process of synchronizing data across multiple
servers.

Purpose of Replication
----------------------

Replication provides redundancy and increases data availability. With
multiple copies of data on different database servers, replication
protects a database from the loss of a single server. Replication also
allows you to recover from hardware failure and service
interruptions. With additional copies of the data, you can dedicate
one to disaster recovery, reporting, or backup.

In some cases, you can use replication to increase read
capacity. Clients have the ability to send read and write operations
to different servers. You can also maintain copies in different data
centers to increase the locality and availability of data for distributed
applications.

Replication in MongoDB
----------------------

A replica set is a group of :program:`mongod` instances that host the
same data set. One :program:`mongod`, the primary, receives all write
operations. All other instances, secondaries, apply operations from
the primary so that they have the same data set.

The **primary** accepts all write operations from clients. Replica set
can have only one primary. Because only one member can accept write
operations, replica sets provide **strict consistency**. To support
replication, the primary logs all changes to its data sets in its
:doc:`oplog </core/replica-set-oplog>`. See :doc:`primary
</core/replica-set-primary>` for more information.

.. include:: /images/replica-set-read-write-operations-primary.rst

The **secondaries** replicate the primary's oplog and apply the
operations to their data sets. Secondaries' data sets reflect the
primary's data set. If the primary is unavailable, the replica set
will elect a secondary to be primary. By default, clients read from
the primary, however, clients can specify a :doc:`read preferences
</core/read-preference>` to send read operations to secondaries. See
:doc:`secondaries </core/replica-set-secondary>` for more information

.. include:: /images/replica-set-primary-with-two-secondaries.rst

You may add an extra :program:`mongod` instance a replica set as an
**arbiter**. Arbiters do not maintain a data set. Arbiters only exist
to vote in elections. If your replica set has an even number of
members, add an arbiter to obtain a majority of votes in an election
for primary. Arbiters do not require dedicated hardware. See
:doc:`arbiter </core/replica-set-arbiter>` for more information.

.. include:: /images/replica-set-primary-with-secondary-and-arbiter.rst

.. note::

   An **arbiter** will always be an arbiter. A **primary** may step
   down and become a **secondary**. A **secondary** may become the
   primary during an election.

Asynchronous Replication
~~~~~~~~~~~~~~~~~~~~~~~~

Secondaries apply operations from the primary asynchronously. By
applying operations after the primary, sets can continue to function
without some members. However, as a result secondaries may not return
the most current data to clients.

See :doc:`/core/replica-set-oplog` and :doc:`/core/replica-set-sync`
for more information. See :doc:`/core/read-preference` for more on
read operations and secondaries.

Automatic Failover
~~~~~~~~~~~~~~~~~~

When a primary does not communicate with the other members of the set
for more than 10 seconds, the replica set will attempt to
select another member to become the new primary. The first secondary
that receives a majority of the votes becomes primary.

.. include:: /images/replica-set-trigger-election.rst

See :doc:`/core/replica-set-elections` and
:doc:`/core/replica-set-rollbacks` for more information.

Additional Features
~~~~~~~~~~~~~~~~~~~

Replica sets provide a number of options to support application
needs. For example, you may deploy a replica set with :doc:`members in
multiple data centers
</core/replica-set-architecture-geographically-distributed>`, or
control the outcome of elections by adjusting the
:data:`~local.system.replset.members[n].priority` of some
members. Replica sets also support dedicated members for reporting,
disaster recovery, or backup functions.

See :doc:`/core/replica-set-priority-0-member`,
:doc:`/core/replica-set-hidden-member` and
:doc:`/core/replica-set-delayed-member` for more information.
.. index:: replica set; sync

.. _replica-set-syncing:

=====================
Replication Processes
=====================

.. default-domain:: mongodb

Members of a :term:`replica set` replicate data continuously. First, a
member uses *initial sync* to capture the data set. Then the member
continuously records and applies every operation that modifies the data
set. Every member records operations in its :doc:`oplog
</core/replica-set-oplog>`, which is a :term:`capped collection`.

.. include:: /includes/toc/dfn-list-replica-set-processes.rst

.. include:: /includes/toc/replica-set-processes.rst
====================
Replication Concepts
====================

.. default-domain:: mongodb

These documents describe and provide examples of replica set
operation, configuration, and behavior. For an overview of
replication, see :doc:`/core/replication-introduction`. For
documentation of the administration of replica sets, see
:doc:`/administration/replica-sets`. The :doc:`/reference/replication`
documents commands and operations specific to replica sets.

.. include:: /includes/toc/dfn-list-spec-replication-core-landing.rst

.. include:: /includes/toc/replication-core-landing.rst
===================================
Security and MongoDB API Interfaces
===================================

.. default-domain:: mongodb

The following section contains strategies to limit risks related to
MongoDB's available interfaces including JavaScript, HTTP, and REST
interfaces.

JavaScript and the Security of the ``mongo`` Shell
--------------------------------------------------

The following JavaScript evaluation behaviors of the :program:`mongo`
shell represents risk exposures.

JavaScript Expression or JavaScript File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :program:`mongo` program can evaluate JavaScript expressions using
the command line :option:`--eval <mongo --eval>` option. Also, the
:program:`mongo` program can evaluate a JavaScript file (``.js``)
passed directly to it (e.g. ``mongo someFile.js``).

Because the :program:`mongo` program evaluates the JavaScript directly,
inputs should only come from trusted sources.

``.mongorc.js`` File
~~~~~~~~~~~~~~~~~~~~

If a ``.mongorc.js`` file exists [#mongorc-location]_, the :program:`mongo` shell will
evaluate a ``.mongorc.js`` file before starting. You can disable this
behavior by passing the :option:`mongo --norc` option.

.. [#mongorc-location] On Linux and Unix systems, :program:`mongo`
   reads the :file:`.mongorc.js` file from :file:`{$HOME}/.mongorc.js`
   (i.e. :file:`~/.mongorc.js`). On Windows, :program:`mongo.exe` reads
   the :file:`.mongorc.js` file from :file:`{%HOME%}\.mongorc.js` or
   :file:`{%HOMEDRIVE%}\{%HOMEPATH%}\.mongorc.js`.

HTTP Status Interface
---------------------

The HTTP status interface provides a web-based interface that includes
a variety of operational data, logs, and status reports regarding the
:program:`mongod` or :program:`mongos` instance. The HTTP interface is
always available on the port numbered ``1000`` greater than the primary
:program:`mongod` port. By default, the HTTP interface port is
``28017``, but is indirectly set using the :setting:`~net.port` option which
allows you to configure the primary :program:`mongod` port.

Without the :setting:`net.http.RESTInterfaceEnabled` setting, this interface is entirely
read-only, and limited in scope; nevertheless, this interface may
represent an exposure. To disable the HTTP interface, set the
:setting:`nohttpinterface` run time option or the
:option:`--nohttpinterface <mongod --nohttpinterface>` command line
option. See also :ref:`security-port-numbers`.

.. _rest-api:

REST API
--------

The REST API to MongoDB provides additional information and write
access on top of the HTTP Status interface. While the REST API does not
provide any support for insert, update, or remove operations, it does
provide administrative access, and its accessibility represents a
vulnerability in a secure environment. The REST interface is *disabled*
by default, and is not recommended for production use.

If you must use the REST API, please control and limit access to the
REST API. The REST API does not include any support for
authentication, even when running with :setting:`~security.authentication`
enabled.

See the following documents for instructions on restricting access to
the REST API interface:

- :doc:`/tutorial/configure-linux-iptables-firewall`
- :doc:`/tutorial/configure-windows-netsh-firewall`
=====================
Security Introduction
=====================

.. default-domain:: mongodb

Maintaining a secure MongoDB deployment requires administrators to
implement controls to ensure that users and applications have
access to only the data that they require. MongoDB provides features that
allow administrators to implement these controls and restrictions for
any MongoDB deployment.

If you are already familiar with security and MongoDB security
practices, consider the :doc:`/administration/security-checklist` for a
collection of recommended actions to protect a MongoDB deployment.

Authentication
--------------

Before gaining access to a system all clients should identify
themselves to MongoDB. This ensures that no client can access the data
stored in MongoDB without being explicitly allowed.

MongoDB includes two mechanism: a password-based challenge and response
protocol and x.509 certificates. Additionally MongoDB includes support
for several external authentication mechanisms to integrate with
existing authentication infrastructure.

When you enable authentication MongoDB, MongoDB will require
authentication for all connections, including all clients and all
other MongoDB instances in a deployment. See
:doc:`/core/authentication` for more information.

Role Based Access Control
-------------------------

Clients should only be able to perform the operations required to
fulfill their approved functions. This is the "principal of least
privilege," and limits the potential risk of a compromised
application.

MongoDB's role-based access control system allows administrators to
control all access and ensure that all granted access applies as
narrowly as possible.

Privileges in MongoDB consist of an *action*, or a set operations
that a user can perform, and a *resource*, or context where
the user can perform that action. Multiple privileges combine to
create a *role*, and users may have one or more role that describes
their access. MongoDB provides several
:doc:`built-in roles </reference/built-in-roles>` and users can
construct specific roles tailored to clients' actual requirements.

See :doc:`/core/authorization` for more information.

Auditing
--------

Auditing provides administrators with the ability to verify that the
implemented security policies are controlling activity in the
system. Retaining audit information ensures that administrators have
enough information to perform forensic investigations and comply with
regulations and polices that require audit data.

See :doc:`/core/auditing` for more information.

Encryption
----------

Transport Encryption
~~~~~~~~~~~~~~~~~~~~

You can use SSL to encrypt all of MongoDB's network traffic. SSL
ensures that MongoDB network traffic is only readable by the intended
client.

See :doc:`/tutorial/configure-ssl` for more information.

Encryption at Rest
~~~~~~~~~~~~~~~~~~

MongoDB has a partnership with Gazzang to encrypt and secure sensitive
data within MongoDB. The solution encrypts data in real time, and Gazzang
provides advanced key management that ensures only authorized processes
can access this data. The Gazzang software ensures that the cryptographic
keys remain safe and ensures compliance with standards including HIPAA,
PCI-DSS, and FERPA.

For more information on the partnership, refer to the following resources:

- `Partnership <https://www.mongodb.com/partners/technology/gazzang>`_

- `Datasheet <http://www.gazzang.com/images/datasheet-zNcrypt-for-MongoDB.pdf>`_

- `Webinar <http://gazzang.com/resources/videos/partner-videos/item/209-gazzang-zncrypt-on-mongodb>`_

Hardening Deployments and Environments
--------------------------------------

In addition to implementing controls within MongoDB, you should also
place controls around MongoDB to reduce the risk exposure of the
entire MongoDB system. This is a *defense in depth*
strategy.

Hardening MongoDB extends the ideas of least privilege, auditing, and
encryption outside of MongoDB. Reducing risk includes: configuring the
network rules to ensure that only trusted hosts have access to MongoDB, and
that the MongoDB processes only have access to the parts of the
filesystem required for operation.
=============================
Network Exposure and Security
=============================

.. default-domain:: mongodb

By default, MongoDB programs (i.e. :program:`mongos` and
:program:`mongod`) will bind to all available network interfaces (i.e.
IP addresses) on a system.

This page outlines various runtime options that allow you to limit
access to MongoDB programs.

.. _security-port-numbers:

Configuration Options
---------------------

You can limit the network exposure with the following :program:`mongod`
and :program:`mongos` configuration options:
:setting:`nohttpinterface`, :setting:`net.http.RESTInterfaceEnabled`, :setting:`bind_ip`, and
:setting:`~net.port`. You can use a :doc:`configuration file
</reference/configuration-options>` to specify these settings.

``nohttpinterface``
~~~~~~~~~~~~~~~~~~~

.. TODO need to reorg page as part of security section rewrite/reorg,
   including probably removing the nohttpinterface section. For now, just
   added the blurb regarding the default behavior for 2.6

The :setting:`nohttpinterface` setting for :program:`mongod` and
:program:`mongos` instances disables the "home" status page.

.. versionchanged:: 2.6

   The :program:`mongod` and :program:`mongos` instances run with the
   http interface *disabled* by default.

The status interface is read-only by default, and the default port for
the status page is ``28017``. Authentication does not control or affect
access to this interface.

.. important:: Disable this interface for production deployments. If
   you *enable* this interface, you should only allow trusted clients
   to access this port. See :ref:`security-firewalls`.

``rest``
~~~~~~~~

The :setting:`net.http.RESTInterfaceEnabled` setting for :program:`mongod` enables a fully
interactive administrative :term:`REST` interface, which is *disabled*
by default. The :setting:`net.http.RESTInterfaceEnabled` configuration makes the http status
interface [#http-interface]_, which is read-only by default, fully
interactive. Use the :setting:`net.http.RESTInterfaceEnabled` setting with the
:setting:`httpinterface` setting.

The REST interface does not support any authentication and you should
always restrict access to this interface to only allow trusted clients
to connect to this port.

You may also enable this interface on the command line as
:option:`mongod --rest <--rest>` :option:`--httpinterface`.

.. important:: Disable this option for production deployments. If
   *do* you leave this interface enabled, you should only allow trusted
   clients to access this port.

.. [#http-interface] Starting in version 2.6, http interface is
   *disabled* by default.

``bind_ip``
~~~~~~~~~~~

The :setting:`bind_ip` setting for :program:`mongod` and
:program:`mongos` instances limits the network interfaces on which
MongoDB programs will listen for incoming connections. You can also
specify a number of interfaces by passing :setting:`bind_ip` a comma
separated list of IP addresses. You can use the :option:`mongod
--bind_ip` and :option:`mongos --bind_ip` option on the command line at
run time to limit the network accessibility of a MongoDB program.

.. important::

   Make sure that your :program:`mongod` and :program:`mongos`
   instances are only accessible on trusted networks. If your system
   has more than one network interface, bind MongoDB programs to the
   private or internal network interface.

``port``
~~~~~~~~

The :setting:`~net.port` setting for :program:`mongod` and :program:`mongos`
instances changes the main port on which the :program:`mongod` or
:program:`mongos` instance listens for connections. The default port is
``27017``. Changing the port does not meaningfully reduce risk or limit
exposure. You may also specify this option on the command line as
:option:`mongod --port` or :option:`mongos --port`. Setting
:setting:`~net.port` also indirectly sets the port for the HTTP status
interface, which is always available on the port numbered ``1000``
greater than the primary :program:`mongod` port.

Only allow trusted clients to connect to the port for the
:program:`mongod` and :program:`mongos` instances. See
:ref:`security-firewalls`.

See also :ref:`configuration-security` and
:doc:`/reference/default-mongodb-port`.

.. _security-firewalls:

Firewalls
---------

Firewalls allow administrators to filter and control access to a system
by providing granular control over what network communications. For
administrators of MongoDB, the following capabilities are important:
limiting incoming traffic on a specific port to specific systems, and
limiting incoming traffic from untrusted hosts.

On Linux systems, the ``iptables`` interface provides access to the
underlying ``netfilter`` firewall. On Windows systems, ``netsh``
command line interface provides access to the underlying Windows
Firewall. For additional information about firewall configuration, see
:doc:`/tutorial/configure-linux-iptables-firewall` and
:doc:`/tutorial/configure-windows-netsh-firewall`.

For best results and to minimize overall exposure, ensure that *only*
traffic from trusted sources can reach :program:`mongod` and
:program:`mongos` instances and that the :program:`mongod` and
:program:`mongos` instances can only connect to trusted outputs.

.. seealso:: For MongoDB deployments on Amazon's web services, see the
   :ecosystem:`Amazon EC2 </platforms/amazon-ec2>` page, which
   addresses Amazon's Security Groups and other EC2-specific security
   features.

Virtual Private Networks
------------------------

Virtual private networks, or VPNs, make it possible to link two
networks over an encrypted and limited-access trusted
network. Typically MongoDB users who use VPNs use SSL rather than
IPSEC VPNs for performance issues.

Depending on configuration and implementation, VPNs provide for
certificate validation and a choice of encryption protocols, which
requires a rigorous level of authentication and identification of all
clients. Furthermore, because VPNs provide a secure tunnel, by using a
VPN connection to control access to your MongoDB instance, you can
prevent tampering and "man-in-the-middle" attacks.
=================
Security Concepts
=================

.. default-domain:: mongodb

These documents introduce and address concepts and strategies related
to security practices in MongoDB deployments.

.. include:: /includes/toc/dfn-list-security-core-landing.rst

.. include:: /includes/toc/security-core-landing.rst
======================
Server-side JavaScript
======================

.. default-domain:: mongodb

.. versionchanged:: 2.4
   The V8 JavaScript engine, which became the default in 2.4, allows
   multiple JavaScript operations to execute at the same time. Prior to
   2.4, MongoDB operations that required the JavaScript interpreter had
   to acquire a lock, and a single :program:`mongod` could only run a
   single JavaScript operation at a time.

.. _server-side-javascript:

Overview
--------

MongoDB supports the execution of JavaScript code for the following
server-side operations:

- :dbcommand:`mapReduce` and the corresponding :program:`mongo` shell
  method :method:`db.collection.mapReduce()`. See
  :doc:`/core/map-reduce` for more information.

- :dbcommand:`eval` command, and the corresponding :program:`mongo`
  shell method :method:`db.eval()`

- :query:`$where` operator

- :ref:`running-js-scripts-in-mongo-on-mongod-host`

.. |javascript-using-operation| replace:: the above operations use
.. include:: /includes/admonition-javascript-prevalence.rst

.. seealso::

   :doc:`/tutorial/store-javascript-function-on-server`

.. include:: /includes/fact-disable-javascript-with-noscript.rst

.. _running-js-scripts-in-mongo-on-mongod-host:

Running ``.js`` files via a ``mongo`` shell Instance on the Server
------------------------------------------------------------------

You can run a JavaScript (``.js``) file using a :program:`mongo` shell
instance on the server. This is a good technique for performing batch
administrative work. When you run :program:`mongo` shell on the server,
connecting via the localhost interface, the connection is fast with low
latency.

The :ref:`command helpers <command-helpers>` provided in the
:program:`mongo` shell are not available in JavaScript files because
they are not valid JavaScript. The following table maps the most common
:program:`mongo` shell helpers to their JavaScript equivalents.

.. include:: /includes/table/helpers-to-javascript.rst

Concurrency
-----------

Refer to the individual method or operator documentation for any
concurrency information. See also the :ref:`concurrency table
<faq-concurrency-operations-locks>`.
.. _sharding-production-architecture:

===============================
Production Cluster Architecture
===============================

.. default-domain:: mongodb

In a production cluster, you must ensure that data is redundant and that
your systems are highly available. To that end, a production
cluster must have the following components:

Components
----------

Config Servers
~~~~~~~~~~~~~~

Three :ref:`config servers <sharding-config-server>`. Each config
server must be on separate machines. A single :term:`sharded
cluster` must have exclusive use of its :ref:`config servers
<sharding-config-server>`. If you have multiple sharded clusters,
you will need to have a group of config servers for each cluster.

Shards
~~~~~~

Two or more :term:`replica sets <replica set>`. These replica sets
are the :term:`shards <shard>`. For information on replica sets, see
:doc:`/replication`.

.. index:: mongos, load balancer, mongos load balancer

Query Routers (``mongos``)
~~~~~~~~~~~~~~~~~~~~~~~~~~

One or more :program:`mongos` instances.  The :program:`mongos`
instances are the routers for the cluster. Typically, deployments have
one :program:`mongos` instance on each application server.

You may also deploy a group of :program:`mongos` instances and use
a proxy/load balancer between the application and the
:program:`mongos`. In these deployments, you *must* configure the load
balancer for *client affinity* so that every connection from a single
client reaches the same :program:`mongos`.

Because cursors and other resources are specific to an single
:program:`mongos` instance, each client must interact with only one
:program:`mongos` instance.

Example
-------

.. include:: /images/sharded-cluster-production-architecture.rst
=================================
Sharded Cluster Test Architecture
=================================

.. default-domain:: mongodb

.. warning:: Use the test cluster architecture for testing and
   development only.

For testing and development, you can deploy a minimal sharded clusters
cluster. These **non-production** clusters have the following
components:

- One :ref:`config server <sharding-config-server>`.

- At least one shard. Shards are either :term:`replica sets <replica
  set>` or a standalone :program:`mongod` instances.

- One :program:`mongos` instance.

.. include:: /images/sharded-cluster-test-architecture.rst

.. see:: :doc:`/core/sharded-cluster-architectures-production`
=============================
Sharded Cluster Architectures
=============================

.. default-domain:: mongodb

The following documents introduce deployment patterns for sharded
clusters.

.. include:: /includes/toc/dfn-list-sharded-cluster-architectures.rst

.. include:: /includes/toc/sharded-cluster-architectures.rst
.. index:: sharding; shards
.. index:: shards
.. _sharding-shards:

==========================
Sharded Cluster Components
==========================

.. default-domain:: mongodb

:term:`Sharded clusters <sharded cluster>` implement
:term:`sharding`. A sharded cluster consists of the following
components:

**Shards**
   A shard is a MongoDB instance that holds a subset of a collection’s
   data. Each shard is either a single :program:`mongod` instance or a
   :term:`replica set`. In production, all shards are replica
   sets. For more information see :doc:`/core/sharded-cluster-shards`.

**Config Servers**
   Each :ref:`config server <sharding-config-server>` is a
   :program:`mongod` instance that holds metadata about the
   cluster. The metadata maps :term:`chunks <chunk>` to shards. For
   more information, see :doc:`/core/sharded-cluster-config-servers`.

**Routing Instances**
   Each router is a :program:`mongos` instance that routes the reads
   and writes from applications to the shards.  Applications do not
   access the shards directly. For more information see
   :doc:`/core/sharded-cluster-query-router`.

.. include:: /images/sharded-cluster.rst

Enable sharding in MongoDB on a per-collection basis. For each
collection you shard, you will specify a :term:`shard key` for that
collection.

.. only:: website

   Further Reading
   ---------------

   .. include:: /includes/toc/dfn-list-sharded-cluster-components.rst

Deploy a sharded cluster, see :doc:`/tutorial/deploy-shard-cluster`.

.. class:: hidden

   .. include:: /includes/toc/sharded-cluster-components.rst
.. index:: sharding; config servers
.. index:: config servers
.. _sharding-config-server:
.. _sharded-cluster-config-server:

==============
Config Servers
==============

.. default-domain:: mongodb

Config servers are special :program:`mongod` instances that store the
:doc:`metadata </core/sharded-cluster-metadata>` for a sharded cluster.
Config servers use a two-phase commit to ensure immediate consistency
and reliability. Config servers *do not* run as replica sets. All
config servers must be available to deploy a sharded cluster or to
make any changes to cluster metadata.

A production sharded cluster has *exactly three* config servers. For
testing purposes you may deploy a cluster with a single config server.
But to ensure redundancy and safety in production, you should always
use three.

.. warning::

   If your cluster has a single config server, then the config server
   is a single point of failure. If the config server is inaccessible,
   the cluster is not accessible. If you cannot recover the data on a
   config server, the cluster will be inoperable.

   **Always** use three config servers for production deployments.

Config servers store metadata for a single sharded cluster. Each
cluster must have its own config servers.

.. tip::

   .. include:: /includes/fact-use-cnames-for-config-servers.rst

.. index:: config databases
.. index:: database, config

Config Database
---------------

Config servers store the metadata in the :doc:`config database
</reference/config-database>`. The :program:`mongos` instances cache
this data and use it to route reads and writes to shards.

.. _config-server-read-write-ops:

Read and Write Operations on Config Servers
-------------------------------------------

MongoDB only writes data to the config server in the following cases:

- To create splits in existing chunks. For more information, see
  :doc:`chunk splitting </core/sharding-chunk-splitting>`.

- To migrate a chunk between shards. For more information, see
  :doc:`chunk migration </core/sharding-chunk-migration>`.

MongoDB reads data from the config server data in the following
cases:

- A new :program:`mongos` starts for the first time, or an existing
  :program:`mongos` restarts.

- After a chunk migration, the :program:`mongos` instances update
  themselves with the new cluster metadata.

MongoDB also uses the config server to manage distributed locks.

Config Server Availability
--------------------------

If one or two config servers become unavailable, the cluster's metadata
becomes *read only*. You can still read and write data from the shards,
but no chunk migrations or splits will occur until all three servers
are available.

If all three config servers are unavailable, you can still use the
cluster if you do not restart the :program:`mongos` instances until
after the config servers are accessible again. If you restart the
:program:`mongos` instances before the config servers are available,
the :program:`mongos` will be unable to route reads and writes.

Clusters become inoperable without the cluster metadata. *Always,*
ensure that the config servers remain available and intact. As such,
backups of config servers are critical. The data on the config server
is small compared to the data stored in a cluster. This means the
config server has a relatively low activity load, and the config
server does not need to be always available to support a sharded
cluster. As a result, it is easy to back up the config servers.

.. include:: /includes/fact-rename-config-servers-requires-cluster-restart.rst

See :ref:`sharding-config-servers-and-availability` for more
information.
.. _sharding-high-availability:

=================================
Sharded Cluster High Availability
=================================

.. default-domain:: mongodb

A :ref:`production <sharding-production-architecture>` :term:`cluster`
has no single point of failure. This section introduces the
availability concerns for MongoDB deployments in general and
highlights potential failure scenarios and available resolutions.

Application Servers or :program:`mongos` Instances Become Unavailable
---------------------------------------------------------------------

If each application server has its own :program:`mongos` instance, other
application servers can continue access the database. Furthermore,
:program:`mongos` instances do not maintain persistent state, and they
can restart and become unavailable without losing any state or data.
When a :program:`mongos` instance starts, it retrieves a copy of the
:term:`config database` and can begin routing queries.

A Single :program:`mongod` Becomes Unavailable in a Shard
---------------------------------------------------------

:doc:`Replica sets </replication>` provide high availability for shards.
If the unavailable :program:`mongod` is a :term:`primary`, then the
replica set will :ref:`elect <replica-set-elections>` a new primary. If
the unavailable :program:`mongod` is a :term:`secondary`, and it
disconnects the primary and secondary will continue to hold all data. In
a three member replica set, even if a single member of the set
experiences catastrophic failure, two other members have full copies of
the data. [#recovery-window]_

Always investigate availability interruptions and failures. If a system
is unrecoverable, replace it and create a new member of the replica set
as soon as possible to replace the lost redundancy.

All Members of a Replica Set Become Unavailable
-----------------------------------------------

If all members of a replica set within a shard are unavailable, all data
held in that shard is unavailable. However, the data on all other shards
will remain available, and it's possible to read and write data to the
other shards. However, your application must be able to deal with
partial results, and you should investigate the cause of the
interruption and attempt to recover the shard as soon as possible.

One or Two Config Databases Become Unavailable
----------------------------------------------

Three distinct :program:`mongod` instances provide the :term:`config
database` using a special two-phase commits to maintain consistent state
between these :program:`mongod` instances. Cluster operation will
continue as normal but :ref:`chunk migration <sharding-balancing>` and
the cluster can create no new :doc:`chunk splits
</tutorial/split-chunks-in-sharded-cluster>`. Replace the config server as soon as
possible. If all config databases become unavailable, the
cluster can become inoperable.

.. include:: /includes/note-config-server-startup.rst

.. [#recovery-window] If an unavailable secondary becomes available
   while it still has current oplog entries, it can catch up to the
   latest state of the set using the normal :term:`replication process
   <sync>`, otherwise it must perform an :term:`initial sync`.

.. _sharding-config-servers-and-availability:

Renaming Config Servers and Cluster Availability
------------------------------------------------

.. include:: /includes/fact-rename-config-servers-requires-cluster-restart.rst

To avoid downtime when renaming config servers, use DNS names
unrelated to physical or virtual hostnames to refer to your
:ref:`config servers <sharding-config-server>`.

Generally, refer to each config server using the DNS alias (e.g. a
CNAME record). When specifying the config server connection string to
:program:`mongos`, use these names. These records make it possible to
change the IP address or rename config servers without changing the
connection string and without having to restart the entire cluster.

Shard Keys and Cluster Availability
-----------------------------------

The most important consideration when choosing a :term:`shard key`
are:

- to ensure that MongoDB will be able to distribute data evenly among
  shards, and

- to scale writes across the cluster, and

- to ensure that :program:`mongos` can isolate most queries to a specific
  :program:`mongod`.

Furthermore:

- Each shard should be a :term:`replica set`, if a specific
  :program:`mongod` instance fails, the replica set members will elect
  another to be :term:`primary` and continue operation. However, if an
  entire shard is unreachable or fails for some reason, that data will
  be unavailable.

- If the shard key allows the :program:`mongos` to isolate most
  operations to a single shard, then the failure of a single shard
  will only render *some* data unavailable.

- If your shard key distributes data required for every operation
  throughout the cluster, then the failure of the entire shard will
  render the entire cluster unavailable.

In essence, this concern for reliability simply underscores the
importance of choosing a shard key that isolates query operations to a
single shard.
==================
Sharding Mechanics
==================

.. default-domain:: mongodb

The following documents describe sharded cluster processes.

.. include:: /includes/toc/dfn-list-sharded-cluster-mechanics.rst

.. include:: /includes/toc/sharded-cluster-mechanics.rst
.. _sharding-internals-config-database:

========================
Sharded Cluster Metadata
========================

.. default-domain:: mongodb

:doc:`Config servers </core/sharded-cluster-config-servers>` store the
metadata for a sharded cluster. The metadata reflects state and
organization of the sharded data sets and system. The metadata
includes the list of chunks on every shard and the ranges that define
the chunks. The :program:`mongos` instances cache this data and use it
to route read and write operations to shards.

Config servers store the metadata in the
:doc:`/reference/config-database`.

.. important:: Always back up the ``config`` database before doing any
   maintenance on the config server.

To access the ``config`` database, issue the following command from the
:program:`mongo` shell:

.. code-block:: javascript

   use config

In general, you should *never* edit the content of the ``config``
database directly. The ``config`` database contains the following
collections:

- :data:`~config.changelog`
- :data:`~config.chunks`
- :data:`~config.collections`
- :data:`~config.databases`
- :data:`~config.lockpings`
- :data:`~config.locks`
- :data:`~config.mongos`
- :data:`~config.settings`
- :data:`~config.shards`
- :data:`~config.version`

For more information on these collections and their role in sharded
clusters, see :doc:`/reference/config-database`. See
:ref:`config-server-read-write-ops` for more information about reads
and updates to the metadata.
========================
Sharded Cluster Behavior
========================

.. default-domain:: mongodb

These documents address the distribution of data and queries to a
sharded cluster as well as specific security and availability
considerations for sharded clusters.

.. include:: /includes/toc/dfn-list-sharded-cluster-operations.rst

.. include:: /includes/toc/sharded-cluster-operations.rst
.. index:: mongos
.. _sharded-cluster-query-routing:
.. _sharding-read-operations:

=============================
Sharded Cluster Query Routing
=============================

.. default-domain:: mongodb

MongoDB :program:`mongos` instances route queries and write operations
to :term:`shards <shard>` in a sharded cluster. :program:`mongos` provide the
only interface to a sharded cluster from the perspective of
applications. Applications never connect or communicate directly with
the shards.

The :program:`mongos` tracks what data is on which shard by caching
the metadata from the :ref:`config servers
<sharded-cluster-config-server>`. The :program:`mongos` uses the
metadata to route operations from applications and clients to the
:program:`mongod` instances. A :program:`mongos` has no *persistent*
state and consumes minimal system resources.

The most common practice is to run :program:`mongos` instances on the
same systems as your application servers, but you can maintain
:program:`mongos` instances on the shards or on other dedicated
resources.

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using the :dbcommand:`aggregate`
   command (i.e. :method:`db.collection.aggregate()`) will cause
   :program:`mongos` instances to require more CPU resources than in
   previous versions. This modified performance profile may dictate
   alternate architecture decisions if you use the :term:`aggregation
   framework` extensively in a sharded environment.

Routing Process
---------------

A :program:`mongos` instance uses the following processes to route
queries and return results.

How ``mongos`` Determines which Shards Receive a Query
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A :program:`mongos` instance routes a query to a :term:`cluster <sharded
cluster>` by:

1. Determining the list of :term:`shards <shard>` that must receive the
   query.

#. Establishing a cursor on all targeted shards.

In some cases, when the :term:`shard key` or a prefix of the shard
key is a part of the query, the :program:`mongos` can route the
query to a subset of the shards. Otherwise, the :program:`mongos`
must direct the query to *all* shards that hold documents for that
collection.

.. example::

   Given the following shard key:

   .. code-block:: javascript

      { zipcode: 1, u_id: 1, c_date: 1 }

   Depending on the distribution of chunks in the cluster, the
   :program:`mongos` may be able to target the query at a subset of
   shards, if the query contains the following fields:

   .. code-block:: javascript

      { zipcode: 1 }
      { zipcode: 1, u_id: 1 }
      { zipcode: 1, u_id: 1, c_date: 1 }

How ``mongos`` Handles Query Modifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the result of the query is not sorted, the :program:`mongos`
instance opens a result cursor that "round robins" results from all
cursors on the shards.

.. versionchanged:: 2.0.5
   In versions prior to 2.0.5, the :program:`mongos` exhausted each
   cursor, one by one.

If the query specifies sorted results using the
:method:`~cursor.sort()` cursor method, the :program:`mongos` instance
passes the :operator:`$orderby` option to the shards. When the
:program:`mongos` receives results it performs an incremental *merge sort*
of the results while returning them to the client.

If the query limits the size of the result set using the
:method:`~cursor.limit()` cursor method, the :program:`mongos`
instance passes that limit to the shards and then re-applies the limit
to the result before returning the result to the client.

If the query specifies a number of records to *skip* using the
:method:`~cursor.skip()` cursor method, the :program:`mongos` *cannot*
pass the skip to the shards, but rather retrieves unskipped results
from the shards and skips the appropriate number of documents when assembling
the complete result. However, when used in conjunction with a
:method:`~cursor.limit()`, the :program:`mongos` will pass the *limit*
plus the value of the :method:`~cursor.skip()` to the shards to
improve the efficiency of these operations.

Detect Connections to :program:`mongos` Instances
-------------------------------------------------

To detect if the MongoDB instance that your client is connected
to is :program:`mongos`, use the :dbcommand:`isMaster` command. When a
client connects to a :program:`mongos`, :dbcommand:`isMaster` returns
a document with a ``msg`` field that holds the string
``isdbgrid``. For example:

.. code-block:: javascript

   {
      "ismaster" : true,
      "msg" : "isdbgrid",
      "maxBsonObjectSize" : 16777216,
      "ok" : 1
   }

If the application is instead connected to a :program:`mongod`, the
returned document does not include the ``isdbgrid`` string.

Broadcast Operations and Targeted Operations
--------------------------------------------

In general, operations in a sharded environment are either:

- Broadcast to all shards in the cluster that hold documents in a
  collection

- Targeted at a single shard or a limited group of shards, based on
  the shard key

For best performance, use targeted operations whenever possible. While
some operations must broadcast to all shards, you can ensure MongoDB
uses targeted operations whenever possible by always including the shard
key.

Broadcast Operations
~~~~~~~~~~~~~~~~~~~~

:program:`mongos` instances broadcast queries to all shards for the
collection **unless** the :program:`mongos` can
determine which shard or subset of shards stores this data.

.. include:: /images/sharded-cluster-scatter-gather-query.rst

Multi-update operations are always broadcast operations.

The :method:`~db.collection.remove()` operation is always a
broadcast operation, unless the operation specifies the shard key in
full.

Targeted Operations
~~~~~~~~~~~~~~~~~~~

All :method:`~db.collection.insert()` operations target to one
shard.

All single :method:`~db.collection.update()` (including :term:`upsert`
operations) and :method:`~db.collection.remove()` operations must
target to one shard.

.. important::

   .. |single-modification-operation-names| replace:: :method:`~db.collection.update()` and :method:`~db.collection.remove()`

   .. |single-modification-operation-option| replace:: ``justOne`` or ``multi: false``

   .. include:: /includes/fact-single-modification-in-sharded-collections.rst

For queries that include the shard key or portion of the shard key,
:program:`mongos` can target the query at a specific shard or set of
shards. This is the case only if the portion of the shard key included
in the query is a *prefix* of the shard key. For example, if the shard
key is:

.. code-block:: javascript

   { a: 1, b: 1, c: 1 }

The :program:`mongos` program *can* route queries that include the full
shard key or either of the following shard key prefixes at a
specific shard or set of shards:

.. code-block:: javascript

   { a: 1 }
   { a: 1, b: 1 }

.. include:: /images/sharded-cluster-targeted-query.rst

Depending on the distribution of data in the cluster and the
selectivity of the query, :program:`mongos` may still have to
contact multiple shards [#possible-all]_ to fulfill these queries.

.. [#possible-all] :program:`mongos` will route some queries, even
   some that include the shard key, to all shards, if needed.

Sharded and Non-Sharded Data
----------------------------

Sharding operates on the collection level. You can shard multiple
collections within a database or have multiple databases with
sharding enabled. [#sharding-databases]_ However, in production
deployments, some databases and collections will use sharding, while
other databases and collections will only reside on a single
shard.

.. include:: /images/sharded-cluster-primary-shard.rst

Regardless of the data architecture of your :term:`sharded cluster`,
ensure that all queries and operations use the :term:`mongos` router to
access the data cluster. Use the :program:`mongos` even for operations
that do not impact the sharded data.

.. include:: /images/sharded-cluster-mixed.rst

.. [#sharding-databases] As you configure sharding, you will use the
   :dbcommand:`enableSharding` command to enable sharding for a
   database. This simply makes it possible to use the
   :dbcommand:`shardCollection` command on a collection within that database.
.. index:: fundamentals; sharding
.. _sharding-fundamentals:

============================
Sharded Cluster Requirements
============================

.. TODO add something about which nodes need to be able to communicate
   with other nodes.

.. default-domain:: mongodb

While sharding is a powerful and compelling feature, sharded clusters
have significant infrastructure requirements and increases the overall
complexity of a deployment. As a result, only deploy sharded clusters
when indicated by application and operational requirements

Sharding is the *only* solution for some classes of deployments. Use
:term:`sharded clusters <sharded cluster>` if:

- your data set approaches or exceeds the storage capacity of a single
  MongoDB instance.

- the size of your system's active :term:`working set` *will soon*
  exceed the capacity of your system's *maximum* RAM.

- a single MongoDB instance cannot meet the demands of your write
  operations, and all other approaches have not reduced contention.

If these attributes are not present in your system, sharding will only
add complexity to your system without adding much benefit.

.. _sharding-capacity-planning:

.. important:: It takes time and resources to deploy sharding. If
   your system has *already* reached or exceeded its capacity, it
   will be difficult to deploy sharding without impacting
   your application.

   As a result, if you think you will need to partition your database
   in the future, **do not** wait until your system is overcapacity to
   enable sharding.

When designing your data model, take into consideration your sharding
needs.

.. _sharding-requirements-data:

Data Quantity Requirements
--------------------------

Your cluster should manage a large quantity of data if sharding is to
have an effect. The default :term:`chunk` size is 64 megabytes. And the
:ref:`balancer <sharding-balancing>` will not begin moving data across
shards until the imbalance of chunks among the shards exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. In
practical terms, unless your cluster has many hundreds of megabytes of
data, your data will remain on a single shard.

In some situations, you may need to shard a small collection of data.
But most of the time, sharding a small collection is not worth the
added complexity and overhead unless you need additional write
capacity. If you have a small data set, a properly configured single
MongoDB instance or a replica set will usually be enough for your
persistence layer needs.

:term:`Chunk <chunk>` size is
:option:`user configurable <mongos --chunkSize>`.
For most deployments, the default value is of 64
megabytes is ideal. See :ref:`sharding-chunk-size` for more information.
======
Shards
======

.. default-domain:: mongodb

.. TODO Intro paragraph .. following is just a placeholder.

A shard is a :term:`replica set` or a single :program:`mongod` that
contains a subset of the data for the sharded cluster. Together, the
cluster's shards hold the entire data set for the cluster.

Typically each shard is a replica set. The replica set provides
redundancy and high availability for the data in each shard.

.. important:: MongoDB shards data on a *per collection* basis. You
   *must* access all data in a sharded cluster via the
   :program:`mongos` instances. If you connect directly to a shard,
   you will see only its fraction of the cluster's data. There is no
   particular order to the data set on a specific shard. MongoDB does
   not guarantee that any two contiguous chunks will reside on a
   single shard.

.. _primary-shard:

Primary Shard
-------------

Every database has a "primary" [#overloaded-primary-term]_ shard that
holds all the un-sharded collections in that database.

.. include:: /images/sharded-cluster-primary-shard.rst

To change the primary shard for a database, use the
:dbcommand:`movePrimary` command.

.. warning::

   The :dbcommand:`movePrimary` command can be expensive because it
   copies all non-sharded data to the new shard. During this time,
   this data will be unavailable for other operations.

When you deploy a new :term:`sharded cluster`, the "first" shard
becomes the primary shard for all existing databases before enabling
sharding. Databases created subsequently may reside on any shard in the
cluster.

.. [#overloaded-primary-term] The term "primary" shard has nothing to
   do with the term :term:`primary` in the context of :term:`replica
   sets <replica set>`.

Shard Status
------------

Use the :method:`sh.status()` method in the :program:`mongo` shell to
see an overview of the cluster. This reports includes which shard is
primary for the database and the :term:`chunk` distribution across the
shards. See :method:`sh.status()` method for more details.
.. index:: balancing
.. _sharding-balancing:

============================
Sharded Collection Balancing
============================

.. default-domain:: mongodb

Balancing is the process MongoDB uses to distribute data of a sharded
collection evenly across a :term:`sharded cluster`. When a
:term:`shard` has too many of a sharded collection's :term:`chunks
<chunk>` compared to other shards, MongoDB automatically balances the
the chunks across the shards. The balancing procedure for
:term:`sharded clusters <sharded cluster>` is entirely transparent to
the user and application layer.

.. index:: balancing; internals
.. _sharding-balancing-internals:
.. _sharding-internals-balancing:

Cluster Balancer
----------------

The :term:`balancer` process is responsible for redistributing the
chunks of a sharded collection evenly among the shards for every
sharded collection. By default, the balancer process is always enabled.

Any :program:`mongos` instance in the cluster can start a balancing
round. When a balancer process is active, the responsible
:program:`mongos` acquires a "lock" by modifying a document in the
``lock`` collection in the :ref:`config-database`.

.. note::

   .. versionchanged:: 2.0
      Before MongoDB version 2.0, large differences in timekeeping
      (i.e. clock skew) between :program:`mongos` instances could lead
      to failed distributed locks. This carries the possibility of
      data loss, particularly with skews larger than 5 minutes.
      Always use the network time protocol (NTP) by running ``ntpd``
      on your servers to minimize clock skew.

To address uneven chunk distribution for a sharded collection, the
balancer :doc:`migrates chunks </core/sharding-chunk-migration>` from
shards with more chunks to shards with a fewer number of chunks. The
balancer migrates the chunks, one at a time, until there is an even
dispersion of chunks for the collection across the shards.

Chunk migrations carry some overhead in terms of bandwidth and
workload, both of which can impact database performance. The
:term:`balancer` attempts to minimize the impact by:

- Moving only one chunk at a time. See also
  :ref:`chunk-migration-queuing`.

- Starting a balancing round **only** when the difference in the
  number of chunks between the shard with the greatest number of chunks
  for a sharded collection and the shard with the lowest number of
  chunks for that collection reaches the :ref:`migration threshold
  <sharding-migration-thresholds>`.

You may disable the balancer temporarily for maintenance. See
:ref:`sharding-balancing-disable-temporally` for details.

You can also limit the window during which the balancer runs to prevent
it from impacting production traffic. See :ref:`Schedule the Balancing
Window <sharding-schedule-balancing-window>` for details.

.. note::

   The specification of the balancing window is relative to the local
   time zone of all individual :program:`mongos` instances in the
   cluster.

.. seealso:: :doc:`/tutorial/manage-sharded-cluster-balancer`.

.. _sharding-migration-thresholds:

Migration Thresholds
--------------------

To minimize the impact of balancing on the cluster, the
:term:`balancer` will not begin balancing until the distribution of
chunks for a sharded collection has reached certain thresholds. The
thresholds apply to the difference in number of :term:`chunks <chunk>`
between the shard with the most chunks for the collection and the shard
with the fewest chunks for that collection. The balancer has the
following thresholds:

.. versionchanged:: 2.2
   The following thresholds appear first in 2.2. Prior to this
   release, a balancing round would only start if the shard with the most
   chunks had 8 more chunks than the shard with the least number of
   chunks.

================  ===================
Number of Chunks  Migration Threshold
----------------  -------------------
Fewer than 20      2
21-80              4
Greater than 80    8
================  ===================

Once a balancing round starts, the balancer will not stop until, for
the collection, the difference between the number of chunks on any two
shards for that collection is *less than two* or a chunk migration
fails.

.. _sharding-shard-size:

Shard Size
----------

By default, MongoDB will attempt to fill all available disk space with
data on every shard as the data set grows. To ensure that the cluster
always has the capacity to handle data growth, monitor disk
usage as well as other performance metrics.

When adding a shard, you may set a "maximum size" for that shard.
This prevents the :term:`balancer` from migrating chunks to the shard
when the value of :data:`~serverStatus.mem.mapped` exceeds the
"maximum size". Use the ``maxSize`` parameter of the
:dbcommand:`addShard` command to set the "maximum size" for the shard.

.. seealso:: :ref:`sharded-cluster-config-max-shard-size` and
   :doc:`/administration/monitoring`.
.. index:: balancing; migration

=============================
Chunk Migration Across Shards
=============================

.. default-domain:: mongodb

Chunk migration moves the chunks of a sharded collection from one shard
to another and is part of the :doc:`balancer
</core/sharding-balancing>` process.

.. include:: /images/sharding-migrating.rst

.. _sharding-chunk-migration:

Chunk Migration
---------------

MongoDB migrates chunks in a :term:`sharded cluster` to distribute the
chunks of a sharded collection evenly among shards. Migrations may be
either:

- Manual. Only use manual migration in limited cases, such as
  to distribute data during bulk inserts. See :doc:`Migrating Chunks
  Manually </tutorial/migrate-chunks-in-sharded-cluster>` for more details.

- Automatic. The :doc:`balancer </core/sharding-balancing>` process
  automatically migrates chunks when there is an uneven distribution of
  a sharded collection's chunks across the shards. See :ref:`Migration
  Thresholds <sharding-migration-thresholds>` for more details.

All chunk migrations use the following procedure:

#. The balancer process sends the :dbcommand:`moveChunk` command to
   the source shard.

#. The source starts the move with an internal :dbcommand:`moveChunk`
   command. During the migration process, operations to the chunk
   route to the source shard. The source shard is responsible for
   incoming write operations for the chunk.

#. The destination shard begins requesting documents in the chunk and
   starts receiving copies of the data.

#. After receiving the final document in the chunk, the destination
   shard starts a synchronization process to ensure that it has the
   changes to the migrated documents that occurred during the migration.

#. When fully synchronized, the destination shard connects to the
   :term:`config database` and updates the cluster metadata with the new
   location for the chunk.

#. After the destination shard completes the update of the metadata,
   and once there are no open cursors on the chunk, the source shard
   deletes its copy of the documents.

   .. versionchanged:: 2.4

      If the balancer needs to perform additional chunk migrations from
      the source shard, the balancer can start the next chunk
      migration without waiting for the current
      migration process to finish this deletion step. See
      :ref:`chunk-migration-queuing`.

The migration process ensures consistency and maximizes the availability of
chunks during balancing.

.. _chunk-migration-queuing:

Chunk Migration Queuing
~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.4

To migrate multiple chunks from a shard, the balancer migrates the
chunks one at a time. However, the balancer does not wait for the
current migration's delete phase to complete before starting the next
chunk migration. See :ref:`sharding-chunk-migration` for the chunk
migration process and the delete phase.

This queuing behavior allows shards to unload chunks more quickly in
cases of heavily imbalanced cluster, such as when performing initial
data loads without pre-splitting and when adding new shards.

This behavior also affect the :dbcommand:`moveChunk` command, and
migration scripts that use the :dbcommand:`moveChunk` command may
proceed more quickly.

In some cases, the delete phases may persist longer. If multiple delete
phases are queued but not yet complete, a crash of the replica set's
primary can orphan data from multiple migrations.

.. _chunk-migration-replication:

Chunk Migration and Replication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, each document move during chunk migration propagates to at
least one secondary before the balancer proceeds with its next
operation.

To override this behavior and allow the balancer to continue before
replicating to a secondary, set the ``_secondaryThrottle`` parameter to
``false``. See :ref:`sharded-cluster-config-secondary-throttle` to
update the ``_secondaryThrottle`` parameter for the balancer.

Independent of the ``secondaryThrottle`` setting, certain operations of
the chunk migration have the following replication policy:

- MongoDB briefly pauses all application writes to the source shard
  before updating the config servers with the new location for the
  chunk, and resumes the application writes after the update. The
  chunk commit requires all writes to be durably replicated to a
  majority of servers in order to proceed and finish.

- When an outgoing chunk migration finishes and cleanup occurs, all
  writes must be replicated to a majority of servers before further
  cleanup (from other outgoing migrations) or new incoming migrations
  can proceed.

.. versionchanged:: 2.4

   In previous versions, the balancer did not wait for the document
   move to replicate to a secondary. For details, see :v2.2:`Secondary
   Throttle in the v2.2 Manual
   </tutorial/configure-sharded-cluster-balancer/#sharded-cluster-config-secondary-throttle>`
=================================
Chunk Splits in a Sharded Cluster
=================================

.. default-domain:: mongodb

As chunks grow beyond the :ref:`specified chunk size
<sharding-chunk-size>` a :program:`mongos` instance will attempt to split the
chunk in half. Splits may lead to an uneven distribution of the
chunks for a collection across the shards. In such cases, the
:program:`mongos` instances will initiate a round of migrations to
redistribute chunks across shards. See :doc:`/core/sharding-balancing`
for more details on balancing chunks across shards.

.. include:: /images/sharding-splitting.rst

.. index:: sharding; chunk size
.. _sharding-chunk-size:

Chunk Size
----------

.. todo:: link this section to <glossary:chunk size>

The default :term:`chunk` size in MongoDB is 64 megabytes. You can
:doc:`increase or reduce the chunk size
</tutorial/modify-chunk-size-in-sharded-cluster>`, mindful of its effect on the
cluster's efficiency.

#. Small chunks lead to a more even distribution of data at the
   expense of more frequent migrations. This creates expense at the
   query routing (:program:`mongos`) layer.

#. Large chunks lead to fewer migrations. This is more efficient both
   from the networking perspective *and* in terms of internal overhead at
   the query routing layer. But, these efficiencies come at
   the expense of a potentially more uneven distribution of data.

For many deployments, it makes sense to avoid frequent and potentially
spurious migrations at the expense of a slightly less evenly
distributed data set.

Limitations
-----------

Changing the chunk size affects when chunks split but there are some
limitations to its effects.

- Automatic splitting only occurs during inserts or updates. If you
  lower the chunk size, it may take time for all chunks to split to the
  new size.

- Splits cannot be "undone". If you increase the chunk size, existing
  chunks must grow through inserts or updates until they reach the new
  size.

.. note::

   Chunk ranges are inclusive of the lower boundary and exclusive of
   the upper boundary.
.. _sharding-introduction:

=====================
Sharding Introduction
=====================

.. default-domain:: mongodb

Sharding is a method for storing data across multiple
machines. MongoDB uses sharding to support deployments with very large
data sets and high throughput operations.

Purpose of Sharding
-------------------

Database systems with large data sets and high throughput applications
can challenge the capacity of a single server. High query rates can
exhaust the CPU capacity of the server. Larger data sets exceed the
storage capacity of a single machine. Finally, working set sizes
larger than the system's RAM stress the I/O capacity of disk drives.

To address these issues of scales, database systems have two basic
approaches: **vertical scaling** and **sharding**.

**Vertical scaling** adds more CPU and storage resources to increase
capacity. Scaling by adding capacity has limitations: high performance
systems with large numbers of CPUs and large amount of RAM are
disproportionately *more expensive* than smaller
systems. Additionally, cloud-based providers may only allow users to
provision smaller instances.  As a result there is a *practical
maximum* capability for vertical scaling.

**Sharding**, or *horizontal scaling*, by contrast, divides the data
set and distributes the data over multiple servers, or **shards**. Each
shard is an independent database, and collectively, the shards make up
a single logical database.

.. include:: /images/sharded-collection.rst

Sharding addresses the challenge of scaling to support high throughput
and large data sets:

- Sharding reduces the number of operations each shard handles. Each
  shard processes fewer operations as the cluster grows. As a result, a
  cluster can increase capacity and throughput *horizontally*.

  For example, to insert data, the application only needs to access the
  shard responsible for that record.

- Sharding reduces the amount of data that each server needs to store.
  Each shard stores less data as the cluster grows.

  For example, if a database has a 1 terabyte data set, and there are
  4 shards, then each shard might hold only 256GB of data. If there
  are 40 shards, then each shard might hold only 25GB of data.

Sharding in MongoDB
-------------------

MongoDB supports sharding through the configuration of a :term:`sharded
clusters <sharded cluster>`.

.. include:: /images/sharded-cluster-production-architecture.rst

Sharded cluster has the following components: :term:`shards <shard>`,
:term:`query routers <mongos>` and :term:`config servers <config server>`.

**Shards** store the data. To provide high availability and data
consistency, in a production sharded cluster, each shard is a
:term:`replica set` [#dev-only-shard-deployment]_. For more
information on replica sets, see :doc:`Replica Sets
</core/replication>`.

**Query Routers**, or :program:`mongos` instances, interface with client
applications and direct operations to the appropriate shard or
shards. The query router processes and targets operations to shards and then
returns results to the clients. A sharded cluster can contain more
than one query router to divide the client request load. A client sends
requests to one query router. Most sharded cluster have many query routers.

**Config servers** store the cluster's metadata. This data contains a
mapping of the cluster's data set to the shards. The query router uses this
metadata to target operations to specific shards. Production sharded
clusters have *exactly* 3 config servers.

.. [#dev-only-shard-deployment] For development and testing purposes
   only, each **shard** can be a single :program:`mongod` instead of a
   replica set. Do **not** deploy production clusters without 3 config
   servers.

.. _sharding-data-partitioning:

Data Partitioning
-----------------

MongoDB distributes data, or shards, at the collection level. Sharding
partitions a collection's data by the **shard key**.

Shard Keys
~~~~~~~~~~

To shard a collection, you need to select a **shard key**. A
:term:`shard key` is either an indexed field or an indexed compound
field that exists in every document in the collection. MongoDB divides
the shard key values into **chunks** and distributes the :term:`chunks
<chunk>` evenly across the shards. To divide the shard key values into
chunks, MongoDB uses either **range based partitioning** and **hash
based partitioning**. See :doc:`/core/sharding-shard-key` for more
information.

Range Based Sharding
~~~~~~~~~~~~~~~~~~~~

For *range-based sharding*, MongoDB divides the data set into ranges
determined by the shard key values to provide **range based
partitioning**. Consider a numeric shard key: If you visualize a
number line that goes from negative infinity to positive infinity,
each value of the shard key falls at some point on that line. MongoDB
partitions this line into smaller, non-overlapping ranges called
**chunks** where a chunk is range of values from some minimum value to
some maximum value.

Given a range based partitioning system, documents with "close" shard
key values are likely to be in the same chunk, and therefore on the
same shard.

.. include:: /images/sharding-range-based.rst

Hash Based Sharding
~~~~~~~~~~~~~~~~~~~

For *hash based partitioning*, MongoDB computes a hash of a field's
value, and then uses these hashes to create chunks.

With hash based partitioning, two documents with "close" shard key
values are *unlikely* to be part of the same chunk. This ensures a
more random distribution of a collection in the cluster.

.. include:: /images/sharding-hash-based.rst

Performance Distinctions between Range and Hash Based Partitioning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Range based partitioning supports more efficient range queries. Given
a range query on the shard key, the query router can easily determine which
chunks overlap that range and route the query to only those shards
that contain these chunks.

However, range based partitioning can result in an uneven distribution
of data, which may negate some of the benefits of sharding. For
example, if the shard key is a linearly increasing field, such as
time, then all requests for a given time range will map to the same
chunk, and thus the same shard. In this situation, a small set of
shards may receive the majority of requests and the system would not
scale very well.

Hash based partitioning, by contrast, ensures an even distribution of
data at the expense of efficient range queries. Hashed key values
results in random distribution of data across chunks and therefore
shards. But random distribution makes it more likely that a range query
on the shard key will not be able to target a few shards but would more
likely query every shard in order to return a result.

Maintaining a Balanced Data Distribution
----------------------------------------

The addition of new data or the addition of new servers can result in
data distribution imbalances within the cluster, such as a particular
shard contains significantly more chunks than another shard or a size
of a chunk is significantly greater than other chunk sizes.

MongoDB ensures a balanced cluster using two background process:
splitting and the balancer.

Splitting
~~~~~~~~~

Splitting is a background process that keeps chunks from growing too
large. When a chunk grows beyond a :ref:`specified chunk size
<sharding-chunk-size>`, MongoDB splits the chunk in half. Inserts and
updates triggers splits. Splits are a efficient meta-data change. To
create splits, MongoDB does *not* migrate any data or affect the
shards.

.. include:: /images/sharding-splitting.rst

Balancing
~~~~~~~~~

The :ref:`balancer <sharding-balancing-internals>` is a background
process that manages chunk migrations. The balancer runs in all of the
query routers in a cluster.

When the distribution of a sharded collection in a cluster is uneven,
the balancer process migrates chunks from the shard that has the
largest number of chunks to the shard with the least number of chunks
until the collection balances. For example: if collection ``users``
has 100 chunks on *shard 1* and 50 chunks on *shard 2*, the balancer
will migrate chunks from *shard 1* to *shard 2* until the collection
achieves balance.

The shards manage *chunk migrations* as a background operation. During
migration, all requests for a chunk's data address the "origin" shard.

In a chunk migration, the *destination shard* receives all the
documents in the chunk from the *origin shard*. Then, the destination
shard captures and applies all changes made to the data during
migration process. Finally, the destination shard updates the metadata
regarding the location of the chunk on *config server*.

If there's an error during the migration, the balancer aborts the
process leaving the chunk on the origin shard. MongoDB removes the
chunk's data from the origin shard **after** the migration completes
successfully.

.. include:: /images/sharding-migrating.rst

Adding and Removing Shards from the Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Adding a shard to a cluster creates an imbalance  since the new
shard has no chunks. While MongoDB begins migrating data to the new
shard immediately, it can take some time before the cluster balances.

When removing a shard, the balancer migrates all chunks from a shard to other
shards. After migrating all data and updating the meta data, you can
safely remove the shard.
.. index:: sharding; shard key indexes
.. _sharding-internals-shard-key-indexes:
.. _sharding-shard-key-indexes:

=================
Shard Key Indexes
=================

.. default-domain:: mongodb

All sharded collections **must** have an index that starts with the
:term:`shard key`. If you shard a collection without any documents and
*without* such an index, the :dbcommand:`shardCollection` command will
create the index on the shard key. If the collection already has
documents, you must create the index before using
:dbcommand:`shardCollection`.

.. todo:: replace the link

.. versionchanged:: 2.2
   The index on the shard key no longer needs to be only on the
   shard key. This index can be an index of the shard key itself,
   or a :term:`compound index` where the
   shard key is a prefix of the index.

.. important:: The index on the shard key **cannot** be a
   :ref:`multikey index <index-type-multikey>`.

A sharded collection named ``people`` has for its shard key the field
``zipcode``. It currently has the index ``{ zipcode: 1 }``. You can
replace this index with a compound index ``{ zipcode: 1, username: 1
}``, as follows:

#. Create an index on ``{ zipcode: 1, username: 1 }``:

   .. code-block:: javascript

      db.people.ensureIndex( { zipcode: 1, username: 1 } );

#. When MongoDB finishes building the index, you can safely drop the
   existing index on ``{ zipcode: 1 }``:

   .. code-block:: javascript

      db.people.dropIndex( { zipcode: 1 } );

Since the index on the shard key cannot be a multikey index, the index
``{ zipcode: 1, username: 1 }`` can only replace the index ``{ zipcode:
1 }`` if there are no array values for the ``username`` field.

If you drop the last valid index for the shard key, recover by
recreating an index on just the shard key.

For restrictions on shard key indexes, see :ref:`limits-shard-keys`.
.. index:: shard key
   single: sharding; shard key

.. _sharding-shard-key:
.. _shard-key:
.. _sharding-internals-shard-keys:

==========
Shard Keys
==========

.. default-domain:: mongodb

The shard key determines the distribution of the collection's
:term:`documents <document>` among the cluster's :term:`shards
<shard>`. The shard key is either an indexed :term:`field` or an
indexed compound field that exists in every document in the
collection.

MongoDB partitions data in the collection using ranges of shard key
values. Each range, or :term:`chunk`, defines a non-overlapping range
of shard key values. MongoDB distributes the chunks, and their
documents, among the shards in the cluster.

.. include:: /images/sharding-range-based.rst

When a chunk grows beyond the :ref:`chunk size <sharding-chunk-size>`,
MongoDB :term:`splits <split>` the chunk into smaller chunks, always
based on ranges in the shard key.

Considerations
--------------

Shard keys are immutable and cannot be changed after
insertion. See the :ref:`system limits for sharded cluster
<limits-sharding>` for more information.

The index on the shard key **cannot** be a :ref:`multikey index
<index-type-multikey>`.

.. _sharding-hashed-sharding:

Hashed Shard Keys
-----------------

.. versionadded:: 2.4

Hashed shard keys use a :ref:`hashed index <index-hashed-index>` of a
single field as the :term:`shard key` to partition data across your
sharded cluster.

The field you choose as your hashed shard key should have a good
cardinality, or large number of different values. Hashed keys work
well with fields that increase monotonically like :term:`ObjectId`
values or timestamps.

If you shard an empty collection using a hashed shard key, MongoDB
will automatically create and migrate chunks so that each shard has
two chunks.  You can control how many chunks MongoDB will create with
the ``numInitialChunks`` parameter to :dbcommand:`shardCollection` or
by manually creating chunks on the empty collection using the
:dbcommand:`split` command.

To shard a collection using a hashed shard key, see
:doc:`/tutorial/shard-collection-with-a-hashed-shard-key`.

.. include:: /includes/tip-applications-do-not-need-to-compute-hashes.rst

Impacts of Shard Keys on Cluster Operations
-------------------------------------------

The shard key affects write and query performance by determining how
the MongoDB partitions data in the cluster and how effectively the
:program:`mongos` instances can direct operations to the
cluster. Consider the following operational impacts of shard key
selection:

.. index:: shard key; write scaling
.. _sharding-shard-key-write-scaling:

Write Scaling
~~~~~~~~~~~~~

Some possible shard keys will allow your application to take advantage of
the increased write capacity that the cluster can provide, while
others do not. Consider the following example where you shard by the
values of the default :term:`_id` field, which is :term:`ObjectId`.

MongoDB generates ``ObjectId`` values upon document creation to
produce a unique identifier for the object. However, the most
significant bits of data in this value represent a time stamp, which
means that they increment in a regular and predictable pattern. Even
though this value has :ref:`high cardinality
<sharding-shard-key-cardinality>`, when using this, *any date, or
other monotonically increasing number* as the shard key, all insert
operations will be storing data into a single chunk, and therefore, a
single shard. As a result, the write capacity of this shard will
define the effective write capacity of the cluster.

A shard key that increases monotonically will not hinder performance
if you have a very low insert rate, or if most of your write
operations are :method:`~db.collection.update()` operations
distributed through your entire data set. Generally, choose shard keys
that have *both* high cardinality and will distribute write operations
across the *entire cluster*.

Typically, a computed shard key that has some amount of "randomness,"
such as ones that include a cryptographic hash (i.e. MD5 or SHA1) of
other content in the document, will allow the cluster to scale write
operations. However, random shard keys do not typically provide
:ref:`query isolation <sharding-shard-key-query-isolation>`, which is
another important characteristic of shard keys.

.. versionadded:: 2.4
   MongoDB makes it possible to shard a collection on a hashed
   index. This can greatly improve write scaling. See
   :doc:`/tutorial/shard-collection-with-a-hashed-shard-key`.

.. _sharding-internals-querying:

Querying
~~~~~~~~

The :program:`mongos` provides an interface for applications to
interact with sharded clusters that hides the complexity of :term:`data
partitioning <partition>`. A :program:`mongos` receives queries from
applications, and uses metadata from the :ref:`config server
<sharding-config-server>`, to route queries to the :program:`mongod`
instances with the appropriate data. While the :program:`mongos`
succeeds in making all querying operational in sharded environments,
the :term:`shard key` you select can have a profound affect on query
performance.

.. seealso:: The :doc:`/core/sharded-cluster-query-router` and
   :ref:`config server <sharding-config-server>` sections for a more
   general overview of querying in sharded environments.

.. index:: shard key; query isolation
.. _sharding-shard-key-query-isolation:

Query Isolation
```````````````

The fastest queries in a sharded environment are those that
:program:`mongos` will route to a single shard, using the
:term:`shard key` and the cluster meta data from the :ref:`config server
<sharding-config-server>`. For queries that don't include the shard
key, :program:`mongos` must query all shards, wait for their response
and then return the result to the application. These "scatter/gather"
queries can be long running operations.

If your query includes the first component of a compound shard
key [#shard-key-index]_, the :program:`mongos` can route the
query directly to a single shard, or a small number of shards, which
provides better performance. Even if you query values of the shard
key reside in different chunks, the :program:`mongos` will route
queries directly to specific shards.

To select a shard key for a collection:

- determine the most commonly included fields in queries for a
  given application

- find which of these operations are most performance dependent.

.. todo:: - link to document produced by DOCS-235

If this field has low cardinality (i.e not sufficiently
selective) you should add a second field to the shard key making a
compound shard key. The data may become more splittable with a
compound shard key.

.. see:: :doc:`/core/sharded-cluster-query-router` for more information on query
   operations in the context of sharded clusters.

.. [#shard-key-index] In many ways, you can think of the shard key a
   cluster-wide unique index. However, be aware that sharded systems
   cannot enforce cluster-wide unique indexes *unless* the unique
   field is in the shard key. Consider the :doc:`/core/indexes` page
   for more information on indexes and compound indexes.

Sorting
```````

In sharded systems, the :program:`mongos` performs a merge-sort of all
sorted query results from the shards. See
:doc:`/core/sharded-cluster-query-router` and :ref:`index-sort` for
more information.
=================
Sharding Concepts
=================

.. default-domain:: mongodb

These documents present the details of sharding in MongoDB. These
include the components, the architectures, and the behaviors of MongoDB
sharded clusters. For an overview of sharding and sharded clusters, see
:doc:`/core/sharding-introduction`.

.. include:: /includes/toc/dfn-list-spec-sharding-core-landing.rst

.. include:: /includes/toc/sharding-core-landing.rst
=================================
Data Types in the ``mongo`` Shell
=================================

.. default-domain:: mongodb

MongoDB :term:`BSON` provides support for additional data types than
:term:`JSON`. :doc:`Drivers </applications/drivers>` provide native
support for these data types in host languages and the
:program:`mongo` shell also provides several helper classes to support
the use of these data types in the :program:`mongo` JavaScript
shell. See :doc:`/reference/mongodb-extended-json` for additional
information.

.. _mongo-shell-data-type:

Types
-----

Date
~~~~

The :program:`mongo` shell provides various options to return the date,
either as a string or as an object:

- ``Date()`` method which returns the current date as a string.

- ``Date()`` constructor which returns an ``ISODate`` object when used
  with the ``new`` operator.

- ``ISODate()`` constructor which returns an ``ISODate`` object when
  used with *or* without the ``new`` operator.

Consider the following examples:

- To return the date as a string, use the ``Date()`` method, as in the
  following example:

  .. code-block:: javascript

     var myDateString = Date();

  - To print the value of the variable, type the variable name in the
    shell, as in the following:

    .. code-block:: javascript

       myDateString

    The result is the value of ``myDateString``:

    .. code-block:: javascript

       Wed Dec 19 2012 01:03:25 GMT-0500 (EST)

  - To verify the type, use the ``typeof`` operator, as in the
    following:

    .. code-block:: javascript

       typeof myDateString

    The operation returns ``string``.

- To get the date as an ``ISODate`` object, instantiate a new instance
  using the ``Date()`` constructor with the ``new`` operator, as in the
  following example:

  .. code-block:: javascript

     var myDateObject = new Date();

  - To print the value of the variable, type the variable name in the
    shell, as in the following:

    .. code-block:: javascript

       myDateObject

    The result is the value of ``myDateObject``:

    .. code-block:: javascript

       ISODate("2012-12-19T06:01:17.171Z")

  - To verify the type, use the ``typeof`` operator, as in the
    following:

    .. code-block:: javascript

       typeof myDateObject

    The operation returns ``object``.

- To get the date as an ``ISODate`` object, instantiate a new instance
  using the ``ISODate()`` constructor *without* the ``new`` operator,
  as in the following example:

  .. code-block:: javascript

     var myDateObject2 = ISODate();

  You can use the ``new`` operator with the ``ISODate()`` constructor
  as well.

  - To print the value of the variable, type the variable name in the
    shell, as in the following:

    .. code-block:: javascript

       myDateObject2

    The result is the value of ``myDateObject2``:

    .. code-block:: javascript

       ISODate("2012-12-19T06:15:33.035Z")

  - To verify the type, use the ``typeof`` operator, as in the
    following:

    .. code-block:: javascript

       typeof myDateObject2

    The operation returns ``object``.

ObjectId
~~~~~~~~

The :program:`mongo` shell provides the ``ObjectId()`` wrapper class
around :term:`ObjectId` data types. To generate a new ObjectId, use
the following operation in the :program:`mongo` shell:

.. code-block:: javascript

   new ObjectId

.. see:: :doc:`/reference/object-id` for full documentation of ObjectIds in
   MongoDB.

.. _shell-type-long:

NumberLong
~~~~~~~~~~

By default, the :program:`mongo` shell treats all numbers as
floating-point values. The :program:`mongo` shell provides the
``NumberLong()`` class to handle 64-bit integers.

The ``NumberLong()`` constructor accepts the long as a string:

.. code-block:: javascript

   NumberLong("2090845886852")

The following examples use the ``NumberLong()`` class to write to the
collection:

.. code-block:: javascript

   db.collection.insert( { _id: 10, calc: NumberLong("2090845886852") } )
   db.collection.update( { _id: 10 },
                         { $set:  { calc: NumberLong("2555555000000") } } )
   db.collection.update( { _id: 10 },
                         { $inc: { calc: NumberLong(5) } } )

Retrieve the document to verify:

.. code-block:: javascript

   db.collection.findOne( { _id: 10 } )

In the returned document, the ``calc`` field contains a
``NumberLong`` object:

.. code-block:: sh

   { "_id" : 10, "calc" : NumberLong("2555555000005") }

If you use the :update:`$inc` to increment the value of a field that
contains a ``NumberLong`` object by a **float**, the data type changes
to a floating point value, as in the following example:

#. Use :update:`$inc` to increment the ``calc`` field by ``5``, which the
   :program:`mongo` shell treats as a float:

   .. code-block:: javascript

      db.collection.update( { _id: 10 },
                            { $inc: { calc: 5 } } )

#. Retrieve the updated document:

   .. code-block:: javascript

      db.collection.findOne( { _id: 10 } )

   In the updated document, the ``calc`` field contains a floating
   point value:

   .. code-block:: sh

      { "_id" : 10, "calc" : 2555555000010 }

.. _shell-type-int:

NumberInt
~~~~~~~~~

By default, the :program:`mongo` shell treats all numbers as
floating-point values. The :program:`mongo` shell provides the
``NumberInt()`` constructor to explicitly specify 32-bit integers.

.. _check-types-in-shell:

Check Types in the ``mongo`` Shell
----------------------------------

To determine the type of fields, the :program:`mongo` shell provides
the following operators:

- ``instanceof`` returns a boolean to test if a value has
  a specific type.

- ``typeof`` returns the type of a field.

.. example::

   Consider the following operations using ``instanceof`` and
   ``typeof``:

   - The following operation tests whether the ``_id`` field is of type
     ``ObjectId``:

     .. code-block:: javascript

        mydoc._id instanceof ObjectId

     The operation returns ``true``.

   - The following operation returns the type of the ``_id`` field:

     .. code-block:: javascript

        typeof mydoc._id

     In this case ``typeof`` will return the more generic ``object``
     type rather than ``ObjectId`` type.
=====================================
Single Purpose Aggregation Operations
=====================================

.. default-domain:: mongodb

Aggregation refers to a broad class of data manipulation operations
that compute a result based on an input *and* a specific
procedure. MongoDB provides a number of aggregation operations that
perform specific aggregation operations on a set of data.

Although limited in scope, particularly compared to the
:doc:`aggregation pipeline </core/aggregation>` and :doc:`map-reduce
</core/map-reduce>`, these operations provide straightforward
semantics for common data processing options.

Count
-----

MongoDB can return a count of the number of documents that match a
query. The :dbcommand:`count` command as well as the
:method:`~db.collection.count()` and :method:`cursor.count()` methods
provide access to counts in the :program:`mongo` shell.

.. example::

   Given a collection named ``records`` with *only* the following
   documents:

   .. code-block:: javascript

      { a: 1, b: 0 }
      { a: 1, b: 1 }
      { a: 1, b: 4 }
      { a: 2, b: 2 }

   The following operation would count all documents in the collection
   and return the number ``4``:

   .. code-block:: javascript

      db.records.count()

   The following operation will count only the documents where the
   value of the field ``a`` is ``1`` and return ``3``:

   .. code-block:: javascript

      db.records.count( { a: 1 } )

Distinct
--------

The *distinct* operation takes a number of documents that match a query
and returns all of the unique values for a field in the matching documents.
The :dbcommand:`distinct` command and :method:`db.collection.distinct()` method provide
this operation in the :program:`mongo` shell. Consider the following examples of a distinct operation:

.. include:: /images/distinct.rst

.. example::

   Given a collection named ``records`` with *only* the following
   documents:

   .. code-block:: javascript

      { a: 1, b: 0 }
      { a: 1, b: 1 }
      { a: 1, b: 1 }
      { a: 1, b: 4 }
      { a: 2, b: 2 }
      { a: 2, b: 2 }

   Consider the following :method:`db.collection.distinct()`
   operation which returns the distinct values of the field ``b``:

   .. code-block:: javascript

      db.records.distinct( "b" )

   The results of this operation would resemble:

   .. code-block:: javascript

      [ 0, 1, 4, 2 ]

Group
-----

The *group* operation takes a number of documents that match a query,
and then collects groups of documents based on the value of a field or
fields. It returns an array of documents with computed results for
each group of documents.

Access the grouping functionality via the :dbcommand:`group` command
or the :method:`db.collection.group()` method in the :program:`mongo`
shell.

.. warning:: :dbcommand:`group` does not support data in sharded
   collections. In addition, the results of the :dbcommand:`group`
   operation must be no larger than 16 megabytes.

Consider the following group operation:

.. example::

   Given a collection named ``records`` with the following documents:

   .. code-block:: javascript

      { a: 1, count: 4 }
      { a: 1, count: 2 }
      { a: 1, count: 4 }
      { a: 2, count: 3 }
      { a: 2, count: 1 }
      { a: 1, count: 5 }
      { a: 4, count: 4 }

   Consider the following :dbcommand:`group` operation which groups documents by the
   field ``a``, where ``a`` is less than ``3``, and sums the field ``count`` for each group:

   .. code-block:: javascript

      db.records.group( {
         key: { a: 1 },
         cond: { a: { $lt: 3 } },
         reduce: function(cur, result) { result.count += cur.count },
         initial: { count: 0 }
      } )

   The results of this group operation would resemble the following:

   .. code-block:: javascript

      [
        { a: 1, count: 15 },
        { a: 2, count: 4 }
      ]

.. seealso:: The :pipeline:`$group` for related functionality in the
   :doc:`aggregation pipeline </core/aggregation-pipeline>`.
=======
Storage
=======

.. default-domain:: mongodb

Data Model
----------

MongoDB stores data in the form of :term:`BSON` documents, which are
rich mappings of keys, or field names, to values. BSON supports a rich
collection of types, and fields in BSON documents may hold arrays of
values or embedded documents. All documents in MongoDB must be less
than 16MB, which is the :limit:`BSON document size <BSON Document Size>`.

Every document in MongoDB is stored in a *record* which contains the
document itself and extra space, or :term:`padding`, which allows the
document to grow as the result of updates.

All records are contiguously located on disk, and when a document
becomes larger than the allocated record, MongoDB must allocate a new
record. New allocations require MongoDB to move a document and update
all indexes that refer to the document, which takes more time than
in-place updates and leads to storage
fragmentation.

All records are part of a :term:`collection`, which is a logical
grouping of documents in a MongoDB database. The documents in a
collection share a set of indexes, and typically these documents share
common fields and structure.

In MongoDB the :term:`database` construct is a group of related
collections. Each database has a distinct set of data files and can
contain a large number of collections. Also, each database has one
distinct write lock, that blocks operations to the database
during write operations. A single MongoDB deployment may have many
databases.

Journal
-------

In order to ensure that all modifications to a MongoDB data set are
durably written to disk, MongoDB records all modifications to a
journal that it writes to disk more frequently than it writes the data
files. The journal allows MongoDB to successfully recover
data from data files after a :program:`mongod` instance exits without
flushing all changes.

See :doc:`/core/journaling` for more information about the journal in
MongoDB.

.. _record-allocation-stratgies:

Record Allocation Strategies
----------------------------

MongoDB supports multiple record allocation strategies that determine
how :program:`mongod` adds padding to a document when creating a
:term:`record`. Because documents in MongoDB may grow after insertion
and all records are contiguous on disk, the padding can reduce the
need to relocate documents on disk following updates. Relocations are
less efficient than in-place updates, and can lead to storage
fragmentation. As a result, all padding strategies trade additional
space for increased efficiency and decreased fragmentation.

Different allocation strategies support different kinds of workloads:
the :ref:`power of 2 allocations <power-of-2-allocation>` are more
efficient for insert/update/delete workloads; while :ref:`exact fit
allocations <exact-fit-allocation>` is ideal for collections *without*
update and delete workloads.

.. _power-of-2-allocation:

Power of 2 Sized Allocations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.6
   For all new collections, :collflag:`usePowerOf2Sizes` became the
   default allocation strategy. To change the default allocation strategy, use
   the :parameter:`newCollectionsUsePowerOf2Sizes` parameter.

:program:`mongod` uses an allocation strategy called
:collflag:`usePowerOf2Sizes` where each record has a size in bytes
that is a power of 2 (e.g. 32, 64, 128, 256, 512...16777216.) The
smallest allocation for a document is 32 bytes. The power of 2 sizes
allocation strategy has two key properties:

- there are a limited number of record allocation sizes, which makes
  it easier for :program:`mongod` to reuse existing allocations, which
  will reduce fragmentation in some cases.

- in many cases, the record allocations are significantly larger than
  the documents they hold. This allows documents to grow while
  minimizing or eliminating the chance that the :program:`mongod` will
  need to allocate a new record if the document grows.

The :collflag:`usePowerOf2Sizes` strategy does not *eliminate* document
reallocation as a result of document growth, but it minimizes its
occurrence in many common operations.

.. _exact-fit-allocation:

Exact Fit Allocation
~~~~~~~~~~~~~~~~~~~~

The exact fit allocation strategy allocates record sizes based on the
size of the document and an additional *padding factor*. Each
collection has its own padding factor, which defaults to ``0`` when you
insert the first document in a collection, and may grow incrementally
to ``2`` as as documents grow as a result of updates.

Multiply the size of a document by the padding factor to determine the
total record size. That is:

.. code-block:: none

   record size = paddingFactor * <document size>.

The size of each record in a collection reflects the size of the
padding factor at the time of allocation. See the
:data:`~collStats.paddingFactor` field in the output of
:method:`db.collection.stats()` to see the current padding factor for
a collection.

On average, this exact fit allocation strategy uses less storage space
than the :collflag:`usePowerOf2Sizes` strategy but will result in
higher levels of storage fragmentation if documents grow beyond the
size of their initial allocation.

The :dbcommand:`compact` and :dbcommand:`repairDatabase` operations
remove padding by default, as do the :program:`mongodump` and
:program:`mongorestore`. :dbcommand:`compact` does allow you to
specify a padding for records during compaction.

Capped Collections
------------------

:term:`Capped collections <capped collection>` are fixed-size
collections that support high-throughput operations that store records
in insertion order. Capped collections work like circular buffers:
once a collection fills its allocated space, it makes room for new
documents by overwriting the oldest documents in the collection.

See :doc:`/core/capped-collections` for more information.
.. _tag-aware-sharding:

==================
Tag Aware Sharding
==================

.. default-domain:: mongodb

MongoDB supports tagging a range of :term:`shard key` values to
associate that range with a shard or group of shards. Those shards
receive all inserts within the tagged range.

The balancer obeys tagged range associations, which enables the
following deployment patterns:

- isolate a specific subset of data on a specific set of shards.

- ensure that the most relevant data reside on shards that are
  geographically closest to the application servers.

This document describes the behavior, operation, and use of tag aware
sharding in MongoDB deployments.

Considerations
--------------

- :term:`Shard key` range tags are distinct from :ref:`replica set
  member tags <replica-set-read-preference-tag-sets>`.

- :term:`Hash-based sharding <hashed shard key>` does not support
  tag-aware sharding.

.. _shards-tag-sets:

Behavior and Operations
-----------------------

The balancer migrates chunks of documents in a sharded collections to
the shards associated with a tag that has a :term:`shard key` range
with an *upper* bound *greater* than the chunk's *lower* bound.

During balancing rounds, if the balancer detects that any chunks
violate configured tags, the balancer migrates chunks in tagged ranges
to shards associated with those tags.

After configuring tags with a shard key range, and associating it with
a shard or shards, the cluster may take some time to balance the data
among the shards. This depends on the division of chunks and the
current distribution of data in the cluster.

Once configured, the balancer respects tag ranges during future
:ref:`balancing rounds <sharding-internals-balancing>`.

.. seealso::

   :doc:`/tutorial/administer-shard-tags`

Chunks that Span Multiple Tag Ranges
------------------------------------

A single chunk may contain data with a :term:`shard key` values that
falls into ranges associated with more than one tag. To accommodate
these situations, the balancer may migrate chunks to shards that
contain shard key values that exceed the upper bound of the selected
tag range.

.. example::

   Given a sharded collection with two configured tag ranges:

   - :term:`Shard key` values between ``100`` and ``200`` have tags to
     direct corresponding chunks to shards tagged ``NYC``.

   - Shard key values between ``200`` and ``300`` have tags to
     direct corresponding chunks to shards tagged ``SFO``.

   For this collection cluster, the balancer will migrate a chunk with :term:`shard
   key` values ranging between ``150`` and ``220`` to a shard tagged
   ``NYC``, since ``150`` is closer to ``200`` than ``300``.

To ensure that your collection has no potentially ambiguously tagged
chunks, :doc:`create splits on your tag boundaries
</tutorial/split-chunks-in-sharded-cluster>`. You can then manually
migrate chunks to the appropriate shards, or wait for the balancer to
automatically migrate these chunks.
.. index:: write concern
.. _write-concern:
.. _write-operations-write-concern:

=============
Write Concern
=============

.. default-domain:: mongodb

.. include:: /includes/introduction-write-concern.rst

.. seealso:: :doc:`/reference/write-concern` for a reference of
   specific write concern configuration. Also consider
   :doc:`/core/write-operations` for a general overview of write
   operations with MongoDB and :doc:`/core/replica-set-write-concern`
   for considerations specific to replica sets.

Considerations
--------------

Default Write Concern
~~~~~~~~~~~~~~~~~~~~~

The :doc:`driver write concern </release-notes/drivers-write-concern>`
change created a new connection class in all of the MongoDB
drivers. The new class, called ``MongoClient`` changes the default
write concern. See the :doc:`release notes
</release-notes/drivers-write-concern>` for this change and the
release notes for your driver.

Read Isolation
~~~~~~~~~~~~~~

.. include:: /includes/fact-write-concern-read-uncommitted.rst

Write Concern Levels
--------------------

Clients issue write operations with some level of :term:`write
concern`.  MongoDB has the following levels of conceptual write
concern, listed from weakest to strongest:

.. _write-concern-unacknowledged:

Unacknowledged
~~~~~~~~~~~~~~

With an *unacknowledged* write concern, MongoDB does not acknowledge
the receipt of write operation. *Unacknowledged* is similar to *errors
ignored*; however, drivers attempt to receive and handle network errors
when possible. The driver's ability to detect network errors depends on
the system's networking configuration.

To set *unacknowledged* write concern, specify ``w`` values of ``0``
to your driver.

Before the releases outlined in :ref:`driver-write-concern-change`,
this was the default write concern.

.. include:: /images/crud-write-concern-unack.rst

.. _write-concern-acknowledged:

Acknowledged
~~~~~~~~~~~~

With a receipt *acknowledged* write concern, the :program:`mongod`
confirms the receipt of the write operation. *Acknowledged* write
concern allows clients to catch network, duplicate key, and other
errors.

To set *acknowledged* write concern, specify ``w`` values of ``1``
to your driver.

MongoDB uses *acknowledged* write concern by default, after the
releases outlined in :ref:`driver-write-concern-change`.

.. include:: /images/crud-write-concern-ack.rst

Internally, the default write concern calls :dbcommand:`getLastError`
with no arguments. For replica sets, you can define the default write
concern settings in the
:data:`~local.system.replset.settings.getLastErrorDefaults`. When
:data:`~local.system.replset.settings.getLastErrorDefaults` does not
define a default write concern setting, :dbcommand:`getLastError`
defaults to basic receipt acknowledgment.

.. _write-concern-replica-journaled:

Journaled
~~~~~~~~~

With a *journaled* write concern, the :program:`mongod` acknowledges the
write operation only after committing the data to the :term:`journal`.
This write concern ensures that MongoDB can recover the data following
a shutdown or power interruption.

To set a *journaled* write concern, specify ``w`` values of ``1`` and
set the ``journal`` or ``j`` option to ``true`` for your driver. You
must have journaling enabled to use this write concern.

With a *journaled* write concern, write operations must wait for the next
journal commit. To reduce latency for these operations, MongoDB
also increases the frequency that it commits operations to the journal. See
:setting:`~storage.journal.commitIntervalMs` for more information.

.. include:: /images/crud-write-concern-journal.rst

.. include:: /includes/note-write-concern-journaled-replication.rst

.. _write-concern-replica-acknowledged:
.. _replica-set-write-concern:

Replica Acknowledged
~~~~~~~~~~~~~~~~~~~~

:term:`Replica sets <replica set>` add several considerations for
write concern. Basic write concerns affect write operations on only
one :program:`mongod` instance. The ``w`` argument to
:dbcommand:`getLastError` provides *replica acknowledged* write
concerns. With *replica acknowledged* you can guarantee that the write
operation propagates to the members of a replica set. See
:doc:`/reference/write-concern` document for the values for ``w`` and
:doc:`Write Concern for Replica Sets <replica-set-write-concern>` for
more information.

To set *replica acknowledged* write concern, specify ``w`` values
greater than ``1`` to your driver.

.. include:: /images/crud-write-concern-w2.rst

.. include:: /includes/note-write-concern-journaled-replication.rst
.. index:: write operations
.. index:: crud; write operations

=========================
Write Operations Overview
=========================

.. default-domain:: mongodb

A write operation is any operation that creates or modifies data in the
MongoDB instance. In MongoDB, write operations target a single
:term:`collection`. All write operations in MongoDB are atomic on the
level of a single :term:`document`.

There are three classes of write operations in MongoDB: insert, update,
and remove. Insert operations add new data to a collection. Update
operations modify existing data, and remove operations delete data
from a collection. No insert, update, or remove can affect more than
one document atomically.

For the update and remove operations, you can specify criteria, or
conditions, that identify the documents to update or remove. These
operations use the same query syntax to specify the criteria as
:doc:`read operations </core/read-operations>`.

After issuing these modification operations, MongoDB allows
applications to determine the level of acknowledgment returned from
the database. See
:doc:`/core/write-concern`.

Create
------

Create operations add new :term:`documents <document>` to a collection.
In MongoDB, the :method:`db.collection.insert()` method perform create
operations.

The following diagram highlights the components of a MongoDB insert
operation:

.. include:: /images/crud-annotated-mongodb-insert.rst

The following diagram shows the same query in SQL:

.. include:: /images/crud-annotated-sql-insert.rst

.. example::

   The following operation inserts a new documents into the ``users``
   collection. The new document has four fields ``name``, ``age``, and
   ``status``, and an ``_id`` field. MongoDB always adds the ``_id``
   field to the new document if that field does not exist.

   .. code-block:: javascript

      db.users.insert(
         {
            name: "sue",
            age: 26,
            status: "A"
         }
      )

For more information, see :method:`db.collection.insert()` and
:doc:`/tutorial/insert-documents`.

Some updates also create records. If an update operation specifies
the :term:`upsert` flag *and* there are no documents that match the
query portion of the update operation, then MongoDB will convert the
update into an insert.

With an :term:`upsert`, applications can decide between
performing an update or an insert operation using just a
single call. Both the :method:`~db.collection.update()` method
and the :method:`~db.collection.save()` method can perform
an :term:`upsert`. See :method:`~db.collection.update()` and
:method:`~db.collection.save()` for details on performing an
:term:`upsert` with these methods.

.. see:: :doc:`/reference/sql-comparison` for additional examples of
   MongoDB write operations and the corresponding SQL statements.

Insert Behavior
~~~~~~~~~~~~~~~

If you add a new document *without* the :term:`_id` field, the client
library or the :program:`mongod` instance adds an ``_id`` field and
populates the field with a unique :term:`ObjectId <objectid>`.

If you specify the ``_id`` field, the value must be unique within the
collection. For operations with :ref:`write concern <write-concern>`,
if you try to create a document with a duplicate ``_id`` value,
:program:`mongod` returns a duplicate key exception.

Update
------

Update operations modify existing :term:`documents <document>` in a
:term:`collection`. In MongoDB, :method:`db.collection.update()` and
the :method:`db.collection.save()` methods perform update operations.
The :method:`db.collection.update()` method can accept query criteria
to determine which documents to update as well as an option to update
multiple rows. The method can also accept options that affect its
behavior such as the ``multi`` option to update multiple documents.

The following diagram highlights the components of a MongoDB update
operation:

.. include:: /images/crud-annotated-mongodb-update.rst

The following diagram shows the same query in SQL:

.. include:: /images/crud-annotated-sql-update.rst

.. example::

   .. code-block:: javascript

      db.users.update(
         { age: { $gt: 18 } },
         { $set: { status: "A" } },
         { multi: true }
      )

   This update operation on the ``users`` collection sets the
   ``status`` field to ``A`` for the documents that match the criteria
   of ``age`` greater than ``18``.

For more information, see :method:`db.collection.update()` and
:method:`db.collection.save()`, and :doc:`/tutorial/modify-documents`
for examples.

.. _update-multiple-documents:

Update Behavior
~~~~~~~~~~~~~~~

By default, the :method:`db.collection.update()` method updates a
**single** document. However, with the ``multi`` option,
:method:`~db.collection.update()` can update all documents in a
collection that match a query.

The :method:`db.collection.update()` method either updates specific
fields in the existing document or replaces the document. See
:method:`db.collection.update()` for details.

.. include:: /includes/fact-update-field-order.rst

The :method:`db.collection.save()` method replaces a document and can
only update a single document. See :method:`db.collection.save()` and
:doc:`/tutorial/insert-documents` for more information

Delete
------

Delete operations remove documents from a collection. In MongoDB,
:method:`db.collection.remove()` method performs delete operations. The
:method:`db.collection.remove()` method accepts a query criteria to
determine which documents to remove.

The following diagram highlights the components of a MongoDB remove
operation:

.. include:: /images/crud-annotated-mongodb-remove.rst

The following diagram shows the same query in SQL:

.. include:: /images/crud-annotated-sql-delete.rst

.. example::

   .. code-block:: javascript

      db.users.remove(
         { status: "D" }
      )

   This delete operation on the ``users`` collection removes all
   documents that match the criteria of ``status`` equal to ``D``.

For more information, see :method:`db.collection.remove()` method and
:doc:`/tutorial/remove-documents`.

Remove Behavior
~~~~~~~~~~~~~~~

By default, :method:`db.collection.remove()` method removes all
documents that match its query. However, the method can accept a flag
to limit the delete operation to a single document.

.. _write-operations-isolation:

Isolation of Write Operations
-----------------------------

The modification of a single document is always atomic, even if the
write operation modifies multiple sub-documents *within* that
document. For write operations that modify multiple documents, the
operation as a whole is not atomic, and other operations may
interleave.

No other operations are atomic. You can, however, attempt to isolate a
write operation that affects multiple documents using the
:doc:`isolation operator </reference/operator/update/isolated>`.

To isolate a sequence of write operations from other read and write
operations, see :doc:`/tutorial/perform-two-phase-commits`.
================
Write Operations
================

.. default-domain:: mongodb

The following documents describe write operations:

.. include:: /includes/toc/dfn-list-crud-write-operations.rst

.. include:: /includes/toc/crud-write-operations.rst
===========================
Write Operation Performance
===========================

.. default-domain:: mongodb

Indexes
-------

After every insert, update, or delete operation, MongoDB must update
*every* index associated with the collection in addition to the data
itself. Therefore, every index on a collection adds some amount of
overhead for the performance of write operations. [#exceptions]_

In general, the performance gains that indexes provide for *read
operations* are worth the insertion penalty. However, in order to
optimize write performance when possible, be careful when creating new
indexes and evaluate the existing indexes to ensure that your queries
actually use these indexes.

For indexes and queries, see :doc:`/core/query-optimization`. For more
information on indexes, see :doc:`/indexes` and
:doc:`/applications/indexes`.

.. [#exceptions] For inserts and updates to un-indexed fields, the
   overhead for :ref:`sparse indexes <index-type-sparse>` is less than
   for non-sparse indexes. Also for non-sparse indexes, updates that
   do not change the record size have less indexing overhead.

Document Growth
---------------

If an update operation causes a document to exceed the currently
allocated :term:`record size`, MongoDB relocates the document on disk
with enough contiguous space to hold the document. These relocations
take longer than in-place updates, particularly if the collection has
indexes. If a collection has indexes, MongoDB must update all index
entries. Thus, for a collection with many indexes, the move will
impact the write throughput.

Some update operations, such as the :update:`$inc` operation, do not
cause an increase in document size. For these update operations,
MongoDB can apply the updates in-place. Other update operations, such
as the :update:`$push` operation, change the size of the document.

In-place-updates are significantly more efficient than updates that
cause document growth. When possible, use :doc:`data models
</core/data-models>` that minimize the need for document growth.

See :doc:`/core/storage` for more information.

Storage Performance
-------------------

Hardware
~~~~~~~~

The capability of the storage system creates some important physical
limits for the performance of MongoDB's write operations. Many unique
factors related to the storage system of the drive affect write
performance, including random access patterns, disk caches,
disk readahead and RAID configurations.

Solid state drives (SSDs) can outperform spinning hard disks (HDDs) by
100 times or more for random workloads.

.. see:: :doc:`/administration/production-notes` for recommendations
   regarding additional hardware and configuration options.

Journaling
~~~~~~~~~~

MongoDB uses *write ahead logging* to an on-disk :term:`journal` to
guarantee :doc:`write operation </core/write-operations>` durability
and to provide crash resiliency. Before applying a change to the data
files, MongoDB writes the change operation to the journal.

While the durability assurance provided by the journal typically
outweigh the performance costs of the additional write operations,
consider the following interactions between the journal and
performance:

- if the journal and the data file reside on the same block device,
  the data files and the journal may have to contend for a finite
  number of available write operations. Moving the journal to a
  separate device may increase the capacity for write operations.

- if applications specify :doc:`write concern </core/write-concern>`
  that includes :ref:`journaled <write-concern-replica-journaled>`,
  :program:`mongod` will decrease the duration between journal
  commits, which can increases the overall write load.

- the duration between journal commits is configurable using the
  :setting:`~storage.journal.commitIntervalMs` run-time option. Decreasing the
  period between journal commits will increase the number of write
  operations, which can limit MongoDB's capacity for write
  operations. Increasing the amount of time between commits may
  decrease the total number of write operation, but also increases the
  chance that the journal will not record a write operation in the
  event of a failure.

For additional information on journaling, see
:doc:`/core/journaling`.
=======================
MongoDB CRUD Operations
=======================

.. default-domain:: mongodb

MongoDB provides rich semantics for reading and manipulating data.
CRUD stands for *create*, *read*, *update*, and *delete*. These
terms are the foundation for all interactions with the database.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`MongoDB
   CRUD Operations <MongoDB-crud-guide.pdf>`.

.. include:: /includes/toc/dfn-list-crud-landing.rst

.. include:: /includes/toc/crud-landing.rst
=====================
Data Center Awareness
=====================

.. default-domain:: mongodb

MongoDB provides a number of features that allow application
developers and database administrators to customize the behavior of a
:term:`sharded cluster` or :term:`replica set` deployment so that
MongoDB may be *more* "data center aware," or allow operational
and location-based separation.

MongoDB also supports segregation based
on functional parameters, to ensure that certain :program:`mongod`
instances are only used for reporting workloads or that certain
high-frequency portions of a sharded collection only exist on specific
shards.

The following documents, *found either in this section or other sections
of this manual*, provide information on customizing a deployment for
operation- and location-based separation:

.. include:: /includes/toc/dfn-list-spec-data-center-awareness.rst

.. include:: /includes/toc/data-center-awareness.rst

Further Reading
---------------

- The :doc:`/core/write-concern` and :doc:`/core/read-preference`
  documents, which address capabilities related to data center
  awareness.

- :doc:`/tutorial/deploy-geographically-distributed-replica-set`.
===========
Data Models
===========

.. default-domain:: mongodb

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. This
flexibility gives you data-modeling choices to match your application
and its performance requirements.

Read the :doc:`/core/data-modeling-introduction` document for a high
level introduction to data modeling, and proceed to the documents in
the :doc:`/core/data-models` section for additional documentation of
the data model design process. The :ref:`data-modeling-patterns`
documents provide examples of different data models. In addition, the
:ecosystem:`MongoDB Use Case Studies </use-cases>` provide overviews of
application design and include example data models with MongoDB.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`Data Model
   Design for MongoDB <MongoDB-data-models-guide.pdf>`.

.. include:: /includes/toc/dfn-list-data-modeling-landing.rst

.. include:: /includes/toc/data-modeling-landing.rst
================
FAQ: Concurrency
================

.. default-domain:: mongodb

.. versionchanged:: 2.2

MongoDB allows multiple clients to read and write a single corpus of
data using a locking system to ensure that all clients receive the
same view of the data *and* to prevent multiple applications
from modifying the exact same pieces of data at the same time. Locks
help guarantee that all writes to a single document occur either in
full or not at all.

.. seealso:: `Presentation on Concurrency and Internals in 2.2 <http://www.mongodb.com/presentations/concurrency-internals-mongodb-2-2>`_

.. _faq-concurrency-locking:

What type of locking does MongoDB use?
--------------------------------------

MongoDB uses a readers-writer [#multi-reader-lock-names]_ lock that
allows concurrent reads access to a database but gives exclusive
access to a single write operation.

When a read lock exists, many read operations may use this lock. However, when
a write lock exists, a single write operation holds the lock
exclusively, and no other read *or* write operations may share the lock.

Locks are "writer greedy," which means writes have preference over
reads. When both a read and write are waiting for a lock, MongoDB
grants the lock to the write.

.. [#multi-reader-lock-names] You may be familiar with a
   "readers-writer" lock as "multi-reader" or "shared exclusive"
   lock. See the Wikipedia page on `Readers-Writer Locks
   <http://en.wikipedia.org/wiki/Readers%E2%80%93writer_lock>`_ for
   more information.

How granular are locks in MongoDB?
----------------------------------

.. versionchanged:: 2.2

Beginning with version 2.2, MongoDB implements locks on a per-database
basis for most read and write operations. Some global operations,
typically short lived operations involving multiple databases, still
require a global "instance" wide lock. Before 2.2, there is only one
"global" lock per :program:`mongod` instance.

For example, if you have six databases and one takes a write lock, the
other five are still available for read and write.

How do I see the status of locks on my :program:`mongod` instances?
-------------------------------------------------------------------

For reporting on lock utilization information on locks, use any of the
following methods:

- :method:`db.serverStatus()`,
- :method:`db.currentOp()`,
- :doc:`mongotop </reference/program/mongotop>`,
- :doc:`mongostat </reference/program/mongostat>`, and/or
- the `MongoDB Management Service (MMS) <http://mms.mongodb.com/>`_

Specifically, the :data:`~serverStatus.locks` document in the :doc:`output of
serverStatus </reference/command/serverStatus>`, or the :data:`~currentOp.locks` field
in the :doc:`current operation reporting </reference/method/db.currentOp>`
provides insight into the type of locks and amount of lock
contention in your :program:`mongod` instance.

To terminate an operation, use :method:`db.killOp()`.

.. _faq-concurrency-yielding:

Does a read or write operation ever yield the lock?
---------------------------------------------------

In some situations, read and write operations can yield their locks.

Long running read and write operations, such as queries, updates, and
deletes, yield under many conditions. In MongoDB 2.0, operations
yielded based on time slices and the number of operations waiting for
the actively held lock. After 2.2, more adaptive algorithms allow
operations to yield based on predicted disk access (i.e. page faults).

.. versionadded:: 2.0
   Read and write operations will yield their locks if the
   :program:`mongod` receives a :term:`page fault` *or* fetches data that
   is unlikely to be in memory. Yielding allows other operations that
   only need to access documents that are already in memory to complete
   while :program:`mongod` loads documents into memory.

   Additionally, write operations that affect multiple documents
   (i.e. :method:`~db.collection.update()` with the ``multi``
   parameter) will yield periodically to allow read operations during
   these long write operations. Similarly, long running read locks will
   yield periodically to ensure that write operations have the
   opportunity to complete.

.. versionchanged:: 2.2
   The use of yielding expanded greatly in MongoDB 2.2. Including the
   "yield for page fault." MongoDB tracks the contents of memory and
   predicts whether data is available before performing a read. If
   MongoDB predicts that the data is not in memory a read operation
   yields its lock while MongoDB loads the data to memory. Once data
   is available in memory, the read will reacquire the lock to
   complete the operation.

.. _faq-concurrency-operations-locks:

Which operations lock the database?
-----------------------------------

.. versionchanged:: 2.2

The following table lists common database operations and the types of
locks they use.

.. todo In the table below (in the include), the issue of blocked
   JavaScript might no longer apply in version 2.4, which will use V8.

.. include:: /includes/table/lock-behavior-per-operation.rst

Which administrative commands lock the database?
------------------------------------------------

Certain administrative commands can exclusively lock the database for
extended periods of time. In some deployments, for large databases,
you may consider taking the :program:`mongod` instance offline so that
clients are not affected. For example, if a :program:`mongod` is part
of a :term:`replica set`, take the :program:`mongod` offline and let
other members of the set service load while maintenance is in progress.

The following administrative operations require an exclusive
(i.e. write) lock on the database for extended periods:

- :method:`db.collection.ensureIndex()`, when issued
  *without* setting ``background`` to ``true``,
- :dbcommand:`reIndex`,
- :dbcommand:`compact`,
- :method:`db.repairDatabase()`,
- :method:`db.createCollection()`, when creating a very large
  (i.e. many gigabytes) capped collection,
- :method:`db.collection.validate()`, and
- :method:`db.copyDatabase()`. This operation may lock all
  databases. See :ref:`faq-concurrency-lock-multiple-dbs`.

The following administrative commands lock the database but only hold
the lock for a very short time:

- :method:`db.collection.dropIndex()`,
- :method:`db.getLastError()`,
- :method:`db.isMaster()`,
- :method:`rs.status()` (i.e. :dbcommand:`replSetGetStatus`),
- :method:`db.serverStatus()`,
- :method:`db.auth()`, and
- :method:`db.addUser()`.

.. _faq-concurrency-lock-multiple-dbs:

Does a MongoDB operation ever lock more than one database?
----------------------------------------------------------

The following MongoDB operations lock multiple databases:

- :method:`db.copyDatabase()` must lock the entire :program:`mongod`
  instance at once.

- :term:`Journaling <journal>`, which is an internal operation, locks
  all databases for short intervals. All databases share a single
  journal.

- :doc:`User authentication </core/authentication>` locks the
  ``admin`` database as well as the database the user is accessing.

- All writes to a replica set's :term:`primary` lock both the database
  receiving the writes and then the ``local`` database for a short
  time. The lock for the ``local`` database allows the
  :program:`mongod` to write to the primary's :term:`oplog` and
  accounts for a small portion of the total time of the operation.

How does sharding affect concurrency?
-------------------------------------

:term:`Sharding <sharding>` improves concurrency by distributing
collections over multiple :program:`mongod` instances, allowing shard
servers (i.e. :program:`mongos` processes) to perform any number of
operations concurrently to the various downstream :program:`mongod`
instances.

Each :program:`mongod` instance is independent of the others in the
shard cluster and uses the MongoDB :ref:`readers-writer lock
<faq-concurrency-locking>`. The operations on one :program:`mongod`
instance do not block the operations on any others.

.. _faq-concurrency-replication:

How does concurrency affect a replica set primary?
--------------------------------------------------

In :term:`replication`, when MongoDB writes to a collection on the
:term:`primary`, MongoDB also writes to the primary's :term:`oplog`,
which is a special collection in the ``local`` database.  Therefore,
MongoDB must lock both the collection's database and the ``local``
database. The :program:`mongod` must lock both databases at the same
time to keep the database consistent and ensure that write operations, even
with replication, are "all-or-nothing" operations.

How does concurrency affect secondaries?
----------------------------------------

In :term:`replication`, MongoDB does not apply writes serially to
:term:`secondaries <secondary>`. Secondaries collect oplog entries in
batches and then apply those batches in parallel. Secondaries do not
allow reads while applying the write operations, and apply write
operations in the order that they appear in the oplog.

MongoDB can apply several writes in parallel on replica set
secondaries, in two phases:

1. During the first *prefer* phase, under a read lock, the
   :program:`mongod` ensures that all documents affected by the
   operations are in memory. During this phase, other clients may
   execute queries against this member.

2. A thread pool using write locks applies all write operations in the
   batch as part of a coordinated write phase.

What kind of concurrency does MongoDB provide for JavaScript operations?
------------------------------------------------------------------------

.. versionchanged:: 2.4
   The V8 JavaScript engine added in 2.4 allows multiple JavaScript
   operations to run at the same time. Prior to 2.4, a single
   :program:`mongod` could only run a *single* JavaScript operation at
   once.
.. _faq-developers:

=======================================
FAQ: MongoDB for Application Developers
=======================================

.. default-domain:: mongodb

This document answers common questions about application
development using MongoDB.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

.. _faq-dev-namespace:

What is a namespace in MongoDB?
-------------------------------

A "namespace" is the concatenation of the :term:`database` name and
the :term:`collection` names [#indexes-are-namespaces]_ with a period
character in between.

Collections are containers for documents that share one or more
indexes. Databases are groups of collections stored on disk using a
single set of data files. [#ns-limit]_

For an example ``acme.users`` namespace, ``acme`` is the database
name and ``users`` is the collection name. Period characters **can**
occur in collection names, so that ``acme.user.history`` is a
valid namespace, with ``acme`` as the database name, and
``user.history`` as the collection name.

While data models like this appear to support nested collections, the
collection namespace is flat, and there is no difference from the
perspective of MongoDB between ``acme``, ``acme.users``, and
``acme.records``.

.. [#indexes-are-namespaces] Each index also has its own namespace.

.. [#ns-limit] MongoDB database have a configurable limit on the
   :limit:`number of namespaces <Number of Namespaces>` in a database.

How do you copy all objects from one collection to another?
-----------------------------------------------------------

In the :program:`mongo` shell, you can use the following operation to
duplicate the entire collection:

.. code-block:: javascript

   db.source.copyTo(newCollection)

.. include:: /includes/warning-copyto-loss-of-type-fidelity.rst

Also consider the :dbcommand:`cloneCollection` :term:`command
<database command>` that may provide some of this functionality.

If you remove a document, does MongoDB remove it from disk?
-----------------------------------------------------------

Yes.

When you use :method:`~db.collection.remove()`, the object will no longer
exist in MongoDB's on-disk data storage.

When does MongoDB write updates to disk?
----------------------------------------

MongoDB flushes writes to disk on a regular interval. In the default
configuration, MongoDB writes data to the main data files on disk
every 60 seconds and commits the :term:`journal` roughly every 100
milliseconds. These values are configurable with the
:setting:`~storage.journal.commitIntervalMs` and :setting:`~storage.syncPeriodSecs`.

These values represent the *maximum* amount of time between the
completion of a write operation and the point when the write is
durable in the journal, if enabled, and when MongoDB flushes data to
the disk. In many cases MongoDB and the operating system flush data to
disk more frequently, so that the above values represents a
theoretical maximum.

However, by default, MongoDB uses a "lazy" strategy to write to
disk. This is advantageous in situations where the database receives a
thousand increments to an object within one second, MongoDB only needs
to flush this data to disk once. In addition to the aforementioned
configuration options, you can also use :dbcommand:`fsync` and
:dbcommand:`getLastError` to modify this strategy.

How do I do transactions and locking in MongoDB?
------------------------------------------------

MongoDB does not have support for traditional locking or complex
transactions with rollback. MongoDB aims to be lightweight, fast, and
predictable in its performance. This is similar to the MySQL MyISAM
autocommit model. By keeping transaction support extremely simple,
MongoDB can provide greater performance especially for
:term:`partitioned <partition>` or :term:`replicated <replication>`
systems with a number of database server processes.

MongoDB *does* have support for atomic operations *within* a single
document. Given the possibilities provided by nested documents, this
feature provides support for a large number of use-cases.

.. seealso:: The :doc:`/tutorial/isolate-sequence-of-operations` page.

How do you aggregate data with MongoDB?
---------------------------------------

In version 2.1 and later, you can use the new :doc:`aggregation
framework </core/aggregation>`, with the
:dbcommand:`aggregate` command.

MongoDB also supports :term:`map-reduce` with the
:dbcommand:`mapReduce` command, as well as basic aggregation with the
:dbcommand:`group`, :dbcommand:`count`, and
:dbcommand:`distinct`. commands.

.. seealso:: The :doc:`/aggregation` page.

Why does MongoDB log so many "Connection Accepted" events?
----------------------------------------------------------

If you see a very large number connection and re-connection messages
in your MongoDB log, then clients are frequently connecting and
disconnecting to the MongoDB server. This is normal behavior for
applications that do not use request pooling, such as CGI. Consider
using FastCGI, an Apache Module, or some other kind of persistent
application server to decrease the connection overhead.

If these connections do not impact your performance you can use the
run-time :setting:`~systemLog.quiet` option or the command-line option
:option:`--quiet <mongod --quiet>` to suppress these messages from the
log.

Does MongoDB run on Amazon EBS?
-------------------------------

Yes.

MongoDB users of all sizes have had a great deal of success using
MongoDB on the EC2 platform using EBS disks.

.. seealso:: :ecosystem:`Amazon EC2 </platforms/amazon-ec2>`

Why are MongoDB's data files so large?
--------------------------------------

MongoDB aggressively preallocates data files to reserve space and
avoid file system fragmentation. You can use the :setting:`smallfiles`
setting to modify the file preallocation strategy.

.. seealso:: :ref:`faq-disk-size`

.. _faq-small-documents:

How do I optimize storage use for small documents?
--------------------------------------------------

Each MongoDB document contains a certain amount of overhead. This
overhead is normally insignificant but becomes significant if all
documents are just a few bytes, as might be the case if the documents
in your collection only have one or two fields.

Consider the following suggestions and strategies for optimizing
storage utilization for these collections:

- Use the ``_id`` field explicitly.

  MongoDB clients automatically add an ``_id`` field to each document
  and generate a unique 12-byte :term:`ObjectId` for the ``_id``
  field. Furthermore, MongoDB always indexes the ``_id`` field. For
  smaller documents this may account for a significant amount of
  space.

  To optimize storage use, users can specify a value for the ``_id`` field
  explicitly when inserting documents into the collection. This
  strategy allows applications to store a value in the ``_id`` field
  that would have occupied space in another portion of the document.

  You can store any value in the ``_id`` field, but because this value
  serves as a primary key for documents in the collection, it must
  uniquely identify them. If the field's value is not unique, then it
  cannot serve as a primary key as there would be collisions in the
  collection.

- Use shorter field names.

  MongoDB stores all field names in every document. For most
  documents, this represents a small fraction of the space used by a
  document; however, for small documents the field names may represent
  a proportionally large amount of space. Consider a collection of
  documents that resemble the following:

  .. code-block:: javascript

     { last_name : "Smith", best_score: 3.9 }

  If you shorten the filed named ``last_name`` to ``lname`` and the
  field name ``best_score`` to ``score``, as follows, you could save 9
  bytes per document.

  .. code-block:: javascript

     { lname : "Smith", score : 3.9 }

  Shortening field names reduces expressiveness and does not provide
  considerable benefit on for larger documents and where document
  overhead is not significant concern. Shorter field names do not
  reduce the size of indexes, because indexes have a predefined
  structure.

  In general it is not necessary to use short field names.

- Embed documents.

  In some cases you may want to embed documents in other documents
  and save on the per-document overhead.

.. _faq-developers-when-to-use-gridfs:

When should I use GridFS?
-------------------------

For documents in a MongoDB collection, you should always use
:term:`GridFS` for storing files larger than 16 MB.

In some situations, storing large files may be more efficient in a
MongoDB database than on a system-level filesystem.

- If your filesystem limits the number of files in a directory, you can
  use GridFS to store as many files as needed.

- When you want to keep your files and metadata automatically synced
  and deployed across a number of systems and facilities.  When using
  :ref:`geographically distributed replica sets
  <replica-set-geographical-distribution>` MongoDB can distribute
  files and their metadata automatically to a number of
  :program:`mongod` instances and facilities.

- When you want to access information from portions of large
  files without having to load whole files into memory, you can use
  GridFS to recall sections of files without reading the entire file
  into memory.

Do not use GridFS if you need to update the content of the entire file
atomically. As an alternative you can store multiple versions of each
file and specify the current version of the file in the metadata. You
can update the metadata field that indicates "latest" status in an
atomic update after uploading the new version of the file, and later
remove previous versions if needed.

Furthermore, if your files are all smaller the 16 MB
:limit:`BSON Document Size` limit, consider storing the file manually
within a single document. You may use the BinData data type to store
the binary data. See your :doc:`drivers </applications/drivers>`
documentation for details on using BinData.

For more information on GridFS, see :doc:`/core/gridfs`.

How does MongoDB address SQL or Query injection?
------------------------------------------------

BSON
~~~~

As a client program assembles a query in MongoDB, it builds a BSON
object, not a string. Thus traditional SQL injection attacks are not a
problem. More details and some nuances are covered below.

MongoDB represents queries as :term:`BSON` objects. Typically
:doc:`client libraries </applications/drivers>` provide a convenient,
injection free, process to build these objects. Consider the following
C++ example:

.. code-block:: cpp

   BSONObj my_query = BSON( "name" << a_name );
   auto_ptr<DBClientCursor> cursor = c.query("tutorial.persons", my_query);

Here, ``my_query`` then will have a value such as ``{ name : "Joe"
}``. If ``my_query`` contained special characters, for example
``,``, ``:``, and ``{``, the query simply wouldn't match any
documents. For example, users cannot hijack a query and convert it to
a delete.

JavaScript
~~~~~~~~~~

.. note::

   .. include:: /includes/fact-disable-javascript-with-noscript.rst

All of the following MongoDB operations permit you to run arbitrary JavaScript
expressions directly on the server:

- :query:`$where`
- :method:`db.eval()`
- :dbcommand:`mapReduce`
- :dbcommand:`group`

You must exercise care in these cases to prevent users from
submitting malicious JavaScript.

Fortunately, you can express most queries in MongoDB without
JavaScript and for queries that require JavaScript, you can mix
JavaScript and non-JavaScript in a single query. Place all the
user-supplied fields directly in a :term:`BSON` field and pass
JavaScript code to the :query:`$where` field.

- If you need to pass user-supplied values in a :query:`$where`
  clause, you may escape these values with the ``CodeWScope``
  mechanism. When you set user-submitted values as variables in the
  scope document,  you can avoid evaluating them on the database
  server.

- If you need to use :method:`db.eval()` with user supplied values, you can
  either use a ``CodeWScope`` or you can supply extra arguments to your
  function. For instance:

  .. code-block:: sh

     db.eval(function(userVal){...},
             user_value);

  This will ensure that your application sends ``user_value`` to the
  database server as data rather than code.

.. _faq-dollar-sign-escaping:

Dollar Sign Operator Escaping
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Field names in MongoDB's query language have semantic meaning. The
dollar sign (i.e ``$``) is a reserved character used to represent
:doc:`operators </reference/operator>` (i.e. :update:`$inc`.) Thus,
you should ensure that your application's users cannot inject operators
into their inputs.

In some cases, you may wish to build a BSON object with a
user-provided key.  In these situations, keys will need to substitute
the reserved ``$`` and ``.`` characters. Any character is sufficient,
but consider using the Unicode full width equivalents: ``U+FF04``
(i.e. "＄") and ``U+FF0E`` (i.e. "．").

Consider the following example:

.. code-block:: cpp

   BSONObj my_object = BSON( a_key << a_name );

The user may have supplied a ``$`` value in the ``a_key`` value. At
the same time, ``my_object`` might be ``{ $where : "things"
}``. Consider the following cases:

- **Insert**. Inserting this into the database does no harm. The
  insert process does not evaluate the object as a query.

  .. note::

     MongoDB client drivers, if properly implemented, check for
     reserved characters in keys on inserts.

- **Update**.  The :method:`~db.collection.update()` operation permits ``$`` operators
  in the update argument but does not support the
  :query:`$where` operator. Still, some users
  may be able to inject operators that can manipulate a single
  document only. Therefore your application should escape keys, as
  mentioned above, if reserved characters are possible.

- **Query** Generally this is not a problem for queries that
  resemble ``{ x : user_obj }``: dollar signs are not top level and
  have no effect. Theoretically it may be possible for the user to
  build a query themselves. But checking the user-submitted content for
  ``$`` characters in key names may help protect against this kind
  of injection.

Driver-Specific Issues
~~~~~~~~~~~~~~~~~~~~~~

See the "`PHP MongoDB Driver Security Notes
<http://us.php.net/manual/en/mongo.security.php>`_" page in the PHP
driver documentation for more information

.. _faq-dev-concurrency:

How does MongoDB provide concurrency?
-------------------------------------

MongoDB implements a readers-writer lock. This means that
at any one time, only one client may be writing or any number
of clients may be reading, but that reading and writing cannot
occur simultaneously.

In standalone and :term:`replica sets <replica set>` the lock's scope
applies to a single :program:`mongod` instance or :term:`primary`
instance. In a sharded cluster, locks apply to each individual shard,
not to the whole cluster.

For more information, see :doc:`/faq/concurrency`.

.. _faq-dev-compare-order-for-BSON-types:

What is the compare order for BSON types?
-----------------------------------------

MongoDB permits documents within a single collection to
have fields with different :term:`BSON` types. For instance,
the following documents may exist within a single collection.

.. code-block:: javascript

   { x: "string" }
   { x: 42 }

.. include:: /includes/fact-sort-order.rst

Consider the following :program:`mongo` example:

.. code-block:: javascript

   db.test.insert( {x : 3 } );
   db.test.insert( {x : 2.9 } );
   db.test.insert( {x : new Date() } );
   db.test.insert( {x : true } );

   db.test.find().sort({x:1});
   { "_id" : ObjectId("4b03155dce8de6586fb002c7"), "x" : 2.9 }
   { "_id" : ObjectId("4b03154cce8de6586fb002c6"), "x" : 3 }
   { "_id" : ObjectId("4b031566ce8de6586fb002c9"), "x" : true }
   { "_id" : ObjectId("4b031563ce8de6586fb002c8"), "x" : "Tue Nov 17 2009 16:28:03 GMT-0500 (EST)" }

The :query:`$type` operator provides access to :term:`BSON type
<BSON types>` comparison in the MongoDB query syntax. See the
documentation on :term:`BSON types` and the :query:`$type` operator
for additional information.

.. include:: /includes/warning-mixing-types.rst

.. seealso::

   - The :doc:`Tailable Cursors </tutorial/create-tailable-cursor>`
     page for an example of a C++ use of ``MinKey``.

.. commenting out: per scott, we shouldn't be referencing source code;
   *and* this header file actually doesn't contain anything other than
   include statements
   The :source:`jsobj.h <src/mongo/db/jsobj.h>` source
   file for the definition of ``MinKey`` and ``MaxKey``.

.. _faq-developers-multiplication-type-conversion:

When multiplying values of mixed types, what type conversion rules apply?
-------------------------------------------------------------------------

The :update:`$mul` multiplies the numeric value of a field by a
number. For multiplication with values of mixed numeric types (32-bit
integer, 64-bit integer, float), the following type conversion rules
apply:

.. list-table::
   :header-rows: 1

   * -
     - 32-bit Integer
     - 64-bit Integer
     - Float

   * - **32-bit Integer**
     - 32-bit or 64-bit Integer
     - 64-bit Integer
     - Float

   * - **64-bit Integer**
     - 64-bit Integer
     - 64-bit Integer
     - Float

   * - **Float**
     - Float
     - Float
     - Float

.. note::

   - If the product of two 32-bit integers exceeds the maximum value
     for a 32-bit integer, the result is a 64-bit integer.

   - Integer operations of any type that exceed the maximum value for a
     64-bit integer produce an error.

.. _faq-developers-query-for-nulls:

How do I query for fields that have null values?
------------------------------------------------

Fields in a document may store ``null`` values, as in a notional
collection, ``test``, with the following documents:

.. code-block:: javascript

   { _id: 1, cancelDate: null }
   { _id: 2 }

Different query operators treat ``null`` values differently:

- The ``{ cancelDate : null }`` query matches documents that either
  contains the ``cancelDate`` field whose value is ``null`` *or* that
  do not contain the ``cancelDate`` field:

  .. code-block:: javascript

     db.test.find( { cancelDate: null } )

  The query returns both documents:

  .. code-block:: javascript

     { "_id" : 1, "cancelDate" : null }
     { "_id" : 2 }

- The ``{ cancelDate : { $type: 10 } }`` query matches documents that
  contains the ``cancelDate`` field whose value is ``null`` *only*;
  i.e. the value of the ``cancelDate`` field is of BSON Type ``Null``
  (i.e. ``10``) :

  .. code-block:: javascript

     db.test.find( { cancelDate : { $type: 10 } } )

  The query returns only the document that contains the ``null`` value:

  .. code-block:: javascript

     { "_id" : 1, "cancelDate" : null }

- The ``{ cancelDate : { $exists: false } }`` query matches documents
  that do not contain the ``cancelDate`` field:

  .. code-block:: javascript

     db.test.find( { cancelDate : { $exists: false } } )

  The query returns only the document that does *not* contain the
  ``cancelDate`` field:

  .. code-block:: javascript

     { "_id" : 2 }

.. seealso:: The reference documentation for the :query:`$type` and
   :query:`$exists` operators.

.. _faq-restrictions-on-collection-names:

Are there any restrictions on the names of Collections?
-------------------------------------------------------

Collection names can be any UTF-8 string with the following
exceptions:

- A collection name should begin with a letter or an underscore.

- The empty string (``""``) is not a valid collection name.

- Collection names cannot contain the ``$`` character. (version 2.2 only)

- Collection names cannot contain the null character: ``\0``

- Do not name a collection using the ``system.`` prefix. MongoDB
  reserves ``system.``
  for system collections, such as the
  ``system.indexes`` collection.

- The maximum size of a collection name is 128 characters, including
  the name of the database. However, for maximum flexibility,
  collections should have names less than 80 characters.

If your collection name includes special characters, such as the
underscore character, then to access the collection use the
:method:`db.getCollection()` method or a :api:`similar method for your
driver <>`.

.. example:: To create a collection ``_foo`` and insert the
   ``{ a : 1 }`` document, use the following operation:

   .. code-block:: javascript

      db.getCollection("_foo").insert( { a : 1 } )

   To perform a query, use the :method:`~db.collection.find()`
   method, in as the following:

   .. code-block:: javascript

      db.getCollection("_foo").find()

.. _faq-developers-isolate-cursors:

How do I isolate cursors from intervening write operations?
-----------------------------------------------------------

MongoDB cursors can return the same document more than once in some
situations. [#duplicate-document-in-result-set]_ You can use the
:method:`~cursor.snapshot()` method on a cursor to isolate
the operation for a very specific case.

:method:`~cursor.snapshot()` traverses the index on the ``_id`` field
and guarantees that the query will return each document (with respect to
the value of the ``_id`` field) no more than once. [#id-is-immutable]_

The :method:`~cursor.snapshot()` does not guarantee that the data
returned by the query will reflect a single moment in time *nor* does it
provide isolation from insert or delete operations.

.. warning::

   - You **cannot** use :method:`~cursor.snapshot()` with
     :term:`sharded collections <sharding>`.

   - You **cannot** use :method:`~cursor.snapshot()` with
     :method:`~cursor.sort()` or :method:`~cursor.hint()` cursor methods.


As an alternative, if your collection has a field or fields that are
never modified, you can use a *unique* index on this field or these
fields to achieve a similar result as the :method:`~cursor.snapshot()`.
Query with :method:`~cursor.hint()` to explicitly force the query to use
that index.

.. [#duplicate-document-in-result-set] As a cursor returns documents other
   operations may interleave with the query: if some of these
   operations are :doc:`updates </core/write-operations>` that cause the
   document to move (in the case of a table scan, caused by document
   growth) or that change the indexed field on the index used by the
   query; then the cursor will return the same document more than
   once.

.. [#id-is-immutable] MongoDB does not permit changes to the value of the
   ``_id`` field; it is not possible for a cursor that transverses
   this index to pass the same document more than once.

.. _faq-developers-embed-documents:

When should I embed documents within other documents?
-----------------------------------------------------

When :doc:`modeling data in MongoDB </core/data-models>`, embedding
is frequently the choice for:

- "contains" relationships between entities.

- one-to-many relationships when the "many" objects *always* appear
  with or are viewed in the context of their parents.

You should also consider embedding for performance reasons if you have
a collection with a large number of small documents. Nevertheless, if
small, separate documents represent the natural model for the data,
then you should maintain that model.

If, however, you can group these small documents by some logical
relationship *and* you frequently retrieve the documents by this
grouping, you might consider "rolling-up" the small documents into
larger documents that contain an array of subdocuments. Keep in mind
that if you often only need to retrieve a subset of the documents
within the group, then "rolling-up" the documents may not provide
better performance.

"Rolling up" these small documents into logical groupings means that queries to
retrieve a group of documents involve sequential reads and fewer random disk
accesses.

.. Will probably need to break up the following sentence:

Additionally, "rolling up" documents and moving common fields to the
larger document benefit the index on these fields. There would be fewer
copies of the common fields *and* there would be fewer associated key
entries in the corresponding index. See :doc:`/core/indexes` for more
information on indexes.

.. Commenting out.. If the data is too large to fit entirely in RAM,
   embedding provides better RAM cache utilization.

.. Commenting out.. If your small documents are approximately the page
   cache unit size, there is no benefit for ram cache efficiency, although
   embedding will provide some benefit regarding random disk I/O.

Where can I learn more about data modeling in MongoDB?
------------------------------------------------------

Begin by reading the documents in the :doc:`/data-modeling`
section. These documents contain a high level introduction to data
modeling considerations in addition to practical examples of data
models targeted at particular issues.

Additionally, consider the following external resources that provide
additional examples:

.. note: commented links are no longer active and I have not been able
   to locate their new URLs (if indeed they exist)

.. `Walkthrough MongoDB Data Modeling
  <http://blog.fiesta.cc/post/11319522700/walkthrough-mongodb-data-modeling>`_

.. `Document Design for MongoDB <http://oreilly.com/catalog/0636920018391>`_

- `Schema Design by Example <http://www.10gen.com/presentations/mongodb-melbourne-2012/schema-design-example>`_

- `Dynamic Schema Blog Post
  <http://dmerr.tumblr.com/post/6633338010/schemaless>`_

- :ecosystem:`MongoDB Data Modeling and Rails
  </tutorial/model-data-for-ruby-on-rails/>`

- `Ruby Example of Materialized Paths
  <http://github.com/banker/newsmonger/blob/master/app/models/comment.rb>`_

- `Sean Cribs Blog Post
  <http://seancribbs.com/tech/2009/09/28/modeling-a-tree-in-a-document-database>`_
  which was the source for much of the :ref:`data-modeling-trees`
  content.

.. _faq-developers-manual-padding:

Can I manually pad documents to prevent moves during updates?
-------------------------------------------------------------

An update can cause a document to move on disk if the document grows in
size. To *minimize* document movements, MongoDB uses
:term:`padding`.

You should not have to pad manually because MongoDB adds
:ref:`padding automatically <record-allocation-stratgies>` and can
adaptively adjust the amount of padding added to documents to prevent
document relocations following updates.
You can change the default :data:`~collStats.paddingFactor`
calculation by using the :dbcommand:`collMod` command with the
:collflag:`usePowerOf2Sizes` flag. The :collflag:`usePowerOf2Sizes`
flag ensures that MongoDB allocates document space in sizes that are
powers of 2, which helps ensure that MongoDB can efficiently reuse
free space created by document deletion or relocation.

However, *if you must* pad a document manually, you can add a
temporary field to the document and then :update:`$unset` the field,
as in the following example.

.. warning:: Do not manually pad documents in a capped
   collection. Applying manual padding to a document in a capped
   collection can break replication. Also, the padding is not
   preserved if you re-sync the MongoDB instance.

.. code-block:: javascript

   var myTempPadding = [ "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
                         "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
                         "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
                         "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"];

   db.myCollection.insert( { _id: 5, paddingField: myTempPadding } );

   db.myCollection.update( { _id: 5 },
                           { $unset: { paddingField: "" } }
                         )

   db.myCollection.update( { _id: 5 },
                           { $set: { realField: "Some text that I might have needed padding for" } }
                         )

.. seealso::

   :ref:`record-allocation-stratgies`
.. _troubleshooting:

========================
FAQ: MongoDB Diagnostics
========================

.. default-domain:: mongodb

This document provides answers to common diagnostic questions and
issues.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

Where can I find information about a ``mongod`` process that stopped running unexpectedly?
------------------------------------------------------------------------------------------

If :program:`mongod` shuts down unexpectedly on a UNIX or UNIX-based
platform, and if :program:`mongod` fails to log a shutdown or error
message, then check your system logs for messages pertaining to MongoDB.
For example, for logs located in ``/var/log/messages``, use the
following commands:

.. code-block:: sh

   sudo grep mongod /var/log/messages
   sudo grep score /var/log/messages

.. _faq-keepalive:

Does TCP ``keepalive`` time affect sharded clusters and replica sets?
---------------------------------------------------------------------

If you experience socket errors between members of a sharded cluster
or replica set, that do not have other reasonable causes, check the
TCP keep alive value, which Linux systems store as the
``tcp_keepalive_time`` value. A common keep alive period is ``7200``
seconds (2 hours); however, different distributions and OS X may have
different settings. For MongoDB, you will have better experiences with
shorter keepalive periods, on the order of ``300`` seconds (five minutes).

On Linux systems you can use the following operation to check the
value of ``tcp_keepalive_time``:

.. code-block:: sh

   cat /proc/sys/net/ipv4/tcp_keepalive_time

You can change the ``tcp_keepalive_time`` value with the following
operation:

.. code-block:: sh

   echo 300 > /proc/sys/net/ipv4/tcp_keepalive_time

The new ``tcp_keepalive_time`` value takes effect without requiring
you to restart the :program:`mongod` or :program:`mongos`
servers. When you reboot or restart your system you will need to set
the new ``tcp_keepalive_time`` value, or see your operating system's
documentation for setting the TCP keepalive value persistently.

For OS X systems, issue the following command to view the keep alive
setting:

.. code-block:: sh

   sysctl net.inet.tcp.keepinit

To set a shorter keep alive period use the following invocation:

.. code-block:: sh

   sysctl -w net.inet.tcp.keepinit=300

If your replica set or sharded cluster experiences keepalive-related
issues, you must alter the ``tcp_keepalive_time`` value on all machines
hosting MongoDB processes. This includes all machines hosting
:program:`mongos` or :program:`mongod` servers.

Windows users should consider the `Windows Server Technet Article on
KeepAliveTime configuration
<http://technet.microsoft.com/en-us/library/dd349797.aspx#BKMK_2>`_
for more information on setting keep alive for MongoDB deployments on
Windows systems.

What tools are available for monitoring MongoDB?
------------------------------------------------

The `MongoDB Management Services <http://mms.mongodb.com>` includes
monitoring. MMS Monitoring is a free, hosted services for monitoring
MongoDB deployments. A full list of third-party tools is available as
part of the :doc:`/administration/monitoring/` documentation. Also
consider the `MMS Documentation <http://mms.mongodb.com/help/>`_.

.. _faq-memory:

Memory Diagnostics
------------------

Do I need to configure swap space?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Always configure systems to have swap space. Without swap, your system
may not be reliant in some situations with extreme memory constraints,
memory leaks, or multiple programs using the same memory.  Think of
the swap space as something like a steam release valve that allows the
system to release extra pressure without affecting the overall
functioning of the system.

Nevertheless, systems running MongoDB *do not* need swap for routine
operation. Database files are :ref:`memory-mapped
<faq-storage-memory-mapped-files>` and should constitute most of your
MongoDB memory use. Therefore, it is unlikely that :program:`mongod`
will ever use any swap space in normal operation. The operating system
will release memory from the memory mapped files without needing
swap and MongoDB can write data to the data files without needing the swap
system.

.. _faq-fundamentals-working-set:

What is "working set" and how can I estimate its size?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The *working set* for a MongoDB database is the portion of your data
that clients access most often. You can estimate size of the working
set, using the :data:`~serverStatus.workingSet` document in the output
of :dbcommand:`serverStatus`. To return :dbcommand:`serverStatus` with
the :data:`~serverStatus.workingSet` document, issue a command in the
following form:

.. code-block:: javascript

   db.runCommand( { serverStatus: 1, workingSet: 1 } )

Must my working set size fit RAM?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your working set should stay in memory to achieve good performance.
Otherwise many random disk IO's will occur, and unless you are using
SSD, this can be quite slow.

One area to watch specifically in managing the size of your working set
is index access patterns. If you are inserting into indexes at random
locations (as would happen with id's that are randomly
generated by hashes), you will continually be updating the whole index.
If instead you are able to create your id's in approximately ascending
order (for example, day concatenated with a random id), all the updates
will occur at the right side of the b-tree and the working set size for
index pages will be much smaller.

It is fine if databases and thus virtual size are much larger than RAM.

.. todo Commenting out for now:

   .. _faq-fundamentals-working-set-size:

   How can I measure working set size?
   -----------------------------------

   Measuring working set size can be difficult; even if it is much
   smaller than total RAM. If the database is much larger than RAM in
   total, all memory will be indicated as in use for the cache. Thus you
   need a different way to estimate the working set size.

   One technique is to use the `eatmem.cpp
   <https://github.com/mongodb/mongo-snippets/blob/master/cpp/eatmem.cpp>`_.
   utility, which reserves a certain amount of system memory for itself.
   You can run the utility with a certain amount specified and see if
   the server continues to perform well. If not, the working set is
   larger than the total RAM minus the consumed RAM. The test will eject
   some data from the file system cache, which might take time to page
   back in after the utility is terminated.

   Running eatmem.cpp continuously with a small percentage of total RAM,
   such as 20%, is a good technique to get an early warning if memory is
   too low. If disk I/O activity increases significantly, terminate
   eatmem.cpp to mitigate the problem for the moment until further steps
   can be taken.

   In :term:`replica sets <replica set>`, if one server is underpowered
   the eatmem.cpp utility could help as an early warning mechanism for
   server capacity. Of course, the server must be receiving
   representative traffic to get an indication.

How do I calculate how much RAM I need for my application?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. todo Improve this FAQ

The amount of RAM you need depends on several factors, including but not
limited to:

- The relationship between :doc:`database storage </faq/storage>` and working set.

- The operating system's cache strategy for LRU (Least Recently Used)

- The impact of :doc:`journaling </core/journaling>`

- The number or rate of page faults and other MMS gauges to detect when
  you need more RAM

MongoDB defers to the operating system when loading data into memory
from disk. It simply :ref:`memory maps <faq-storage-memory-mapped-files>` all
its data files and relies on the operating system to cache data. The OS
typically evicts the least-recently-used data from RAM when it runs low
on memory. For example if clients access  indexes more frequently than
documents, then indexes will more likely stay in RAM, but it depends on
your particular usage.

To calculate how much RAM you need, you must calculate your working set
size, or the portion of your data that clients use most often. This
depends on your access patterns, what indexes you have, and the size of
your documents.

If page faults are infrequent, your
working set fits in RAM. If fault rates rise higher than that, you risk
performance degradation. This is less critical with SSD drives than
with spinning disks.

How do I read memory statistics in the UNIX ``top`` command
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because :program:`mongod` uses :ref:`memory-mapped files
<faq-storage-memory-mapped-files>`, the memory statistics in ``top``
require interpretation in a special way. On a large database, ``VSIZE``
(virtual bytes) tends to be the size of the entire database. If the
:program:`mongod` doesn't have other processes running, ``RSIZE``
(resident bytes) is the total memory of the machine, as this counts
file system cache contents.

For Linux systems, use the ``vmstat`` command to help determine how
the system uses memory. On OS X systems use ``vm_stat``.

Sharded Cluster Diagnostics
---------------------------

The two most important factors in maintaining a successful sharded cluster are:

- :ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and

- :ref:`sufficient capacity to support current and future operations
  <sharding-capacity-planning>`.

You can prevent most issues encountered with sharding by ensuring that
you choose the best possible :term:`shard key` for your deployment and
ensure that you are always adding additional capacity to your cluster
well before the current resources become saturated. Continue reading
for specific issues you may encounter in a production environment.

.. _sharding-troubleshooting-not-splitting:

In a new sharded cluster, why does all data remains on one shard?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Your cluster must have sufficient data for sharding to make
sense. Sharding works by migrating chunks between the shards until
each shard has roughly the same number of chunks.

The default chunk size is 64 megabytes. MongoDB will not begin
migrations until the imbalance of chunks in the cluster exceeds the
:ref:`migration threshold <sharding-migration-thresholds>`. While the
default chunk size is configurable with the :setting:`~sharding.chunkSize`
setting, these behaviors help prevent unnecessary chunk migrations,
which can degrade the performance of your cluster as a whole.

If you have just deployed a sharded cluster, make sure that you have
enough data to make sharding effective. If you do not have sufficient
data to create more than eight 64 megabyte chunks, then all data will
remain on one shard. Either lower the :ref:`chunk size
<sharding-chunk-size>` setting, or add more data to the cluster.

As a related problem, the system will split chunks only on
inserts or updates, which means that if you configure sharding and do not
continue to issue insert and update operations, the database will not
create any chunks. You can either wait until your application inserts
data *or* :doc:`split chunks manually </tutorial/split-chunks-in-sharded-cluster>`.

Finally, if your shard key has a low :ref:`cardinality
<sharding-shard-key-cardinality>`, MongoDB may not be able to create
sufficient splits among the data.

Why would one shard receive a disproportion amount of traffic in a sharded cluster?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In some situations, a single shard or a subset of the cluster will
receive a disproportionate portion of the traffic and workload. In
almost all cases this is the result of a shard key that does not
effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`.

It's also possible that you have "hot chunks." In this case, you may
be able to solve the problem by splitting and then migrating parts of
these chunks.

In the worst case, you may have to consider re-sharding your data
and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>`
to correct this pattern.

What can prevent a sharded cluster from balancing?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have just deployed your sharded cluster, you may want to
consider the :ref:`troubleshooting suggestions for a new cluster where
data remains on a single shard <sharding-troubleshooting-not-splitting>`.

If the cluster was initially balanced, but later developed an uneven
distribution of data, consider the following possible causes:

- You have deleted or removed a significant amount of data from the
  cluster. If you have added additional data, it may have a
  different distribution with regards to its shard key.

- Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>`
  and MongoDB cannot split the chunks any further.

- Your data set is growing faster than the balancer can distribute
  data around the cluster. This is uncommon and
  typically is the result of:

  - a :ref:`balancing window <sharding-schedule-balancing-window>` that
    is too short, given the rate of data growth.

  - an uneven distribution of :ref:`write operations
    <sharding-shard-key-write-scaling>` that requires more data
    migration. You may have to choose a different shard key to resolve
    this issue.

  - poor network connectivity between shards, which may lead to chunk
    migrations that take too long to complete. Investigate your
    network configuration and interconnections between shards.

Why do chunk migrations affect sharded cluster performance?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If migrations impact your cluster or application's performance,
consider the following options, depending on the nature of the impact:

#. If migrations only interrupt your clusters sporadically, you can
   limit the :ref:`balancing window
   <sharding-schedule-balancing-window>` to prevent balancing activity
   during peak hours. Ensure that there is enough time remaining to
   keep the data from becoming out of balance again.

#. If the balancer is always migrating chunks to the detriment of
   overall cluster performance:

   - You may want to attempt :doc:`decreasing the chunk size </tutorial/modify-chunk-size-in-sharded-cluster>`
     to limit the size of the migration.

   - Your cluster may be over capacity, and you may want to attempt to
     :ref:`add one or two shards <sharding-procedure-add-shard>` to
     the cluster to distribute load.

It's also possible that your shard key causes your
application to direct all writes to a single shard. This kind of
activity pattern can require the balancer to migrate most data soon after writing
it. Consider redeploying your cluster  with a shard key that provides
better :ref:`write scaling <sharding-shard-key-write-scaling>`.
=========================
FAQ: MongoDB Fundamentals
=========================

.. default-domain:: mongodb

This document addresses basic high level questions about MongoDB and
its use.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

What kind of database is MongoDB?
---------------------------------

MongoDB is a :term:`document`\-oriented DBMS. Think of MySQL but with
:term:`JSON`-like objects comprising the data model, rather than RDBMS
tables. Significantly, MongoDB supports neither joins nor transactions.
However, it features secondary indexes, an expressive query language,
atomic writes on a per-document level, and fully-consistent reads.

Operationally, MongoDB features master-slave replication with automated
failover and built-in horizontal scaling via automated range-based
partitioning.

.. note::

   MongoDB uses :term:`BSON`, a binary object format similar
   to, but more expressive than :term:`JSON`.

Do MongoDB databases have tables?
---------------------------------

Instead of tables, a MongoDB database stores its data in
:term:`collections <collection>`, which are the rough equivalent of RDBMS
tables. A collection holds one or more :term:`documents
<document>`, which corresponds to a record or a row in a relational
database table, and each document has
one or more fields, which corresponds to a column in a relational
database table.

Collections have important differences from RDBMS tables. Documents in a
single collection may have a unique combination and set of fields.
Documents need not have identical fields. You can add a field to some
documents in a collection without adding that field to all documents in
the collection.

.. see:: :doc:`/reference/sql-comparison`

.. _faq-schema-free:

Do MongoDB databases have schemas?
----------------------------------

MongoDB uses dynamic schemas. You can create collections without
defining the structure, i.e. the fields or the types of their values,
of the documents in the collection. You can change the structure of
documents simply by adding new fields or deleting existing ones.
Documents in a collection need not have an identical set of fields.

In practice, it is common for the documents in a collection to have
a largely homogeneous structure; however, this is not a
requirement. MongoDB's flexible schemas mean that schema migration and
augmentation are very easy in practice, and you will rarely, if ever,
need to write scripts that perform "alter table" type operations,
which simplifies and facilitates iterative software development with
MongoDB.

.. see:: :doc:`/reference/sql-comparison`

What languages can I use to work with MongoDB?
----------------------------------------------

MongoDB :term:`client drivers <driver>` exist for
all of the most popular programming languages, and many
other ones. See the :ecosystem:`latest list of
drivers </drivers>`
for details.

.. seealso:: :doc:`/applications/drivers`.

Does MongoDB support SQL?
-------------------------

No.

However, MongoDB does support a rich, ad-hoc query language
of its own.

.. seealso:: :doc:`/reference/operator`

What are typical uses for MongoDB?
----------------------------------

MongoDB has a general-purpose design, making it appropriate for a large
number of use cases. Examples include content management
systems, mobile applications, gaming, e-commerce, analytics,
archiving, and logging.

Do not use MongoDB for systems that require SQL,
joins, and multi-object transactions.

Does MongoDB support ACID transactions?
---------------------------------------

MongoDB does not support multi-document transactions.

However, MongoDB does provide atomic operations on a single
document. Often these document-level atomic operations are sufficient
to solve problems that would require ACID transactions in a relational
database.

For example, in MongoDB, you can embed related data in nested arrays or
nested documents within a single document and update the entire
document in a single atomic operation. Relational databases might
represent the same kind of data with multiple tables and rows, which
would require transaction support to update the data atomically.

.. include:: /includes/fact-write-concern-read-uncommitted.rst

Does MongoDB require a lot of RAM?
----------------------------------

Not necessarily. It's certainly possible to run MongoDB
on a machine with a small amount of free RAM.

MongoDB automatically uses all free memory on the machine as its
cache. System resource monitors show that MongoDB uses a lot of
memory, but its usage is dynamic. If another process suddenly needs
half the server's RAM, MongoDB will yield cached memory to the other process.

Technically, the operating system's virtual memory subsystem manages
MongoDB's memory. This means that MongoDB will use as much free memory
as it can, swapping to disk as needed. Deployments with enough memory
to fit the application's working data set in RAM will achieve the best
performance.

.. seealso:: :doc:`/faq/diagnostics` for answers to additional
   questions about MongoDB and Memory use.

How do I configure the cache size?
----------------------------------

MongoDB has no configurable cache. MongoDB uses all *free* memory on
the system automatically by way of memory-mapped files. Operating
systems use the same approach with their file system caches.

.. _faq-database-and-caching:

Does MongoDB require a separate caching layer for application-level caching?
----------------------------------------------------------------------------

No. In MongoDB, a document's representation in the database is similar
to its representation in application memory. This means the database
already stores the usable form of data, making the data usable in both
the persistent store and in the application cache. This eliminates the
need for a separate caching layer in the application.

This differs from relational databases, where caching data is more
expensive. Relational databases must transform data into object
representations that applications can read and must store the
transformed data in a separate cache: if these transformation from
data to application objects require joins, this process increases the
overhead related to using the database which increases the importance
of the caching layer.

Does MongoDB handle caching?
----------------------------

Yes. MongoDB keeps all of the most recently used data in RAM. If you
have created indexes for your queries and your working data set fits
in RAM, MongoDB serves all queries from memory.

MongoDB does not implement a query cache: MongoDB serves all queries
directly from the indexes and/or data files.

Are writes written to disk immediately, or lazily?
--------------------------------------------------

Writes are physically written to the :doc:`journal </core/journaling>`
within 100 milliseconds, by default. At that point, the write is "durable" in the
sense that after a pull-plug-from-wall event, the data will still be
recoverable after a hard restart. See :setting:`~storage.journal.commitIntervalMs`
for more information on the journal commit window.

While the journal commit is nearly instant, MongoDB writes to the data
files lazily. MongoDB may wait to write data to the data files for as
much as one minute by default. This does not affect durability, as the journal
has enough information to ensure crash recovery. To change the interval
for writing to the data files, see :setting:`~storage.syncPeriodSecs`.

What language is MongoDB written in?
------------------------------------

MongoDB is implemented in C++. :term:`Drivers <driver>` and client libraries
are typically written in their respective languages, although some
drivers use C extensions for better performance.

.. _faq-32-bit-limitations:

What are the limitations of 32-bit versions of MongoDB?
-------------------------------------------------------

MongoDB uses :ref:`memory-mapped files <faq-storage-memory-mapped-files>`.
When running a 32-bit build of
MongoDB, the total storage size for the server, including data and
indexes, is 2 gigabytes. For this reason, do not deploy MongoDB to
production on 32-bit machines.

If you're running a 64-bit build of MongoDB, there's virtually no
limit to storage size. For production deployments, 64-bit builds and
operating systems are strongly recommended.

.. seealso:: "`Blog Post: 32-bit Limitations <http://blog.mongodb.org/post/137788967/32-bit-limitations>`_"

.. note::

   32-bit builds disable :term:`journaling <journal>` by default
   because journaling further limits the maximum amount of data that
   the database can
============
FAQ: Indexes
============

.. default-domain:: mongodb

This document addresses common questions regarding MongoDB indexes.

If you don't find the answer you're looking for, check the
:doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.
See also :doc:`/administration/indexes`.

Should you run ``ensureIndex()`` after every insert?
----------------------------------------------------

No. You only need to create an index once for a single
collection. After initial creation, MongoDB automatically updates the
index as data changes.

While running :method:`~db.collection.ensureIndex()` is usually ok,
if an index doesn't exist because of ongoing administrative work, a
call to :method:`~db.collection.ensureIndex()` may disrupt database
availability. Running :method:`~db.collection.ensureIndex()` can render
a replica set inaccessible as the index creation is happening. See
:ref:`index-building-replica-sets`.

How do you know what indexes exist in a collection?
---------------------------------------------------

To list a collection's indexes, use the
:method:`db.collection.getIndexes()` method or a similar :api:`method
for your driver <>`.

How do you determine the size of an index?
------------------------------------------

To check the sizes of the indexes on a collection, use :method:`db.collection.stats()`.

.. todo:: FAQ How do I determine if an index fits into RAM?

What happens if an index does not fit into RAM?
-----------------------------------------------

When an index is too large to fit into RAM, MongoDB must read the index
from disk, which is a much slower operation than reading from RAM. Keep
in mind an index fits into RAM when your server has RAM available for
the index combined with the rest of the :term:`working set`.

In certain cases, an index does not need to fit *entirely* into RAM. For
details, see :ref:`indexing-right-handed`.

.. todo:: FAQ How does MongoDB determine what index to use?

How do you know what index a query used?
----------------------------------------

To inspect how MongoDB processes a query, use the
:method:`~cursor.explain()` method in the :program:`mongo` shell, or in
your application driver.

How do you determine what fields to index?
------------------------------------------

A number of factors determine what fields to index, including
:ref:`selectivity <index-selectivity>`, fitting indexes into RAM,
reusing indexes in multiple queries when possible, and creating indexes
that can support all the fields in a given query. For detailed
documentation on choosing which fields to index, see
:doc:`/administration/indexes`.

.. todo:: FAQ How do I guarantee a query uses an index?
   MongoDB's :ref:`query optimizer <read-operations-query-optimization>`
   always looks for the most advantageous
   index to use. You cannot guarantee use of a particular index, but you
   can write indexes with your queries in mind. For detailed
   documentation on creating optimal indexes, see
   :doc:`/administration/indexes`.

How do write operations affect indexes?
---------------------------------------

Any write operation that alters an indexed field requires an update to
the index in addition to the document itself. If you update a document
that causes the document to grow beyond the allotted record size, then
MongoDB must update all indexes that include this document as part of
the update operation.

Therefore, if your application is write-heavy, creating too many
indexes might affect performance.

Will building a large index affect database performance?
--------------------------------------------------------

Building an index can be an IO-intensive operation, especially if you
have a large collection. This is true on any database system that
supports secondary indexes, including MySQL. If you need to build an
index on a large collection, consider building the index in the
background. See :ref:`index-creation-operations`.

If you build a large index without the background option, and if doing
so causes the database to stop responding,
do one of the following:

- Wait for the index to finish building.

- Kill the current operation (see :method:`db.killOp()`). The partial
  index will be deleted.

.. _faq-index-min-max:

Can I use index keys to constrain query matches?
------------------------------------------------

You can use the :method:`~cursor.min()` and :method:`~cursor.max()`
methods to constrain the results of the cursor returned from
:method:`~db.collection.find()` by using index keys.

Using ``$ne`` and ``$nin`` in a query is slow. Why?
---------------------------------------------------

The :query:`$ne` and :query:`$nin` operators are not selective.
See :ref:`index-selectivity`. If you need to use these,
it is often best to make sure that an additional, more selective
criterion is part of the query.

Can I use a multi-key index to support a query for a whole array?
-----------------------------------------------------------------

Not entirely. The index can partially support these queries because
it can speed the selection of the first element of the array;
however, comparing all subsequent items in the array cannot use the
index and must scan the documents individually.

How can I effectively use indexes strategy for attribute lookups?
-----------------------------------------------------------------

For simple attribute lookups that don't require sorted result sets or
range queries, consider creating a field that contains an array of
documents where each document has a field (e.g. ``attrib`` ) that
holds a specific type of attribute. You can index this ``attrib``
field.

For example, the ``attrib`` field in the following document allows you
to add an unlimited number of attributes types:

.. code-block:: javascript

   { _id : ObjectId(...),
     attrib : [
               { k: "color", v: "red" },
               { k: "shape": v: "rectangle" },
               { k: "color": v: "blue" },
               { k: "avail": v: true }
              ]
   }

*Both* of the following queries could use the same ``{ "attrib.k": 1,
"attrib.v": 1 }`` index:

.. code-block:: javascript

   db.mycollection.find( { attrib: { $elemMatch : { k: "color", v: "blue" } } } )
   db.mycollection.find( { attrib: { $elemMatch : { k: "avail", v: true } } } )
========================
FAQ: The ``mongo`` Shell
========================

.. default-domain:: mongodb

.. _mongo-shell-tips:

How can I enter multi-line operations in the ``mongo`` shell?
-------------------------------------------------------------

If you end a line with an open parenthesis (``'('``), an open brace
(``'{'``), or an open bracket (``'['``), then the subsequent lines start
with ellipsis (``"..."``) until you enter the corresponding closing
parenthesis (``')'``), the closing brace (``'}'``) or the closing
bracket (``']'``). The :program:`mongo` shell waits for the closing
parenthesis, closing brace, or the closing bracket before evaluating
the code, as in the following example:

.. code-block:: javascript

   > if ( x > 0 ) {
   ... count++;
   ... print (x);
   ... }

You can exit the line continuation mode if you enter two blank
lines, as in the following example:

.. code-block:: javascript

   > if (x > 0
   ...
   ...
   >

.. _mongo-shell-getSiblingDB:

How can I access different databases temporarily?
-------------------------------------------------

You can use :method:`db.getSiblingDB()` method to access another
database without switching databases, as in the following example which
first switches to the ``test`` database and then accesses the
``sampleDB`` database from the ``test`` database:

.. code-block:: javascript

   use test

   db.getSiblingDB('sampleDB').getCollectionNames();

.. _mongo-shell-keyboard-shortcuts:

Does the ``mongo`` shell support tab completion and other keyboard shortcuts?
-----------------------------------------------------------------------------

The :program:`mongo` shell supports keyboard shortcuts. For example,

- Use the up/down arrow keys to scroll through command history. See
  :ref:`.dbshell <mongo-dbshell-file>` documentation for more
  information on the ``.dbshell`` file.

- Use ``<Tab>`` to autocomplete or to list the completion
  possibilities, as in the following example which uses ``<Tab>`` to
  complete the method name starting with the letter ``'c'``:

  .. code-block:: javascript

     db.myCollection.c<Tab>

  Because there are many collection methods starting with the letter
  ``'c'``, the ``<Tab>`` will list the various methods that start with
  ``'c'``.

For a full list of the shortcuts, see :ref:`Shell Keyboard Shortcuts <mongo-keyboard-shortcuts>`

How can I customize the ``mongo`` shell prompt?
-----------------------------------------------

.. versionadded:: 1.9

You can change the :program:`mongo` shell prompt by setting the
``prompt`` variable. This makes it possible to display additional
information in the prompt.

Set ``prompt`` to any string or arbitrary JavaScript code that returns
a string, consider the following examples:

- Set the shell prompt to display the hostname and the database issued:

  .. code-block:: javascript

     var host = db.serverStatus().host;
     var prompt = function() { return db+"@"+host+"> "; }

  The :program:`mongo` shell prompt should now reflect the new prompt:

  .. code-block:: none

     test@my-machine.local>

- Set the shell prompt to display the database statistics:

  .. code-block:: javascript

     var prompt = function() {
                     return "Uptime:"+db.serverStatus().uptime+" Documents:"+db.stats().objects+" > ";
                  }

  The :program:`mongo` shell prompt should now reflect the new prompt:

  .. code-block:: none

     Uptime:1052 Documents:25024787 >

You can add the logic for the prompt in the :ref:`.mongorc.js
<mongo-mongorc-file>` file to set the prompt each time you start up the
:program:`mongo` shell.

Can I edit long shell operations with an external text editor?
--------------------------------------------------------------

.. versionadded: 2.1

You can use your own editor in the :program:`mongo` shell by setting
the :envvar:`EDITOR` environment variable before starting the
:program:`mongo` shell. Once in the :program:`mongo` shell, you can
edit with the specified editor by typing ``edit <variable>`` or ``edit
<function>``, as in the following example:

#. Set the :envvar:`EDITOR` variable from the command line prompt:

   .. code-block:: sh

      EDITOR=vim

#. Start the :program:`mongo` shell:

   .. code-block:: sh

      mongo

#. Define a function ``myFunction``:

   .. code-block:: javascript

      function myFunction () { }

#. Edit the function using your editor:

   .. code-block:: javascript

      edit myFunction

   The command should open the ``vim`` edit session. Remember to save
   your changes.

#. Type ``myFunction`` to see the function definition:

   .. code-block:: javascript

      myFunction

   The result should be the changes from your saved edit:

   .. code-block:: javascript

      function myFunction() {
          print("This was edited");
      }
=================================
FAQ: Replication and Replica Sets
=================================

.. default-domain:: mongodb

This document answers common questions about database replication
in MongoDB.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

What kinds of replication does MongoDB support?
-----------------------------------------------

MongoDB supports master-slave replication and a variation
on master-slave replication known as replica sets. Replica
sets are the recommended replication topology.

What do the terms "primary" and "master" mean?
----------------------------------------------

:term:`Primary` and :term:`master` nodes are the nodes
that can accept writes. MongoDB's replication is
"single-master:" only one node can accept write operations at a time.

In a replica set, if the current "primary" node fails or becomes
inaccessible, the other members can autonomously :term:`elect
<election>` one of the other members of the set to be the new "primary".

By default, clients send all reads to the primary;
however, :term:`read preference` is configurable at the
client level on a per-connection basis, which makes it possible to
send reads to secondary nodes instead.

What do the terms "secondary" and "slave" mean?
-----------------------------------------------

:term:`Secondary` and :term:`slave` nodes are read-only nodes
that replicate from the :term:`primary`.

Replication operates by way of an :term:`oplog`, from which secondary/slave
members apply new operations to themselves. This replication process
is asynchronous, so secondary/slave nodes may not always reflect the
latest writes to the primary. But usually, the gap between the primary and
secondary nodes is just few milliseconds on a local network connection.

How long does replica set failover take?
----------------------------------------

It varies, but a replica set will select a new primary within a minute.

It may take 10-30 seconds for the members of a :term:`replica
set` to declare a :term:`primary` inaccessible. This
triggers an :term:`election`. During the election, the cluster
is unavailable for writes.

The election itself may take another 10-30 seconds.

.. note::

   :term:`Eventually consistent <eventual consistency>` reads, like the ones that will return
   from a replica set are only possible with a :term:`write concern`
   that permits reads from :term:`secondary` members.


Does replication work over the Internet and WAN connections?
------------------------------------------------------------

Yes.

For example, a deployment may maintain a :term:`primary` and :term:`secondary`
in an East-coast data center along with a :term:`secondary` member for disaster
recovery in a West-coast data center.

.. seealso:: :doc:`/tutorial/deploy-geographically-distributed-replica-set`

Can MongoDB replicate over a "noisy" connection?
------------------------------------------------

Yes, but not without connection failures and the obvious latency.

Members of the set will attempt to reconnect to the other members of
the set in response to networking flaps. This does not require
administrator intervention. However, if the network connections
among the nodes in the replica set are very slow, it might not be
possible for the members of the node to keep up with the replication.

If the TCP connection between the secondaries and the :term:`primary`
instance breaks, a :term:`replica set` will automatically
elect one of the :term:`secondary` members of the set as primary.

What is the preferred replication method: master/slave or replica sets?
-----------------------------------------------------------------------

.. versionadded:: 1.8

:term:`Replica sets <replica set>` are the preferred
:term:`replication` mechanism in MongoDB. However, if your deployment
requires more than 12 nodes, you must use master/slave replication.

What is the preferred replication method: replica sets or replica pairs?
------------------------------------------------------------------------

.. deprecated:: 1.6

:term:`Replica sets <replica set>` replaced :term:`replica pairs` in
version 1.6. :term:`Replica sets <replica set>` are the preferred
:term:`replication` mechanism in MongoDB.

Why use journaling if replication already provides data redundancy?
-------------------------------------------------------------------

:term:`Journaling <journal>` facilitates faster crash recovery.
Prior to journaling, crashes often required :dbcommand:`database repairs <repairDatabase>`
or full data resync. Both were slow, and the first was unreliable.

Journaling is particularly useful for protection
against power failures, especially if your replica set resides in a single data
center or power circuit.

When a :term:`replica set` runs with journaling, :program:`mongod`
instances can safely restart without any administrator intervention.

.. note::

   Journaling requires some resource overhead for write
   operations. Journaling has no effect on read performance, however.

   Journaling is enabled by default on all 64-bit
   builds of MongoDB v2.0 and greater.

Are write operations durable if write concern does not acknowledge writes?
--------------------------------------------------------------------------

Yes.

However, if you want confirmation that a given write has arrived at
the server, use :ref:`write concern <write-concern>`. The
:dbcommand:`getLastError` command provides the facility for write
concern. However, after the :ref:`default write concern change
<driver-write-concern-change>`, the default write concern acknowledges
all write operations, and unacknowledged writes must be explicitly
configured. See the :doc:`/applications/drivers` documentation for
your driver for more information.

How many arbiters do replica sets need?
---------------------------------------

Some configurations do not require any :term:`arbiter`
instances. Arbiters vote in :term:`elections <election>`
for :term:`primary` but do not replicate the data like
:term:`secondary` members.

:term:`Replica sets <replica set>` require a majority of the
remaining nodes present to elect a primary. Arbiters allow you
to construct this majority without the overhead of adding replicating
nodes to the system.

There are many possible replica set :doc:`architectures
</core/replica-set-architectures>`.

A replica set with an odd number of voting nodes does not need an arbiter.

A common configuration consists of two replicating nodes that include a
:term:`primary` and a :term:`secondary`, as well as an :term:`arbiter`
for the third node. This configuration makes it possible for the set
to elect a primary in the event of failure, without requiring three
replicating nodes.

You may also consider adding an arbiter to a set if it has an equal
number of nodes in two facilities and network partitions between the
facilities are possible. In these cases, the arbiter will break
the tie between the two facilities and allow the set to elect a new
primary.

.. seealso:: :doc:`/core/replica-set-architectures`

What information do arbiters exchange with the rest of the replica set?
-----------------------------------------------------------------------

Arbiters never receive the contents of a collection but do exchange the
following data with the rest of the replica set:

- Credentials used to authenticate the arbiter with the replica set. All
  MongoDB processes within a replica set use keyfiles. These exchanges
  are encrypted.

- Replica set configuration data and voting data. This information is
  not encrypted. Only credential exchanges are encrypted.

If your MongoDB deployment uses SSL, then all communications between
arbiters and the other members of the replica set are secure. See the
documentation for :doc:`/tutorial/configure-ssl` for more
information. Run all arbiters on secure networks, as with all MongoDB
components.

.. see:: The overview of :ref:`Arbiter Members of Replica Sets
   <replica-set-arbiters>`.

Which members of a replica set vote in elections?
-------------------------------------------------

All members of a replica set, unless the value of :data:`votes
<local.system.replset.members[n].votes>` is equal to ``0``, vote in
elections. This includes all :ref:`delayed
<replica-set-delayed-members>`, :ref:`hidden
<replica-set-hidden-members>` and :ref:`secondary-only
<replica-set-secondary-only-members>` members, as well as the
:ref:`arbiters <replica-set-arbiters>`.

Additionally, the :data:`~replSetGetStatus.members.state` of the voting
members also determine whether the member can vote. Only voting members
in the following states are eligible to vote:

- ``PRIMARY``

- ``SECONDARY``

- ``RECOVERING``

- ``ARBITER``

- ``ROLLBACK``

.. seealso:: :ref:`replica-set-elections`

Do hidden members vote in replica set elections?
------------------------------------------------

:ref:`Hidden members <replica-set-hidden-members>` of :term:`replica
sets <replica set>` *do* vote in elections. To exclude a member from voting in an
:term:`election`, change the value of the member's
:data:`~local.system.replset.members[n].votes` configuration to ``0``.

.. seealso:: :ref:`replica-set-elections`

Is it normal for replica set members to use different amounts of disk space?
----------------------------------------------------------------------------

Yes.

Factors including: different oplog sizes, different levels of storage
fragmentation, and MongoDB's data file pre-allocation can lead to some
variation in storage utilization between nodes. Storage use
disparities will be most pronounced when you add members at different
times.
==========================
FAQ: Sharding with MongoDB
==========================

.. default-domain:: mongodb

This document answers common questions about horizontal scaling
using MongoDB's :term:`sharding`.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

Is sharding appropriate for a new deployment?
---------------------------------------------

Sometimes.

If your data set fits on a single server, you should begin
with an unsharded deployment.

Converting an unsharded database to a :term:`sharded cluster` is easy
and seamless, so there is *little advantage* in configuring sharding
while your data set is small.

Still, all production deployments should use :term:`replica sets
<replication>` to provide high availability and disaster recovery.

How does sharding work with replication?
----------------------------------------

To use replication with sharding, deploy each :term:`shard` as a
:term:`replica set`.

.. _faq-change-shard-key:

Can I change the shard key after sharding a collection?
-------------------------------------------------------

No.

There is no automatic support in MongoDB for changing a shard key
after sharding a collection. This reality underscores
the importance of choosing a good :ref:`shard key <shard-key>`. If you
*must* change a shard key after sharding a collection, the best option is to:

- dump all data from MongoDB into an external format.

- drop the original sharded collection.

- configure sharding using a more ideal shard key.

- :doc:`pre-split </tutorial/create-chunks-in-sharded-cluster>` the shard
  key range to ensure initial even distribution.

- restore the dumped data into MongoDB.

See :dbcommand:`shardCollection`, :method:`sh.shardCollection()`,
the :ref:`Shard Key <sharding-internals-shard-keys>`,
:doc:`/tutorial/deploy-shard-cluster`, and :issue:`SERVER-4000` for
more information.

What happens to unsharded collections in sharded databases?
-----------------------------------------------------------

In the current implementation, all databases in a :term:`sharded
cluster` have a "primary :term:`shard`." All unsharded
collection within that database will reside on the same shard.

How does MongoDB distribute data across shards?
-----------------------------------------------

Sharding must be specifically enabled on a collection. After enabling
sharding on the collection, MongoDB will assign various ranges of
collection data to the different shards in the cluster. The cluster
automatically corrects imbalances between shards by migrating ranges
of data from one shard to another.

What happens if a client updates a document in a chunk during a migration?
--------------------------------------------------------------------------

The :program:`mongos` routes the operation to the "old" shard, where
it will succeed immediately. Then the :term:`shard` :program:`mongod`
instances will replicate the modification to the "new" shard before
the :term:`sharded cluster` updates that chunk's "ownership," which
effectively finalizes the migration process.

What happens to queries if a shard is inaccessible or slow?
-----------------------------------------------------------

If a :term:`shard` is inaccessible or unavailable, queries will return
with an error.

However, a client may set the ``partial`` query bit, which will then
return results from all available shards, regardless of whether a
given shard is unavailable.

If a shard is responding slowly, :program:`mongos` will merely wait
for the shard to return results.

How does MongoDB distribute queries among shards?
-------------------------------------------------

.. versionchanged:: 2.0

The exact method for distributing queries to :term:`shards <shard>` in a
:term:`cluster <sharded cluster>` depends on the nature of the query and the configuration of
the sharded cluster. Consider a sharded collection, using the
:term:`shard key` ``user_id``, that has ``last_login`` and
``email`` attributes:

- For a query that selects one or more values for the ``user_id``
  key:

  :program:`mongos` determines which shard or shards contains the
  relevant data, based on the cluster metadata, and directs a query to
  the required shard or shards, and returns those results to the
  client.

- For a query that selects ``user_id`` and also performs a sort:

  :program:`mongos` can make a straightforward translation of this
  operation into a number of queries against the relevant shards,
  ordered by ``user_id``. When the sorted queries return from all
  shards, the :program:`mongos` merges the sorted results and returns
  the complete result to the client.

- For queries that select on ``last_login``:

  These queries must run on all shards: :program:`mongos` must
  parallelize the query over the shards and perform a merge-sort on
  the ``email`` of the documents found.

How does MongoDB sort queries in sharded environments?
------------------------------------------------------

If you call the :method:`cursor.sort()` method on a query in a sharded
environment, the :program:`mongod` for each shard will sort its
results, and the :program:`mongos` merges each shard's results before returning
them to the client.

How does MongoDB ensure unique ``_id`` field values when using a shard key *other* than ``_id``?
------------------------------------------------------------------------------------------------

If you do not use ``_id`` as the shard key, then your
application/client layer must be responsible for keeping the ``_id``
field unique. It is problematic for collections to have duplicate
``_id`` values.

If you're not sharding your collection by the ``_id`` field, then you
should be sure to store a globally unique identifier in that
field. The default :doc:`BSON ObjectId </reference/object-id>` works well in
this case.

I've enabled sharding and added a second shard, but all the data is still on one server. Why?
---------------------------------------------------------------------------------------------

First, ensure that you've declared a :term:`shard key` for your
collection. Until you have configured the shard key, MongoDB will not
create :term:`chunks <chunk>`, and :term:`sharding` will not occur.

Next, keep in mind that the default chunk size is 64 MB. As a result,
in most situations, the collection needs to have at least 64 MB of data before a
migration will occur.

Additionally, the system which balances chunks among the servers
attempts to avoid superfluous migrations. Depending on the number of
shards, your shard key, and the amount of data, systems often require
at least 10 chunks of data to trigger migrations.

You can run :method:`db.printShardingStatus()` to see all the chunks
present in your cluster.

Is it safe to remove old files in the ``moveChunk`` directory?
--------------------------------------------------------------

Yes. :program:`mongod` creates these files as backups during normal
:term:`shard` balancing operations.

Once these migrations are complete, you may delete these files.

How does ``mongos`` use connections?
------------------------------------

Each client maintains a connection to a :program:`mongos` instance.
Each :program:`mongos` instance maintains a pool of connections to the
members of a replica set supporting the sharded cluster.  Clients use
connections between :program:`mongos` and :program:`mongod` instances
one at a time. Requests are not multiplexed or pipelined. When client
requests complete, the :program:`mongos` returns the connection to the
pool.

See the :ref:`System Resource Utilization
<system-resource-utilization>` section of the
:doc:`/reference/ulimit` document.

Why does ``mongos`` hold connections open?
------------------------------------------

:program:`mongos` uses a set of connection pools to communicate with
each :term:`shard`.  These pools do not shrink when the number of
clients decreases.

This can lead to an unused :program:`mongos` with a large number
of open connections. If the :program:`mongos` is no longer in use,
it is safe to restart the process to close existing connections.

Where does MongoDB report on connections used by ``mongos``?
------------------------------------------------------------

Connect to the :program:`mongos` with the :program:`mongo` shell, and
run the following command:

.. code-block:: sh

   db._adminCommand("connPoolStats");

.. _faq-writebacklisten:

What does ``writebacklisten`` in the log mean?
----------------------------------------------

The writeback listener is a process that opens a long poll to relay
writes back from a :program:`mongod` or :program:`mongos` after
migrations to make sure they have not gone to the wrong server.  The
writeback listener sends writes back to the correct server if
necessary.

These messages are a key part of the sharding infrastructure and should
not cause concern.

How should administrators deal with failed migrations?
------------------------------------------------------

Failed migrations require no administrative intervention. Chunk
migrations always preserve a consistent state. If a migration
fails to complete for some reason, the :term:`cluster` retries
the operation. When the migration completes successfully, the data
resides only on the new shard.

What is the process for moving, renaming, or changing the number of config servers?
-----------------------------------------------------------------------------------

See :doc:`/administration/sharded-clusters` for information on
migrating and replacing config servers.

When do the ``mongos`` servers detect config server changes?
------------------------------------------------------------

:program:`mongos` instances maintain a cache of the :term:`config
database` that holds the metadata for the :term:`sharded cluster`. This
metadata includes the mapping of :term:`chunks <chunk>` to
:term:`shards <shard>`.

:program:`mongos` updates its cache lazily by issuing a request to a
shard and discovering that its metadata is out of date.  There is no
way to control this behavior from the client, but you can run the
:dbcommand:`flushRouterConfig` command against any :program:`mongos`
to force it to refresh its cache.

Is it possible to quickly update ``mongos`` servers after updating a replica set configuration?
-----------------------------------------------------------------------------------------------

The :program:`mongos` instances will detect these changes without
intervention over time. However, if you want to force the
:program:`mongos` to reload its configuration, run the
:dbcommand:`flushRouterConfig` command against to each
:program:`mongos` directly.

What does the ``maxConns`` setting on ``mongos`` do?
----------------------------------------------------

The :setting:`~net.maxIncomingConnections` option limits the number of connections
accepted by :program:`mongos`.

If your client driver or application creates a large number of
connections but allows them to time out rather than closing them
explicitly, then it might make sense to limit the number of
connections at the :program:`mongos` layer.

Set :setting:`~net.maxIncomingConnections` to a value slightly higher than the
maximum number of connections that the client creates, or the maximum
size of the connection pool. This setting prevents the
:program:`mongos` from causing connection spikes on the individual
:term:`shards <shard>`. Spikes like these may disrupt the operation
and memory allocation of the :term:`sharded cluster`.

How do indexes impact queries in sharded systems?
-------------------------------------------------

If the query does not include the :term:`shard key`, the
:program:`mongos` must send the query to all shards as a
"scatter/gather" operation. Each shard will, in turn, use *either* the
shard key index or another more efficient index to fulfill the query.

If the query includes multiple sub-expressions that reference the
fields indexed by the shard key *and* the secondary index, the
:program:`mongos` can route the queries to a specific shard and the
shard will use the index that will allow it to fulfill most
efficiently. See `this presentation <http://www.slideshare.net/mongodb/how-queries-work-with-sharding>`_
for more information.

Can shard keys be randomly generated?
-------------------------------------

:term:`Shard keys <shard key>` can be random. Random keys ensure
optimal distribution of data across the cluster.

:term:`Sharded clusters <sharded cluster>`, attempt to route queries to
*specific* shards when queries include the shard key as a parameter,
because these directed queries are more efficient. In many cases,
random keys can make it difficult to direct queries to specific
shards.

.. STUB :wiki:`Sharding`

Can shard keys have a non-uniform distribution of values?
---------------------------------------------------------

Yes. There is no requirement that documents be evenly distributed by
the shard key.

However, documents that have the same shard key *must* reside in the same
*chunk* and therefore on the same server. If your sharded data set has
too many documents with the exact same shard key you will not be able
to distribute *those* documents across your sharded cluster.

.. STUB link to shard key granularity.

Can you shard on the ``_id`` field?
-----------------------------------

You can use any field for the shard key. The ``_id`` field is a common
shard key.

Be aware that ``ObjectId()`` values, which are the default value of
the ``_id`` field, increment as a timestamp. As a result, when used as
a shard key, all new documents inserted into the collection will
initially belong to the same chunk on a single shard. Although the
system will eventually divide this chunk and migrate its contents to
distribute data more evenly, at any moment the cluster can only direct
insert operations at a single shard. This can limit the throughput of
inserts. If most of your write operations are updates, this limitation
should not impact your performance. However, if you have a high insert
volume, this may be a limitation.

To address this issue, MongoDB 2.4 provides :ref:`hashed shard keys
<sharding-hashed-sharding>`.

What do ``moveChunk commit failed`` errors mean?
------------------------------------------------

At the end of a :ref:`chunk migration <sharding-chunk-migration>`, the
:term:`shard` must connect to the :term:`config database` to update
the chunk's record in the cluster metadata. If the :term:`shard`
fails to connect to the :term:`config database`, MongoDB reports the
following error:

.. code-block:: none

   ERROR: moveChunk commit failed: version is at <n>|<nn> instead of
   <N>|<NN>" and "ERROR: TERMINATING"

When this happens, the :term:`primary` member of the shard's replica
set then terminates to protect data consistency. If a :term:`secondary`
member can access the config database, data on the shard becomes
accessible again after an election.

The user will need to resolve the chunk migration failure
independently. If you encounter this issue, contact the `MongoDB
User Group <http://groups.google.com/group/mongodb-user>`_ or
:about:`MongoDB Support </support>` to address this issue.

How does draining a shard affect the balancing of uneven chunk distribution?
----------------------------------------------------------------------------

The sharded cluster balancing process controls both migrating chunks
from decommissioned shards (i.e. draining) and normal cluster
balancing activities. Consider the following behaviors for different
versions of MongoDB in situations where you remove a shard in a
cluster with an uneven chunk distribution:

- After MongoDB 2.2, the balancer first removes the chunks from the
  draining shard and then balances the remaining uneven chunk
  distribution.

- Before MongoDB 2.2, the balancer handles the uneven chunk
  distribution and *then* removes the chunks from the draining shard.

.. COMMENT: This is probably a knowledge base article, but doesn't belong in
            the documentation here.

   Although this balancer prioritization is **not** configurable, you can,
   however, achieve the desired behavior with one of the following manual

   - Option 1

     #. Stop the balancer.

     #. Use the :dbcommand:`moveChunk` command to move chunks off of the
        draining shard.

     #. Restart the balancer.

   - Option 2

   #. Set the ``maxSize`` on the new shards to some low value so that
      they cannot accept more chunks. This should stop the balancing
      operation and allow the draining to continue with chunks moved
      to the full shards.

   #. Restore the original ``maxSize`` on the new shards.

      .. note::

         When changing the ``maxSize`` in the :term:`config database`:

         - in versions 2.0+, run the :dbcommand:`flushRouterConfig` command
           to refresh the :program:`mongos`.

         - in versions pre-2.0, you need to restart :program:`mongos`.
           solutions:
====================
FAQ: MongoDB Storage
====================

.. default-domain:: mongodb

This document addresses common questions regarding MongoDB's storage
system.

If you don't find the answer you're looking for, check
the :doc:`complete list of FAQs </faq>` or post your question to the
`MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_.

.. _faq-storage-memory-mapped-files:

What are memory mapped files?
-----------------------------

A memory-mapped file is a file with data that the operating system
places in memory by way of the ``mmap()`` system call. ``mmap()`` thus
*maps* the file to a region of virtual memory. Memory-mapped files are
the critical piece of the storage engine in MongoDB. By using memory
mapped files MongoDB can treat the contents of its data files as if
they were in memory. This provides MongoDB with an extremely fast and
simple method for accessing and manipulating data.

How do memory mapped files work?
--------------------------------

Memory mapping assigns files to a block of virtual memory with a
direct byte-for-byte correlation. Once mapped, the relationship
between file and memory allows MongoDB to interact with the data in
the file as if it were memory.

How does MongoDB work with memory mapped files?
-----------------------------------------------

MongoDB uses memory mapped files for managing and interacting with all
data. MongoDB memory maps data files to memory as it accesses
documents. Data that isn't accessed is *not* mapped to memory.

.. _faq-storage-page-faults:

What are page faults?
---------------------

Page faults will occur if you're attempting to access part of a
memory-mapped file that *isn't* in memory.

If there is free memory, then the operating system can find the page
on disk and load it to memory directly. However, if there is no free
memory, the operating system must:

- find a page in memory that is stale or no longer needed, and write
  the page to disk.

- read the requested page from disk and load it into memory.

This process, particularly on an active system can take a long time,
particularly in comparison to reading a page that is already in
memory.

What is the difference between soft and hard page faults?
---------------------------------------------------------

:term:`Page faults <page fault>` occur when MongoDB needs access to
data that isn't currently in active memory. A "hard" page fault
refers to situations when MongoDB must access a disk to access the
data. A "soft" page fault, by contrast, merely moves memory pages from
one list to another, such as from an operating system file
cache. In production, MongoDB will rarely encounter soft page faults.

.. _faq-tools-for-measuring-storage-use:

What tools can I use to investigate storage use in MongoDB?
-----------------------------------------------------------

The :method:`db.stats()` method in the :program:`mongo` shell,
returns the current state of the "active" database. The
:doc:`dbStats command </reference/command/dbStats>` document describes
the fields in the :method:`db.stats()` output.

.. _faq-working-set:

What is the working set?
------------------------

Working set represents the total body of data that the application
uses in the course of normal operation. Often this is a subset of the
total data size, but the specific size of the working set depends on
actual moment-to-moment use of the database.

If you run a query that requires MongoDB to scan every document in a
collection, the working set will expand to include every
document. Depending on physical memory size, this may cause documents
in the working set to "page out," or to be removed from physical memory by
the operating system. The next time MongoDB needs to access these
documents, MongoDB may incur a hard page fault.

If you run a query that requires MongoDB to scan every
:term:`document` in a collection, the working set includes every
active document in memory.

For best performance, the majority of your *active* set should fit in
RAM.

.. _faq-disk-size:

Why are the files in my data directory larger than the data in my database?
---------------------------------------------------------------------------

The data files in your data directory, which is the :file:`/data/db`
directory in default configurations, might be larger than the data set
inserted into the database. Consider the following possible causes:

- Preallocated data files.

  In the data directory, MongoDB preallocates data files to a
  particular size, in part to prevent file system
  fragmentation. MongoDB names the first data file ``<databasename>.0``,
  the next ``<databasename>.1``, etc. The first file :program:`mongod`
  allocates is 64 megabytes, the next 128 megabytes, and so on, up to
  2 gigabytes, at which point all subsequent files are 2
  gigabytes. The data files include files with allocated space but
  that hold no data. :program:`mongod` may allocate a 1 gigabyte data
  file that may be 90% empty. For most larger databases, unused
  allocated space is small compared to the database.

  On Unix-like systems, :program:`mongod` preallocates an additional data file and
  initializes the disk space to ``0``.  Preallocating data files in
  the background prevents significant delays when a new database file
  is next allocated.

  You can disable preallocation with the :setting:`noprealloc` run time
  option. However :setting:`noprealloc` is **not** intended for use in
  production environments: only use :setting:`noprealloc` for testing
  and with small data sets where you frequently drop databases.

  On Linux systems you can use ``hdparm`` to get an idea of how costly
  allocation might be:

  .. code-block:: sh

     time hdparm --fallocate $((1024*1024)) testfile

- The :term:`oplog`.

  If this :program:`mongod` is a member of a replica set, the data
  directory includes the :term:`oplog.rs <oplog>` file, which is a
  preallocated :term:`capped collection` in the ``local``
  database. The default allocation is approximately 5% of disk space
  on 64-bit installations, see :ref:`Oplog Sizing
  <replica-set-oplog-sizing>` for more information. In most cases, you
  should not need to resize the oplog. However, if you do, see
  :doc:`/tutorial/change-oplog-size`.

- The :term:`journal`.

  The data directory contains the journal files, which store write
  operations on disk prior to MongoDB applying them to databases. See
  :doc:`/core/journaling`.

- Empty records.

  MongoDB maintains lists of empty records in data files when
  deleting documents and collections. MongoDB can reuse this space,
  but will never return this space to the operating system.

  To de-fragment allocated storage, use :dbcommand:`compact`, which
  de-fragments allocated space. By de-fragmenting storage, MongoDB
  can effectively use the allocated space. :dbcommand:`compact`
  requires up to 2 gigabytes of extra disk space to run. Do not
  use :dbcommand:`compact` if you are critically low on disk space.

  .. important:: :dbcommand:`compact` only removes fragmentation
     from MongoDB data files and does not return any disk space to
     the operating system.

  To reclaim deleted space, use :dbcommand:`repairDatabase`, which
  rebuilds the database which de-fragments the storage and may release
  space to the operating system. :dbcommand:`repairDatabase` requires
  up to 2 gigabytes of extra disk space to run. Do not use
  :dbcommand:`repairDatabase` if you are critically low on disk space.

  .. warning::
     :dbcommand:`repairDatabase` requires enough free disk space to
     hold both the old and new database files while the repair is
     running. Be aware that :dbcommand:`repairDatabase` will block
     all other operations and may take a long time to complete.

How can I check the size of a collection?
-----------------------------------------

To view the size of a collection and other information, use the
:method:`db.collection.stats()` method from the :program:`mongo` shell.
The following example issues :method:`db.collection.stats()` for the
``orders`` collection:

.. code-block:: javascript

   db.orders.stats();

To view specific measures of size, use these methods:

- :method:`db.collection.dataSize()`: data size in bytes for the collection.
- :method:`db.collection.storageSize()`: allocation size in bytes, including unused space.
- :method:`db.collection.totalSize()`: the data size plus the index size in bytes.
- :method:`db.collection.totalIndexSize()`: the index size in bytes.

Also, the following scripts print the statistics for each database and
collection:

.. code-block:: javascript

   db._adminCommand("listDatabases").databases.forEach(function (d) {mdb = db.getSiblingDB(d.name); printjson(mdb.stats())})

.. code-block:: javascript

   db._adminCommand("listDatabases").databases.forEach(function (d) {mdb = db.getSiblingDB(d.name); mdb.getCollectionNames().forEach(function(c) {s = mdb[c].stats(); printjson(s)})})

How can I check the size of indexes?
------------------------------------

To view the size of the data allocated for an index, use one of the
following procedures in the :program:`mongo` shell:

- Use the :method:`db.collection.stats()` method using the
  index namespace. To retrieve a list of namespaces, issue the
  following command:

  .. code-block:: javascript

     db.system.namespaces.find()

- Check the value of :data:`~collStats.indexSizes` in the output of the
  :method:`db.collection.stats()` command.

.. example:: Issue the following command to retrieve index namespaces:

   .. code-block:: javascript

      db.system.namespaces.find()

   The command returns a list similar to the following:

   .. code-block:: javascript

      {"name" : "test.orders"}
      {"name" : "test.system.indexes"}
      {"name" : "test.orders.$_id_"}

   View the size of the data allocated for the ``orders.$_id_`` index
   with the following sequence of operations:

   .. code-block:: javascript

      use test
      db.orders.$_id_.stats().indexSizes

How do I know when the server runs out of disk space?
-----------------------------------------------------

If your server runs out of disk space for data files, you will see
something like this in the log:

.. code-block:: none

   Thu Aug 11 13:06:09 [FileAllocator] allocating new data file dbms/test.13, filling with zeroes...
   Thu Aug 11 13:06:09 [FileAllocator] error failed to allocate new file: dbms/test.13 size: 2146435072 errno:28 No space left on device
   Thu Aug 11 13:06:09 [FileAllocator]     will try again in 10 seconds
   Thu Aug 11 13:06:19 [FileAllocator] allocating new data file dbms/test.13, filling with zeroes...
   Thu Aug 11 13:06:19 [FileAllocator] error failed to allocate new file: dbms/test.13 size: 2146435072 errno:28 No space left on device
   Thu Aug 11 13:06:19 [FileAllocator]     will try again in 10 seconds

The server remains in this state forever, blocking all writes including
deletes. However, reads still work. To delete some data and compact,
using the :dbcommand:`compact` command, you must restart the server
first.

If your server runs out of disk space for journal files, the server
process will exit. By default, :program:`mongod` creates journal files
in a sub-directory of :setting:`~storage.dbPath` named ``journal``. You may
elect to put the journal files on another storage device using a
filesystem mount or a symlink.

.. note::

   If you place the journal files on a separate storage device you
   will not be able to use a file system snapshot tool to capture a
   valid snapshot of your data files and journal files.

.. todo the following "journal FAQ" content is from the wiki. Must add
   this content to the manual, perhaps on this page.

   If I am using replication, can some members use journaling and others not?
   --------------------------------------------------------------------------

   Yes. It is OK to use journaling on some replica set members and not
   others.

   Can I use the journaling feature to perform safe hot backups?
   -------------------------------------------------------------

   Yes, see :doc:`/administration/backups`.

   32 bit nuances?
   ---------------

   There is extra memory mapped file activity with journaling. This will
   further constrain the limited db size of 32 bit builds. Thus, for now
   journaling by default is disabled on 32 bit systems.

   When did the --journal option change from --dur?
   ------------------------------------------------

   In 1.8 the option was renamed to --journal, but the old name is still
   accepted for backwards compatibility; please change to --journal if
   you are using the old option.

   Will the journal replay have problems if entries are incomplete (like the failure happened in the middle of one)?
   -----------------------------------------------------------------------------------------------------------------

   Each journal (group) write is consistent and won't be replayed during
   recovery unless it is complete.

   How many times is data written to disk when replication and journaling are both on?
   -----------------------------------------------------------------------------------

   In v1.8, for an insert, four times. The object is written to the main
   collection and also the oplog collection. Both of those writes are
   also journaled as a single mini-transaction in the journal files in
   /data/db/journal.

   The above applies to collection data and inserts which is the worst
   case scenario. Index updates are written to the index and the
   journal, but not the oplog, so they should be 2X today not 4X.
   Likewise updates with things like $set, $addToSet, $inc, etc. are
   compactly logged all around so those are generally small.
==========================
Frequently Asked Questions
==========================

.. toctree::
   :maxdepth: 2

   /faq/fundamentals
   /faq/developers
   /faq/mongo
   /faq/concurrency
   /faq/sharding
   /faq/replica-sets
   /faq/storage
   /faq/indexes
   /faq/diagnostics
:orphan:

============================
The MongoDB |version| Manual
============================

.. default-domain:: mongodb

Welcome to the MongoDB Manual! MongoDB is an open-source,
document-oriented database designed for ease of development and
scaling.

The Manual introduces MongoDB and continues to describe the query
language, operational considerations and procedures, administration,
and application development patterns, and other aspects of MongoDB use
and administration. See the :doc:`/core/introduction` for an overview of
MongoDB's key features. The Manual also has a thorough reference
section of the MongoDB interface and tools.

This manual is under constant development. See the :doc:`/about` for
more information on the MongoDB Documentation project.

.. admonition:: MongoDB 2.6 Released

   See :doc:`/release-notes/2.6` for new features in MongoDB 2.6.

.. class:: index-table

   .. as an alternative, the following built table is a bit more
      wide

   .. include /includes/table-index-nav.rst

.. list-table::
   :header-rows: 1
   :class: index-table

   * - Getting Started
     - Developers
     - Administrators
     - Reference

   * - :doc:`/core/introduction`

       :doc:`Installation Guides </installation>`

       :doc:`First Steps </tutorial/getting-started>`

       :doc:`/faq`

     - :doc:`Database Operations </crud>`

       :doc:`Aggregation </core/aggregation>`

       :doc:`SQL to MongoDB Mapping </reference/sql-comparison>`

       :doc:`/indexes`

     - :doc:`Operations </administration>`

       :doc:`Replica Sets </replication>`

       :doc:`Sharded Clusters </sharding>`

       :doc:`MongoDB Security </security>`

     - :doc:`Shell Methods </reference/method>`

       :doc:`Query Operators </reference/operator>`

       :doc:`Complete System Reference </reference>`

       :doc:`/reference/glossary`

   * -
     -
     -
     -

Community
~~~~~~~~~

MongoDB has an :about:`active community </community>`. You'll get a quick
response to MongoDB questions posted to `Stack Overflow`_.

.. _`Stack Overflow`: http://stackoverflow.com/questions/tagged/mongodb

Additional Resources
~~~~~~~~~~~~~~~~~~~~

The following resources provide additional information:

.. class:: toc

   `MongoDB, Inc. <http://www.mongodb.com/>`_
      The company behind MongoDB.

   `MongoDB Events <http://www.mongodb.com/events/>`_
      Upcoming events where you can learn more and meet members of the
      MongoDB community. `MongoDB World
      <http://world.mongodb.com/?utm_source=docs&utm_medium=org&utm_campaign=MongoDB-World&utm_content=Additional-Resources&utm_term=>`_
      MongoDB World is the annual user conference, held in New York
      City on June 23-25 2014.

   `Planet MongoDB <http://planet.mongodb.org/>`_
      Aggregator of popular MongoDB blogs.

   `Slides and Video <http://www.mongodb.com/presentations/>`_
      Presentations and videos from past MongoDB events.

   `MongoDB Management Service <http://mms.mongodb.com/>`_
      Free cloud-based service for monitoring and backing up MongoDB deployments. Also
      consider the `MMS documentation <http://mms.mongodb.com/help/>`_.

   `MongoDB Books <http://mongodb.org/books>`_
      Books that provide additional information and background on
      MongoDB.
.. _indexes:

=======
Indexes
=======

.. default-domain:: mongodb

Indexes provide high performance read operations for frequently used
queries.

This section introduces indexes in MongoDB, describes the types and
configuration options for indexes, and describes special types of
indexing MongoDB supports. The section also provides tutorials
detailing procedures and operational concerns, and providing
information on how applications may use indexes.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`Indexes
   and MongoDB <MongoDB-indexes-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-indexes-landing.rst

.. include:: /includes/toc/indexes-landing.rst
===============
Install MongoDB
===============

.. default-domain:: mongodb

.. index:: tutorials; installation
.. index:: installation tutorials
.. index:: installation guides
.. index:: installation

MongoDB runs on most platforms and supports both 32-bit and 64-bit
architectures.

.. _tutorials-installation:
.. _tutorial-installation:

Installation Guides
--------------------

See the :doc:`/release-notes` for information about specific releases
of MongoDB.

.. include:: /includes/toc/dfn-list-spec-installation.rst

.. include:: /includes/toc/install-on-linux-landing.rst

.. include:: /includes/toc/installation-osx.rst

.. include:: /includes/toc/installation-windows.rst

.. include:: /includes/toc/installation-enterprise-landing.rst

First Steps with MongoDB
------------------------

After you have installed MongoDB, consider the following documents as
you begin to learn about MongoDB:

.. include:: /includes/toc/dfn-list-getting-started.rst

.. include:: /includes/toc/getting-started.rst

.. seealso:: :doc:`/core/crud` and :doc:`/data-modeling`.
:orphan:

=======================
Authentication Required
=======================

You must log in to access the URL you requested.
:orphan:

=============
Access Denied
=============

You do not have access to the URL you requested.
:orphan:

==============
File not found
==============

The URL you requested does not exist or has been removed.
:orphan:

============
File Deleted
============

The URL you requested has been deleted.
:orphan:

======================
MongoDB Administration
======================

.. include:: /administration.txt
   :start-after: admin-introduction-start
   :end-before: admin-introduction-end

.. include:: /includes/toc/administration-landing.rst

Appendix
--------

.. toctree::
   :titlesonly:

   /administration/replica-sets
   /administration/sharded-clusters
==================================
MongoDB Documentation Build System
==================================

This document contains more direct instructions for building the
MongoDB documentation.

Getting Started
---------------

Install Dependencies
~~~~~~~~~~~~~~~~~~~~

The MongoDB Documentation project depends on the following tools:

- GNU Make
- GNU Tar
- Python
- Git
- Sphinx (documentation management toolchain)
- Pygments (syntax highlighting)
- PyYAML (for the generated tables)
- Droopy (Python package for static text analysis)
- Fabric (Python package for scripting and orchestration)
- Inkscape (Image generation.)
- python-argparse (For Python 2.6.)
- LaTeX/PDF LaTeX (typically texlive; for building PDFs)
- Common Utilities (rsync, tar, gzip, sed)

OS X
````

Install Sphinx, Docutils, and their dependencies with ``easy_install``
the following command:

.. code-block:: sh

   easy_install Sphinx Jinja2 Pygments docutils  PyYAML droopy fabric

Feel free to use ``pip`` rather than ``easy_install`` to install
python packages.

To generate the images used in the documentation, `download and
install Inkscape <http://inkscape.org/download/>`_.

.. optional::

   To generate PDFs for the full production build, install a TeX
   distribution (for building the PDF.) If you do not have a LaTeX
   installation, use `MacTeX <http://www.tug.org/mactex/2011/>`_. This
   is **only** required to build PDFs.

Arch Linux
``````````

Install packages from the system repositories with the following command:

.. code-block:: sh

   pacman -S python2-sphinx python2-yaml inkscape python2-pip

Then install the following Python packages:

.. code-block:: sh

   pip install droopy fabric

.. optional::

   To generate PDFs for the full production build, install the
   following packages from the system repository:

   .. code-block:: sh

      pacman -S texlive-bin texlive-core texlive-latexextra

Debian/Ubuntu
`````````````

Install the required system packages with the following command:

.. code-block:: sh

   apt-get install python-sphinx python-yaml python-argparse inkscape python-pip

Then install the following Python packages:

.. code-block:: sh

   pip install droopy fabric

.. optional::

   To generate PDFs for the full production build, install the
   following packages from the system repository:

   .. code-block:: sh

      apt-get install texlive-latex-recommended texlive-latex-recommended

Setup and Configuration
~~~~~~~~~~~~~~~~~~~~~~~

Clone the repository:

.. code-block:: sh

   git clone git://github.com/mongodb/docs.git

Then run the ``bootstrap.py`` script in the ``docs/`` repository, to
configure the build dependencies:

.. code-block:: sh

   python bootstrap.py

This downloads and configures the `mongodb/docs-tools
<http://github.com/mongodb/docs-tools/>`_ repository, which contains
the authoritative build system shared between branches of the MongoDB
Manual and other MongoDB documentation projects.

You can run ``bootstrap.py`` regularly to update build system.

Building the Documentation
--------------------------

The MongoDB documentation build system is entirely accessible via
``make`` targets. For example, to build an HTML version of the
documentation issue the following command:

.. code-block:: sh

   make html

You can find the build output in ``build/<branch>/html``, where
``<branch>`` is the name of the current branch.

In addition to the ``html`` target, the build system provides the
following targets:

``publish``
   Builds and integrates all output for the production build. Build
   output is in ``build/public/<branch>/``. When you run ``publish``
   in the ``master``, the build will generate some output in
   ``build/public/``.

``push``; ``stage``
   Uploads the production build to the production or staging web
   servers. Depends on ``publish``. Requires access production or
   staging environment.

``push-all``; ``stage-all``
   Uploads the entire content of ``build/public/`` to the web
   servers. Depends on ``publish``. Not used in common practice.

``push-with-delete``; ``stage-with-delete``
   Modifies the action of ``push`` and ``stage`` to remove remote file
   that don't exist in the local build. Use with caution.

``html``; ``latex``; ``dirhtml``; ``epub``; ``texinfo``; ``man``; ``json``
   These are standard targets derived from the default Sphinx
   Makefile, with adjusted dependencies. Additionally, for all of
   these targets you can append ``-nitpick`` to increase Sphinx's
   verbosity, or ``-clean`` to remove all Sphinx build artifacts.

   ``latex`` performs several additional post-processing steps on
   ``.tex`` output generated by Sphinx. This target will also compile
   PDFs using ``pdflatex``.

   ``html`` and ``man`` also generates a ``.tar.gz`` file of the build
   outputs for inclusion in the final releases.

Build Mechanics and Tools
-------------------------

Internally the build system has a number of components and
processes. See the `docs-tools README
<https://github.com/mongodb/docs-tools/blob/master/README.rst>`_ for
more information on the internals. This section documents a few
of these components from a very high level and lists useful operations
for contributors to the documentation.

.. _build-fabric:

Fabric
~~~~~~

Fabric is an orchestration and scripting package for Python. The
documentation uses Fabric to handle the deployment of the build
products to the web servers and also unifies a number of independent
build operations. Fabric commands have the following form:

.. code-block:: sh

   fab <module>.<task>[:<argument>]

The ``<argument>`` is optional in most cases. Additionally some tasks
are available at the root level, without a module. To see a full list
of fabric tasks, use the following command:

.. code-block:: sh

   fab -l

You can chain fabric tasks on a single command line, although this
doesn't always make sense.

Important fabric tasks include:

``tools.bootstrap``
   Runs the ``bootstrap.py`` script. Useful for re-initializing the
   repository without needing to be in root of the repository.

``tools.dev``; ``tools.reset``
   ``tools.dev`` switches the ``origin`` remote of the ``docs-tools``
   checkout in ``build`` directory, to ``../docs-tools`` to facilitate
   build system testing and development. ``tools.reset`` resets the
   ``origin`` remote for normal operation.

``tools.conf``
   ``tools.conf`` returns the content of the configuration object for
   the current project. These data are useful during development.

``stats.report:<filename>``
   Returns, a collection of readability statistics. Specify file names
   relative to ``source/`` tree.

``make``
   Provides a thin wrapper around Make calls. Allows you to start make
   builds from different locations in the project repository.

``process.refresh_dependencies``
   Updates the time stamp of ``.txt`` source files with changed
   include files, to facilitate Sphinx's incremental rebuild
   process. This task runs internally as part of the build process.

Buildcloth
~~~~~~~~~~

`Buildcloth <https://pypi.python.org/pypi/buildcloth/>`_ is a
meta-build tool, used to generate Makefiles programmatically. This
makes the build system easier to maintain, and makes it easier to use
the same fundamental code to generate various branches of the Manual
as well as related documentation projects. See `makecloth/ in the
docs-tools repository
<https://github.com/mongodb/docs-tools/tree/master/makecloth>`_ for
the relevant code.

Running ``make`` with no arguments will regenerate these parts of the
build system automatically.

Rstcloth
~~~~~~~~

`Rstcloth <https://pypi.python.org/pypi/rstcloth>`_ is a library for
generating reStructuredText programmatically. This makes it possible to
generate content for the documentation, such as tables, tables of
contents, and API reference material programmatically and
transparently. See `rstcloth/ in the docs-tools repository
<https://github.com/mongodb/docs-tools/tree/master/rstcloth>`_ for
the relevant code.
:orphan:

.. This is entirely generated as to provide an introspective aid into
   our generated content. See ``rstcloth/includes.py`` in the
   docs-tools repository and ``/includes/metadata.yaml`` in this
   repository for more information.

.. include:: /includes/generated/overview.rst
:orphan:

=======================
MongoDB Manual Contents
=======================

See :doc:`/about` for more information about the MongoDB Documentation
project, this Manual and additional editions of this text.

.. only:: latex

   .. note::

      This version of the PDF does *not* include the reference section,
      see :hardlink:`MongoDB Reference Manual
      <MongoDB-reference-manual.pdf>` for a PDF edition of all MongoDB
      Reference Material.

.. toctree::
   :titlesonly:

   /core/introduction
   /installation
   /crud
   /data-modeling
   /administration
   /security
   /aggregation
   /indexes
   /replication
   /sharding
   /faq
   /release-notes
   /about
.. _documentation-organization:

===========================
MongoDB Manual Organization
===========================

This document provides an overview of the global organization of the
documentation resource. Refer to the notes below if you are having
trouble understanding the reasoning behind a file's current location,
or if you want to add new documentation but aren't sure how to
integrate it into the existing resource.

If you have questions, don't hesitate to open a ticket in the
`Documentation Jira Project <https://jira.mongodb.org/browse/DOCS>`_
or contact the `documentation team <mailto:docs@mongodb.com>`_.

Global Organization
-------------------

Indexes and Experience
~~~~~~~~~~~~~~~~~~~~~~

The documentation project has two "index files": ``/contents.txt`` and
``/index.txt``. The "contents" file
provides the documentation's tree structure, which Sphinx uses
to create the left-pane navigational structure, to power the "Next" and
"Previous" page functionality, and to provide all overarching outlines of the
resource. The "index" file is not included in the "contents" file (and
thus builds will produce a warning here) and is the page that users
first land on when visiting the resource.

Having separate "contents" and "index" files provides a bit more
flexibility with the organization of the resource while also making it
possible to customize the primary user experience.

Topical Organization
~~~~~~~~~~~~~~~~~~~~

The placement of files in the repository depends on the *type* of
documentation rather than the *topic* of the content. Like the
difference between ``contents.txt`` and ``index.txt``, by decoupling
the organization of the files from the organization of the information
the documentation can be more flexible and can more adequately address
changes in the product and in users' needs.

*Files* in the ``source/`` directory represent the tip of a logical
tree of documents, while *directories* are containers of types of
content. The ``administration`` and ``applications`` directories,
however, are legacy artifacts and with a few exceptions contain
sub-navigation pages.

With several exceptions in the ``reference/`` directory, there is only
one level of sub-directories in the ``source/`` directory.

Tools
-----

The organization of the site, like all Sphinx sites derives from the
:rst:dir:`toctree <sphinx:toctree>` structure. However, in order to annotate
the table of contents and provide additional flexibility, the MongoDB
documentation generates :rst:dir:`toctree` structures using
data from YAML files stored in the ``source/includes/``
directory. These files start with ``ref-toc`` or ``toc`` and generate
output in the ``source/includes/toc/`` directory. Briefly this system
has the following behavior:

- files that start with ``ref-toc`` refer to the documentation of API
  objects (i.e. commands, operators and methods), and the build
  system generates files that hold :rst:dir:`toctree <sphinx:toctree>`
  directives as well as files that hold *tables* that list objects and
  a brief description.

- files that start with ``toc`` refer to all other documentation and
  the build system generates files that hold :rst:dir:`toctree
  <sphinx:toctree>`  directives as well as files that hold
  *definition lists* that contain links to the documents and short
  descriptions the content.

- file names that have ``spec`` following ``toc`` or ``ref-toc`` will
  generate aggregated tables or definition lists and allow ad-hoc
  combinations of documents for landing pages and quick reference guides.
=============================================
MongoDB Documentation Practices and Processes
=============================================

This document provides an overview of the practices and processes.

Commits
-------

When relevant, include a Jira case identifier in a commit
message. Reference documentation cases when applicable, but feel free to
reference other cases from `jira.mongodb.org <http://jira.mongodb.org/>`_.

Err on the side of creating a larger number of discrete commits rather
than bundling large set of changes into one commit.

For the sake of consistency, remove trailing whitespaces in
the source file.

"Hard wrap" files to between 72 and 80 characters per-line.

Standards and Practices
-----------------------

- At least two people should vet all non-trivial
  changes to the documentation before publication. One of the
  reviewers should have significant technical experience with the
  material covered in the documentation.

- All development and editorial work should transpire on GitHub branches
  or forks that editors can then merge into
  the publication branches.

Collaboration
-------------

To propose a change to the documentation, do either of the following:

- Open a ticket in the `documentation project
  <https://jira.mongodb.org/browse/DOCS>`_ proposing the change.
  Someone on the documentation team will make the change and be in
  contact with you so that you can review the change.

- Using `GitHub <https://github.com/>`_, fork the `mongodb/docs
  repository <https://github.com/mongodb/docs>`_, commit your changes,
  and issue a pull request. Someone on the documentation team will
  review and incorporate your change into the documentation.

Builds
------

Building the documentation is useful because `Sphinx
<http://sphinx.pocoo.org/>`_ and docutils can catch numerous errors in
the format and syntax of the documentation. Additionally, having
access to an example documentation as it *will* appear to the users is
useful for providing more effective basis for the review
process. Besides Sphinx, Pygments, and Python-Docutils, the
documentation repository contains all requirements for building the
documentation resource.

Talk to someone on the documentation team if you are having problems
running builds yourself.

Publication
-----------

The makefile for this repository contains targets that automate the
publication process. Use ``make html`` to publish a test build of
the documentation in the ``build/`` directory of your
repository. Use ``make publish`` to build the full contents of the
manual from the current branch in the ``../public-docs/`` directory
relative the docs repository.

Other targets include:

- ``man`` - builds UNIX Manual pages for all Mongodb utilities.
- ``push`` - builds and deploys the contents of the
  ``../public-docs/``.
- ``pdfs`` - builds a PDF version of the manual (requires LaTeX
  dependencies.)

Branches
--------

This section provides an overview of the git branches in the MongoDB
documentation repository and their use.

At the present time, future work transpires in the ``master``, with
the main publication being ``current``. As the documentation
stabilizes, the documentation team will begin to maintain branches of
the documentation for specific MongoDB releases.

Migration from Legacy Documentation
-----------------------------------

The MongoDB.org Wiki contains
a wealth of information. As the transition to the Manual (i.e. this
project and resource) continues, it's *critical* that no information
disappears or goes missing. The following process outlines *how* to
migrate a wiki page to the manual:

1. Read the relevant sections of the Manual, and see what the new
   documentation has to offer on a specific topic.

   In this process you should follow cross references and gain an
   understanding of both the underlying information and how the parts of
   the new content relates its constituent parts.

2. Read the wiki page you wish to redirect, and take note of all of the
   factual assertions, examples presented by the wiki page.

3. Test the factual assertions of the wiki page to the greatest extent
   possible. Ensure that example output is accurate. In the case of
   commands and reference material, make sure that documented options
   are accurate.

4. Make corrections to the manual page or pages to reflect any missing
   pieces of information.

   The target of the redirect need *not* contain every piece of
   information on the wiki page, **if** the manual as a whole does, and
   relevant section(s) with the information from the wiki page are
   accessible from the target of the redirection.

5. As necessary, get these changes reviewed by another writer and/or
   someone familiar with the area of the information in question.

   At this point, update the relevant Jira case with the target that
   you've chosen for the redirect, and make the ticket unassigned.

6. When someone has reviewed the changes and published those changes
   to Manual, you, or preferably someone else on the team, should make
   a final pass at both pages with fresh eyes and then make the
   redirect.

   Steps 1-5 should ensure that no information is lost in the
   migration, and that the final review in step 6 should be trivial to
   complete.

Review Process
--------------

Types of Review
~~~~~~~~~~~~~~~

The content in the Manual undergoes many types of review, including
the following:

Initial Technical Review
````````````````````````

Review by an engineer familiar with MongoDB and the topic area of
the documentation. This review focuses on technical content, and
correctness of the procedures and facts presented, but can improve
any aspect of the documentation that may still be lacking. When both
the initial technical review and the content review are complete,
the piece may be "published."

Content Review
``````````````

Textual review by another writer to ensure stylistic consistency with
the rest of the manual. Depending on the content, this may precede or
follow the initial technical review. When both the initial technical
review and the content review are complete, the piece may be
"published."

Consistency Review
``````````````````

This occurs post-publication and is content focused. The goals of
consistency reviews are to increase the internal consistency of the
documentation as a whole. Insert relevant cross-references, update the
style as needed, and provide background fact-checking.

When possible, consistency reviews should be as systematic as possible
and we should avoid encouraging stylistic and information drift by
editing only small sections at a time.

Subsequent Technical Review
```````````````````````````

If the documentation needs to be updated following a change in
functionality of the server or following the resolution of a user
issue, changes may be significant enough to warrant additional
technical review. These reviews follow the same form as the "initial
technical review," but is often less involved and covers a smaller
area.

Review Methods
~~~~~~~~~~~~~~

If you're not a usual contributor to the documentation and would like
to review something, you can submit reviews in any of the following
methods:

- If you're reviewing an open pull request in GitHub, the best way to
  comment is on the "overview diff," which you can find by clicking on
  the "diff" button in the upper left portion of the screen. You can
  also use the following URL to reach this interface:

  .. code-block:: none

     https://github.com/mongodb/docs/pull/[pull-request-id]/files

  Replace ``[pull-request-id]`` with the identifier of the pull
  request. Make all comments inline, using GitHub's comment system.

  You may also provide comments directly on commits, or on the pull
  request itself but these commit-comments are archived in less
  coherent ways and generate less useful emails, while comments on the
  pull request lead to less specific changes to the document.

- Leave feedback on Jira cases in the `DOCS
  <http://jira.mongodb.org/browse/DOCS>`_ project. These are better
  for more general changes that aren't necessarily tied to a specific
  line, or affect multiple files.

- Create a fork of the repository in your GitHub account, make any
  required changes and then create a pull request with your changes.

  If you insert lines that begin with any of the following
  annotations:

  .. code-block:: none

     .. TODO:
     TODO:
     .. TODO
     TODO

  followed by your comments, it will be easier for the original writer
  to locate your comments. The two dots ``..`` format is a comment in
  reStructured Text, which will hide your comments from Sphinx and
  publication if you're worried about that.

  This format is often easier for reviewers with larger portions of
  content to review.
:orphan:

========================
MongoDB Reference Manual
========================

This document contains all of the reference material from the
:doc:`MongoDB Manual </contents>`, reflecting the |release|
release. See the full manual, for complete documentation of MongoDB,
it's operation, and use.

.. toctree::
   :hidden:

   /about

Interfaces Reference
--------------------

.. toctree::
   :titlesonly:
   :maxdepth: 25

   /reference/method
   /reference/command
   /reference/operator
   /reference/aggregation

MongoDB and SQL Interface Comparisons
-------------------------------------

.. toctree::
   :titlesonly:

   /reference/sql-comparison
   /reference/sql-aggregation-comparison

Program and Tool Reference Pages
--------------------------------

.. toctree::
   :titlesonly:

   /reference/program

Internal Metadata
-----------------

.. toctree::
   :titlesonly:

   /reference/config-database
   /reference/local-database
   /reference/system-collections

General System Reference
------------------------

.. toctree::
   :titlesonly:

   /reference/exit-codes
   /reference/limits
   /reference/glossary

.. include:: /release-notes.txt
   :start-after: start-include-here
   :end-before: end-include-here
.. _documentation-style-guide:

=========================================
Style Guide and Documentation Conventions
=========================================

This document provides an overview of the style for the MongoDB
documentation stored in this repository. The overarching goal of this
style guide is to provide an accessible base style to ensure that our
documentation is easy to read, simple to use, and straightforward to
maintain.

For information regarding the MongoDB Manual organization, see
:ref:`documentation-organization`.

Document History
----------------

**2011-09-27**: Document created with a (very) rough list of style
guidelines, conventions, and questions.

**2012-01-12**: Document revised based on slight shifts in practice,
and as part of an effort of making it easier for people outside of the
documentation team to contribute to documentation.

**2012-03-21**: Merged in content from the Jargon, and cleaned up
style in light of recent experiences.

**2012-08-10**: Addition to the "Referencing" section.

**2013-02-07**: Migrated this document to the manual. Added "map-reduce"
terminology convention. Other edits.

**2013-11-15**: Added new table of preferred terms.

Naming Conventions
------------------

This section contains guidelines on naming files, sections, documents
and other document elements.

- File naming Convention:

  - For Sphinx, all files should have a ``.txt`` extension.

  - Separate words in file names with hyphens (i.e. ``-``.)

  - For most documents, file names should have a terse one or two word
    name that describes the material covered in the document. Allow
    the path of the file within the document tree to add some of the
    required context/categorization. For example it's acceptable to
    have ``/core/sharding.rst`` and ``/administration/sharding.rst``.

  - For tutorials, the full title of the document should be in the
    file name. For example,
    ``/tutorial/replace-one-configuration-server-in-a-shard-cluster.rst``

- Phrase headlines and titles so users can determine what questions the
  text will answer, and material that will be addressed, without needing
  them to read the content. This shortens the amount of time that
  people spend looking for answers, and improvise search/scanning, and
  possibly "SEO."

- Prefer titles and headers in the form of "Using foo" over "How to Foo."

- When using target references (i.e. ``:ref:`` references in
  documents), use names that include enough context to be intelligible
  through all documentation. For example, use
  "``replica-set-secondary-only-node``" as opposed to
  "``secondary-only-node``". This makes the source more usable
  and easier to maintain.

Style Guide
-----------

This includes the local typesetting, English, grammatical, conventions
and preferences that all documents in the manual should use. The goal
here is to choose good standards, that are clear, and have a stylistic
minimalism that does not interfere with or distract from the
content. A uniform style will improve user experience and minimize
the effect of a multi-authored document.

Punctuation
~~~~~~~~~~~

- Use the Oxford comma.

  Oxford commas are the commas in a list of things (e.g. "something,
  something else, and another thing") before the conjunction
  (e.g. "and" or "or.").

- Do not add two spaces after terminal punctuation, such as
  periods.

- Place commas and periods inside quotation marks.

.. NOTE: The above bullet replaces the commented bullet below. American style
   refers only to quote marks and not to parenthesis. I consulted several sites,
   including this one: http://www.thepunctuationguide.com/british-versus-american-style.html

.. - Use "American style" punctuation when enclosing punctuation
     (i.e. parentheses, quotes, and similar) interacts with terminal
     punctuation (i.e. periods, commas, colons, and similar.) This rule
     dictates that the terminal punctuation always goes *within* the
     enclosing punctuation, even if the terminal punctuation affects the
     sentence/clause outside of the quote or parenthetical.

..   This is counter to the "logical style" which is default in British
     English, because its easier to be consistent and edit using this
     rule, and because the style of the documents tends towards American
     English.

.. NOTE This last paragraph in the bullet (below) uses an example we no longer do.
   We no longer put parens around literals.

..   The one exception is if a literal string or inline code snippet is
     enclosed in quotations or parenthesis, it may make sense to move a
     period or comma further away from the literal string to avoid
     copy-and-paste errors or other confusion. For example, in this case,
     "``cfg.members[0].priorty = 1``." is worse than
     "``cfg.members[0].priorty = 1``".

Headings
~~~~~~~~

Use title case for headings and document titles. Title case capitalizes the
first letter of the first, last, and all significant words.

Verbs
~~~~~

Verb tense and mood preferences, with examples:

- **Avoid** the first person. For example do not say, "We will begin
  the backup process by locking the database," or "I begin the backup
  process by locking my database instance."

- **Use** the second person. "If you need to back up your database,
  start by locking the database first." In practice, however, it's
  more concise to imply second person using the imperative, as in
  "Before initiating a backup, lock the database."

- When indicated, use the imperative mood. For example: "Backup your
  databases often" and "To prevent data loss, back up your databases."

- The future perfect is also useful in some cases. For example,
  "Creating disk snapshots without locking the database will lead to
  an invalid state."

- Avoid helper verbs, as possible, to increase clarity and
  concision. For example, attempt to avoid "this does foo" and "this
  will do foo" when possible. Use "does foo" over "will do foo" in
  situations where "this foos" is unacceptable.

Referencing
~~~~~~~~~~~

- To refer to future or planned functionality in MongoDB or a driver,
  *always* link to the Jira case. The Manual's ``conf.py`` provides
  an ``:issue:`` role that links directly to a Jira case
  (e.g. ``:issue:\`SERVER-9001\```).

- For non-object references (i.e. functions, operators, methods,
  database commands, settings) always reference only the first
  occurrence of the reference in a section. You should *always*
  reference objects, except in section headings.

- Structure references with the *why* first; the link second.

  For example, instead of this:

  Use the :doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`
  procedure if you have an existing replica set.

  Type this:

  To deploy a sharded cluster for an existing replica set, see
  :doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`.

General Formulations
~~~~~~~~~~~~~~~~~~~~

- Contractions are acceptable insofar as they are necessary to
  increase readability and flow. Avoid otherwise.

- Make lists grammatically correct.

  - Do not use a period after every item unless the list item
    completes the unfinished sentence before the list.

  - Use appropriate commas and conjunctions in the list items.

  - Typically begin a bulleted list with an introductory sentence or
    clause, with a colon or comma.

- The following terms are one word:

  - standalone
  - workflow

- Use "unavailable," "offline," or "unreachable" to refer to a
  ``mongod`` instance that cannot be accessed. Do not use the
  colloquialism "down."

- Always write out units (e.g. "megabytes") rather than using
  abbreviations (e.g. "MB".)

Structural Formulations
~~~~~~~~~~~~~~~~~~~~~~~

- There should be at least two headings at every nesting level. Within
  an "h2" block, there should be either: no "h3" blocks, 2 "h3"
  blocks, or more than 2 "h3" blocks.

- Section headers are in title case (capitalize first, last, and
  all important words) and should effectively describe the contents
  of the section. In a single document you should strive to have
  section titles that are not redundant and grammatically consistent
  with each other.

- Use paragraphs and paragraph breaks to increase clarity and
  flow. Avoid burying critical information in the middle of long
  paragraphs. Err on the side of shorter paragraphs.

- Prefer shorter sentences to longer sentences. Use complex
  formations only as a last resort, if at all (e.g. compound
  complex structures that require semi-colons).

- Avoid paragraphs that consist of single sentences as
  they often represent a sentence that has unintentionally become too
  complex or incomplete. However, sometimes such paragraphs are useful
  for emphasis, summary, or introductions.

  As a corollary, most sections should have multiple paragraphs.

- For longer lists and more complex lists, use bulleted items rather
  than integrating them inline into a sentence.

- Do not expect that the content of any example (inline or blocked)
  will be self explanatory. Even when it feels redundant, make sure
  that the function and use of every example is clearly described.

ReStructured Text and Typesetting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Place spaces between nested parentheticals and elements in
  JavaScript examples. For example, prefer ``{ [ a, a, a ] }`` over
  ``{[a,a,a]}``.

- For underlines associated with headers in RST, use:

  - ``=`` for heading level 1 or h1s. Use underlines and overlines for
    document titles.
  - ``-`` for heading level 2 or h2s.
  - ``~`` for heading level 3 or h3s.
  - ````` for heading level 4 or h4s.

- Use hyphens (``-``) to indicate items of an ordered list.

- Place footnotes and other references, if you use them, at the end of
  a section rather than the end of a file.

  Use the footnote format that includes automatic numbering and a
  target name for ease of use. For instance a footnote tag may look
  like: ``[#note]_`` with the corresponding directive holding the
  body of the footnote that resembles the following: ``.. [#note]``.

  Do **not** include ``.. code-block:: [language]`` in footnotes.

- As it makes sense, use the ``.. code-block:: [language]`` form to
  insert literal blocks into the text. While the double colon,
  ``::``, is functional, the ``.. code-block:: [language]`` form makes the source easier to
  read and understand.

- For all mentions of referenced types (i.e. commands, operators,
  expressions, functions, statuses, etc.) use the reference types to
  ensure uniform formatting and cross-referencing.

Jargon and Common Terms
-----------------------

.. list-table::
   :header-rows: 1
   :widths: 10, 20, 10, 60

   * - Preferred Term
     - Concept
     - Dispreferred Alternatives
     - Notes

   * - :term:`document`
     - A single, top-level object/record in a MongoDB collection.
     - record, object, row
     - Prefer document over object because of concerns about
       cross-driver language handling of objects. Reserve record for
       "allocation" of storage. Avoid "row," as possible.

   * - :term:`database`
     - A group of collections. Refers to a group of data files. This
       is the "logical" sense of the term "database."
     -
     - Avoid genericizing "database." Avoid using database to refer to
       a server process or a data set. This applies both to the
       datastoring contexts as well as other (related) operational
       contexts (command context, authentication/authorization
       context.)

   * - instance
     - A daemon process. (e.g. :program:`mongos` or :program:`mongod`)
     - process (acceptable sometimes), node (never acceptable), server.
     - Avoid using instance, unless it modifies something
       specifically. Having a descriptor for a process/instance makes
       it possible to avoid needing to make mongod or mongos
       plural. Server and node are both vague and contextually
       difficult to disambiguate with regards to application servers,
       and underlying hardware.

   * - :term:`field` name
     - The identifier of a value in a document.
     - key, column
     - Avoid introducing unrelated terms for a single field. In the
       documentation we've rarely had to discuss the identifier of a
       field, so the extra word here isn't burdensome.

   * - :term:`field`/value
     - The name/value pair that describes a unit of data in MongoDB.
     - key, slot, attribute
     - Use to emphasize the difference between the name of a field and
       its value For example, "_id" is the field and the default
       value is an ObjectId.

   * - value
     - The data content of a field.
     - data
     -

   * - MongoDB
     - A group of processes, or deployment that implement the MongoDB
       interface.
     - mongo, mongodb, cluster
     - Stylistic preference, mostly. In some cases it's useful to be
       able to refer generically to instances (that may be either
       :program:`mongod` or :program:`mongos`.)

   * - sub-document
     - An embedded or nested document within a document or an array.
     - embedded document, nested document
     -

   * - :term:`map-reduce`
     - An operation performed by the mapReduce command.
     - mapReduce, map reduce, map/reduce
     - Avoid confusion with the command, shell helper, and driver
       interfaces. Makes it possible to discuss the operation
       generally.

   * - cluster
     - A sharded cluster.
     - grid, shard cluster, set, deployment
     - Cluster is a great word for a group of processes; however, it's
       important to avoid letting the term become generic. Do not use
       for any group of MongoDB processes or deployments.

   * - sharded cluster
     - A :term:`sharded cluster`.
     - shard cluster, cluster, sharded system
     -

   * - :term:`replica set`
     - A deployment of replicating :program:`mongod` programs that
       provide redundancy and automatic failover.
     - set, replication deployment
     -

   * - deployment
     - A group of MongoDB processes, or a standalone :program:`mongod`
       instance.
     - cluster, system
     - Typically in the form MongoDB deployment. Includes standalones,
       replica sets and sharded clusters.

   * - data set
     - The collection of physical databases provided by a MongoDB
       deployment.
     - database, data
     - Important to keep the distinction between the data provided by
       a mongod or a sharded cluster as distinct from each "database"
       (i.e. a logical database that refers to a group of collections
       stored in a single series of data files.)

   * - :term:`primary`
     - The only member of a replica set that can accept writes.
     - master
     - Avoid "primary member" construction.

   * - secondary
     - Read-only members of a replica set that apply operations from
       the primary's oplog.
     - slave
     - Accept "secondary member" as needed.

   * - primary shard
     - The shard in a cluster that's "primary" for a database.
     - primary
     - Avoid ambiguity with primary in the context of replica
       sets.

   * - range based sharding
     - Refers to sharding based on regular shard keys where the range
       is the value of the field(s) selected as the shard key.
     -
     -

   * - hash based sharding
     - Refers to sharding based on hashed shard keys where the range
       is the hashed value of the field selected as the shard key.
     -
     - Even though hashed sharding is based on ranges of hashes, the
       sequence of hashes aren't meaningful to users, and the
       range-based aspect of hashed shard keys is an implementation
       detail.

   * - sharding
     - Describes the practice of horizontal scaling or partitioning as
       implemented in sharded clusters.
     - partitioning, horizontal scaling
     - Only use the terms "partitioning" and "horizontal scaling" to
       describe what sharding does, and its operation. Don't refer to
       sharding as "the partitioning system."

   * - metadata
     - data about data
     - meta-data, meta data
     -

Database Systems and Processes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- To indicate the entire database system, use "MongoDB," not mongo or
  Mongo.

- To indicate the database process or a server instance, use ``mongod``
  or ``mongos``. Refer to these as "processes" or "instances." Reserve
  "database" for referring to a database structure, i.e., the structure
  that holds collections and refers to a group of files on disk.

Distributed System Terms
~~~~~~~~~~~~~~~~~~~~~~~~

- Refer to partitioned systems as "sharded clusters." Do not use
  shard clusters or sharded systems.

- Refer to configurations that run with replication as "replica sets" (or
  "master/slave deployments") rather than "clusters" or other variants.

Data Structure Terms
~~~~~~~~~~~~~~~~~~~~

- "document" refers to "rows" or "records" in a MongoDB
  database. Potential confusion with "JSON Documents."

  Do not refer to documents as "objects," because drivers (and
  MongoDB) do not preserve the order of fields when fetching data. If
  the order of objects matter, use an array.

- "field" refers to a "key" or "identifier" of data within a MongoDB
  document.

- "value" refers to the contents of a "field".

- "sub-document" describes a nested document.

Other Terms
~~~~~~~~~~~

- Use ``example.net`` (and ``.org`` or ``.com`` if needed) for all
  examples and samples.

- Hyphenate "map-reduce" in order to avoid ambiguous reference to the
  command name. Do not camel-case.

Notes on Specific Features
--------------------------

- Geo-Location

  #. While MongoDB *is capable* of storing coordinates in
     sub-documents, in practice, users should only store coordinates
     in arrays. (See: `DOCS-41 <https://jira.mongodb.org/browse/DOCS-41>`_.)
.. _internationalization:

==========================
MongoDB Manual Translation
==========================

.. default-domain:: mongodb

The original authorship language for all MongoDB documentation is
American English. However, ensuring that speakers of other languages
can read and understand the documentation is of critical importance to
the documentation project.

In this direction, the MongoDB Documentation project uses the service
provided by `Smartling <http://smartling.com/>`_ to translate the
MongoDB documentation into additional non-English languages. This
translation project is largely supported by the work of volunteer
translators from the MongoDB community who contribute to the
translation effort.

If you would like to volunteer to help translate the MongoDB
documentation, please:

- complete the `MongoDB Contributor Agreement
  <http://www.mongodb.com/legal/contributor-agreement>`_, and

- create an account on Smartling at `translate.docs.mongodb.org
  <http://translate.docs.mongodb.org/>`_.

Please use the same email address you use to sign the contributor as
you use to create your Smartling account.

The `mongodb-translators`_ user group exists to facilitate
collaboration between translators and the documentation team at large.
You can join the Google Group without signing the contributor's
agreement.

We currently have the following languages configured:

- `Arabic <http://ar.docs.mongodb.org>`_
- `Chinese <http://cn.docs.mongodb.org>`_
- `Czech <http://cs.docs.mongodb.org>`_
- `French <http://fr.docs.mongodb.org>`_
- `German <http://de.docs.mongodb.org>`_
- `Hungarian <http://hu.docs.mongodb.org>`_
- `Indonesian <http://id.docs.mongodb.org>`_
- `Italian <http://it.docs.mongodb.org>`_
- `Japanese <http://jp.docs.mongodb.org>`_
- `Korean <http://ko.docs.mongodb.org>`_
- `Lithuanian <http://lt.docs.mongodb.org>`_
- `Polish <http://pl.docs.mongodb.org>`_
- `Portuguese <http://pt.docs.mongodb.org>`_
- `Romanian <http://ro.docs.mongodb.org>`_
- `Russian <http://ru.docs.mongodb.org>`_
- `Spanish <http://es.docs.mongodb.org>`_
- `Turkish <http://tr.docs.mongodb.org>`_
- `Ukrainian <http://uk.docs.mongodb.org>`_

If you would like to initiate a translation project to an additional
language, please report this issue using the "*Report a Problem*" link
above or by posting to the `mongodb-translators`_ list.

Currently the translation project only publishes rendered translation.
While the translation effort is currently focused on the web site we are
evaluating how to retrieve the translated phrases for use in other
media.

.. _`mongodb-translators`: http://groups.google.com/group/mongodb-translators

.. seealso::

   - :ref:`meta-contributing`
   - :doc:`/meta/style-guide`
   - :doc:`/meta/organization`
   - :doc:`/meta/practices`
   - :doc:`/meta/build`
========================
Administration Reference
========================

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-administration-reference.rst

.. include:: /includes/toc/administration-reference.rst
===============================
Aggregation Commands Comparison
===============================

.. default-domain:: mongodb

The following table provides a brief overview of the features of the
MongoDB aggregation commands.

.. include:: /includes/table/aggregation-xref.rst
========================
Variables in Aggregation
========================

.. default-domain:: mongodb

Aggregation expressions can use both user-defined and system variables.

Variables can hold any :doc:`BSON type data </reference/bson-types>`.
To access the value of the variable, use a string with the variable
name prefixed with double dollar signs (``$$``).

If the variable references an object, to access a specific field in the
object, use the dot notation; i.e. ``"$$<variable>.<field>"``.

.. _agg-user-variables:

User Variables
--------------

User variable names can contain the ascii characters ``[_a-zA-Z0-9]``
and any non-ascii character.

User variable names must begin with a lowercase ascii letter ``[a-z]``
or a non-ascii character.

.. _agg-system-variables:

System Variables
----------------

MongoDB offers the following system variables:

.. list-table::
   :header-rows: 1
   :widths: 15 50

   * - Variable

     - Description

   * - .. variable:: ROOT

     - References the root document, i.e. the top-level document, currently
       being processed in the aggregation pipeline stage.

   * - .. variable:: CURRENT

     - References the start of the field path being processed in the
       aggregation pipeline stage. Unless documented otherwise, all
       stages start with :variable:`CURRENT` the same as
       :variable:`ROOT`.

       :variable:`CURRENT` is modifiable. However, since ``$<field>``
       is equivalent to ``$$CURRENT.<field>``, rebinding
       :variable:`CURRENT` changes the meaning of ``$`` accesses.

   * - .. variable:: DESCEND

     - One of the allowed results of a :pipeline:`$redact` expression.

   * - .. variable:: PRUNE

     - One of the allowed results of a :pipeline:`$redact` expression.

   * - .. variable:: KEEP

     - One of the allowed results of a :pipeline:`$redact` expression.

.. seealso:: :expression:`$let`, :pipeline:`$redact`
=====================
Aggregation Reference
=====================

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-spec-aggregation-reference.rst

.. include:: /includes/toc/aggregation-reference.rst
===========================
System Event Audit Messages
===========================

.. default-domain:: mongodb

.. include:: /includes/note-audit-in-enterprise-only.rst

The :doc:`event auditing feature </core/auditing>` can record
events in JSON format. The recorded JSON messages have the following
syntax:

.. code-block:: none

   {
     atype: <String>,
     ts : { "$date": <timestamp> },
     local: { ip: <String>, port: <int> },
     remote: { ip: <String>, port: <int> },
     users : [ { user: <String>, db: String> }, ... ],
     params: <document>,
     result: <int>
   }

.. include:: /reference/audit-message-fields.rst

.. _audit-action-details-results:

Event Actions, Details, and Results
-----------------------------------

The following table lists for each ``atype`` or action type, the
associated ``params`` details and the ``result`` values, if any.

.. include:: /includes/table/system-event-audit-params-results.rst

Additional Information
----------------------

.. _audit-message-role:

``role`` Document
~~~~~~~~~~~~~~~~~

The ``<role>`` document in the ``roles`` array has the following form:

.. code-block:: javascript

   {
     role: <role name>,
     db: <database>
   }

.. _audit-message-privilege:

``privilege`` Document
~~~~~~~~~~~~~~~~~~~~~~

The ``<privilege>`` document in the ``privilege`` array has the following form:

.. code-block:: javascript

   {
     resource: <resource document> ,
     actions: [ <action>, ... ]
   }

See :ref:`resource-document` for details on the resource document. For
a list of actions, see :ref:`security-user-actions`.
===============================
The ``bios`` Example Collection
===============================

.. default-domain:: mongodb

The ``bios`` collection provides example data for experimenting with
MongoDB. Many of this guide's examples on :method:`insert
<db.collection.insert()>`, :method:`update <db.collection.update()>` and
:method:`read <db.collection.find()>` operations create or query data
from the ``bios`` collection.

The following documents comprise the ``bios`` collection. In the
examples, the data might be different, as the examples themselves make
changes to the data.

.. code-block:: javascript

   {
       "_id" : 1,
       "name" : {
           "first" : "John",
           "last" : "Backus"
       },
       "birth" : ISODate("1924-12-03T05:00:00Z"),
       "death" : ISODate("2007-03-17T04:00:00Z"),
       "contribs" : [
           "Fortran",
           "ALGOL",
           "Backus-Naur Form",
           "FP"
       ],
       "awards" : [
           {
               "award" : "W.W. McDowell Award",
               "year" : 1967,
               "by" : "IEEE Computer Society"
           },
           {
               "award" : "National Medal of Science",
               "year" : 1975,
               "by" : "National Science Foundation"
           },
           {
               "award" : "Turing Award",
               "year" : 1977,
               "by" : "ACM"
           },
           {
               "award" : "Draper Prize",
               "year" : 1993,
               "by" : "National Academy of Engineering"
           }
       ]
   }

   {
       "_id" : ObjectId("51df07b094c6acd67e492f41"),
       "name" : {
           "first" : "John",
           "last" : "McCarthy"
       },
       "birth" : ISODate("1927-09-04T04:00:00Z"),
       "death" : ISODate("2011-12-24T05:00:00Z"),
       "contribs" : [
           "Lisp",
           "Artificial Intelligence",
           "ALGOL"
       ],
       "awards" : [
           {
               "award" : "Turing Award",
               "year" : 1971,
               "by" : "ACM"
           },
           {
               "award" : "Kyoto Prize",
               "year" : 1988,
               "by" : "Inamori Foundation"
           },
           {
               "award" : "National Medal of Science",
               "year" : 1990,
               "by" : "National Science Foundation"
           }
       ]
   }

   {
       "_id" : 3,
       "name" : {
           "first" : "Grace",
           "last" : "Hopper"
       },
       "title" : "Rear Admiral",
       "birth" : ISODate("1906-12-09T05:00:00Z"),
       "death" : ISODate("1992-01-01T05:00:00Z"),
       "contribs" : [
           "UNIVAC",
           "compiler",
           "FLOW-MATIC",
           "COBOL"
       ],
       "awards" : [
           {
               "award" : "Computer Sciences Man of the Year",
               "year" : 1969,
               "by" : "Data Processing Management Association"
           },
           {
               "award" : "Distinguished Fellow",
               "year" : 1973,
               "by" : " British Computer Society"
           },
           {
               "award" : "W. W. McDowell Award",
               "year" : 1976,
               "by" : "IEEE Computer Society"
           },
           {
               "award" : "National Medal of Technology",
               "year" : 1991,
               "by" : "United States"
           }
       ]
   }

   {
       "_id" : 4,
       "name" : {
           "first" : "Kristen",
           "last" : "Nygaard"
       },
       "birth" : ISODate("1926-08-27T04:00:00Z"),
       "death" : ISODate("2002-08-10T04:00:00Z"),
       "contribs" : [
           "OOP",
           "Simula"
       ],
       "awards" : [
           {
               "award" : "Rosing Prize",
               "year" : 1999,
               "by" : "Norwegian Data Association"
           },
           {
               "award" : "Turing Award",
               "year" : 2001,
               "by" : "ACM"
           },
           {
               "award" : "IEEE John von Neumann Medal",
               "year" : 2001,
               "by" : "IEEE"
           }
       ]
   }

   {
       "_id" : 5,
       "name" : {
           "first" : "Ole-Johan",
           "last" : "Dahl"
       },
       "birth" : ISODate("1931-10-12T04:00:00Z"),
       "death" : ISODate("2002-06-29T04:00:00Z"),
       "contribs" : [
           "OOP",
           "Simula"
       ],
       "awards" : [
           {
               "award" : "Rosing Prize",
               "year" : 1999,
               "by" : "Norwegian Data Association"
           },
           {
               "award" : "Turing Award",
               "year" : 2001,
               "by" : "ACM"
           },
           {
               "award" : "IEEE John von Neumann Medal",
               "year" : 2001,
               "by" : "IEEE"
           }
       ]
   }

   {
       "_id" : 6,
       "name" : {
           "first" : "Guido",
           "last" : "van Rossum"
       },
       "birth" : ISODate("1956-01-31T05:00:00Z"),
       "contribs" : [
           "Python"
       ],
       "awards" : [
           {
               "award" : "Award for the Advancement of Free Software",
               "year" : 2001,
               "by" : "Free Software Foundation"
           },
           {
               "award" : "NLUUG Award",
               "year" : 2003,
               "by" : "NLUUG"
           }
       ]
   }

   {
       "_id" : ObjectId("51e062189c6ae665454e301d"),
       "name" : {
           "first" : "Dennis",
           "last" : "Ritchie"
       },
       "birth" : ISODate("1941-09-09T04:00:00Z"),
       "death" : ISODate("2011-10-12T04:00:00Z"),
       "contribs" : [
           "UNIX",
           "C"
       ],
       "awards" : [
           {
               "award" : "Turing Award",
               "year" : 1983,
               "by" : "ACM"
           },
           {
               "award" : "National Medal of Technology",
               "year" : 1998,
               "by" : "United States"
           },
           {
               "award" : "Japan Prize",
               "year" : 2011,
               "by" : "The Japan Prize Foundation"
           }
       ]
   }

   {
       "_id" : 8,
       "name" : {
           "first" : "Yukihiro",
           "aka" : "Matz",
           "last" : "Matsumoto"
       },
       "birth" : ISODate("1965-04-14T04:00:00Z"),
       "contribs" : [
           "Ruby"
       ],
       "awards" : [
           {
               "award" : "Award for the Advancement of Free Software",
               "year" : "2011",
               "by" : "Free Software Foundation"
           }
       ]
   }

   {
       "_id" : 9,
       "name" : {
           "first" : "James",
           "last" : "Gosling"
       },
       "birth" : ISODate("1955-05-19T04:00:00Z"),
       "contribs" : [
           "Java"
       ],
       "awards" : [
           {
               "award" : "The Economist Innovation Award",
               "year" : 2002,
               "by" : "The Economist"
           },
           {
               "award" : "Officer of the Order of Canada",
               "year" : 2007,
               "by" : "Canada"
           }
       ]
   }

   {
       "_id" : 10,
       "name" : {
           "first" : "Martin",
           "last" : "Odersky"
       },
       "contribs" : [
           "Scala"
       ]
   }
==========
BSON Types
==========

.. default-domain:: mongodb

:term:`BSON` is a binary serialization format used to store documents
and make remote procedure calls in MongoDB. The BSON specification is
located at `bsonspec.org <http://bsonspec.org/>`_.

BSON supports the following data types as values in documents. Each data
type has a corresponding number that can be used with the
:query:`$type` operator to query documents by BSON type.

=======================  ==========
**Type**                 **Number**
-----------------------  ----------
Double                       1
String                       2
Object                       3
Array                        4
Binary data                  5
Object id                    7
Boolean                      8
Date                         9
Null                        10
Regular Expression          11
JavaScript                  13
Symbol                      14
JavaScript (with scope)     15
32-bit integer              16
Timestamp                   17
64-bit integer              18
Min key                    255
Max key                    127
=======================  ==========

.. include:: /includes/fact-sort-order.rst

To determine a field's type, see :ref:`check-types-in-shell`.

If you convert BSON to JSON, see
:doc:`/reference/mongodb-extended-json`.

The next sections describe special considerations for particular BSON
types.

.. _document-bson-type-object-id:

ObjectId
--------

ObjectIds are: small, likely unique, fast to generate, and ordered.
These values consists of 12-bytes, where the first four bytes are a
timestamp that reflect the ObjectId's creation. Refer to the
:doc:`ObjectId </reference/object-id>` documentation for more
information.

.. _document-bson-type-string:

String
------

BSON strings are UTF-8. In general, drivers for each programming
language convert from the language's string format to UTF-8 when
serializing and deserializing BSON. This makes it possible to store
most international characters in BSON strings with ease.
[#sort-string-internationalization]_ In addition, MongoDB
:query:`$regex` queries support UTF-8 in the regex string.

.. [#sort-string-internationalization] Given strings using UTF-8
   character sets, using :method:`sort() <cursor.sort()>` on strings
   will be reasonably correct. However, because internally
   :method:`sort() <cursor.sort()>` uses the C++ ``strcmp`` api, the
   sort order may handle some characters incorrectly.

.. _document-bson-type-timestamp:

Timestamps
----------

BSON has a special timestamp type for *internal* MongoDB use and is
**not** associated with the regular :ref:`document-bson-type-date`
type. Timestamp values are a 64 bit value where:

- the first 32 bits are a ``time_t`` value (seconds since the Unix epoch)

- the second 32 bits are an incrementing ``ordinal`` for operations
  within a given second.

Within a single :program:`mongod` instance, timestamp values are
always unique.

In replication, the :term:`oplog` has a ``ts`` field. The values in
this field reflect the operation time, which uses a BSON timestamp
value.

.. note::

   The BSON Timestamp type is for *internal* MongoDB use. For most
   cases, in application development, you will want to use the BSON
   date type. See :ref:`document-bson-type-date` for more
   information.

If you create a BSON Timestamp using the empty constructor (e.g. ``new
Timestamp()``), MongoDB will only generate a timestamp *if* you use
the constructor in the first field of the document. [#id_exception]_
Otherwise, MongoDB will generate an empty timestamp value
(i.e. ``Timestamp(0, 0)``.)

.. versionchanged:: 2.1
   :program:`mongo` shell displays the Timestamp value with the wrapper:

   .. code-block:: javascript

      Timestamp(<time_t>, <ordinal>)

   Prior to version 2.1, the :program:`mongo` shell display the
   Timestamp value as a document:

   .. code-block:: javascript

      { t : <time_t>, i : <ordinal> }

.. [#id_exception] If the first field in the document is ``_id``, then
   you can generate a timestamp in the *second* field
   of a document.

   .. only:: html or dirhtml or singlehtml or epub or gettext

      In the following example, MongoDB will generate a Timestamp
      value, even though the ``Timestamp()`` constructor is *not* in
      the first field in the document:

      .. code-block:: javascript

         db.bios.insert( { _id: 9, last_updated: new Timestamp() } )

.. _document-bson-type-date:

Date
----

BSON Date is a 64-bit integer that represents the number of
milliseconds since the Unix epoch (Jan 1, 1970). The `official BSON
specification <http://bsonspec.org/#/specification>`_ refers to the
BSON Date type as the *UTC datetime*.

.. versionchanged:: 2.0
   BSON Date type is signed. [#unsigned-date]_ Negative values
   represent dates before 1970.

.. example:: Construct a Date using the ``new Date()`` constructor in the
   :program:`mongo` shell:

   .. code-block:: javascript

      var mydate1 = new Date()

.. example:: Construct a Date using the ``ISODate()`` constructor in the
   :program:`mongo` shell:

   .. code-block:: javascript

      var mydate2 = ISODate()

.. example:: Return the ``Date`` value as string:

   .. code-block:: javascript

      mydate1.toString()

.. example:: Return the month portion of the Date value; months are zero-indexed,
   so that January is month ``0``:

   .. code-block:: javascript

      mydate1.getMonth()

.. [#unsigned-date] Prior to version 2.0, ``Date`` values were
   incorrectly interpreted as *unsigned* integers, which affected
   sorts, range queries, and indexes on ``Date`` fields. Because
   indexes are not recreated when upgrading, please re-index if you
   created an index on ``Date`` values with an earlier version, and
   dates before 1970 are relevant to your application.
.. _built-in-roles:

==============
Built-In Roles
==============

.. default-domain:: mongodb

MongoDB grants access to data and commands through :ref:`role-based
authorization <roles>` and provides built-in roles that provide the different
levels of access commonly needed in a database system. You can additionally
create :ref:`user-defined roles <user-defined-roles>`.

A role grants privileges to perform sets of :ref:`actions
<security-user-actions>` on defined :ref:`resources <resource-document>`. A
given role applies to the database on which it is defined and can grant access
down to a collection level of granularity.

Each of MongoDB's built-in roles defines access at the database level for all
*non*-system collections in the role's database and at the collection level
for all :doc:`system collections </reference/system-collections>`.

MongoDB provides the built-in :ref:`database user <database-user-roles>` and
:ref:`database administration <database-administration-roles>` roles on
*every* database. MongoDB provides all other built-in roles only on the
``admin`` database.

This section describes the privileges for each built-in role. You can also
view the privileges for a built-in role at any time by issuing the
:dbcommand:`rolesInfo` command with the ``showPrivileges`` and
``showBuiltinRoles`` fields both set to ``true``.

.. _database-user-roles:

Database User Roles
-------------------

Every database includes the following client roles:

.. authrole:: read

   Provides the ability to read data on all *non*-system collections and on
   the following system collections: :data:`system.indexes
   <<database>.system.indexes>`, :data:`system.js <<database>.system.js>`, and
   :data:`system.namespaces <<database>.system.namespaces>` collections. The
   role provides read access by granting the following :ref:`actions
   <security-user-actions>`:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`

.. authrole:: readWrite

   Provides all the privileges of the :authrole:`read` role plus ability to
   modify data on all *non*-system collections and the :data:`system.js
   <<database>.system.js>` collection. The role provides the following
   actions on those collections:

   - :authaction:`collStats`
   - :authaction:`convertToCapped`
   - :authaction:`createCollection`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`dropCollection`
   - :authaction:`createIndex`
   - :authaction:`dropIndex`
   - :authaction:`emptycapped`
   - :authaction:`find`
   - :authaction:`insert`
   - :authaction:`killCursors`
   - :authaction:`remove`
   - :authaction:`renameCollectionSameDB`
   - :authaction:`update`

.. _database-administration-roles:

Database Administration Roles
-----------------------------

Every database includes the following database administration roles:

.. authrole:: dbAdmin

   Provides the following :ref:`actions <security-user-actions>` on the
   database's :data:`system.indexes <<database>.system.indexes>`,
   :data:`system.namespaces <<database>.system.namespaces>`, and
   :data:`system.profile <<database>.system.profile>` collections:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`
   - :authaction:`dropCollection` on :data:`system.profile
     <<database>.system.profile>` *only*

   Provides the following actions on all *non*-system collections. This role
   *does not* include full read access on non-system collections:

   - :authaction:`collMod`
   - :authaction:`collStats`
   - :authaction:`compact`
   - :authaction:`convertToCapped`
   - :authaction:`createCollection`
   - :authaction:`createIndex`
   - :authaction:`dbStats`
   - :authaction:`dropCollection`
   - :authaction:`dropDatabase`
   - :authaction:`dropIndex`
   - :authaction:`enableProfiler`
   - :authaction:`indexStats`
   - :authaction:`reIndex`
   - :authaction:`renameCollectionSameDB`
   - :authaction:`repairDatabase`
   - :authaction:`storageDetails`
   - :authaction:`validate`

.. authrole:: dbOwner

   The database owner can perform any administrative action on the database.
   This role combines the privileges granted by the :authrole:`readWrite`,
   :authrole:`dbAdmin` and :authrole:`userAdmin` roles.

.. authrole:: userAdmin

   Provides the ability to create and modify roles and users on the current
   database. This role also indirectly provides :ref:`superuser <superuser>`
   access to either the database or, if scoped to the ``admin`` database, the
   cluster. The :authrole:`userAdmin` role allows users to grant any user any
   privilege, including themselves.

   The :authrole:`userAdmin` role explicitly provides the following actions:

   - :authaction:`changeCustomData`
   - :authaction:`changePassword`
   - :authaction:`createRole`
   - :authaction:`createUser`
   - :authaction:`dropRole`
   - :authaction:`dropUser`
   - :authaction:`grantRole`
   - :authaction:`revokeRole`
   - :authaction:`viewRole`
   - :authaction:`viewUser`

.. _cluster-admin-roles:
.. _admin-roles:

Cluster Administration Roles
----------------------------

The ``admin`` database includes the following roles for administering the
whole system rather than just a single database. These roles include but are
not limited to :term:`replica set` and :term:`sharded cluster` administrative
functions.

.. authrole:: clusterAdmin

   Provides the greatest cluster-management access. This role combines the
   privileges granted by the :authrole:`clusterManager`,
   :authrole:`clusterMonitor`, and :authrole:`hostManager` roles.
   Additionally, the role provides the :authaction:`dropDatabase` action.

.. authrole:: clusterManager

   Provides management and monitoring actions on the cluster. A user with
   this role can access the ``config`` and ``local`` databases, which are
   used in sharding and replication, respectively.

   Provides the following actions on the cluster as a whole:

   - :authaction:`addShard`
   - :authaction:`applicationMessage`
   - :authaction:`cleanupOrphaned`
   - :authaction:`flushRouterConfig`
   - :authaction:`listShards`
   - :authaction:`removeShard`
   - :authaction:`replSetConfigure`
   - :authaction:`replSetGetStatus`
   - :authaction:`replSetStateChange`
   - :authaction:`resync`

   Provides the following actions on *all* databases in the cluster:

   - :authaction:`enableSharding`
   - :authaction:`moveChunk`
   - :authaction:`splitChunk`
   - :authaction:`splitVector`

   On the ``config`` database, provides the following actions on the
   :data:`~config.settings` collection:

   - :authaction:`insert`
   - :authaction:`remove`
   - :authaction:`update`

   On the ``config`` database, provides the following actions on all
   configuration collections and on the :data:`system.indexes
   <<database>.system.indexes>`, :data:`system.js <<database>.system.js>`, and
   :data:`system.namespaces <<database>.system.namespaces>` collections:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`

   On the ``local`` database, provides the following actions on the
   :data:`~local.system.replset` collection:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`

.. authrole:: clusterMonitor

   Provides read-only access to monitoring tools, such as the `MongoDB
   Management Service (MMS) <http://mms.mongodb.com/help/>`_ monitoring agent.

   Provides the following actions on the cluster as a whole:

   - :authaction:`connPoolStats`
   - :authaction:`cursorInfo`
   - :authaction:`getCmdLineOpts`
   - :authaction:`getLog`
   - :authaction:`getParameter`
   - :authaction:`getShardMap`
   - :authaction:`hostInfo`
   - :authaction:`inprog`
   - :authaction:`listDatabases`
   - :authaction:`listShards`
   - :authaction:`netstat`
   - :authaction:`replSetGetStatus`
   - :authaction:`serverStatus`
   - :authaction:`shardingState`
   - :authaction:`top`

   Provides the following actions on *all* databases in the cluster:

   - :authaction:`collStats`
   - :authaction:`dbStats`
   - :authaction:`getShardVersion`

   Provides the :authaction:`find` action on all :data:`system.profile
   <<database>.system.profile>` collections in the cluster.

   Provides the following actions on the ``config`` database's
   configuration collections and :data:`system.indexes
   <<database>.system.indexes>`, :data:`system.js <<database>.system.js>`, and
   :data:`system.namespaces <<database>.system.namespaces>` collections:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`

.. authrole:: hostManager

   Provides the ability to monitor and manager servers.

   Provides the following actions on the cluster as a whole:

   - :authaction:`applicationMessage`
   - :authaction:`closeAllDatabases`
   - :authaction:`connPoolSync`
   - :authaction:`cpuProfiler`
   - :authaction:`diagLogging`
   - :authaction:`flushRouterConfig`
   - :authaction:`fsync`
   - :authaction:`invalidateUserCache`
   - :authaction:`killop`
   - :authaction:`logRotate`
   - :authaction:`resync`
   - :authaction:`setParameter`
   - :authaction:`shutdown`
   - :authaction:`touch`
   - :authaction:`unlock`

   Provides the following actions on *all* databases in the cluster:

   - :authaction:`killCursors`
   - :authaction:`repairDatabase`

.. _backup-and-restore-roles:

Backup and Restoration Roles
----------------------------

The ``admin`` database includes the following roles for backing up and
restoring data:

.. authrole:: backup

   Provides minimal privileges needed for backing up data.
   This role provides sufficient privileges to use the
   `MongoDB Management Service (MMS) <http://mms.mongodb.com/help/>`_
   backup agent, or to use :program:`mongodump` to back up an entire
   :program:`mongod` instance.

   .. todo: should we document the mms.backup collection in the
            system-collections document?

   Provides the following :ref:`actions <security-user-actions>` on the
   ``mms.backup`` collection in the ``admin`` database:

   - :authaction:`insert`
   - :authaction:`update`

   Provides the :authaction:`listDatabases` action on the cluster as a whole.

   Provides the :authaction:`find` action on the following:

   - all *non*-system collections in the cluster

   - all the following system collections in the cluster:
     :data:`system.indexes <<database>.system.indexes>`,
     :data:`system.namespaces <<database>.system.namespaces>`, and
     :data:`system.js <<database>.system.js>`

   - the :data:`admin.system.users` and :data:`admin.system.roles` collections

   - legacy ``system.users`` collections from versions of MongoDB prior to 2.6

   .. todo: Also provides the :authaction:`find` action on:
             - system.new_users
             - system.backup_users
             - system.version
            Do we want to document these?

.. authrole:: restore

   Provides minimal privileges needed for restoring data from backups.
   This role provides sufficient privileges to use the :program:`mongorestore` tool
   to restore an entire :program:`mongod` instance.

   Provides the following actions on all *non*-system collections and
   :data:`system.js <<database>.system.js>` collections in the cluster; on
   the :data:`admin.system.users` and :data:`admin.system.roles`
   collections in the ``admin`` database; and on legacy ``system.users``
   collections from versions of MongoDB prior to 2.6:

   - :authaction:`collMod`
   - :authaction:`createCollection`
   - :authaction:`createIndex`
   - :authaction:`dropCollection`
   - :authaction:`insert`

   .. todo: Also provides the above actions on system.new_users,
            system.backup_users, system.version

   Provides the following *additional* actions on :data:`admin.system.users`
   and legacy ``system.users`` collections:

   - :authaction:`find`
   - :authaction:`remove`
   - :authaction:`update`

   Provides the :authaction:`find` action on all the :data:`system.namespaces
   <<database>.system.namespaces>` collections in the cluster.

.. _auth-any-database-roles:

All-Database Roles
------------------

The ``admin`` database provides the following roles that apply to all
databases in a :program:`mongod` instance and are roughly equivalent to their
single-database equivalents:

.. authrole:: readAnyDatabase

   Provides the same read-only permissions as :authrole:`read`, except it
   applies to *all* databases in the cluster. The role also provides the
   :authaction:`listDatabases` action on the cluster as a whole.

.. authrole:: readWriteAnyDatabase

   Provides the same read and write permissions as :authrole:`readWrite`,
   except it applies to *all* databases in the cluster. The role also provides
   the :authaction:`listDatabases` action on the cluster as a whole.

.. authrole:: userAdminAnyDatabase

   Provides the same access to user administration operations as
   :authrole:`userAdmin`, except it applies to *all* databases in the
   cluster. The role also provides the following actions on the cluster
   as a whole:

   - :authaction:`authSchemaUpgrade`
   - :authaction:`invalidateUserCache`
   - :authaction:`listDatabases`

   The role also provides the following actions on the
   :data:`admin.system.users` and :data:`admin.system.roles` collections on
   the ``admin`` database, and on legacy ``system.users`` collections from
   versions of MongoDB prior to 2.6:

   - :authaction:`collStats`
   - :authaction:`dbHash`
   - :authaction:`dbStats`
   - :authaction:`find`
   - :authaction:`killCursors`
   - :authaction:`planCacheRead`

   The :authrole:`userAdminAnyDatabase` role does not restrict the permissions
   that a user can grant. As a result, :authrole:`userAdminAnyDatabase` users
   can grant themselves privileges in excess of their current
   privileges and even can grant themselves *all privileges*, even though the
   role does not explicitly authorize privileges beyond user administration.
   This role is effectively a MongoDB system :ref:`superuser <superuser>`.

   .. todo: Also provides the above actions on system.new_users, system.backup_users,
            and system.version. Do we want to document these?

.. authrole:: dbAdminAnyDatabase

   Provides the same access to database administration operations as
   :authrole:`dbAdmin`, except it applies to *all* databases in the cluster.
   The role also provides the :authaction:`listDatabases` action
   on the cluster as a whole.

.. _superuser:

Superuser Roles
---------------

Several roles provide either indirect or direct system-wide superuser access.

The following roles provide the ability to assign any user any privilege on
any database, which means that users with one of these roles can assign
*themselves* any privilege on any database:

- :authrole:`dbOwner` role, when scoped to the ``admin`` database

- :authrole:`userAdmin` role, when scoped to the ``admin`` database

- :authrole:`userAdminAnyDatabase` role

The following role provides full privileges on all resources:

.. authrole:: root

   Provides access to all the operations and all the resources of *of all
   other roles combined*. The role provides full privileges on all resources
   in the cluster.

Internal Role
-------------

.. authrole:: __system

   MongoDB assigns this role to user objects that represent cluster members,
   such as replica set members and :program:`mongos` instances. The role
   entitles its holder to take any action against any object in the database.

   **Do not** assign this role to user objects representing applications or
   human administrators, other than in exceptional circumstances.

   If you need access to all actions on all resources, for example to run
   the :dbcommand:`eval` or :dbcommand:`applyOps` commands, do not
   assign this role. Instead,:ref:`create a user-defined role
   <define-roles>` that grants :authaction:`anyAction` on
   :ref:`resource-anyresource` and ensure that only the users who
   needs access to these operations has this access.
========
addShard
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: addShard

   Adds either a database instance or a :term:`replica set` to a
   :term:`sharded cluster`. The optimal configuration is to deploy
   shards across replica sets.

   Run :dbcommand:`addShard` when connected to a :program:`mongos`
   instance.  The command takes the following form when adding a single
   database instance as a shard:

   .. code-block:: javascript

      { addShard: "<hostname><:port>", maxSize: <size>, name: "<shard_name>" }

   When adding a replica set as a shard, use the following form:

   .. code-block:: javascript

      { addShard: "<replica_set>/<hostname><:port>", maxSize: <size>, name: "<shard_name>" }

   The command contains the following fields:

   .. include:: /reference/command/addShard-field.rst

   The :dbcommand:`addShard` command stores shard configuration
   information in the :term:`config database`. Always run
   :dbcommand:`addShard` when using the ``admin`` database.

   Specify a ``maxSize`` when you have machines with different disk
   capacities, or if you want to limit the amount of data on some
   shards. The ``maxSize`` constraint prevents the :term:`balancer` from
   migrating chunks to the shard when the value of :data:`mem.mapped
   <serverStatus.mem.mapped>` exceeds the value of ``maxSize``.

.. |cmd-name| replace:: :dbcommand:`addShard`
.. include:: /includes/important-add-shard-not-compatible-with-hidden-members.rst

Examples
--------

The following command adds the database instance running on
port ``27027`` on the host ``mongodb0.example.net`` as a shard:

.. code-block:: javascript

   use admin
   db.runCommand({addShard: "mongodb0.example.net:27027"})

.. warning::

   Do not use ``localhost`` for the hostname unless your
   :term:`configuration server <config database>` is also running on
   ``localhost``.

The following command adds a replica set as a shard:

.. code-block:: javascript

   use admin
   db.runCommand( { addShard: "repl0/mongodb3.example.net:27327"} )

You may specify all members in the replica set. All additional
hostnames must be members of the same replica set.
=========
aggregate
=========

.. default-domain:: mongodb

.. dbcommand:: aggregate

   .. versionadded:: 2.2

   Performs aggregation operation using the :doc:`aggregation pipeline
   </reference/operator/aggregation-pipeline>`. The pipeline allows
   users to process data from a collection with a sequence of
   stage-based manipulations.

   .. TODO will undergo a thorough rewrite during the agg doc rewrite

   .. versionchanged:: 2.6

   - The :dbcommand:`aggregate` command adds support for returning a
     cursor, supports the ``explain`` option, and enhances its sort
     operations with an external sort facility.

   - :doc:`aggregation pipeline
     </reference/operator/aggregation-pipeline>` introduces the
     :pipeline:`$out` operator to allow :dbcommand:`aggregate`
     command to store results to a collection.

   The command has following syntax:

   .. versionchanged:: 2.6

   .. code-block:: javascript

      {
        aggregate: "<collection>",
        pipeline: [ <stage>, <...> ],
        explain: <boolean>,
        allowDiskUse: <boolean>,
        cursor: <document>
      }

   The :dbcommand:`aggregate` command takes the following fields as
   arguments:

   .. include:: /reference/command/aggregate-field.rst

For more information about the aggregation pipeline
:doc:`/core/aggregation-pipeline`, :doc:`/reference/aggregation`, and
:doc:`/core/aggregation-pipeline-limits`.

Example
-------

Aggregate Data with Multi-Stage Pipeline
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``articles`` contains documents such as the following:

.. code-block:: javascript

   {
      _id: ObjectId("52769ea0f3dc6ead47c9a1b2"),
      author: "abc123",
      title: "zzz",
      tags: [ "programming", "database", "mongodb" ]
   }

The following example performs an :dbcommand:`aggregate` operation on
the ``articles`` collection to calculate the count of each distinct
element in the ``tags`` array that appears in the collection.

.. code-block:: javascript

   db.runCommand(
      { aggregate: "articles",
        pipeline: [
                    { $project: { tags: 1 } },
                    { $unwind: "$tags" },
                    { $group: {
                                _id: "$tags",
                                count: { $sum : 1 }
                              }
                    }
                  ]
      }
   )

In the :program:`mongo` shell, this operation can use the
:method:`~db.collection.aggregate()` helper as in the following:

.. code-block:: javascript

   db.articles.aggregate(
                          [
                             { $project: { tags: 1 } },
                             { $unwind: "$tags" },
                             { $group: {
                                         _id: "$tags",
                                         count: { $sum : 1 }
                                       }
                             }
                          ]
   )

.. note:: In 2.6 and later, the :method:`~db.collection.aggregate()`
   helper always returns a cursor.

.. include:: /includes/fact-agg-helper-exception.rst

Return Information on the Aggregation Operation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation operation sets the optional field ``explain``
to ``true`` to return information about the aggregation operation.

.. code-block:: javascript

   db.runCommand( { aggregate: "orders",
                    pipeline: [
                                { $match: { status: "A" } },
                                { $group: { _id: "$cust_id", total: { $sum: "$amount" } } },
                                { $sort: { total: -1 } }
                              ],
                    explain: true
                 } )

.. note:: The intended readers of the ``explain`` output document are humans, and
   not machines, and the output format is subject to change between
   releases.

.. seealso:: :method:`db.collection.aggregate()` method

Aggregate Data using External Sort
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Aggregation pipeline stages have :ref:`maximum memory use limit
<agg-memory-restrictions>`. To handle large datasets, set
``allowDiskUse`` option to ``true`` to enable writing data to
temporary files, as in the following example:

.. code-block:: javascript

   db.runCommand(
      { aggregate: "stocks",
        pipeline: [
                    { $project : { cusip: 1, date: 1, price: 1, _id: 0 } },
                    { $sort : { cusip : 1, date: 1 } }
                  ],
        allowDiskUse: true
      }
   )

.. seealso:: :method:`db.collection.aggregate()`


Aggregate Command Returns a Cursor
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::
   Using the :dbcommand:`aggregate` command to return a cursor is a
   low-level operation, intended for authors of drivers. Most users
   should use the :method:`db.collection.aggregate()` helper provided
   in the :program:`mongo` shell or in their driver. In 2.6 and
   later, the :method:`~db.collection.aggregate()` helper always
   returns a cursor.

The following command returns a document that contains results with
which to instantiate a cursor object.

.. code-block:: javascript

   db.runCommand(
      { aggregate: "records",
        pipeline: [
           { $project: { name: 1, email: 1, _id: 0 } },
           { $sort: { name: 1 } }
        ],
        cursor: { }
      }
   )

To specify an *initial* batch size, specify the ``batchSize`` in the
``cursor`` field, as in the following example:

.. code-block:: javascript

   db.runCommand(
      { aggregate: "records",
        pipeline: [
           { $project: { name: 1, email: 1, _id: 0 } },
           { $sort: { name: 1 } }
        ],
        cursor: { batchSize: 0 }
      }
   )

The ``{batchSize: 0 }`` document specifies the size of the *initial*
batch size only. Specify subsequent batch sizes to :ref:`OP_GET_MORE
<wire-op-get-more>` operations as with other MongoDB cursors. A
``batchSize`` of ``0`` means an empty first batch and is useful if you
want to quickly get back a cursor or failure message, without doing
significant server-side work.

.. seealso:: :method:`db.collection.aggregate()`
========
applyOps
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: applyOps

   Applies specified :term:`oplog` entries to a :program:`mongod`
   instance. The :dbcommand:`applyOps` command is primarily an internal
   command to support :term:`sharded clusters <sharded cluster>`.

   .. |eval-object| replace:: :dbcommand:`applyOps`
   .. include:: /includes/access-eval.rst

   The :dbcommand:`applyOps` command has the following prototype form:

   .. code-block:: javascript

      db.runCommand( { applyOps: [ <operations> ],
                       preCondition: [ { ns: <namespace>, q: <query>, res: <result> } ] } )

   The :dbcommand:`applyOps` command takes a document with the following fields:

   .. include:: /reference/command/applyOps-field.rst

   The ``preCondition`` array takes one or more documents with the following fields:

   .. include:: /reference/command/applyOps-preCondition-field.rst

   .. include:: /includes/warning-blocking-global.rst

   .. write-lock

   .. see: DOCS-133; SERVER-4259
============
authenticate
============

.. default-domain:: mongodb

.. dbcommand:: authenticate

   Clients use :dbcommand:`authenticate` to authenticate a
   connection. When using the shell, use the :method:`db.auth()`
   helper as follows:

   .. code-block:: javascript

       db.auth( "username", "password" )

   .. see:: :method:`db.auth()` and :doc:`/core/security`
      for more information.

   .. read-lock, slave-ok
=================
authSchemaUpgrade
=================

.. default-domain:: mongodb

.. versionadded:: 2.6

.. dbcommand:: authSchemaUpgrade

   :dbcommand:`authSchemagUpgrade` supports the upgrade from 2.4 to 2.6
   process for existing systems that use :term:`authentication` and
   :term:`authorization`. Between 2.4 and 2.6 the schema for user
   credential documents changed requiring the
   :dbcommand:`authScheamUpgrade` process.

   See :doc:`/release-notes/2.6-upgrade-authorization` for more
   information.
=====================
availableQueryOptions
=====================

.. default-domain:: mongodb

.. dbcommand:: availableQueryOptions

   :dbcommand:`availableQueryOptions` is an internal command that is only
   available on :program:`mongos` instances.
=========
buildInfo
=========

.. default-domain:: mongodb

.. dbcommand:: buildInfo

   The :dbcommand:`buildInfo` command is an administrative command which
   returns a build summary for the current
   :program:`mongod`. :dbcommand:`buildInfo` has the following
   prototype form:

   .. code-block:: javascript

      { buildInfo: 1 }

   In the :program:`mongo` shell, call :dbcommand:`buildInfo` in the
   following form:

   .. code-block:: javascript

      db.runCommand( { buildInfo: 1 } )

   .. example::

      The output document of :dbcommand:`buildInfo` has the following
      form:

      .. code-block:: javascript

         {
           "version" : "<string>",
           "gitVersion" : "<string>",
           "sysInfo" : "<string>",
           "loaderFlags" : "<string>",
           "compilerFlags" : "<string>",
           "allocator" : "<string>",
           "versionArray" : [ <num>, <num>, <...> ],
           "javascriptEngine" : "<string>",
           "bits" : <num>,
           "debug" : <boolean>,
           "maxBsonObjectSize" : <num>,
           "ok" : <num>
         }

   Consider the following documentation of the output of
   :dbcommand:`buildInfo`:

   .. data:: buildInfo

      The document returned by the :dbcommand:`buildInfo` command.

   .. data:: buildInfo.gitVersion

      The commit identifier that identifies the state of the code used
      to build the :program:`mongod`.

   .. data:: buildInfo.sysInfo

      A string that holds information about the operating system,
      hostname, kernel, date, and Boost version used to compile the
      :program:`mongod`.

   .. data:: buildInfo.loaderFlags

      The flags passed to the loader that loads the :program:`mongod`.

   .. data:: buildInfo.compilerFlags

      The flags passed to the compiler that builds the
      :program:`mongod` binary.

   .. data:: buildInfo.allocator

      .. versionchanged:: 2.2

      The memory allocator that :program:`mongod` uses. By default
      this is ``tcmalloc`` after version 2.2, and ``system`` before
      2.2.

   .. data:: buildInfo.versionArray

      An array that conveys version information about the
      :program:`mongod` instance. See :data:`~buildInfo.version` for a
      more readable version of this string.

   .. TODO:: document buildInfo.version

   .. data:: buildInfo.javascriptEngine

      .. versionchanged:: 2.4

      A string that reports the JavaScript engine used in the
      :program:`mongod` instance. By default, this is ``V8`` after
      version 2.4, and ``SpiderMonkey`` before 2.4.

   .. data:: buildInfo.bits

      A number that reflects the target processor architecture of the
      :program:`mongod` binary.

   .. data:: buildInfo.debug

      A boolean. ``true`` when built with debugging options.

   .. data:: buildInfo.maxBsonObjectSize

      A number that reports the :limit:`Maximum BSON Document Size
      <BSON Document Size>`.
========
captrunc
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: captrunc

   :dbcommand:`captrunc` is a command that truncates capped
   collections for diagnostic and testing purposes and is not part of
   the stable client facing API. The command takes the following form:

   .. code-block:: javascript

      { captrunc: "<collection>", n: <integer>, inc: <true|false> }.

   :dbcommand:`captrunc` has the following fields:

   .. include:: /reference/command/captrunc-field.rst

.. |dbcommand| replace:: :dbcommand:`captrunc`
.. include:: /includes/note-enabletestcommands.rst

Examples
--------

The following command truncates 10 older documents from the collection
``records``:

.. code-block:: javascript

   db.runCommand({captrunc: "records" , n: 10})

The following command truncates 100 documents and the 101st document:

.. code-block:: javascript

   db.runCommand({captrunc: "records", n: 100, inc: true})

.. write-lock
.. testcommand
==================
checkShardingIndex
==================

.. default-domain:: mongodb

.. dbcommand:: checkShardingIndex

   :dbcommand:`checkShardingIndex` is an internal command that supports the
   sharding functionality.

   .. read-lock
=====
clean
=====

.. default-domain:: mongodb

.. dbcommand:: clean

   :dbcommand:`clean` is an internal command.

   .. warning::

      This command obtains a write lock on the affected database and will
      block other operations until it has completed.

   .. write-lock, slave-ok
===============
cleanupOrphaned
===============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: cleanupOrphaned

   .. versionadded:: 2.6

   Deletes from a shard the :term:`orphaned documents <orphaned
   document>` whose shard key values fall into a single or a single
   contiguous range that do not belong to the shard. For example, if
   two contiguous ranges do not belong to the shard, the
   :dbcommand:`cleanupOrphaned` examines both ranges for orphaned
   documents.

   :dbcommand:`cleanupOrphaned` has the following syntax:

   .. code-block:: javascript

      db.runCommand( {
         cleanupOrphaned: "<database>.<collection>",
         startingAtKey: <minimumShardKeyValue>,
         secondaryThrottle: <boolean>
      } )

   :dbcommand:`cleanupOrphaned` has the following fields:

   .. include:: /reference/command/cleanupOrphaned-field.rst

Behavior
--------

Run :dbcommand:`cleanupOrphaned` in the ``admin`` database directly on
the :program:`mongod` instance that is the primary replica set member
of the shard. Do not run :dbcommand:`cleanupOrphaned` on a
:program:`mongos` instance.

You do not need to disable the balancer before running
:dbcommand:`cleanupOrphaned`.

.. _cleanupOrphaned-determine-range:

Determine Range
~~~~~~~~~~~~~~~

The :dbcommand:`cleanupOrphaned` command uses the ``startingAtKey``
value, if specified, to determine the start of the range to examine for
orphaned document:

- If the ``startingAtKey`` value falls into a range for a chunk not
  owned by the shard, :dbcommand:`cleanupOrphaned` begins examining at
  the start of this range, which may not necessarily be the
  ``startingAtKey``.

- If the ``startingAtKey`` value falls into a range for a chunk owned
  by the shard, :dbcommand:`cleanupOrphaned` moves onto the next range
  until it finds a range for a chunk not owned by the shard.

The :dbcommand:`cleanupOrphaned` deletes orphaned documents from the
start of the determined range and ends at the start of the chunk range
that belongs to the shard.

Consider the following key space with documents distributed across
``Shard A`` and ``Shard B``.

.. include:: /images/sharding-cleanup-orphaned.rst

``Shard A`` owns:

- ``Chunk 1`` with the range ``{ x: minKey } --> { x: -75 }``,

- ``Chunk 2`` with the range ``{ x: -75 } --> { x: 25 }``, and

- ``Chunk 4`` with the range ``{ x: 175 } --> { x: 200 }``.

``Shard B`` owns:

- ``Chunk 3`` with the range ``{ x: 25 } --> { x: 175 }`` and

- ``Chunk 5`` with the range ``{ x: 200 } --> { x: maxKey }``.

If on ``Shard A``, the :dbcommand:`cleanupOrphaned` command runs with
``startingAtKey: { x: -70 }`` or any other value belonging to range for
``Chunk 1`` or ``Chunk 2``, the :dbcommand:`cleanupOrphaned` command examines
the ``Chunk 3`` range of ``{ x: 25 } --> { x: 175 }`` to delete
orphaned data.

If on ``Shard B``, the :dbcommand:`cleanupOrphaned` command runs with
the ``startingAtKey: { x: -70 }`` or any other value belonging to range
for ``Chunk 1``, the :dbcommand:`cleanupOrphaned` command examines the
combined contiguous range for ``Chunk 1`` and ``Chunk 2``, namely ``{
x: minKey } --> { x: 25 }`` to delete orphaned data.

Required Access
---------------

On systems running with :setting:`~security.authentication`, you must have
:authrole:`clusterAdmin` privileges to run :dbcommand:`cleanupOrphaned`.

.. _cleanupOrphaned-output:

Output
------

Return Document
~~~~~~~~~~~~~~~

Each :dbcommand:`cleanupOrphaned` command returns a document containing
a subset of the following fields:

.. data:: cleanupOrphaned.ok

   Equal to ``1`` on success.

   A value of ``1`` indicates that :dbcommand:`cleanupOrphaned` scanned
   the specified shard key range, deleted any orphaned documents
   found in that range, and confirmed that all deletes replicated to a
   majority of the members of that shard's replica set. If confirmation
   does not arrive within 1 hour, :dbcommand:`cleanupOrphaned`
   times out.

   A value of ``0`` could indicate either of two cases:

   - :dbcommand:`cleanupOrphaned` found orphaned documents on the
     shard but could not delete them.

   - :dbcommand:`cleanupOrphaned` found and deleted orphaned
     documents, but could not confirm replication before the 1
     hour timeout. In this case, replication does occur, but only
     after :dbcommand:`cleanupOrphaned` returns.

.. data:: cleanupOrphaned.stoppedAtKey

   The upper bound of the cleanup range of shard keys. If present, the
   value corresponds to the lower bound of the next chunk on the shard.
   The absence of the field signifies that the cleanup range was the
   uppermost range for the shard.

Log Files
~~~~~~~~~

The :dbcommand:`cleanupOrphaned` command prints the number of deleted
documents to the :program:`mongod` log. For example:

.. code-block:: none

   m30000| 2013-10-31T15:17:28.972-0400 [conn1] Deleter starting delete for: foo.bar from { _id: -35.0 } -> { _id: -10.0 }, with opId: 128
   m30000| 2013-10-31T15:17:28.972-0400 [conn1] rangeDeleter deleted 0 documents for foo.bar from { _id: -35.0 } -> { _id: -10.0 } { "stoppedAtKey": { "_id": -10 }, "ok": 1 }

.. _cleanuporphaned-examples:

Examples
--------

The following examples run the :dbcommand:`cleanupOrphaned` command
directly on the primary of the shard.

Remove Orphaned Documents for a Specific Range
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a sharded collection ``info`` in the ``test`` database, a shard
owns a single chunk with the range: ``{ x: MinKey } --> { x: 10 }``.

The shard also contains documents whose shard keys values fall in a
range for a chunk *not* owned by the shard: ``{ x: 10 } --> { x: MaxKey
}``.

To remove orphaned documents within the ``{ x: 10 } => { x: MaxKey }``
range, you can specify a ``startingAtKey`` with a value that falls into
this range, as in the following example:

.. code-block:: javascript

   use admin
   db.runCommand( {
      "cleanupOrphaned": "test.info",
      "startingAtKey": { x: 10 },
      "secondaryThrottle": true
   } )

Or you can specify a ``startingAtKey`` with a value that falls into the
previous range, as in the following:

.. code-block:: javascript

   use admin
   db.runCommand( {
      "cleanupOrphaned": "test.info",
      "startingAtKey": { x: 2 },
      "secondaryThrottle": true
   } )

Since ``{ x: 2 }`` falls into a range that belongs to a chunk owned by
the shard, :dbcommand:`cleanupOrphaned` examines the next range to find
a range not owned by the shard, in this case ``{ x: 10 } => { x: MaxKey
}``.

Remove All Orphaned Documents from a Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:dbcommand:`cleanupOrphaned` examines documents from a single
contiguous range of shard keys. To remove all orphaned documents from
the shard, you can run :dbcommand:`cleanupOrphaned` in a loop, using
the returned ``stoppedAtKey`` as the next ``startingFromKey``, as in
the following:

.. code-block:: javascript

   use admin
   var nextKey = { };

   while ( nextKey = db.runCommand( {
                         cleanupOrphaned: "test.user",
                         startingFromKey: nextKey
                     } ).stoppedAtKey ) {
      printjson(nextKey);
   }

.. admin-only
=======
 clone
=======

.. default-domain:: mongodb

.. dbcommand:: clone

   The :dbcommand:`clone` command clones a database from a
   remote MongoDB instance to the current host. :dbcommand:`clone` copies the
   database on the remote instance with the same name as the current
   database. The command takes the following form:

   .. code-block:: javascript

        { clone: "db1.example.net:27017" }

   Replace ``db1.example.net:27017`` above with the resolvable hostname for the
   MongoDB instance you wish to copy from. Note the following
   behaviors:

   - :dbcommand:`clone` can run against a :term:`slave` or a
     non-:term:`primary` member of a :term:`replica set`.

   - :dbcommand:`clone` does not snapshot the database. If any clients
     update the database you're copying at any point during the clone
     operation, the resulting database may be inconsistent.

   - You must run :dbcommand:`clone` on the **destination server**.

   - The destination server is not locked for the duration of the
     :dbcommand:`clone` operation. This means that :dbcommand:`clone` will occasionally yield to
     allow other operations to complete.

   See :dbcommand:`copydb` for similar functionality with greater flexibility.

   .. warning::

      This command obtains an intermittent :term:`write lock` on the
      destination server, which can block other operations until it
      completes.

.. intermittent write lock on destination server.
===============
cloneCollection
===============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: cloneCollection

   Copies a collection from a remote :program:`mongod` instance to the
   current :program:`mongod` instance. :dbcommand:`cloneCollection`
   creates a collection in a database with the same name as the remote
   collection's database.  :dbcommand:`cloneCollection` takes the
   following form:

   .. code-block:: javascript

      { cloneCollection: "<collection>", from: "<hostname>", query: { <query> } }

   .. important:: You cannot clone a collection through a
      :program:`mongos` but must connect directly to the
      :program:`mongod` instance.

   :dbcommand:`cloneCollection` has the following fields:

   .. include:: /reference/command/cloneCollection-field.rst

Example
-------

.. code-block:: javascript

   { cloneCollection: "users.profiles", from: "mongodb.example.net:27017", query: { active: true } }

This operation copies the ``profiles`` collection from the ``users``
database on the server at ``mongodb.example.net``. The operation only
copies documents that satisfy the query ``{ active: true }`` and does
not copy indexes. :dbcommand:`cloneCollection` copies indexes by
default. The ``query`` arguments is optional.

If, in the above example, the ``profiles`` collection exists in the
``users`` database, then MongoDB appends documents from the remote
collection to the destination collection.
=======================
cloneCollectionAsCapped
=======================

.. default-domain:: mongodb

.. dbcommand:: cloneCollectionAsCapped

   The :dbcommand:`cloneCollectionAsCapped` command creates a new
   :term:`capped collection` from an existing, non-capped collection
   within the same database. The operation does not affect the
   original non-capped collection.

   The command has the following syntax:

   .. code-block:: javascript

      { cloneCollectionAsCapped: <existing collection>, toCollection: <capped collection>, size: <capped size> }

   The command copies an ``existing collection`` and creates a new
   ``capped collection`` with a maximum size specified by the ``capped
   size`` in bytes. The name of the new capped collection must be
   distinct and cannot be the same as that of the original existing
   collection. To replace the original non-capped collection with a
   capped collection, use the :dbcommand:`convertToCapped` command.

   During the cloning, the :dbcommand:`cloneCollectionAsCapped` command
   exhibit the following behavior:

   - MongoDB will transverse the documents in the original collection
     in :term:`natural order <natural order>` as they're loaded.

   - If the ``capped size`` specified for the new collection is
     smaller than the size of the original uncapped collection, then
     MongoDB will begin overwriting earlier documents in
     insertion order, which is *first in, first out* (e.g "FIFO").
=================
closeAllDatabases
=================

.. default-domain:: mongodb

.. dbcommand:: closeAllDatabases

   :dbcommand:`closeAllDatabases` is an internal command that
   invalidates all cursors and closes the open database files. The
   next operation that uses the database will reopen the file.

   .. include:: /includes/warning-blocking-global.rst
=======
collMod
=======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: collMod

   .. versionadded:: 2.2

   :dbcommand:`collMod` makes it possible to add flags to a collection
   to modify the behavior of MongoDB. Flags include
   :collflag:`usePowerOf2Sizes` and :collflag:`index`. The command
   takes the following prototype form:

   .. code-block:: javascript

      db.runCommand( {"collMod" : <collection> , "<flag>" : <value> } )

   In this command substitute ``<collection>`` with the name of a
   collection in the current database, and ``<flag>`` and ``<value>``
   with the flag and value you want to set.

   Use the :data:`~collStats.userFlags` field in the
   :method:`db.collection.stats()` output to check enabled collection
   flags.

.. Commenting out the following after DOCS-717, it does take
   a lock but its to cover a very small metadata change.

   This command obtains a write lock on the affected database
   and will block other operations until it has completed.

Flags
-----

Powers of Two Record Allocation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: document; space allocation
.. index:: usePowerOf2Sizes
.. _usePowerOf2Sizes:
.. collflag:: usePowerOf2Sizes

   .. versionchanged:: 2.6
      :collflag:`usePowerOf2Sizes` became the default allocation
      strategy for all new collections. Use
      :parameter:`newCollectionsUsePowerOf2Sizes` to use the *exact
      fit allocation strategy*.

   The :collflag:`usePowerOf2Sizes` flag changes the method that
   MongoDB uses to allocate space on disk for documents in this
   collection. By setting :collflag:`usePowerOf2Sizes`, you ensure
   that MongoDB will allocate space for documents in sizes that
   are powers of 2 (e.g. 32, 64, 128, 256, 512...16777216.) The
   smallest allocation for a document is 32 bytes.

   With :collflag:`usePowerOf2Sizes` MongoDB will be able to more
   effectively reuse space.

   With :collflag:`usePowerOf2Sizes` MongoDB, allocates
   records that have power of 2 sizes, until record sizes
   equal 4 megabytes. For records larger than 4 megabytes
   with :collflag:`usePowerOf2Sizes` set, :program:`mongod`
   will allocate records in full megabytes by rounding up to
   the nearest megabyte.

   Use :collflag:`usePowerOf2Sizes` for collections where applications
   insert and delete large numbers of documents to avoid storage
   fragmentation and ensure that MongoDB will effectively use space on
   disk.

TTL Collection Expiration Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. index:: expireAfterSeconds
.. collflag:: index

   The :collflag:`index` flag changes the expiration time of a
   :doc:`TTL Collection </tutorial/expire-data>`.

   Specify the key and new expiration time with a document of the form:

   .. code-block:: javascript

      {keyPattern: <index_spec>, expireAfterSeconds: <seconds> }

   In this example, ``<index_spec>`` is an existing index in the
   collection and ``seconds`` is the number of seconds to subtract
   from the current time.

   On success :dbcommand:`collMod` returns a document with fields
   ``expireAfterSeconds_old`` and ``expireAfterSeconds_new`` set to
   their respective values.

   On failure, :dbcommand:`collMod` returns a document with ``no
   expireAfterSeconds field to update`` if there is no existing
   ``expireAfterSeconds`` field or ``cannot find index { **key**:
   1.0 } for ns **namespace**`` if the specified ``keyPattern``
   does not exist.

Examples
--------

Enable Powers of Two Allocation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To enable :collflag:`usePowerOf2Sizes` on the collection
named ``products``, use the following operation:

.. code-block:: javascript

   db.runCommand( {collMod: "products", usePowerOf2Sizes : true })

To disable :collflag:`usePowerOf2Sizes` on the collection
``products``, use the following operation:

.. code-block:: javascript

   db.runCommand( { collMod: "products", usePowerOf2Sizes: false })

:collflag:`usePowerOf2Sizes` only affects subsequent
allocations caused by document insertion or record relocation
as a result of document growth, and *does not* affect
existing allocations.

Change Expiration Value for Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To update the expiration value for a collection
named ``sessions`` indexed on a ``lastAccess`` field from 30
minutes to 60 minutes, use the following operation:

.. code-block:: javascript

   db.runCommand({collMod: "sessions",
                  index: {keyPattern: {lastAccess:1},
                          expireAfterSeconds: 3600}})

Which will return the document:

.. code-block:: javascript

   { "expireAfterSeconds_old" : 1800, "expireAfterSeconds_new" : 3600, "ok" : 1 }
=========
collStats
=========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: collStats

   The :dbcommand:`collStats` command returns a variety of storage statistics
   for a given collection. Use the following syntax:

   .. code-block:: javascript

      { collStats: "collection" , scale : 1024 }

   Specify the ``collection`` you want statistics for, and
   use the ``scale`` argument to scale the output. The above example
   will display values in kilobytes.

   Examine the following example output, which uses the
   :method:`db.collection.stats()` helper in the :program:`mongo` shell.

   .. code-block:: javascript

      > db.users.stats()
      {
              "ns" : "app.users",             // namespace
              "count" : 9,                    // number of documents
              "size" : 432,                   // collection size in bytes
              "avgObjSize" : 48,              // average object size in bytes
              "storageSize" : 3840,           // (pre)allocated space for the collection in bytes
              "numExtents" : 1,               // number of extents (contiguously allocated chunks of datafile space)
              "nindexes" : 2,                 // number of indexes
              "lastExtentSize" : 3840,        // size of the most recently created extent in bytes
              "paddingFactor" : 1,            // padding can speed up updates if documents grow
              "flags" : 1,
              "totalIndexSize" : 16384,       // total index size in bytes
              "indexSizes" : {                // size of specific indexes in bytes
                      "_id_" : 8192,
                      "username" : 8192
              },
              "ok" : 1
      }

   .. note::

      The scale factor rounds values to whole numbers. This can
      produce unpredictable and unexpected results in some situations.

Output
------

.. data:: collStats.ns

   The namespace of the current collection, which follows the format
   ``[database].[collection]``.

.. data:: collStats.count

   The number of objects or documents in this collection.

.. data:: collStats.size

   The total size of all records in a collection. This value does not
   include the record header, which is 16 bytes per record, but *does*
   include the record's :term:`padding`. Additionally
   :data:`~collStats.size` does not include the size of any indexes
   associated with the collection, which the
   :data:`~collStats.totalIndexSize` field reports.

   The ``scale`` argument affects this value.

.. data:: collStats.avgObjSize

   The average size of an object in the collection. The ``scale``
   argument affects this value.

.. data:: collStats.storageSize

   The total amount of storage allocated to this collection for
   :term:`document` storage.  The ``scale`` argument affects this
   value. The :data:`~collStats.storageSize` does not decrease as you remove or
   shrink documents.

   .. sum of all extents (no indexes or the $freelist)
   .. include links to eventual documentation of storage management

.. data:: collStats.numExtents

   The total number of contiguously allocated data file regions.

.. data:: collStats.nindexes

   The number of indexes on the collection. All collections have at
   least one index on the :term:`_id` field.

   .. versionchanged:: 2.2
      Before 2.2, capped collections did not necessarily have an
      index on the ``_id`` field, and some capped collections created
      with pre-2.2 versions of :program:`mongod` may not have an
      ``_id`` index.


.. data:: collStats.lastExtentSize

   The size of the last extent allocated. The ``scale`` argument
   affects this value.

.. data:: collStats.paddingFactor

   The amount of space added to the end of each document at insert
   time. The document padding provides a small amount of extra space
   on disk to allow a document to grow slightly without needing to
   move the document. :program:`mongod` automatically calculates this
   padding factor

.. data:: collStats.flags

   .. versionchanged:: 2.2
      Removed in version 2.2 and replaced with the :data:`~collStats.userFlags`
      and :data:`~collStats.systemFlags` fields.

   Indicates the number of flags on the current collection. In version
   2.0, the only flag notes the existence of an :term:`index` on the
   :term:`_id` field.

.. data:: collStats.systemFlags

   .. versionadded:: 2.2

   Reports the flags on this collection that reflect internal server
   options. Typically this value is ``1`` and reflects the existence
   of an :term:`index` on the ``_id`` field.

.. data:: collStats.userFlags

   .. versionadded:: 2.2

   Reports the flags on this collection set by the user. In version
   2.2 the only user flag is :collflag:`usePowerOf2Sizes`.
   If :collflag:`usePowerOf2Sizes` is enabled, :data:`~collStats.userFlags` will
   be set to ``1``, otherwise :data:`~collStats.userFlags` will be ``0``.

   See the :dbcommand:`collMod` command for more information on setting user
   flags and :collflag:`usePowerOf2Sizes`.

.. data:: collStats.totalIndexSize

   The total size of all indexes. The ``scale`` argument affects this
   value.

.. data:: collStats.indexSizes

   This field specifies the key and size of every existing index on
   the collection. The ``scale`` argument affects this value.

Example
-------

The following is an example of :method:`db.collection.stats()` and
:dbcommand:`collStats` output:

.. code-block:: javascript

   {
        "ns" : "<database>.<collection>",
        "count" : <number>,
        "size" : <number>,
        "avgObjSize" : <number>,
        "storageSize" : <number>,
        "numExtents" : <number>,
        "nindexes" : <number>,
        "lastExtentSize" : <number>,
        "paddingFactor" : <number>,
        "systemFlags" : <bit>,
        "userFlags" : <bit>,
        "totalIndexSize" : <number>,
        "indexSizes" : {
                "_id_" : <number>,
                "a_1" : <number>
        },
        "ok" : 1
   }
.. _compact:

=======
compact
=======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: compact

   .. versionadded:: 2.0

   Rewrites and defragments all data in a collection, as well as all
   of the indexes on that collection. :dbcommand:`compact` has the
   following form:

   .. code-block:: javascript

      { compact: <collection name> }

   :dbcommand:`compact` has the following fields:

   .. include:: /reference/command/compact-field.rst

   :dbcommand:`compact` is similar to :dbcommand:`repairDatabase`;
   however, :dbcommand:`repairDatabase` operates on an entire
   database.

.. warning:: Always have an up-to-date backup before performing server
   maintenance such as the :dbcommand:`compact` operation.

.. _compact-paddingFactor:

paddingFactor
~~~~~~~~~~~~~

.. versionadded:: 2.2

The ``paddingFactor`` field takes the following range of values:

- Default: ``1.0``

- Minimum: ``1.0`` (no padding)

- Maximum: ``4.0``

If your updates increase the size of the documents, padding will
increase the amount of space allocated to each document and avoid
expensive document relocation operations within the data files.

You can calculate the padding size by subtracting the document size from
the record size or, in terms of the ``paddingFactor``, by subtracting
``1`` from the ``paddingFactor``:

.. code-block:: none

   padding size = (paddingFactor - 1) * <document size>.

For example, a ``paddingFactor`` of ``1.0`` specifies a padding size of
``0`` whereas a ``paddingFactor`` of ``1.2`` specifies a padding size of
``0.2`` or 20 percent (20%) of the document size.

With the following command, you can use the ``paddingFactor`` option of
the :dbcommand:`compact` command to set the record size to ``1.1`` of
the document size, or a padding factor of 10 percent (10%):

.. code-block:: javascript

   db.runCommand ( { compact: '<collection>', paddingFactor: 1.1 } )

:dbcommand:`compact` compacts existing documents but does not reset
``paddingFactor`` statistics for the collection. After the
:dbcommand:`compact` MongoDB will use the existing ``paddingFactor``
when allocating new records for documents in this collection.

.. _compact-paddingBytes:

paddingBytes
~~~~~~~~~~~~

.. versionadded:: 2.2

Specifying ``paddingBytes`` can be useful if your documents start small
but then increase in size significantly. For example, if your documents
are initially 40 bytes long and you grow them by 1KB, using
``paddingBytes: 1024`` might be reasonable since using ``paddingFactor:
4.0`` would specify a record size of 160 bytes (``4.0`` times the
initial document size), which would only provide a padding of 120 bytes
(i.e. record size of 160 bytes minus the document size).

With the following command, you can use the ``paddingBytes`` option of
the :dbcommand:`compact` command to set the padding size to ``100``
bytes on the collection named by ``<collection>``:

.. code-block:: javascript

   db.runCommand ( { compact: '<collection>', paddingBytes: 100 } )

Behaviors
---------

The :dbcommand:`compact` has the behaviors described here.

Blocking
~~~~~~~~

In MongoDB 2.2, :dbcommand:`compact` blocks activities only for its
database. Prior to 2.2, the command blocked all activities.

You may view the intermediate progress either by viewing the
:program:`mongod` log file or by running the :method:`db.currentOp()`
in another shell instance.

Operation Termination
~~~~~~~~~~~~~~~~~~~~~

If you terminate the operation with the :method:`db.killOp()
<db.killOp()>` method or restart the server before the
:dbcommand:`compact` operation has finished:

- If you have journaling enabled, the data remains valid and
  usable, regardless of the state of the :dbcommand:`compact` operation.
  You may have to manually rebuild the indexes.

- If you do not have journaling enabled and the :program:`mongod` or
  :dbcommand:`compact` terminates during the operation, it is impossible
  to guarantee that the data is in a valid state.

- In either case, much of the existing free space in the collection may
  become un-reusable. In this scenario, you should rerun the compaction
  to completion to restore the use of this free space.


Disk Space
~~~~~~~~~~

:dbcommand:`compact` generally uses less disk space than
:dbcommand:`repairDatabase` and is faster. However, the
:dbcommand:`compact` command is still slow and blocks other database
use. Only use :dbcommand:`compact` during scheduled maintenance periods.

:dbcommand:`compact` requires up to 2 gigabytes of additional disk
space while running. Unlike :dbcommand:`repairDatabase`,
:dbcommand:`compact` does *not* free space on the file system.

To see how the storage space changes for the collection, run the
:dbcommand:`collStats` command before and after compaction.


Size and Number of Data Files
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:dbcommand:`compact` may increase the total size and number of your data
files, especially when run for the first time. However, this will not
increase the total collection storage space since storage size is the
amount of data allocated within the database files, and not the
size/number of the files on the file system.

Replica Sets
~~~~~~~~~~~~

:dbcommand:`compact` commands do not replicate to secondaries in a
:term:`replica set`:

- Compact each member separately.

- Ideally run :dbcommand:`compact` on a secondary. See option
  ``force:true`` above for information regarding compacting the primary.

.. |cmd-name| replace:: :dbcommand:`compact`
.. include:: /includes/fact-command-puts-secondary-into-recovering.rst

Sharded Clusters
~~~~~~~~~~~~~~~~

:dbcommand:`compact` is a command issued to a :program:`mongod`. In a
sharded environment, run :dbcommand:`compact` on each shard separately
as a maintenance operation.

You cannot issue :dbcommand:`compact` against a :program:`mongos` instance.

Capped Collections
~~~~~~~~~~~~~~~~~~

It is not possible to compact :term:`capped collections <capped
collection>` because they don't have padding, and documents cannot grow
in these collections. However, the documents of a :term:`capped
collection` are not subject to fragmentation.
==================
configureFailPoint
==================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: configureFailPoint

   Configures a failure point that you can turn on and off while
   MongoDB runs. :dbcommand:`configureFailPoint` is an internal
   command for testing purposes that takes the following form:

   .. code-block:: javascript

      {configureFailPoint: "<failure_point>", mode: <behavior> }

   You must issue :dbcommand:`configureFailPoint` against the
   :term:`admin database`. :dbcommand:`configureFailPoint` has the
   following fields:

   .. include:: /reference/command/configureFailPoint-field.rst

.. |dbcommand| replace:: :dbcommand:`configureFailPoint`
.. include:: /includes/note-enabletestcommands.rst

Example
-------

.. code-block:: javascript

   db.adminCommand( { configureFailPoint: "blocking_thread", mode: {times: 21} } )
=============
connPoolStats
=============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: connPoolStats

   .. include:: /includes/note-conn-pool-stats.rst

   The command :dbcommand:`connPoolStats` returns information
   regarding the number of open connections to the current database
   instance, including client connections and server-to-server
   connections for replication and clustering. The command takes the
   following form:

   .. code-block:: javascript

      { connPoolStats: 1 }

   The value of the argument (i.e. ``1`` ) does not affect the
   output of the command.

   .. include:: /includes/note-conn-pool-stats.rst

Output
------

.. data:: connPoolStats.hosts

   The sub-documents of the :data:`~connPoolStats.hosts` :term:`document` report connections
   between the :program:`mongos` or :program:`mongod` instance and each component
   :program:`mongod` of the :term:`sharded cluster`.

   .. data:: connPoolStats.hosts.[host].available

      :data:`~connPoolStats.hosts.[host].available` reports the total number of
      connections that the :program:`mongos` or :program:`mongod`
      could use to connect to this :program:`mongod`.

   .. data:: connPoolStats.hosts.[host].created

      :data:`~connPoolStats.hosts.[host].created` reports the number of connections
      that this :program:`mongos` or :program:`mongod` has ever created for this host.

.. data:: connPoolStats.replicaSets

   :data:`~connPoolStats.replicaSets` is a :term:`document` that contains
   :term:`replica set` information for the :term:`sharded cluster`.

   .. data:: connPoolStats.replicaSets.shard

      The :data:`~connPoolStats.replicaSets.shard` :term:`document` reports
      on each :term:`shard` within the :term:`sharded cluster`

   .. data:: connPoolStats.replicaSets.[shard].host

      The :data:`~connPoolStats.replicaSets.[shard].host` field holds an array of
      :term:`document` that reports on each host within the
      :term:`shard` in the :term:`replica set`.

      These values derive from the :doc:`replica set status
      </reference/command/replSetGetStatus>` values.

      .. data:: connPoolStats.replicaSets.[shard].host[n].addr

         :data:`~connPoolStats.replicaSets.[shard].host[n].addr` reports the address
         for the host in the :term:`sharded cluster` in the format of
         "``[hostname]:[port]``".

      .. data:: connPoolStats.replicaSets.[shard].host[n].ok

         :data:`~connPoolStats.replicaSets.[shard].host[n].ok` reports ``false``
         when:

         - the :program:`mongos` or :program:`mongod` cannot connect
           to instance.

         - the :program:`mongos` or :program:`mongod` received a
           connection exception or error.

         This field is for internal use.

      .. data:: connPoolStats.replicaSets.[shard].host[n].ismaster

         :data:`~connPoolStats.replicaSets.[shard].host[n].ismaster` reports ``true``
         if this :data:`~connPoolStats.replicaSets.[shard].host` is the
         :term:`primary` member of the :term:`replica set`.

      .. data:: connPoolStats.replicaSets.[shard].host[n].hidden

         :data:`~connPoolStats.replicaSets.[shard].host[n].hidden` reports ``true``
         if this :data:`~connPoolStats.replicaSets.[shard].host` is a :term:`hidden
         member` of the :term:`replica set`.

      .. data:: connPoolStats.replicaSets.[shard].host[n].secondary

         :data:`~connPoolStats.replicaSets.[shard].host[n].secondary` reports
         ``true`` if this :data:`~connPoolStats.replicaSets.[shard].host` is a
         :term:`secondary` member of the :term:`replica set`.

      .. data:: connPoolStats.replicaSets.[shard].host[n].pingTimeMillis

         :data:`~connPoolStats.replicaSets.[shard].host[n].pingTimeMillis` reports
         the ping time in milliseconds from the :program:`mongos` or
         :program:`mongod` to this :data:`~connPoolStats.replicaSets.[shard].host`.

      .. data:: connPoolStats.replicaSets.[shard].host[n].tags

         .. versionadded:: 2.2

         :data:`~connPoolStats.replicaSets.[shard].host[n].tags` reports the
         :data:`~local.system.replset.members[n].tags`, if this member of the set has tags
         configured.

   .. data:: connPoolStats.replicaSets.[shard].master

      :data:`~connPoolStats.replicaSets.[shard].master` reports the ordinal identifier
      of the host in the :data:`~connPoolStats.replicaSets.[shard].host` array that is
      the :term:`primary` of the :term:`replica set`.

   .. data:: connPoolStats.replicaSets.[shard].nextSlave

      .. deprecated:: 2.2

      :data:`~connPoolStats.replicaSets.[shard].nextSlave` reports the
      :term:`secondary` member that the :program:`mongos` will use to
      service the next request for this :term:`replica set`.

.. data:: connPoolStats.createdByType

   :data:`~connPoolStats.createdByType` :term:`document` reports the number of each
   type of connection that :program:`mongos` or :program:`mongod` has created in all
   connection pools.

   :program:`mongos` connect to :program:`mongod` instances using one
   of three types of connections. The following sub-document reports
   the total number of connections by type.

   .. data:: connPoolStats.createdByType.master

      :data:`~connPoolStats.createdByType.master` reports the total number of
      connections to the :term:`primary` member in each :term:`cluster`.

   .. data:: connPoolStats.createdByType.set

      :data:`~connPoolStats.createdByType.set` reports the total number of
      connections to a :term:`replica set` member.

   .. data:: connPoolStats.createdByType.sync

      :data:`~connPoolStats.createdByType.sync` reports the total number of
      :term:`config database` connections.

.. data:: connPoolStats.totalAvailable

   :data:`~connPoolStats.totalAvailable` reports the running total of connections
   from the :program:`mongos` or :program:`mongod` to all :program:`mongod` instances in
   the :term:`sharded cluster` available for use.

.. data:: connPoolStats.totalCreated

   :data:`~connPoolStats.totalCreated` reports the total number of connections ever
   created from the :program:`mongos` or :program:`mongod` to all :program:`mongod`
   instances in the :term:`sharded cluster`.

.. data:: connPoolStats.numDBClientConnection

   :data:`~connPoolStats.numDBClientConnection` reports the total number of
   connections from the :program:`mongos` or :program:`mongod` to all of the :program:`mongod`
   instances in the :term:`sharded cluster`.

.. data:: connPoolStats.numAScopedConnection

   :data:`~connPoolStats.numAScopedConnection` reports the number of exception safe
   connections created from :program:`mongos` or :program:`mongod` to all :program:`mongod`
   in the :term:`sharded cluster`. The :program:`mongos` or :program:`mongod` releases these
   connections after receiving a socket exception from the
   :program:`mongod`.
============
connPoolSync
============

.. default-domain:: mongodb

.. dbcommand:: connPoolSync

   :dbcommand:`connPoolSync` is an internal command.

   .. slave-ok
===============
convertToCapped
===============

.. default-domain:: mongodb

.. dbcommand:: convertToCapped

   The :dbcommand:`convertToCapped` command converts an existing,
   non-capped collection to a :term:`capped collection` within the same
   database.

   The command has the following syntax:

   .. code-block:: javascript

      {convertToCapped: <collection>, size: <capped size> }

   :dbcommand:`convertToCapped` takes an existing collection
   (``<collection>``) and transforms it into a capped collection with
   a maximum size in bytes, specified to the ``size`` argument
   (``<capped size>``).

   During the conversion process, the :dbcommand:`convertToCapped`
   command exhibit the following behavior:

   - MongoDB transverses the documents in the original collection in
     :term:`natural order` and loads the documents into a new
     capped collection.

   - If the ``capped size`` specified for the capped collection is
     smaller than the size of the original uncapped collection, then
     MongoDB will overwrite documents in the capped collection based
     on insertion order, or *first in, first out* order.

   - Internally, to convert the collection, MongoDB uses the following
     procedure

     - :dbcommand:`cloneCollectionAsCapped` command creates the capped
       collection and imports the data.

     - MongoDB drops the original collection.

     - :dbcommand:`renameCollection` renames the new capped collection
       to the name of the original collection.

   .. note::

      MongoDB does not support the :dbcommand:`convertToCapped`
      command in a sharded cluster.

   .. warning::

      The :dbcommand:`convertToCapped` will not recreate indexes from
      the original collection on the new collection, other than the
      index on the ``_id`` field. If you need indexes on this
      collection you will need to create these indexes after the
      conversion is complete.

   .. seealso:: :dbcommand:`create`

   .. include:: /includes/warning-blocking-global.rst
======
copydb
======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: copydb

   Copies a database from a remote host to the current host or copies a
   database to another database within the current host. Run
   :dbcommand:`copydb` in the ``admin`` database of the destination
   server with the following syntax:

   .. code-block:: javascript

      { copydb: 1,
        fromhost: <hostname>,
        fromdb: <database>,
        todb: <database>,
        slaveOk: <bool>,
        username: <username>,
        nonce: <nonce>,
        key: <key> }

   :dbcommand:`copydb` accepts the following options:

   .. include:: /reference/command/copydb-fields.rst

   The :program:`mongo` shell provides the :method:`db.copyDatabase()`
   wrapper for the :dbcommand:`copydb` command.

Behavior
--------

.. |copydb| replace:: :dbcommand:`copydb`

.. include:: /includes/fact-copydb-behavior.rst

Required Access
---------------

.. versionchanged:: 2.6

On systems running with :setting:`~security.authentication`, the :dbcommand:`copydb`
command requires the following authorization on the target and source
databases.

Source Database (``fromdb``)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Source is non-``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. source-not-admin
   :end-before: .. source-admin

Source is ``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. source-admin

Source Database is on a Remote Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If copying from a remote server and the remote server has
authentication enabled, you must authenticate to the remote host as a
user with the proper authorization. See
:ref:`copydb-authentication`.

Target Database (``todb``)
~~~~~~~~~~~~~~~~~~~~~~~~~~

Copy from non-``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.. include:: /includes/access-copydb.rst
   :start-after: .. target-non-admin-source
   :end-before: .. target-admin-source

Copy from ``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. target-admin-source
   :end-before: .. source-not-admin

.. _copydb-authentication:

Authentication
--------------

If copying from a remote server and the remote server has
authentication enabled, then you must include a ``username``,
``nonce``, and ``key``.

The ``nonce`` is a one-time password that you request from the remote
server using the :dbcommand:`copydbgetnonce` command, as in the following:

.. code-block:: javascript

   use admin
   mynonce = db.runCommand( { copydbgetnonce : 1, fromhost: <hostname> } ).nonce

If running the :dbcommand:`copydbgetnonce` command directly on the
remote host, you can omit the ``fromhost`` field in the
:dbcommand:`copydbgetnonce` command.

The ``key`` is a hash generated as follows:

.. code-block:: javascript

   hex_md5(mynonce + username + hex_md5(username + ":mongo:" + password))

Replica Sets
------------

With :term:`read preference` configured to set the ``slaveOk`` option
to ``true``, you may run :dbcommand:`copydb` on a :term:`secondary`
member of a :term:`replica set`.

Sharded Clusters
----------------

- Do not use :dbcommand:`copydb` from a :program:`mongos` instance.

- Do not use :dbcommand:`copydb` to copy databases that contain sharded
  collections.

Examples
--------

Copy on the Same Host
~~~~~~~~~~~~~~~~~~~~~

To copy from the same host, omit the ``fromhost`` field.

The following command copies the ``test`` database to a new ``records``
database on the current :program:`mongod` instance:

.. code-block:: javascript

   use admin
   db.runCommand({
      copydb: 1,
      fromdb: "test",
      todb: "records"
   })

Copy from a Remote Host to the Current Host
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To copy from a remote host, include the ``fromhost`` field.

The following command copies the ``test`` database from the remote host
``example.net`` to a new ``records`` database on the current
:program:`mongod` instance:

.. code-block:: javascript

   use admin
   db.runCommand({
      copydb: 1,
      fromdb: "test",
      todb: "records",
      fromhost: "example.net"
   })

Copy Databases from Remote ``mongod`` Instances that Enforce Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To copy from a remote host that enforces authentication, include the
``fromhost``, ``username``, ``nonce`` and ``key`` fields.

The following command copies the ``test`` database from a remote host
``example.net`` that runs with :setting:`~security.authentication` to a new ``records``
database on the local :program:`mongod` instance. Because the
``example.net`` has :setting:`~security.authentication` enabled, the command includes the
``username``, ``nonce`` and ``key`` fields:

.. code-block:: javascript

   use admin
   db.runCommand({
      copydb: 1,
      fromdb: "test",
      todb: "records",
      fromhost: "example.net",
      username: "reportingAdmin",
      nonce: "<nonce>",
      key: "<passwordhash>"
   })

.. seealso::

   - :method:`db.copyDatabase()`

   - :dbcommand:`clone` and :method:`db.cloneDatabase()`

   - :doc:`/core/backups` and :doc:`/core/import-export`
==============
copydbgetnonce
==============

.. default-domain:: mongodb

.. dbcommand:: copydbgetnonce

   Client libraries use :dbcommand:`copydbgetnonce` to get a one-time
   password for use with the :dbcommand:`copydb` command.

   .. note::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed; however, the
      write lock for this operation is short lived.

   .. write-lock, admin-only
=====
count
=====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: count

   Counts the number of documents in a collection. Returns a document
   that contains this count and as well as the command status.
   :dbcommand:`count` has the following form:

   .. versionchanged:: 2.6
      :dbcommand:`count` now accepts the ``hint`` option to specify an
      index.

   .. code-block:: javascript

      { count: <collection>, query: <query>, limit: <limit>, skip: <skip>, hint: <hint> }

   :dbcommand:`count` has the following fields:

   .. include:: /reference/command/count-field.rst

   MongoDB also provides the :method:`~cursor.count()` and
   :method:`db.collection.count()` wrapper methods in the
   :program:`mongo` shell.

Examples
--------

The following sections provide examples of the :dbcommand:`count`
command.

Count All Documents
~~~~~~~~~~~~~~~~~~~

The following operation counts the number of all documents in the
``orders`` collection:

.. code-block:: javascript

   db.runCommand( { count: 'orders' } )

In the result, the ``n``, which represents the count, is ``26``,
and the command status ``ok`` is ``1``:

.. code-block:: javascript

   { "n" : 26, "ok" : 1 }

Count Documents That Match a Query
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns a count of the documents in the
``orders`` collection where the value of the ``ord_dt`` field is
greater than ``Date('01/01/2012')``:

.. code-block:: javascript

   db.runCommand( { count:'orders',
                    query: { ord_dt: { $gt: new Date('01/01/2012') } }
                  } )

In the result, the ``n``, which represents the count, is ``13``
and the command status ``ok`` is ``1``:

.. code-block:: javascript

   { "n" : 13, "ok" : 1 }

Skip Documents in Count
~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns a count of the documents in the
``orders`` collection where the value of the ``ord_dt`` field is
greater than ``Date('01/01/2012')`` and skip the first ``10`` matching
documents:

.. code-block:: javascript

   db.runCommand( { count:'orders',
                    query: { ord_dt: { $gt: new Date('01/01/2012') } },
                    skip: 10 }  )

In the result, the ``n``, which represents the count, is ``3`` and
the command status ``ok`` is ``1``:

.. code-block:: javascript

   { "n" : 3, "ok" : 1 }

Specify the Index to Use
~~~~~~~~~~~~~~~~~~~~~~~~

The following operation uses the index ``{ status: 1 }`` to return a
count of the documents in the ``orders`` collection where the value of
the ``ord_dt`` field is greater than ``Date('01/01/2012')`` and the
``status`` field is equal to ``"D"``:

.. code-block:: javascript

   db.runCommand(
      {
        count:'orders',
        query: {
                 ord_dt: { $gt: new Date('01/01/2012') },
                 status: "D"
               },
        hint: { status: 1 }
      }
   )

In the result, the ``n``, which represents the count, is ``1`` and
the command status ``ok`` is ``1``:

.. code-block:: javascript

   { "n" : 1, "ok" : 1 }======
create
======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: create

   Explicitly creates a collection. :dbcommand:`create` has the
   following form:

   .. code-block:: javascript

      { create: <collection_name>,
        capped: <true|false>,
        autoIndexId: <true|false>,
        size: <max_size>,
        max: <max_documents>,
        flags: <0|1>
      }

   :dbcommand:`create` has the following fields:

   .. include:: /reference/command/create-field.rst

   For more information on the ``autoIndexId`` field in versions before
   2.2, see :ref:`2.2-id-indexes-capped-collections`.

   The :method:`db.createCollection()` method wraps the
   :dbcommand:`create` command.

Considerations
--------------

The :dbcommand:`create` command obtains a write lock on the
affected database and will block other operations until it has
completed. The write lock for this operation is typically short
lived. However, allocations for large capped collections may take
longer.

Example
-------

To create a :term:`capped collection` limited to 64 kilobytes, issue
the command in the following form:

.. code-block:: javascript

   db.runCommand( { create: "collection", capped: true, size: 64 * 1024 } )
=============
createIndexes
=============

.. default-domain:: mongodb

.. versionadded:: 2.6

Definition
----------

.. dbcommand:: createIndexes

   Builds one or more indexes on a collection. The
   :dbcommand:`createIndexes` command takes the following form:

   .. code-block:: javascript

      db.runCommand(
        {
          createIndexes: <collection>,
          indexes: [
              {
                  key: {
                      <key-value_pair>,
                      <key-value_pair>,
                      ...
                  },
                  name: <index_name>,
                  <option1>,
                  <option2>,
                  ...
              },
              { ... },
              { ... }
          ]
        }
      )

   The :dbcommand:`createIndexes` command takes the following fields:

   .. include:: /reference/command/createIndexes-field.rst

   Each document in the ``indexes`` array can take the following fields:

   .. include:: /reference/command/createIndexes-indexes-field.rst

Considerations
--------------

An index name, including the :term:`namespace`, cannot be longer than
the :ref:`Index Name Length <limit-index-name-length>` limit.

Behavior
--------

Non-background indexing operations block all other operations on a
database. If you specify multiple indexes to
:dbcommand:`createIndexes`, MongoDB builds the indexes serially.

If you create an index with one set of options and then issue
:dbcommand:`createIndexes` with the same index fields but different options,
MongoDB will not change the options nor rebuild the index. To
change index options, drop the existing index with
:method:`db.collection.dropIndex()` before running the new
:dbcommand:`createIndexes` with the new options.

Example
-------

The following command builds two indexes on the ``inventory`` collection of
the ``products`` database:

.. code-block:: javascript

   db.getSiblingDB("products").runCommand(
     {
       createIndexes: "inventory",
       indexes: [
           {
               key: {
                   item: 1,
                   manufacturer: 1,
                   model: 1
               },
               name: "item_manufacturer_model",
               unique: true
           },
           {
               key: {
                   item: 1,
                   supplier: 1,
                   model: 1
               },
               name: "item_supplier_model",
               unique: true
           }
       ]
     }
   )

When the indexes successfully finish building, MongoDB returns a results
document that includes a status of ``"ok" : 1``.

Output
------

The :dbcommand:`createIndexes` command returns a document that indicates
the success of the operation. The document contains some but not all of
the following fields, depending on outcome:

.. data:: createIndexes.createdCollectionAutomatically

   If ``true``, then the collection didn't exist and was created in the
   process of creating the index.

.. data:: createIndexes.numIndexesBefore

   The number of indexes at the start of the command.

.. data:: createIndexes.numIndexesAfter

   The number of indexes at the end of the command.

.. data:: createIndexes.ok

   A value of ``1`` indicates the indexes are in place. A value of
   ``0`` indicates an error.

.. data:: createIndexes.note

   This ``note`` is returned if an existing index or indexes already
   exist. This indicates that the index was not created or changed.

.. data:: createIndexes.errmsg

   Returns information about any errors.

.. data:: createIndexes.code

   The error code representing the type of error.
==========
createRole
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: createRole

   Creates a role and specifies its :ref:`privileges <privileges>`.
   The role applies to the
   database on which you run the command. The :dbcommand:`createRole`
   command returns a *duplicate role* error if the role already exists in
   the database.

   The :dbcommand:`createRole` command uses the following syntax:

   .. code-block:: javascript

      { createRole: "<new role>",
        privileges: [
          { resource: { <resource> }, actions: [ "<action>", ... ] },
          ...
        ],
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: <write concern document>
      }

   The :dbcommand:`createRole` command has the following fields:

   .. include:: /reference/command/createRole-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`createRole`
   .. include:: /includes/fact-roles-array-contents.rst

Behavior
--------

A role's privileges apply to the database where the role is created. The
role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>` and can inherit
privileges from roles in other databases.

Required Access
---------------

.. include:: /includes/access-create-role.rst

Example
-------

The following :dbcommand:`createRole` command creates the
``myClusterwideAdmin`` role on the ``admin`` database:

.. code-block:: javascript

   use admin
   db.runCommand({ createRole: "myClusterwideAdmin",
     privileges: [
       { resource: { cluster: true }, actions: [ "addShard" ] },
       { resource: { db: "config", collection: "" }, actions: [ "find", "update", "insert", "remove" ] },
       { resource: { db: "users", collection: "usersCollection" }, actions: [ "update", "insert", "remove" ] },
       { resource: { db: "", collection: "" }, actions: [ "find" ] }
     ],
     roles: [
       { role: "read", db: "admin" }
     ],
     writeConcern: { w: "majority" , wtimeout: 5000 }
   })
==========
createUser
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: createUser

   .. |local-cmd-name| replace:: :dbcommand:`createUser`

   Creates a new user on the database where you run the command. The
   :dbcommand:`createUser` command returns a *duplicate user* error if the
   user exists.
   The :dbcommand:`createUser` command uses the following syntax:

   .. code-block:: javascript

      { createUser: "<name>",
        pwd: "<cleartext password>",
        customData: { <any information> },
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: { <write concern> }
      }

   :dbcommand:`createUser` has the following fields:

   .. include:: /reference/command/createUser-field.rst

   .. include:: /includes/fact-roles-array-contents.rst

.. TODO rename section (or make it subsection or something)

Behavior
--------

:dbcommand:`createUser` sends password to the MongoDB instance in
cleartext. To encrypt the password in transit, use :doc:`SSL
</tutorial/configure-ssl>`.

Users created on the ``$external`` database should have credentials
stored externally to MongoDB, as, for example, with :doc:`MongoDB
Enterprise installations that use Kerberos
</tutorial/control-access-to-mongodb-with-kerberos-authentication>`.

.. _createUser-required-access:

Required Access
---------------

.. include:: /includes/access-create-user.rst

Example
-------

The following :dbcommand:`createUser` command creates a user ``accountAdmin01`` on the
``products`` database. The command gives ``accountAdmin01`` the
``clusterAdmin`` and ``readAnyDatabase`` roles on the ``admin`` database
and the ``readWrite`` role on the ``products`` database:

.. code-block:: javascript

   db.getSiblingDB("products").runCommand( { createUser: "accountAdmin01",
                                             pwd: "cleartext password",
                                             customData: { employeeId: 12345 },
                                             roles: [
                                                      { role: "clusterAdmin", db: "admin" },
                                                      { role: "readAnyDatabase", db: "admin" },
                                                      "readWrite"
                                                    ],
                                             writeConcern: { w: "majority" , wtimeout: 5000 }
                                          } )
==========
cursorInfo
==========

.. default-domain:: mongodb

.. deprecated:: 2.6

.. dbcommand:: cursorInfo

   The :dbcommand:`cursorInfo` command returns information about current cursor
   allotment and use. Use the following form:

   .. code-block:: javascript

      { cursorInfo: 1 }

   The value (e.g. ``1`` above) does not affect the output of the
   command.

   :dbcommand:`cursorInfo` returns the total number of open cursors
   (``totalOpen``), the size of client cursors in current use
   (``clientCursors_size``), and the number of timed out cursors
   since the last server restart (``timedOut``).
========
dataSize
========

.. default-domain:: mongodb

.. dbcommand:: dataSize

   The :dbcommand:`dataSize` command returns the data size for a set
   of data within a certain range:

   .. code-block:: javascript

        { dataSize: "database.collection", keyPattern: { field: 1 }, min: { field: 10 }, max: { field: 100 } }

   This will return a document that contains the size of all matching
   documents. Replace ``database.collection`` value with database
   and collection from your deployment. The ``keyPattern``, ``min``,
   and ``max`` parameters are options.

   The amount of time required to return :dbcommand:`dataSize` depends on the
   amount of data in the collection.

   .. read-lock
======
dbHash
======

.. default-domain:: mongodb

.. dbcommand:: dbHash

   :dbcommand:`dbHash` is a command that supports :term:`config
   servers <config server>` and is not part of the stable client
   facing API.

   .. slave-ok, read-lock
=======
dbStats
=======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: dbStats

   The :dbcommand:`dbStats` command returns storage statistics for a
   given database. The command takes the following syntax:

   .. code-block:: javascript

      { dbStats: 1, scale: 1 }

   The values of the options above do not affect the output of the
   command. The ``scale`` option allows you to specify how to scale byte
   values. For example, a ``scale`` value of ``1024`` will display the
   results in kilobytes rather than in bytes:

   .. code-block:: javascript

      { dbStats: 1, scale: 1024 }

   .. note::

      Because scaling rounds values to whole numbers, scaling may return
      unlikely or unexpected results.

   The time required to run the command depends on the total size of the
   database. Because the command must touch all data files, the command
   may take several seconds to run.

   In the :program:`mongo` shell, the :method:`db.stats()` function
   provides a wrapper around :dbcommand:`dbStats`.

Output
------

.. data:: dbStats.db

   Contains the name of the database.

.. data:: dbStats.collections

   Contains a count of the number of collections in that database.

.. data:: dbStats.objects

   Contains a count of the number of objects (i.e. :term:`documents <document>`) in
   the database across all collections.

.. data:: dbStats.avgObjSize

   The average size of each document in bytes. This is the
   :data:`~dbStats.dataSize` divided by the number of documents.

.. data:: dbStats.dataSize

   The total size in bytes of the data held in this database including the
   :term:`padding factor`. The ``scale`` argument affects this
   value. The :data:`~dbStats.dataSize` will not decrease when :term:`documents
   <document>` shrink, but will decrease when you remove documents.

   .. sum of all records not counting deleted records

.. data:: dbStats.storageSize

   The total amount of space in bytes allocated to collections in this database
   for :term:`document` storage. The ``scale`` argument affects this
   value. The :data:`~dbStats.storageSize` does not decrease as you remove or
   shrink documents.

   .. sum of all extents (no indexes or the $freelist)
   .. include links to eventual documentation of storage management

.. data:: dbStats.numExtents

   Contains a count of the number of extents in the database across
   all collections.

.. data:: dbStats.indexes

   Contains a count of the total number of indexes across all
   collections in the database.

.. data:: dbStats.indexSize

   The total size in bytes of all indexes created on this database. The
   ``scale`` arguments affects this value.

   .. uses the dataSize member

.. data:: dbStats.fileSize

   The total size in bytes of the data files that hold the database. This value
   includes preallocated space and the :term:`padding factor`. The
   value of :data:`~dbStats.fileSize` only reflects the size of the data files
   for the database and not the namespace file.

   The ``scale`` argument affects this value.

.. data:: dbStats.nsSizeMB

   The total size of the :term:`namespace` files (i.e. that end with
   ``.ns``) for this database. You cannot change the size of the
   namespace file after creating a database, but you can change the
   default size for all new namespace files with the
   :setting:`nssize` runtime option.

   .. seealso:: The :setting:`nssize` option, and :ref:`Maximum Namespace File Size <limit-size-of-namespace-file>`

.. data:: dbStats.dataFileVersion

   .. versionadded:: 2.4

   Document that contains information about the on-disk format of the data
   files for the database.

.. data:: dbStats.dataFileVersion.major

   .. versionadded:: 2.4

   The major version number for the on-disk format of the data files for
   the database.

.. data:: dbStats.dataFileVersion.minor

   .. versionadded:: 2.4

   The minor version number for the on-disk format of the data files for
   the database.
======
delete
======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: delete

   .. versionadded:: 2.6

   The :dbcommand:`delete` command removes documents from a collection.
   A single :dbcommand:`delete` command can contain multiple delete
   specifications. The command cannot operate on :doc:`capped
   collections </core/capped-collections>`. The remove methods provided
   by the MongoDB drivers use this command internally.

   The :dbcommand:`delete` command has the following syntax:

   .. code-block:: javascript

      {
         delete: <collection>,
         deletes: [
            { q : <query>, limit : <integer> },
            { q : <query>, limit : <integer> },
            { q : <query>, limit : <integer> },
            ...
         ],
         ordered: <boolean>,
         writeConcern: { <write concern> }
      }

   The command takes the following fields:

   .. include:: delete-field.rst

   Each element of the ``deletes`` array contains the following sub-fields:

   .. include:: delete-field-deletes.rst

   :return:

      A document that contains the status of the operation.
      See :ref:`delete-command-output` for details.

Behavior
--------

The total size of all the queries (i.e. the ``q`` field values) in the
``deletes`` array must be less than or equal to the :limit:`maximum
BSON document size <BSON Document Size>`.

The total number of delete documents in the ``deletes`` array must be
less than or equal to the :limit:`maximum bulk size
<Bulk Operation Size>`.

Examples
--------

Limit the Number of Documents Deleted
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example deletes from the ``orders`` collection one
document that has the ``status`` equal to ``D`` by specifying the
``limit`` of ``1``:

.. code-block:: javascript

   db.runCommand(
      {
         delete: "orders",
         deletes: [ { q: { status: "D" }, limit: 1 } ]
      }
   )

The returned document shows that the command deleted ``1`` document.
See :ref:`delete-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 1 }

Delete All Documents That Match a Condition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example deletes from the ``orders`` collection all
documents that have the ``status`` equal to ``D`` by specifying the
``limit`` of ``0``:

.. code-block:: javascript

   db.runCommand(
      {
         delete: "orders",
         deletes: [ { q: { status: "D" }, limit: 0 } ],
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

The returned document shows that the command found and deleted ``13``
documents. See :ref:`delete-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 13 }

Delete All Documents from a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Delete all documents in the ``orders`` collection by specifying an
empty query condition *and* a ``limit`` of ``0``:

.. code-block:: javascript

   db.runCommand(
      {
         delete: "orders",
         deletes: [ { q: { }, limit: 0 } ],
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

The returned document shows that the command found and deleted ``35``
documents in total. See :ref:`delete-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 35 }

Bulk Delete
~~~~~~~~~~~

The following example performs multiple delete operations on the
``orders`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         delete: "orders",
         deletes: [
            { q: { status: "D" }, limit: 0 },
            { q: { cust_num: 99999, item: "abc123", status: "A" }, limit: 1 }
         ],
         ordered: false,
         writeConcern: { w: 1 }
      }
   )

The returned document shows that the command found and deleted ``21``
documents in total for the two delete statements. See
:ref:`delete-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 21 }

.. _delete-command-output:

Output
------

The returned document contains a subset of the following fields:

.. data:: delete.ok

   The status of the command.

.. data:: delete.n

   The number of documents deleted.

.. data:: delete.writeErrors

   An array of documents that contains information regarding any error
   encountered during the delete operation. The
   :data:`~delete.writeErrors` array contains an error document for
   each delete statement that errors.

   Each error document contains the following information:

   .. data:: delete.writeErrors.index

      An integer that identifies the delete statement in the
      ``deletes`` array, which uses a zero-based index.

   .. data:: delete.writeErrors.code

      An integer value identifying the error.

   .. data:: delete.writeErrors.errmsg

      A description of the error.

.. data:: delete.writeConcernError

   Document that describe error related to write concern and contains
   the field:

   .. data:: delete.writeConcernError.code

      An integer value identifying the cause of the write concern error.

   .. data:: delete.writeConcernError.errmsg

      A description of the cause of the write concern error.

The following is an example document returned for a successful
:dbcommand:`delete` command:

.. code-block:: javascript

   { ok: 1, n: 1 }

The following is an example document returned for a :dbcommand:`delete`
command that encountered an error:

.. code-block:: javascript

   {
      "ok" : 1,
      "n" : 0,
      "writeErrors" : [
         {
            "index" : 0,
            "code" : 10101,
            "errmsg" : "can't remove from a capped collection: test.cappedLog"
         }
      ]
   }
===========
diagLogging
===========

.. default-domain:: mongodb

.. dbcommand:: diagLogging

   :dbcommand:`diagLogging` is a command that captures additional data
   for diagnostic purposes and is not part of the stable client
   facing API.

:dbcommand:`diaglogging`  obtains a write lock on the affected database and
will block other operations until it completes.

   .. write-lock, slave-ok,
========
distinct
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: distinct

   Finds the distinct values for a specified field across a single
   collection. :dbcommand:`distinct` returns a document that contains
   an array of the distinct values. The return document also contains
   a subdocument with query statistics and the query plan.

   When possible, the :dbcommand:`distinct` command uses an index to
   find documents and return values.

   The command takes the following form:

   .. code-block:: javascript

      { distinct: "<collection>", key: "<field>", query: <query> }

   The command contains the following fields:

   .. include:: /reference/command/distinct-field.rst

Examples
--------

Return an array of the distinct values of the field ``ord_dt`` from all
documents in the ``orders`` collection:

.. code-block:: javascript

   db.runCommand ( { distinct: "orders", key: "ord_dt" } )

Return an array of the distinct values of the field ``sku`` in the
subdocument ``item`` from all documents in the ``orders`` collection:

.. code-block:: javascript

   db.runCommand ( { distinct: "orders", key: "item.sku" } )

Return an array of the distinct values of the field ``ord_dt`` from the
documents in the ``orders`` collection where the ``price`` is greater
than ``10``:

.. code-block:: javascript

   db.runCommand ( { distinct: "orders",
                     key: "ord_dt",
                     query: { price: { $gt: 10 } }
                   } )

.. note::

   MongoDB also provides the shell wrapper method
   :method:`db.collection.distinct()` for the :dbcommand:`distinct`
   command. Additionally, many MongoDB :term:`drivers <driver>` also
   provide a wrapper method. Refer to the specific driver
   documentation.
=============
driverOIDTest
=============

.. default-domain:: mongodb

.. dbcommand:: driverOIDTest

   :dbcommand:`driverOIDTest` is an internal command.

   .. slave-ok
====
drop
====

.. default-domain:: mongodb

.. dbcommand:: drop

   The :dbcommand:`drop` command removes an entire collection from a
   database. The command has following syntax:

   .. code-block:: javascript

        { drop: <collection_name> }

   The :program:`mongo` shell provides the equivalent helper method
   :method:`db.collection.drop()`.

   This command also removes any indexes associated with the dropped
   collection.

   .. warning::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed.
========================
dropAllRolesFromDatabase
========================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: dropAllRolesFromDatabase

   Deletes all :ref:`user-defined <user-defined-roles>` roles
   on the database where you run the command.

   .. warning::

      The :dbcommand:`dropAllRolesFromDatabase` removes *all*
      :ref:`user-defined <user-defined-roles>` roles from the database.

   The :dbcommand:`dropAllRolesFromDatabase` command takes the following
   form:

   .. code-block:: javascript

      {
        dropAllRolesFromDatabase: 1,
        writeConcern: { <write concern> }
      }

   The command has the following fields:

   .. include:: /reference/command/dropAllRolesFromDatabase-field.rst

Required Access
---------------

.. include:: /includes/access-drop-role.rst

Example
-------

The following operations drop all :ref:`user-defined
<user-defined-roles>` roles from the ``products`` database:

.. code-block:: javascript

   use products
   db.runCommand(
      {
        dropAllRolesFromDatabase: 1,
        writeConcern: { w: "majority" }
      }
   )

The ``n`` field in the results document reports the number of roles
dropped:

.. code-block:: javascript

   { "n" : 4, "ok" : 1 }
========================
dropAllUsersFromDatabase
========================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: dropAllUsersFromDatabase

   Removes all users from the database on which you run the
   command.

   .. warning::

      The :dbcommand:`dropAllUsersFromDatabase` removes all users from the database.

   The :dbcommand:`dropAllUsersFromDatabase` command has the following
   syntax:

   .. code-block:: javascript

      { dropAllUsersFromDatabase: 1,
        writeConcern: { <write concern> }
      }

   The :dbcommand:`dropAllUsersFromDatabase` document has the following
   fields:

   .. include:: /reference/command/dropAllUsersFromDatabase-field.rst

Required Access
---------------

.. |local-cmd-name| replace:: :command:`dropAllUsersFromDatabase`

.. include:: /includes/access-drop-user.rst

Example
-------

The following sequence of operations in the :program:`mongo` shell drops
every user from the ``products`` database:

.. code-block:: javascript

   use products
   db.runCommand( { dropAllUsersFromDatabase: 1, writeConcern: { w: "majority" } } )

The ``n`` field in the results document shows the number of users
removed:

.. code-block:: javascript

   { "n" : 12, "ok" : 1 }
============
dropDatabase
============

.. default-domain:: mongodb

.. dbcommand:: dropDatabase

   The :dbcommand:`dropDatabase` command drops a database, deleting
   the associated data files. :dbcommand:`dropDatabase` operates on the
   current database.

   In the shell issue the ``use <database>``
   command, replacing ``<database>`` with the name of the database
   you wish to delete. Then use the following command form:

   .. code-block:: javascript

      { dropDatabase: 1 }

   The :program:`mongo` shell also provides the following equivalent helper method:

   .. code-block:: javascript

      db.dropDatabase();

   .. include:: /includes/warning-blocking-global.rst

   .. write-lock
===========
dropIndexes
===========

.. default-domain:: mongodb

.. dbcommand:: dropIndexes

   The :dbcommand:`dropIndexes` command drops one or all indexes from
   the current collection.
   To drop all indexes, issue the command like so:

   .. code-block:: javascript

      { dropIndexes: "collection", index: "*" }

   To drop a single, issue the command by specifying the name
   of the index you want to drop. For example, to drop the index
   named ``age_1``, use the following command:

   .. code-block:: javascript

      { dropIndexes: "collection", index: "age_1" }

   The shell provides a useful command helper. Here's the equivalent command:

   .. code-block:: javascript

      db.collection.dropIndex("age_1");

   .. warning::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed.
========
dropRole
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: dropRole

   Deletes a :ref:`user-defined <user-defined-roles>` role from the
   database on which you run the command.

   The :dbcommand:`dropRole` command uses the following syntax:

   .. code-block:: javascript

      {
        dropRole: "<role>",
        writeConcern: { <write concern> }
      }

   The :dbcommand:`dropRole` command has the following fields:

   .. include:: /reference/command/dropRole-field.rst

Required Access
---------------

.. include:: /includes/access-drop-role.rst

Example
-------

The following operations remove the ``readPrices`` role from the
``products`` database:

.. code-block:: javascript

   use products
   db.runCommand(
      {
        dropRole: "readPrices",
        writeConcern: { w: "majority" }
      }
   )
========
dropUser
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: dropUser

   Removes the user from the database on which you run the command. The
   :dbcommand:`dropUser` command has the following syntax:

   .. code-block:: javascript

      {
        dropUser: "<user>",
        writeConcern: { <write concern> }
      }

   The :dbcommand:`dropUser` command document has the following
   fields:

   .. include:: /reference/command/dropUser-field.rst

Required Access
---------------

.. |local-cmd-name| replace:: :command:`dropUser`

.. include:: /includes/access-drop-user.rst

Example
-------

The following sequence of operations in the :program:`mongo` shell removes
``accountAdmin01`` from the ``products`` database:

.. code-block:: javascript

   use products
   db.runCommand( { dropUser: "accountAdmin01",
                    writeConcern: { w: "majority", wtimeout: 5000 }
                    } )
===========
emptycapped
===========

.. default-domain:: mongodb

.. dbcommand:: emptycapped

   The ``emptycapped`` command removes all documents from a capped
   collection. Use the following syntax:

   .. code-block:: javascript

      { emptycapped: "events" }

   This command removes all records from the capped collection named
   ``events``.

   .. warning::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed.

   .. |dbcommand| replace:: :dbcommand:`emptycapped`
   .. include:: /includes/note-enabletestcommands.rst
==============
enableSharding
==============

.. default-domain:: mongodb

.. dbcommand:: enableSharding

   The :dbcommand:`enableSharding` command enables sharding on a per-database
   level. Use the following command form:

   .. code-block:: javascript

      { enableSharding: "<database name>" }

   Once you've enabled sharding in a database, you can use the :dbcommand:`shardCollection`
   command to begin the process of distributing data among the shards.
====
eval
====

.. Edits to this page should be carried over to the method db.eval.txt
   file. HOWEVER, the parameters are different between the command and method.

.. default-domain:: mongodb

.. dbcommand:: eval

   The :dbcommand:`eval` command evaluates JavaScript functions on the
   database server.

   .. |eval-object| replace:: :dbcommand:`eval`
   .. include:: /includes/access-eval.rst

   The :dbcommand:`eval` command has the following form:

   .. code-block:: none

      {
        eval: <function>,
        args: [ <arg1>, <arg2> ... ],
        nolock: <boolean>
      }

   The command contains the following fields:

   .. include:: /reference/command/eval-param.rst

   .. |javascript-using-operation| replace:: :dbcommand:`eval` uses
   .. include:: /includes/admonition-javascript-prevalence.rst

Behavior
--------

The following example uses :dbcommand:`eval` to
perform an increment and calculate the average on the server:

   .. include:: /includes/examples-eval.rst
      :start-after: eval-command-example
      :end-before: .. eval-method-example

The ``db`` in the function refers to the current database.

The :program:`mongo` shell provides a helper method
:method:`db.eval()` [#eval-shell-helper]_, so you can express the
above as follows:

.. include:: /includes/examples-eval.rst
   :start-after: .. eval-method-example

If you want to use the server's interpreter, you must run
:dbcommand:`eval`. Otherwise, the :program:`mongo` shell's
JavaScript interpreter evaluates functions entered directly into the
shell.

If an error occurs, :dbcommand:`eval` throws an exception.
The following invalid function uses the variable ``x`` without
declaring it as an argument:

.. code-block:: javascript

   db.runCommand(
                  {
                    eval: function() { return x + x; },
                    args: [ 3 ]
                  }
                )

The statement will result in the following exception:

.. code-block:: javascript

   {
      "errmsg" : "exception: JavaScript execution failed: ReferenceError: x is not defined near '{ return x + x; }' ",
      "code" : 16722,
      "ok" : 0
   }

.. |object| replace:: :dbcommand:`eval`
.. |nolockobject| replace:: :dbcommand:`eval` command
.. include:: /includes/admonitions-eval.rst

.. seealso:: :doc:`/core/server-side-javascript`

.. [#eval-shell-helper]
   .. include:: /includes/fact-eval-helper-method.rst
========
features
========

.. default-domain:: mongodb

.. dbcommand:: features

   :dbcommand:`features` is an internal command that returns the build-level
   feature settings.

   .. slave-ok
=======
filemd5
=======

.. default-domain:: mongodb

.. dbcommand:: filemd5

   The :dbcommand:`filemd5` command returns the :term:`md5` hashes for a single
   file stored using the :term:`GridFS` specification. Client libraries
   use this command to verify that files are correctly written to MongoDB.
   The command takes the ``files_id`` of the file in question and the
   name of the GridFS root collection as arguments. For example:

   .. code-block:: javascript

      { filemd5: ObjectId("4f1f10e37671b50e4ecd2776"), root: "fs" }

   .. read-lock
=============
findAndModify
=============

.. default-domain:: mongodb

.. EDITS to findAndModify.txt must be carried over (possibly w
   modifications) to the method db.collection.findAndModify.txt and
   vice versa

.. dbcommand:: findAndModify

   The :dbcommand:`findAndModify` command atomically modifies and
   returns a single document. By default, the returned document does
   not include the modifications made on the update. To return the
   document with the modifications made on the update, use the ``new``
   option.

   The command has the following syntax:

   .. code-block:: javascript

      {
        findAndModify: <string>,
        query: <document>,
        sort: <document>,
        remove: <boolean>,
        update: <document>,
        new: <boolean>,
        fields: <document>,
        upsert: <boolean>
      }

   The :dbcommand:`findAndModify` command takes the following
   fields:

   :field string findAndModify:

      Required. The collection against which to run the command.

   :field document query:

      Optional. Specifies the selection criteria for the
      modification. The ``query`` field employs the same :ref:`query
      selectors <query-selectors>` as used in the
      :method:`~db.collection.find()` method. Although the query may
      match multiple documents, :dbcommand:`findAndModify` will only
      select one document to modify.

   :field document sort:

      Optional. Determines which document the operation will modify if
      the query selects multiple documents. :dbcommand:`findAndModify`
      will modify the first document in the sort order specified by
      this argument.

   :field boolean remove:

      Must specify either the ``remove`` or the ``update`` field in the
      :dbcommand:`findAndModify` command. When ``true``, removes the
      selected document. The default is ``false``.

   :field document update:

      Must specify either the ``remove`` or the ``update`` field in the
      :dbcommand:`findAndModify` command. The ``update`` field employs
      the same :ref:`update operators <update-operators>` or ``field:
      value`` specifications to modify the selected document.

   :field boolean new:

      Optional. When ``true``, returns the modified document rather
      than the original. The :dbcommand:`findAndModify` method ignores
      the ``new`` option for ``remove`` operations. The default is
      ``false``.

   :field document fields:

      Optional. A subset of fields to return. The ``fields`` document
      specifies an inclusion of a field with ``1``, as in the
      following:

      .. code-block:: javascript

         fields: { <field1>: 1, <field2>: 1, ... }

      See :ref:`projection <read-operations-projection>`.

   :field boolean upsert:

      Optional. Used in conjunction with the ``update`` field. When
      ``true``, the :dbcommand:`findAndModify` command creates a new
      document if the ``query`` returns no documents. The default is
      ``false``.

   The :dbcommand:`findAndModify` command returns a document, similar
   to the following:

   .. code-block:: javascript

      {
         lastErrorObject: {
                              updatedExisting: <boolean>,
                              upserted: <boolean>,
                              n: <num>,
                              connectionId: <num>,
                              err: <string>,
                              ok: <num>
         }
         value: <document>,
         ok: <num>
      }

   The return document contains the following fields:

   - The ``lastErrorObject`` field that returns the details of the
     command:

     - The ``updatedExisting`` field **only** appears if the command is
       either an ``update`` or an ``upsert``.

     - The ``upserted`` field **only** appears if the command is an
       ``upsert``.

   - The ``value`` field that returns either:

     - the original (i.e. pre-modification) document if ``new`` is
       false, or

     - the modified or inserted document if ``new: true``.

   - The ``ok`` field that returns the status of the command.

   .. note::

      If the :dbcommand:`findAndModify` finds no matching document,
      then:

      - for ``update`` or ``remove`` operations, ``lastErrorObject``
        does not appear in the return document and the ``value`` field
        holds a ``null``.

        .. code-block:: javascript

           { "value" : null, "ok" : 1 }

      - for an ``upsert`` operation that performs an insert, when ``new``
        is ``false``, **and** includes a ``sort`` option, the return
        document has ``lastErrorObject``, ``value``, and ``ok``
        fields, but the ``value`` field holds an empty document
        ``{}``.

      - for an ``upsert`` that performs an insert, when ``new`` is
        ``false`` **without** a specified ``sort`` the return
        document has ``lastErrorObject``, ``value``, and ``ok``
        fields, but the ``value`` field holds a ``null``.

        .. versionchanged:: 2.2
           Previously, the command returned an empty document (e.g.
           ``{}``) in the ``value`` field. See :ref:`the 2.2 release
           notes <2.2-findandmodify-returns-null>` for more information.

   Consider the following examples:

   - The following command updates an existing document in the
     ``people`` collection where the document matches the ``query``
     criteria:

     .. code-block:: javascript

        db.runCommand(
                        {
                          findAndModify: "people",
                          query: { name: "Tom", state: "active", rating: { $gt: 10 } },
                          sort: { rating: 1 },
                          update: { $inc: { score: 1 } }
                        }
                     )

     This command performs the following actions:

     #. The ``query`` finds a document in the ``people`` collection where the
        ``name`` field has the value ``Tom``, the ``state`` field has
        the value ``active`` and the ``rating`` field has a value
        :operator:`greater than <$gt>` 10.

     #. The ``sort`` orders the results of the query in ascending order.
        If multiple documents meet the ``query`` condition, the command
        will select for modification the first document as ordered by
        this ``sort``.

     #. The ``update`` :operator:`increments <$inc>` the value of the
        ``score`` field by 1.

     #. The command returns a document with the following fields:

        - The ``lastErrorObject`` field that contains the details of
          the command, including the field ``updatedExisting`` which is
          ``true``, and

        - The ``value`` field that contains the original (i.e.
          pre-modification) document selected for this update:

        .. code-block:: javascript

           {
             "lastErrorObject" : {
                "updatedExisting" : true,
                "n" : 1,
                "connectionId" : 1,
                "err" : null,
                "ok" : 1
             },
             "value" : {
                "_id" : ObjectId("50f1d54e9beb36a0f45c6452"),
                "name" : "Tom",
                "state" : "active",
                "rating" : 100,
                "score" : 5
             },
             "ok" : 1
           }

        To return the modified document in the ``value`` field, add the
        ``new:true`` option to the command.

        If no document match the ``query`` condition, the command
        returns a document that contains ``null`` in the ``value``
        field:

        .. code-block:: javascript

           { "value" : null, "ok" : 1 }

     The :program:`mongo` shell and many :term:`drivers <driver>`
     provide a :method:`~db.collection.findAndModify()` helper method.
     Using the shell helper, this previous operation can take the
     following form:

     .. code-block:: javascript

        db.people.findAndModify( {
           query: { name: "Tom", state: "active", rating: { $gt: 10 } },
           sort: { rating: 1 },
           update: { $inc: { score: 1 } }
        } );

     However, the :method:`~db.collection.findAndModify()` shell helper
     method returns just the unmodified document, or the modified
     document when ``new`` is ``true``.

     .. code-block:: javascript

        {
           "_id" : ObjectId("50f1d54e9beb36a0f45c6452"),
           "name" : "Tom",
           "state" : "active",
           "rating" : 100,
           "score" : 5
        }

   - The following :dbcommand:`findAndModify` command includes the
     ``upsert: true`` option to insert a new document if no document
     matches the ``query`` condition:

     .. code-block:: javascript

        db.runCommand(
                       {
                         findAndModify: "people",
                         query: { name: "Gus", state: "active", rating: 100 },
                         sort: { rating: 1 },
                         update: { $inc: { score: 1 } },
                         upsert: true
                       }
                     )

     If the command does **not** find a matching document, the command
     performs an upsert and returns a document with the following
     fields:

     - The ``lastErrorObject`` field that contains the details of the
       command, including the field ``upserted`` that contains the
       ``ObjectId`` of the newly inserted document, and

     - The ``value`` field that contains an empty document ``{}`` as
       the original document because the command included the ``sort``
       option:

     .. code-block:: sh

        {
          "lastErrorObject" : {
             "updatedExisting" : false,
             "upserted" : ObjectId("50f2329d0092b46dae1dc98e"),
             "n" : 1,
             "connectionId" : 1,
             "err" : null,
             "ok" : 1
          },
          "value" : {

          },
          "ok" : 1
        }

     If the command did **not** include the ``sort`` option, the ``value`` field
     would contain ``null``:

     .. code-block:: sh

        {
          "value" : null,
          "lastErrorObject" : {
             "updatedExisting" : false,
             "n" : 1,
             "upserted" : ObjectId("5102f7540cb5c8be998c2e99")
          },
          "ok" : 1
        }

   - The following :dbcommand:`findAndModify` command includes both
     ``upsert: true`` option and the ``new:true`` option to return the
     newly inserted document in the ``value`` field if a document
     matching the ``query`` is not found:

     .. code-block:: javascript

        db.runCommand(
                       {
                         findAndModify: "people",
                         query: { name: "Pascal", state: "active", rating: 25 },
                         sort: { rating: 1 },
                         update: { $inc: { score: 1 } },
                         upsert: true,
                         new: true
                       }
                     )

     The command returns the newly inserted document in the ``value``
     field:

     .. code-block:: sh

        {
          "lastErrorObject" : {
             "updatedExisting" : false,
             "upserted" : ObjectId("50f47909444c11ac2448a5ce"),
             "n" : 1,
             "connectionId" : 1,
             "err" : null,
             "ok" : 1
          },
          "value" : {
             "_id" : ObjectId("50f47909444c11ac2448a5ce"),
             "name" : "Pascal",
             "rating" : 25,
             "score" : 1,
             "state" : "active"
          },
          "ok" : 1
        }

   When the :dbcommand:`findAndModify` command includes the ``upsert:
   true`` option **and** the query field(s) is not uniquely indexed, the
   method could insert a document multiple times in certain
   circumstances. For instance, if multiple clients issue the
   :dbcommand:`findAndModify` command and these commands complete the
   ``find`` phase before any one starts the ``modify`` phase, these
   commands could insert the same document.

   Consider an example where no document with the name ``Andy`` exists
   and multiple clients issue the following command:

   .. code-block:: javascript

      db.runCommand(
                     {
                       findAndModify: "people",
                       query: { name: "Andy" },
                       sort: { rating: 1 },
                       update: { $inc: { score: 1 } },
                       upsert: true
                     }
                   )

   If all the commands finish the ``query`` phase before any command
   starts the ``modify`` phase, **and** there is no unique index on the
   ``name`` field, the commands may all perform an upsert. To prevent
   this condition, create a :ref:`unique index <index-type-unique>` on
   the ``name`` field. With the unique index in place, then the multiple
   :dbcommand:`findAndModify` commands would observe one of the
   following behaviors:

   - Exactly one :dbcommand:`findAndModify` would successfully insert a
     new document.

   - Zero or more :dbcommand:`findAndModify` commands would update the
     newly inserted document.

   - Zero or more :dbcommand:`findAndModify` commands would fail when
     they attempted to insert a duplicate. If the command fails due to
     a unique index constraint violation, you can retry the command.
     Absent a delete of the document, the retry should not fail.

   .. warning::

      When using :dbcommand:`findAndModify` in a :term:`sharded
      <sharding>` environment, the ``query`` must contain the
      :term:`shard key` for all operations against the shard
      cluster. :dbcommand:`findAndModify` operations issued against
      :program:`mongos` instances for non-sharded collections function
      normally.

   .. note::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed; however,
      typically the write lock is short lived and equivalent to other
      similar :method:`~db.collection.update()` operations.
=================
flushRouterConfig
=================

.. default-domain:: mongodb

.. dbcommand:: flushRouterConfig

   :dbcommand:`flushRouterConfig` clears the current cluster
   information cached by a :program:`mongos` instance and reloads all
   :term:`sharded cluster` metadata from the :term:`config database`.

   This forces an update when the configuration database holds data
   that is newer than the data cached in the :program:`mongos`
   process.

   .. warning::

      Do not modify the config data, except as explicitly
      documented. A config database cannot typically tolerate manual
      manipulation.

   :dbcommand:`flushRouterConfig` is an administrative command that is
   only available for :program:`mongos` instances.

   .. versionadded:: 1.8.2
==========
forceerror
==========

.. default-domain:: mongodb

.. dbcommand:: forceerror

   The :dbcommand:`forceerror` command is for testing purposes
   only. Use :dbcommand:`forceerror` to force a user assertion
   exception. This command always returns an ``ok`` value of 0.
=====
fsync
=====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: fsync

   Forces the :program:`mongod` process to flush all pending writes
   from the storage layer to disk. Optionally, you can use
   :dbcommand:`fsync` to lock the :program:`mongod` instance and block
   write operations for the purpose of capturing backups.

   As applications write data, MongoDB records the data in the storage
   layer and then writes the data to disk within the :setting:`~storage.syncPeriodSecs`
   interval, which is 60 seconds by default. Run :dbcommand:`fsync` when
   you want to flush writes to disk ahead of that interval.

   The :dbcommand:`fsync` command has the following syntax:

   .. code-block:: javascript

      { fsync: 1, async: <Boolean>, lock: <Boolean> }

   The :dbcommand:`fsync` command has the following fields:

   .. include:: fsync-field.rst

Behavior
--------

An :dbcommand:`fsync` lock is only possible on *individual*
:program:`mongod` instances of a
sharded cluster, not on the entire cluster. To backup an entire sharded
cluster, please see :doc:`/administration/backup-sharded-clusters` for
more information.

If your :program:`mongod` has :term:`journaling <journal>` enabled,
consider using :ref:`another method <backup-with-journaling>` to create a
back up of the data set.

After :dbcommand:`fsync`, with lock, runs on a :program:`mongod`,
all write operations will block until a subsequent unlock. Read operations *may* also
block. As a result, :dbcommand:`fsync`,
with lock, is not a reliable mechanism for making
a :program:`mongod` instance operate in a read-only mode.

.. include:: /includes/note-disable-profiling-fsynclock.rst

Examples
--------

Run Asynchronously
~~~~~~~~~~~~~~~~~~

The :dbcommand:`fsync` operation is synchronous by default To run
:dbcommand:`fsync` asynchronously, use the ``async`` field set to
``true``:

.. code-block:: javascript

   { fsync: 1, async: true }

The operation returns immediately. To view the status of the
:dbcommand:`fsync` operation, check the output of
:method:`db.currentOp()`.

Lock ``mongod`` Instance
~~~~~~~~~~~~~~~~~~~~~~~~

The primary use of :dbcommand:`fsync` is to lock the :program:`mongod`
instance in order to back up the files withing :program:`mongod`\ 's :setting:`~storage.dbPath`.
The operation flushes all data to the storage layer and
blocks all write operations until you unlock the :program:`mongod` instance.

To lock the database, use the ``lock`` field set to ``true``:

.. code-block:: javascript

   { fsync: 1, lock: true }

You may continue to perform read operations on a :program:`mongod` instance that has a
:dbcommand:`fsync` lock. However, after the first write operation all
subsequent read operations wait until you unlock the :program:`mongod` instance.

Check Lock Status
~~~~~~~~~~~~~~~~~

To check the state of the fsync lock, use :method:`db.currentOp()`. Use
the following JavaScript function in the shell to test if :program:`mongod` instance is
currently locked:

.. code-block:: javascript

   serverIsLocked = function () {
                        var co = db.currentOp();
                        if (co && co.fsyncLock) {
                            return true;
                        }
                        return false;
                    }

After loading this function into your :program:`mongo` shell session
call it, with the following syntax:

.. code-block:: javascript

   serverIsLocked()

This function will return ``true`` if the :program:`mongod` instance is
currently locked and ``false`` if the :program:`mongod` is not locked. To
unlock the :program:`mongod`, make a request for an unlock using the
following operation:

.. code-block:: javascript

   db.getSiblingDB("admin").$cmd.sys.unlock.findOne();

Unlock ``mongod`` Instance
~~~~~~~~~~~~~~~~~~~~~~~~~~

To unlock the :program:`mongod` instance, use :method:`db.fsyncUnlock()`:

.. code-block:: javascript

   db.fsyncUnlock();
=======
geoNear
=======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: geoNear

   Specifies a point for which a :term:`geospatial` query returns the
   closest documents first. The query returns the documents from nearest
   to farthest. The :dbcommand:`geoNear` command provides an alternative
   to the :query:`$near` operator. In addition to the functionality
   of :query:`$near`, :dbcommand:`geoNear` returns additional
   diagnostic information.

   The :dbcommand:`geoNear` command can use either a :term:`GeoJSON`
   point or :term:`legacy coordinate pairs`. Queries that use a ``2d``
   index return a limit of 100 documents.

   The :dbcommand:`geoNear` command accepts a :term:`document` that
   contains the following fields. Specify all distances in the same
   units as the document coordinate system:

   .. include:: /reference/command/geoNear-field.rst

Command Format
--------------

To query a :doc:`2dsphere </core/2dsphere>` index, use the
following syntax:

.. code-block:: javascript

   db.runCommand( { geoNear : <collection> ,
                    near : { type : "Point" ,
                             coordinates: [ <coordinates> ] } ,
                    spherical : true } )

To query a :doc:`2d </core/2d>` index, use:

.. code-block:: javascript

   { geoNear : <collection> , near : [ <coordinates> ] }

.. read-lock, slave-ok
=========
geoSearch
=========

.. default-domain:: mongodb

.. dbcommand:: geoSearch

   The :dbcommand:`geoSearch` command provides an interface to
   MongoDB's :term:`haystack index` functionality. These indexes are
   useful for returning results based on location coordinates *after*
   collecting results based on some other query (i.e. a "haystack.")
   Consider the following example:

   .. code-block:: javascript

      { geoSearch : "places", near : [33, 33], maxDistance : 6, search : { type : "restaurant" }, limit : 30 }

   The above command returns all documents with a ``type`` of
   ``restaurant`` having a maximum distance of 6 units from the
   coordinates ``[30,33]`` in the collection ``places`` up to a
   maximum of 30 results.

   Unless specified otherwise, the :dbcommand:`geoSearch` command
   limits results to 50 documents.

   .. important:: :dbcommand:`geoSearch` is not supported for sharded
      clusters.

   .. read-lock, slave-ok
=======
geoWalk
=======

.. default-domain:: mongodb

.. dbcommand:: geoWalk

   :dbcommand:`geoWalk` is an internal command.

   .. read-lock, slave-ok
==============
getCmdLineOpts
==============

.. default-domain:: mongodb

.. dbcommand:: getCmdLineOpts

   The :dbcommand:`getCmdLineOpts` command returns a document containing
   command line options used to start the given :program:`mongod`:

   .. code-block:: javascript

      { getCmdLineOpts: 1 }

   This command returns a document with two fields, ``argv`` and
   ``parsed``. The ``argv`` field contains an array with each item
   from the command string used to invoke :program:`mongod`. The document
   in the ``parsed`` field includes all runtime options, including
   those parsed from the command line and those specified in the
   configuration file, if specified.

   Consider the following example output of
   :dbcommand:`getCmdLineOpts`:

   .. code-block:: javascript

      {
              "argv" : [
                     "/usr/bin/mongod",
                     "--config",
                     "/etc/mongodb.conf",
                     "--fork"
              ],
              "parsed" : {
                     "bind_ip" : "127.0.0.1",
                     "config" : "/etc/mongodb/mongodb.conf",
                     "dbpath" : "/srv/mongodb",
                     "fork" : true,
                     "logappend" : "true",
                     "logpath" : "/var/log/mongodb/mongod.log",
                     "quiet" : "true"
              },
              "ok" : 1
      }
============
getLastError
============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: getLastError

   Returns the error status of the preceding operation on the *current
   connection*. Clients typically use :dbcommand:`.getLastError` in
   combination with write operations to ensure that the write succeeds.

   :dbcommand:`getLastError` uses the following prototype form:

   .. code-block:: javascript

      { getLastError: 1 }

   :dbcommand:`getLastError` uses the following fields:

   .. include:: /reference/command/getLastError-fields.rst

   .. seealso:: :doc:`Write Concern </core/write-concern>`,
      :doc:`/reference/write-concern`, and :ref:`replica-set-write-concern`.

Output
------

Each :dbcommand:`~db.collection.getLastError()` command returns a document containing a
subset of the fields listed below.

.. data:: getLastError.ok

   :data:`~getLastError.ok` is ``true`` when the
   :dbcommand:`getLastError` command completes successfully.

   .. note:: A value of ``true`` does *not* indicate that the preceding
      operation did not produce an error.

.. data:: getLastError.err

   :data:`~getLastError.err` is ``null`` unless an error occurs. When
   there was an error with the preceding operation, ``err`` contains
   a textual description of the error.

.. data:: getLastError.code

   :data:`~getLastError.code` reports the preceding operation's error
   code.

.. data:: getLastError.connectionId

   The identifier of the connection.

.. data:: getLastError.lastOp

   When issued against a replica set member and the preceding
   operation was a write or update, :data:`~getLastError.lastOp` is the
   *optime* timestamp in the :term:`oplog`  of the change.

.. data:: getLastError.n

   :data:`~getLastError.n` reports the number of documents updated or
   removed, if the preceding operation was an update or remove
   operation.

.. data:: getLastError.updatedExisting

   :data:`~getLastError.updatedExisting` is ``true`` when an update
   affects at least one document and does not result in an
   :term:`upsert`.

.. data:: getLastError.upserted

   If the update results in an insert, :data:`~getLastError.upserted`
   is the value of ``_id`` field of the document.

   .. versionchanged:: 2.6
      Earlier versions of MongoDB included
      :data:`~getLastError.upserted` only if ``_id`` was an
      :term:`ObjectId <objectid>`.

.. data:: getLastError.wnote

   If set, ``wnote`` indicates that the preceding operation's error
   relates to using the ``w`` parameter to :dbcommand:`getLastError`.

   .. see:: :doc:`/reference/write-concern` for more information about
      ``w`` values.

.. data:: getLastError.wtimeout

   :data:`~getLastError.wtimeout` is ``true`` if the
   :dbcommand:`getLastError` timed out because of the ``wtimeout``
   setting to :dbcommand:`getLastError`.

.. data:: getLastError.waited

   If the preceding operation specified a timeout using the
   ``wtimeout`` setting to :dbcommand:`getLastError`, then
   :data:`~getLastError.waited` reports the number of milliseconds
   :dbcommand:`getLastError` waited before timing out.

.. data:: getLastError.wtime

   :data:`getLastError.wtime` is the number of milliseconds spent
   waiting for the preceding operation to complete. If
   :dbcommand:`getLastError` timed out, :data:`~getLastError.wtime` and
   :dbcommand:`getLastError.waited` are equal.

.. _gle-examples:

Examples
--------

Confirm Replication to Two Replica Set Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example ensures the operation has replicated to two
members (the primary and one other member):

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: 2 } )

Confirm Replication to a Majority of a Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example ensures the write operation has replicated to a
majority of the configured members of the set.

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "majority" } )

.. include:: /includes/fact-master-slave-majority.rst

Set a Timeout for a ``getLastError`` Response
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Unless you specify a timeout, a :dbcommand:`getLastError` command may
block forever if MongoDB cannot satisfy the requested write
concern. To specify a timeout of 5000 milliseconds, use an invocation
that resembles the following:

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: 2, wtimeout:5000 } )

When ``wtimeout`` is ``0``, the :dbcommand:`getLastError` operation
will never time out.
======
getLog
======

.. default-domain:: mongodb

.. dbcommand:: getLog

   The :dbcommand:`getLog` command returns a document with a ``log``
   array that contains recent messages from the :program:`mongod`
   process log. The :dbcommand:`getLog` command has the following
   syntax:

   .. code-block:: javascript

      { getLog: <log> }

   Replace ``<log>`` with one of the following values:

   - ``global`` - returns the combined output of all recent log
     entries.

   - ``rs`` - if the :program:`mongod` is part of a :term:`replica
     set`, :dbcommand:`getLog` will return recent notices related to
     replica set activity.

   - ``startupWarnings`` - will return logs that *may* contain errors
     or warnings from MongoDB's log from when the current process
     started. If :program:`mongod` started without warnings, this
     filter may return an empty array.

   You may also specify an asterisk (e.g. ``*``) as the ``<log>``
   value to return a list of available log filters. The following
   interaction from the :program:`mongo` shell connected to a replica
   set:

   .. code-block:: javascript

      db.adminCommand({getLog: "*" })
      { "names" : [ "global", "rs", "startupWarnings" ], "ok" : 1 }

   :dbcommand:`getLog` returns events from a RAM cache of the
   :program:`mongod` events and *does not* read log data from the log
   file.
========
getnonce
========

.. default-domain:: mongodb

.. dbcommand:: getnonce

   Client libraries use :dbcommand:`getnonce`  to generate a one-time
   password for authentication.

   .. slave-ok
=========
getoptime
=========

.. default-domain:: mongodb

.. dbcommand:: getoptime

   :dbcommand:`getoptime` is an internal command.

   .. slave-ok
============
getParameter
============

.. default-domain:: mongodb

.. dbcommand:: getParameter

   :dbcommand:`getParameter` is an administrative command for
   retrieving the value of options normally set on the command
   line. Issue commands against the :term:`admin database` as follows:

   .. code-block:: javascript

      { getParameter: 1, <option>: 1 }

   The values specified for ``getParameter`` and ``<option>`` do not
   affect the output. The command works with the following options:

   - **quiet**
   - **notablescan**
   - **logLevel**
   - **syncdelay**

   .. seealso:: :dbcommand:`setParameter` for more about these parameters.

   .. slave-ok, admin-only
============
getPrevError
============

.. default-domain:: mongodb

.. dbcommand:: getPrevError

   The :dbcommand:`getPrevError` command returns the errors since the
   last :dbcommand:`resetError` command.

   .. seealso:: :method:`db.getPrevError()`
===========
getShardMap
===========

.. default-domain:: mongodb

.. dbcommand:: getShardMap

   :dbcommand:`getShardMap` is an internal command that supports the sharding
   functionality.

   .. slave-ok, admin-only
===============
getShardVersion
===============

.. default-domain:: mongodb

.. dbcommand:: getShardVersion

   :dbcommand:`getShardVersion` is a command that supports sharding
   functionality and is not part of the stable client facing API.

   .. admin-only
=========
godinsert
=========

.. default-domain:: mongodb

.. dbcommand:: godinsert

   :dbcommand:`godinsert` is an internal command for testing purposes only.

   .. note::

      This command obtains a write lock on the affected database and will
      block other operations until it has completed.

   .. |dbcommand| replace:: :dbcommand:`godinsert`
   .. include:: /includes/note-enabletestcommands.rst


   .. write-lock, slave-ok
=====================
grantPrivilegesToRole
=====================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: grantPrivilegesToRole

   Assigns additional :ref:`privileges <privileges>` to a :ref:`user-defined
   <user-defined-roles>` role defined on the database on which the
   command is run. The :dbcommand:`grantPrivilegesToRole` command uses
   the following syntax:

   .. code-block:: javascript

      {
        grantPrivilegesToRole: "<role>",
        privileges: [
            {
              resource: { <resource> }, actions: [ "<action>", ... ]
            },
            ...
        ],
        writeConcern: { <write concern> }
      }

   The :dbcommand:`grantPrivilegesToRole` command has the following
   fields:

   .. include:: /reference/command/grantPrivilegesToRole-field.rst

Behavior
--------

A role's privileges apply to the database where the role is created. A
role created on the ``admin`` database can include privileges that apply
to all databases or to the :ref:`cluster <resource-cluster>`.

Required Access
---------------

.. include:: /includes/access-grant-privileges.rst

Example
-------

The following :dbcommand:`grantPrivilegesToRole` command grants two
additional privileges to the ``service`` role that exists in the
``products`` database:

.. code-block:: javascript

   use products
   db.runCommand(
      {
        grantPrivilegesToRole: "service",
        privileges: [
            {
              resource: { db: "products", collection: "" }, actions: [ "find" ]
            },
            {
              resource: { db: "products", collection: "system.indexes" }, actions: [ "find" ]
            }
        ],
        writeConcern: { w: "majority" , wtimeout: 5000 }
      }
   )

The first privilege in the ``privileges`` array allows the user to
search on all non-system collections in the ``products`` database. The
privilege does not allow searches on :doc:`system collections
</reference/system-collections>`, such as the :data:`system.indexes
<<database>.system.indexes>` collection. To grant access to these
system collections, explicitly provision access in the ``privileges``
array. See :doc:`/reference/resource-document`.

The second privilege explicitly allows the :authaction:`find` action on
:data:`system.indexes <<database>.system.indexes>` collections on all
databases.
================
grantRolesToRole
================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: grantRolesToRole

   Grants roles to a :ref:`user-defined role <user-defined-roles>`.

   The :dbcommand:`grantRolesToRole` command affects roles on the
   database where the command runs. :dbcommand:`grantRolesToRole` has
   the following syntax:

   .. code-block:: javascript

      { grantRolesToRole: "<role>",
        roles: [
                   { role: "<role>", db: "<database>" },
                   ...
               ],
        writeConcern: { <write concern> }
      }

   The :dbcommand:`grantRolesToRole` command has the following fields:

   .. include:: /reference/command/grantRolesToRole-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`grantRolesToRole`
   .. include:: /includes/fact-roles-array-contents.rst

Behavior
--------

A role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can inherit privileges from roles in
any database.

Required Access
---------------

.. include:: /includes/access-grant-roles.rst

Example
-------


The following :dbcommand:`grantRolesToRole` command updates the
``productsReaderWriter`` role in the ``products`` database to :ref:`inherit
<inheritance>` the :ref:`privileges <privileges>` of the ``productsReader``
role in the ``products`` database:

.. code-block:: javascript

   use products
   db.runCommand(
      { grantRolesToRole: "productsReaderWriter",
        roles: [
                 "productsReader"
        ],
        writeConcern: { w: "majority" , wtimeout: 5000 }
      }
   )
================
grantRolesToUser
================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: grantRolesToUser

   Grants additional roles to a user.

   The :dbcommand:`grantRolesToUser` command uses the following syntax:

   .. code-block:: javascript

      { grantRolesToUser: "<user>",
        roles: [ <roles> ],
        writeConcern: { <write concern> }
      }

   The command has the following fields:

   .. include:: /reference/command/grantRolesToUser-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`grantRolesToUser`
   .. include:: /includes/fact-roles-array-contents.rst

Required Access
---------------

.. include:: /includes/access-grant-roles.rst

Example
-------

Given a user ``accountUser01`` in the ``products`` database with the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       }
   ]

The following :dbcommand:`grantRolesToUser` operation gives ``accountUser01`` the
:authrole:`read` role on the ``stock`` database and the
:authrole:`readWrite` role on the ``products`` database.

.. code-block:: javascript

   use products
   db.runCommand( { grantRolesToUser: "accountUser01",
                    roles: [
                       { role: "read", db: "stock"},
                       "readWrite"
                    ],
                    writeConcern: { w: "majority" , wtimeout: 2000 }
                } )

The user ``accountUser01`` in the ``products`` database now has the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       },
       { "role" : "read",
         "db" : "stock"
       },
       { "role" : "readWrite",
         "db" : "products"
       }
   ]
=====
group
=====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: group

   Groups documents in a collection by
   the specified key and performs simple aggregation functions, such as
   computing counts and sums. The command is analogous to a ``SELECT
   <...> GROUP BY`` statement in SQL. The command returns a document with
   the grouped records as well as the command meta-data.

   The :dbcommand:`group` command takes the following prototype
   form:

   .. code-block:: none

      {
        group:
         {
           ns: <namespace>,
           key: <key>,
           $reduce: <reduce function>,
           $keyf: <key function>,
           cond: <query>,
           finalize: <finalize function>
         }
      }

   The command accepts a document with the following fields:

   .. |obj-name| replace:: :dbcommand:`group`

   .. include:: /reference/command/group-field.rst

   For the shell, MongoDB provides a wrapper method
   :method:`db.collection.group()`. However, the
   :method:`db.collection.group()` method takes the ``keyf`` field and
   the ``reduce`` field whereas the :dbcommand:`group` command takes
   the ``$keyf`` field and the ``$reduce`` field.

Behavior
--------

Limits and Restrictions
~~~~~~~~~~~~~~~~~~~~~~~

The :dbcommand:`group` command does not work with :term:`sharded
clusters <sharded cluster>`. Use the :term:`aggregation framework` or
:term:`map-reduce` in :term:`sharded environments <sharding>`.

The result set must fit within the :ref:`maximum BSON document size
<limit-bson-document-size>`.

Additionally, in version 2.2, the returned array can contain at most
20,000 elements; i.e. at most 20,000 unique groupings. For group by
operations that results in more than 20,000 unique groupings, use
:dbcommand:`mapReduce`. Previous versions had a limit of 10,000
elements.

Prior to 2.4, the :dbcommand:`group` command took the :program:`mongod`
instance's JavaScript lock which blocked all other JavaScript execution.

``mongo`` Shell JavaScript Functions/Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.4

.. include:: /includes/fact-group-map-reduce-where-limitations-in-24.rst

.. |javascript-using-operation| replace:: :dbcommand:`group` uses
.. include:: /includes/admonition-javascript-prevalence.rst

Examples
--------

The following are examples of the :method:`db.collection.group()` method.
The examples assume an ``orders`` collection with documents of the
following prototype:

.. code-block:: javascript

   {
     _id: ObjectId("5085a95c8fada716c89d0021"),
     ord_dt: ISODate("2012-07-01T04:00:00Z"),
     ship_dt: ISODate("2012-07-02T04:00:00Z"),
     item:
       {
         sku: "abc123",
         price: 1.99,
         uom: "pcs",
         qty: 25
       }
   }

Group by Two Fields
~~~~~~~~~~~~~~~~~~~

The following example groups by the ``ord_dt`` and ``item.sku``
fields those documents that have ``ord_dt`` greater than
``01/01/2012``:

.. code-block:: javascript

   db.runCommand(
      {
        group:
          {
            ns: 'orders',
            key: { ord_dt: 1, 'item.sku': 1 },
            cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
            $reduce: function ( curr, result ) { },
            initial: { }
          }
      }
   )

The result is a documents that contain the ``retval`` field which
contains the group by records, the ``count`` field which contains
the total number of documents grouped, the ``keys`` field which
contains the number of unique groupings (i.e. number of elements
in the ``retval``), and the ``ok`` field which contains the
command status:

.. code-block:: javascript

   { "retval" :
         [ { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc123"},
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc456"},
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "bcd123"},
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "efg456"},
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "abc123"},
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "efg456"},
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "ijk123"},
           { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc123"},
           { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc456"},
           { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc123"},
           { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc456"}
         ],
     "count" : 13,
     "keys" : 11,
     "ok" : 1 }


The method call is analogous to the SQL statement:

.. code-block:: sql

   SELECT ord_dt, item_sku
   FROM orders
   WHERE ord_dt > '01/01/2012'
   GROUP BY ord_dt, item_sku

Calculate the Sum
~~~~~~~~~~~~~~~~~

The following example groups by the ``ord_dt`` and ``item.sku``
fields those documents that have ``ord_dt`` greater than
``01/01/2012`` and calculates the sum of the ``qty`` field for each
grouping:

.. code-block:: javascript

   db.runCommand(
      { group:
          {
            ns: 'orders',
            key: { ord_dt: 1, 'item.sku': 1 },
            cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
            $reduce: function ( curr, result ) {
                        result.total += curr.item.qty;
                     },
            initial: { total : 0 }
          }
       }
   )

The ``retval`` field of the returned document is an array of
documents that contain the group by fields and the calculated
aggregation field:

.. code-block:: javascript

   { "retval" :
         [ { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc456", "total" : 25 },
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "bcd123", "total" : 10 },
           { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "efg456", "total" : 10 },
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "efg456", "total" : 15 },
           { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "ijk123", "total" : 20 },
           { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc123", "total" : 45 },
           { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc456", "total" : 25 },
           { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
           { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc456", "total" : 25 }
         ],
     "count" : 13,
     "keys" : 11,
     "ok" : 1 }

The method call is analogous to the SQL statement:

.. code-block:: sql

   SELECT ord_dt, item_sku, SUM(item_qty) as total
   FROM orders
   WHERE ord_dt > '01/01/2012'
   GROUP BY ord_dt, item_sku

Calculate Sum, Count, and Average
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example groups by the calculated ``day_of_week`` field,
those documents that have ``ord_dt`` greater than ``01/01/2012`` and
calculates the sum, count, and average of the ``qty`` field for each
grouping:

.. code-block:: javascript

   db.runCommand(
      {
        group:
          {
            ns: 'orders',
            $keyf: function(doc) {
                       return { day_of_week: doc.ord_dt.getDay() };
                   },
            cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
            $reduce: function( curr, result ) {
                         result.total += curr.item.qty;
                         result.count++;
                     },
            initial: { total : 0, count: 0 },
            finalize: function(result) {
                         var weekdays = [
                              "Sunday", "Monday", "Tuesday",
                              "Wednesday", "Thursday",
                              "Friday", "Saturday"
                             ];
                         result.day_of_week = weekdays[result.day_of_week];
                         result.avg = Math.round(result.total / result.count);
                      }
          }
      }
   )

The ``retval`` field of the returned document is an array of
documents that contain the group by fields and the calculated
aggregation field:

.. code-block:: javascript

   {
     "retval" :
         [
           { "day_of_week" : "Sunday", "total" : 70, "count" : 4, "avg" : 18 },
           { "day_of_week" : "Friday", "total" : 110, "count" : 6, "avg" : 18 },
           { "day_of_week" : "Tuesday", "total" : 70, "count" : 3, "avg" : 23 }
         ],
     "count" : 13,
     "keys" : 3,
     "ok" : 1
   }

.. seealso:: :doc:`/core/aggregation`

.. read-lock
=========
handshake
=========

.. default-domain:: mongodb

.. dbcommand:: handshake

   :dbcommand:`handshake` is an internal command.

   .. slave-ok, no-lock
================
_hashBSONElement
================

.. default-domain:: mongodb

Description
-----------

.. dbcommand:: _hashBSONElement

   .. versionadded:: 2.4

   An internal command that computes the MD5 hash of a BSON element. The
   :dbcommand:`_hashBSONElement` command returns 8 bytes from the 16
   byte MD5 hash.

   The :dbcommand:`_hashBSONElement` command has the following form:

   .. code-block:: javascript

      db.runCommand({ _hashBSONElement: <key> , seed: <seed> })

   The :dbcommand:`_hashBSONElement` command has the following fields:

   .. include:: /reference/command/hashBSONElement-field.rst

   .. |dbcommand| replace:: :dbcommand:`_hashBSONElement`
   .. include:: /includes/note-enabletestcommands.rst

Output
------

The :dbcommand:`_hashBSONElement` command returns a document
that holds the following fields:

.. data:: _hashBSONElement.key

   The original BSON element.

.. data:: _hashBSONElement.seed

   The seed used for the hash, defaults to ``0``.

.. data:: _hashBSONElement.out

   The decimal result of the hash.

.. data:: _hashBSONElement.ok

   Holds the ``1`` if the function returns successfully, and ``0`` if
   the operation encountered an error.

Example
-------

Invoke a :program:`mongod` instance with test commands enabled:

.. code-block:: sh

   mongod --setParameter enableTestCommands=1

Run the following to compute the hash of an ISODate string:

.. code-block:: javascript

   db.runCommand({_hashBSONElement: ISODate("2013-02-12T22:12:57.211Z")})

The command returns the following document:

.. code-block:: javascript

   {
     "key" : ISODate("2013-02-12T22:12:57.211Z"),
     "seed" : 0,
     "out" : NumberLong("-4185544074338741873"),
     "ok" : 1
   }

Run the following to hash the same ISODate string but this time to
specify a seed value:

.. code-block:: javascript

   db.runCommand({_hashBSONElement: ISODate("2013-02-12T22:12:57.211Z"), seed:2013})

The command returns the following document:

.. code-block:: javascript

   {
     "key" : ISODate("2013-02-12T22:12:57.211Z"),
     "seed" : 2013,
     "out" : NumberLong("7845924651247493302"),
     "ok" : 1
   }
========
hostInfo
========

.. default-domain:: mongodb

.. dbcommand:: hostInfo

   .. versionadded:: 2.2

   :returns: A document with information about the underlying system
             that the :program:`mongod` or :program:`mongos` runs on.
             Some of the returned fields are only included on some
             platforms.

   You must run the :dbcommand:`hostInfo` command, which takes no
   arguments, against the ``admin`` database. Consider the following
   invocations of :dbcommand:`hostInfo`:

   .. code-block:: javascript

      db.hostInfo()
      db.adminCommand( { "hostInfo" : 1 } )

   In the :program:`mongo` shell you can use :method:`db.hostInfo()`
   as a helper to access :dbcommand:`hostInfo`. The output of
   :dbcommand:`hostInfo` on a Linux system will resemble the
   following:

   .. code-block:: javascript

      {
         "system" : {
                "currentTime" : ISODate("<timestamp>"),
                "hostname" : "<hostname>",
                "cpuAddrSize" : <number>,
                "memSizeMB" : <number>,
                "numCores" : <number>,
                "cpuArch" : "<identifier>",
                "numaEnabled" : <boolean>
         },
         "os" : {
                "type" : "<string>",
                "name" : "<string>",
                "version" : "<string>"
         },
         "extra" : {
                "versionString" : "<string>",
                "libcVersion" : "<string>",
                "kernelVersion" : "<string>",
                "cpuFrequencyMHz" : "<string>",
                "cpuFeatures" : "<string>",
                "pageSize" : <number>,
                "numPages" : <number>,
                "maxOpenFiles" : <number>
         },
         "ok" : <return>
      }

   Consider the following documentation of these fields:

   .. data:: hostInfo

      The document returned by the :dbcommand:`hostInfo`.

   .. data:: hostInfo.system

      A sub-document about the underlying environment of the system
      running the :program:`mongod` or :program:`mongos`

   .. data:: hostInfo.system.currentTime

      A time stamp of the current system time.

   .. data:: hostInfo.system.hostname

      The system name, which should correspond to the output of
      ``hostname -f`` on Linux systems.

   .. data:: hostInfo.system.cpuAddrSize

      A number reflecting the architecture of the system. Either
      ``32`` or ``64``.

   .. data:: hostInfo.system.memSizeMB

      The total amount of system memory (RAM) in megabytes.

   .. data:: hostInfo.system.numCores

      The total number of available logical processor cores.

   .. data:: hostInfo.system.cpuArch

      A string that represents the system architecture. Either ``x86``
      or ``x86_64``.

   .. data:: hostInfo.system.numaEnabled

      A boolean value. ``false`` if NUMA is interleaved (i.e. disabled),
      otherwise ``true``.

   .. data:: hostInfo.os

      A sub-document that contains information about the operating
      system running the :program:`mongod` and :program:`mongos`.

   .. data:: hostInfo.os.type

      A string representing the type of operating system, such as
      ``Linux`` or ``Windows``.

   .. data:: hostInfo.os.name

      If available, returns a display name for the operating
      system.

   .. data:: hostInfo.os.version

      If available, returns the name of the distribution or operating
      system.

   .. data:: hostInfo.extra

      A sub-document with extra information about the operating system
      and the underlying hardware. The content of the
      :data:`~hostInfo.extra` sub-document depends on the operating
      system.

   .. data:: hostInfo.extra.versionString

      A complete string of the operating system version and
      identification. On Linux and OS X systems, this contains output
      similar to ``uname -a``.

   .. data:: hostInfo.extra.libcVersion

      The release of the system ``libc``.

      :data:`~hostInfo.extra.libcVersion` only appears on Linux
      systems.

   .. data:: hostInfo.extra.kernelVersion

      The release of the Linux kernel in current use.

      :data:`~hostInfo.extra.kernelVersion` only appears on Linux
      systems.

   .. data:: hostInfo.extra.alwaysFullSync

      :data:`~hostInfo.extra.alwaysFullSync` only appears on OS X
      systems.

   .. data:: hostInfo.extra.nfsAsync

      :data:`~hostInfo.extra.nfsAsync` only appears on OS X
      systems.

   .. data:: hostInfo.extra.cpuFrequencyMHz

      Reports the clock speed of the system's processor in megahertz.

   .. data:: hostInfo.extra.cpuFeatures

      Reports the processor feature flags. On Linux systems this the
      same information that ``/proc/cpuinfo`` includes in the
      ``flags`` fields.

   .. data:: hostInfo.extra.pageSize

      Reports the default system page size in bytes.

   .. data:: hostInfo.extra.numPages

      :data:`~hostInfo.extra.numPages` only appears on Linux systems.

   .. data:: hostInfo.extra.maxOpenFiles

      Reports the current system limits on open file handles. See
      :doc:`/reference/ulimit` for more information.

      :data:`~hostInfo.extra.maxOpenFiles` only appears on Linux
      systems.

   .. data:: hostInfo.extra.scheduler

      Reports the active I/O scheduler.
      :data:`~hostInfo.extra.scheduler` only appears on OS
      X systems.


   .. admin-only
==========
indexStats
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: indexStats

   The :dbcommand:`indexStats` command aggregates statistics
   for the B-tree data structure that stores data for a MongoDB index.

   .. warning:: This command is not intended for production deployments.

   The command can be run *only* on a :program:`mongod` instance that uses
   the ``--enableExperimentalIndexStatsCmd`` option.

   To aggregate statistics, issue the command like so:

   .. code-block:: javascript

      db.runCommand( { indexStats: "<collection>", index: "<index name>" } )

Output
------

The :method:`db.collection.indexStats()` method and equivalent
:dbcommand:`indexStats` command aggregate statistics for the B-tree data
structure that stores data for a MongoDB index. The commands aggregate
statistics firstly for the entire B-tree and secondly for each
individual level of the B-tree. The output displays the following
values.

.. data:: indexStats.index

   The :ref:`index name <index-names>`.

.. data:: indexStats.version

   The index version. For more information on index version numbers, see
   the ``v`` option in :method:`db.collection.ensureIndex()`.

.. data:: indexStats.isIdIndex

   If ``true``, the index is the default ``_id`` index for the collection.

.. data:: indexStats.keyPattern

   The indexed keys.

.. data:: indexStats.storageNs

   The namespace of the index's underlying storage.

.. data:: indexStats.bucketBodyBytes

   The fixed size, in bytes, of a B-tree bucket in the index, not
   including the record header. All indexes for a given version have the
   same value for this field. MongoDB allocates fixed size buckets on disk.

.. data:: indexStats.depth

   The number of levels in the B-tree, not including the root level.

.. data:: indexStats.overall

   This section of the output displays statistics for the entire B-tree.

   .. data:: indexStats.overall.numBuckets

      The number of buckets in the entire B-tree, including all levels.

   .. data:: indexStats.overall.keyCount

      Statistics about the number of keys in a bucket, evaluated on a
      per-bucket level.

   .. data:: indexStats.overall.usedKeyCount

      Statistics about the number of used keys in a bucket, evaluated on
      a per-bucket level. Used keys are keys not marked as deleted.

   .. data:: indexStats.overall.bsonRatio

      Statistics about the percentage of the bucket body that is
      occupied by the key objects themselves, excluding associated
      metadata.

      For example, if you have the document ``{ name: "Bob Smith" }``
      and an index on ``{ name: 1 }``, the key object is the string
      ``Bob Smith``.

   .. data:: indexStats.overall.keyNodeRatio

      Statistics about the percentage of the bucket body that is
      occupied by the key node objects (the metadata and links
      pertaining to the keys). This does not include the key itself. In
      the current implementation, a key node's objects consist of: the
      pointer to the key data (in the same bucket), the pointer to the
      record the key is for, and the pointer to a child bucket.

   .. data:: indexStats.overall.fillRatio

      The sum of the :data:`bsonRatio <indexStats.overall.bsonRatio>`
      and the :data:`keyNodeRatio <indexStats.overall.keyNodeRatio>`.
      This shows how full the buckets are. This will be much higher for
      indexes with sequential inserts.

.. data:: indexStats.perLevel

   This section of the output displays statistics for each level of the
   B-tree separately, starting with the root level. This section
   displays a different document for each B-tree level.

   .. data:: indexStats.perLevel.numBuckets

      The number of buckets at this level of the B-tree.

   .. data:: indexStats.perLevel.keyCount

      Statistics about the number of keys in a bucket, evaluated on a
      per-bucket level.

   .. data:: indexStats.perLevel.usedKeyCount

      Statistics about the number of used keys in a bucket, evaluated on
      a per-bucket level. Used keys are keys not marked as deleted.

   .. data:: indexStats.perLevel.bsonRatio

      Statistics about the percentage of the bucket body that is
      occupied by the key objects themselves, excluding associated
      metadata.

   .. data:: indexStats.perLevel.keyNodeRatio

      Statistics about the percentage of the bucket body that is
      occupied by the key node objects (the metadata and links
      pertaining to the keys).

   .. data:: indexStats.perLevel.fillRatio

      The sum of the :data:`bsonRatio <indexStats.perLevel.bsonRatio>`
      and the :data:`keyNodeRatio <indexStats.perLevel.keyNodeRatio>`.
      This shows how full the buckets are. This will be much higher in
      the following cases:

      - For indexes with sequential inserts, such as the ``_id`` index
        when using ObjectId keys.

      - For indexes that were recently built in the foreground with
        existing data.

      - If you recently ran :dbcommand:`compact` or :option:`--repair
        <mongod --repair>`.

Example
-------

The following is an example of :method:`db.collection.indexStats()` and
:dbcommand:`indexStats` output.

.. code-block:: javascript

   {
       "index" : "type_1_traits_1",
       "version" : 1,
       "isIdIndex" : false,
       "keyPattern" : {
           "type" : 1,
           "traits" : 1
       },
       "storageNs" : "test.animals.$type_1_traits_1",
       "bucketBodyBytes" : 8154,
       "depth" : 2,
       "overall" : {
           "numBuckets" : 45513,
           "keyCount" : {
               "count" : NumberLong(45513),
               "mean" : 253.89602970579836,
               "stddev" : 21.784799875240708,
               "min" : 52,
               "max" : 290,
               "quantiles" : {
                   "0.01" : 201.99785091648775,
                   // ...
                   "0.99" : 289.9999655156967
               }
           },
           "usedKeyCount" : {
               "count" : NumberLong(45513),
               // ...
               "quantiles" : {
                   "0.01" : 201.99785091648775,
                   // ...
                   "0.99" : 289.9999655156967
               }
           },
           "bsonRatio" : {
               "count" : NumberLong(45513),
               // ...
               "quantiles" : {
                   "0.01" : 0.4267797891997124,
                   // ...
                   "0.99" : 0.5945548174629648
               }
           },
           "keyNodeRatio" : {
               "count" : NumberLong(45513),
               // ...
               "quantiles" : {
                   "0.01" : 0.3963656628236211,
                   // ...
                   "0.99" : 0.5690457993930765
               }
           },
           "fillRatio" : {
               "count" : NumberLong(45513),
               // ...
               "quantiles" : {
                   "0.01" : 0.9909134214926929,
                   // ...
                   "0.99" : 0.9960755457453732
               }
           }
       },
       "perLevel" : [
           {
               "numBuckets" : 1,
               "keyCount" : {
                   "count" : NumberLong(1),
                   "mean" : 180,
                   "stddev" : 0,
                   "min" : 180,
                   "max" : 180
               },
               "usedKeyCount" : {
                   "count" : NumberLong(1),
                   // ...
                   "max" : 180
               },
               "bsonRatio" : {
                   "count" : NumberLong(1),
                   // ...
                   "max" : 0.3619082658817758
               },
               "keyNodeRatio" : {
                   "count" : NumberLong(1),
                   // ...
                   "max" : 0.35320088300220753
               },
               "fillRatio" : {
                   "count" : NumberLong(1),
                   // ...
                   "max" : 0.7151091488839834
               }
           },
           {
               "numBuckets" : 180,
               "keyCount" : {
                   "count" : NumberLong(180),
                   "mean" : 250.84444444444443,
                   "stddev" : 26.30057503009355,
                   "min" : 52,
                   "max" : 290
               },
               "usedKeyCount" : {
                   "count" : NumberLong(180),
                   // ...
                   "max" : 290
               },
               "bsonRatio" : {
                   "count" : NumberLong(180),
                   // ...
                   "max" : 0.5945548197203826
               },
               "keyNodeRatio" : {
                   "count" : NumberLong(180),
                   // ...
                   "max" : 0.5690458670591121
               },
               "fillRatio" : {
                   "count" : NumberLong(180),
                   // ...
                   "max" : 0.9963208241353937
               }
           },
           {
               "numBuckets" : 45332,
               "keyCount" : {
                   "count" : NumberLong(45332),
                   "mean" : 253.90977675813994,
                   "stddev" : 21.761620836279018,
                   "min" : 167,
                   "max" : 290,
                   "quantiles" : {
                       "0.01" : 202.0000012563603,
                       // ...
                       "0.99" : 289.99996486571894
                   }
               },
               "usedKeyCount" : {
                   "count" : NumberLong(45332),
                   // ...
                   "quantiles" : {
                       "0.01" : 202.0000012563603,
                       // ...
                       "0.99" : 289.99996486571894
                   }
               },
               "bsonRatio" : {
                   "count" : NumberLong(45332),
                   // ...
                   "quantiles" : {
                       "0.01" : 0.42678446958950583,
                       // ...
                       "0.99" : 0.5945548175411283
                   }
               },
               "keyNodeRatio" : {
                   "count" : NumberLong(45332),
                   // ...
                   "quantiles" : {
                       "0.01" : 0.39636988227885306,
                       // ...
                       "0.99" : 0.5690457981176729
                   }
               },
               "fillRatio" : {
                   "count" : NumberLong(45332),
                   // ...
                   "quantiles" : {
                       "0.01" : 0.9909246995605362,
                       // ...
                       "0.99" : 0.996075546919481
                   }
               }
           }
       ],
       "ok" : 1
   }

Additional Resources
--------------------

For more information on the command's limits and output, see the following:

- The equivalent :method:`db.collection.indexStats()` method,

- :doc:`/reference/command/indexStats`, and

- `https://github.com/mongodb-labs/storage-viz#readme <https://github.com/mongodb-labs/storage-viz#readme>`_.
======
insert
======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: insert

   .. versionadded:: 2.6

   The :dbcommand:`insert` command inserts one or more documents and
   returns a document containing the status of all inserts. The insert
   methods provided by the MongoDB drivers use this command internally.

   The command has the following syntax:

   .. code-block:: javascript

      {
         insert: <collection>,
         documents: [ <document>, <document>, <document>, ... ],
         ordered: <boolean>,
         writeConcern: { <write concern> }
      }

   The :dbcommand:`insert` command takes the following fields:

   .. include:: insert-field.rst

   :return:

      A document that contains the status of the operation.
      See :ref:`insert-command-output` for details.

Behavior
--------

The total size of all the ``documents`` array elements must be less
than or equal to the :limit:`maximum BSON document size
<BSON Document Size>`.

The total number of documents in the ``documents`` array must be less
than or equal to the :limit:`maximum bulk size <Bulk Operation Size>`.

Examples
--------

Insert a Single Document
~~~~~~~~~~~~~~~~~~~~~~~~

Insert a document into the ``users`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         insert: "users",
         documents: [ { _id: 1, user: "abc123", status: "A" } ]
      }
   )

The returned document shows that the command successfully inserted a
document. See :ref:`insert-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 1 }

Bulk Insert
~~~~~~~~~~~

Insert three documents into the ``users`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         insert: "users",
         documents: [
            { _id: 2, user: "ijk123", status: "A" },
            { _id: 3, user: "xyz123", status: "P" },
            { _id: 4, user: "mop123", status: "P" }
         ],
         ordered: false,
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

The returned document shows that the command successfully inserted the
three documents. See :ref:`insert-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "n" : 3 }

.. _insert-command-output:

Output
------

The returned document contains a subset of the following fields:

.. data:: insert.ok

   The status of the command.

.. data:: insert.n

   The number of documents inserted.

.. data:: insert.writeErrors

   An array of documents that contains information regarding any error
   encountered during the insert operation. The
   :data:`~insert.writeErrors` array contains an error document for
   each insert that errors.

   Each error document contains the following fields:

   .. data:: insert.writeErrors.index

      An integer that identifies the document in the
      ``documents`` array, which uses a zero-based index.

   .. data:: insert.writeErrors.code

      An integer value identifying the error.

   .. data:: insert.writeErrors.errmsg

      A description of the error.

.. data:: insert.writeConcernError

   Document that describe error related to write concern and contains
   the field:

   .. data:: insert.writeConcernError.code

      An integer value identifying the cause of the write concern error.

   .. data:: insert.writeConcernError.errmsg

      A description of the cause of the write concern error.

The following is an example document returned for a successful
:dbcommand:`insert` of a single document:

.. code-block:: javascript

   { ok: 1, n: 1 }

The following is an example document returned for an
:dbcommand:`insert` of two documents that successfully inserted one
document but encountered an error with the other document:

.. code-block:: javascript

   {
      "ok" : 1,
      "n" : 1,
      "writeErrors" : [
         {
            "index" : 1,
            "code" : 11000,
            "errmsg" : "insertDocument :: caused by :: 11000 E11000 duplicate key error index: test.users.$_id_  dup key: { : 1.0 }"
         }
      ]
   }
===================
invalidateUserCache
===================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: invalidateUserCache

   .. versionadded:: 2.6

   Flushes user information from in-memory cache, including removal of each user's
   credentials and roles. This allows you to purge the cache
   at any given moment, regardless of the
   interval set in the :parameter:`userCacheInvalidationIntervalSecs` parameter.

   :dbcommand:`invalidateUserCache` has the following syntax:

   .. code-block:: javascript

      db.runCommand( { invalidateUserCache: 1 } )

Required Access
---------------

You must have privileges that include the
:authaction:`invalidateUserCache` action on the cluster resource in order
to use this command.
========
isdbgrid
========

.. default-domain:: mongodb

.. dbcommand:: isdbgrid

   This command verifies that a process is a :program:`mongos`.

   If you issue the :dbcommand:`isdbgrid` command when connected to a
   :program:`mongos`, the response document includes the ``isdbgrid``
   field set to ``1``. The returned document is similar to the
   following:

   .. code-block:: javascript

      { "isdbgrid" : 1, "hostname" : "app.example.net", "ok" : 1 }

   If you issue the :dbcommand:`isdbgrid` command when connected to a
   :program:`mongod`, MongoDB returns an error document. The
   :dbcommand:`isdbgrid` command is not available to :program:`mongod`.
   The error document, however, also includes a line that reads
   ``"isdbgrid" : 1``, just as in the document returned for a
   :program:`mongos`. The error document is similar to the following:

   .. code-block:: javascript

      {
         "errmsg" : "no such cmd: isdbgrid",
         "bad cmd" : {
               "isdbgrid" : 1
         },
         "ok" : 0
      }

   You can instead use the :dbcommand:`isMaster` command to determine
   connection to a :program:`mongos`. When connected to a
   :program:`mongos`, the :dbcommand:`isMaster` command returns a document that
   contains the string ``isdbgrid`` in the ``msg`` field.
========
isMaster
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: isMaster

   :dbcommand:`isMaster` returns a document that describes the role of
   the :program:`mongod` instance.

   If the instance is a member of a
   replica set, then :dbcommand:`isMaster` returns a subset of the
   replica set configuration and status including whether or not the
   instance is the :term:`primary` of the replica set.

   When sent to a :program:`mongod` instance that is not a member of a
   replica set, :dbcommand:`isMaster` returns a subset of this
   information.

   MongoDB :term:`drivers <driver>` and :term:`clients <client>` use
   :dbcommand:`isMaster` to determine the state of the replica set
   members and to discover additional members of a :term:`replica
   set`.

   The :method:`db.isMaster()` method in the :program:`mongo` shell
   provides a wrapper around :dbcommand:`isMaster`.

   The command takes the following form:

   .. code-block:: javascript

      { isMaster: 1 }

.. seealso::

   :method:`db.isMaster()`

Output
------

All Instances
~~~~~~~~~~~~~

The following :dbcommand:`isMaster` fields are common across all
roles:

.. data:: isMaster.ismaster

   A boolean value that reports when this node is writable. If
   ``true``, then this instance is a :term:`primary` in a
   :term:`replica set`, or a :term:`master` in a master-slave
   configuration, or a :program:`mongos` instance, or a standalone
   :program:`mongod`.

   This field will be ``false`` if the instance is a
   :term:`secondary` member of a replica set or if the member is an
   :term:`arbiter` of a replica set.

.. data:: isMaster.maxBsonObjectSize

   The maximum permitted size of a :term:`BSON` object in bytes for
   this :program:`mongod` process. If not provided, clients should
   assume a max size of "``16 * 1024 * 1024``".

.. data:: isMaster.maxMessageSizeBytes

   .. versionadded:: 2.4

   The maximum permitted size of a :term:`BSON` wire protocol message.
   The default value is ``48000000`` bytes.

.. data:: isMaster.localTime

   .. versionadded:: 2.2

   Returns the local server time in UTC. This value is an
   :term:`ISO date <ISODate>`.

.. data:: isMaster.minWireVersion

   .. versionadded:: 2.6

   The earliest version of the wire protocol that this
   :program:`mongod` or :program:`mongos` instance is capable of using
   to communicate with clients.

   Clients may use :data:`~isMaster.minWireVersion` to help negotiate
   compatibility with MongoDB.

.. data:: isMaster.maxWireVersion

   .. versionadded:: 2.6

   The latest version of the wire protocol that this :program:`mongod`
   or :program:`mongos` instance is capable of using to communicate
   with clients.

   Clients may use :data:`~isMaster.maxWireVersion` to help negotiate
   compatibility with MongoDB.

Sharded Instances
~~~~~~~~~~~~~~~~~

:program:`mongos` instances add the following field to the
:dbcommand:`isMaster` response document:

.. data:: isMaster.msg

   Contains the value ``isdbgrid`` when :dbcommand:`isMaster`
   returns from a :program:`mongos` instance.

Replica Sets
~~~~~~~~~~~~

:dbcommand:`isMaster` contains these fields when returned by a member
of a replica set:

.. data:: isMaster.setName

   The name of the current :replica set.

.. data:: isMaster.secondary

   A boolean value that, when ``true``, indicates if the
   :program:`mongod` is a :term:`secondary` member of a :term:`replica
   set`.

.. data:: isMaster.hosts

   An array of strings in the format of ``"[hostname]:[port]"`` that
   lists all members of the :term:`replica set` that are neither
   :term:`hidden <hidden member>`, :term:`passive <passive member>`,
   nor :term:`arbiters <arbiter>`.

   Drivers use this array and the :data:`isMaster.passives` to determine
   which members to read from.

.. data:: isMaster.passives

   An array of strings in the format of ``"[hostname]:[port]"``
   listing all members of the :term:`replica set` which have a
   :data:`~local.system.replset.members[n].priority` of ``0``.

   This field only appears if there is at least one member with a
   :data:`~local.system.replset.members[n].priority` of ``0``.

   Drivers use this array and the :data:`isMaster.hosts` to determine
   which members to read from.

.. data:: isMaster.arbiters

   An array of strings  in the format of ``"[hostname]:[port]"``
   listing all members of the :term:`replica set` that are
   :term:`arbiters <arbiter>`.

   This field only appears if there is at least one arbiter in the
   replica set.

.. data:: isMaster.primary

   A string in the format of ``"[hostname]:[port]"`` listing the
   current :term:`primary` member of the replica set.

.. data:: isMaster.arbiterOnly

   A boolean value that , when ``true``, indicates that the current
   instance is an :term:`arbiter`.  The :data:`~isMaster.arbiterOnly`
   field is only present, if the instance is an arbiter.

.. data:: isMaster.passive

   A boolean value that, when ``true``, indicates that the current
   instance is :term:`hidden <hidden member>`.  The
   :data:`~isMaster.passive` field is only present for hidden members.

.. data:: isMaster.hidden

   A boolean value that, when ``true``, indicates that the current
   instance is :term:`hidden <hidden member>`.  The
   :data:`~isMaster.hidden` field is only present for hidden members.

.. data:: isMaster.tags

   A document that lists any tags assigned to this member. This field
   is only present if there are tags assigned to the member. See
   :doc:`/tutorial/configure-replica-set-tag-sets` for more
   information.

.. data:: isMaster.me

   The ``[hostname]:[port]`` of the member that returned
   :dbcommand:`isMaster`.
======
isSelf
======

.. default-domain:: mongodb

.. dbcommand:: _isSelf

   :dbcommand:`_isSelf` is an internal command.

   .. slave-ok
==================
journalLatencyTest
==================

.. default-domain:: mongodb

.. dbcommand:: journalLatencyTest

   :dbcommand:`journalLatencyTest` is an administrative command that tests the
   length of time required to write and perform a file system sync
   (e.g. :term:`fsync`) for a file in the journal directory.
   You must issue the
   :dbcommand:`journalLatencyTest`
   command against the :term:`admin database` in the form:

   .. code-block:: javascript

      { journalLatencyTest: 1 }

   The value (i.e. ``1`` above), does not affect the operation of the
   command.

   .. |dbcommand| replace:: :dbcommand:`journalLatencyTest`
   .. include:: /includes/note-enabletestcommands.rst
============
listCommands
============

.. default-domain:: mongodb

.. dbcommand:: listCommands

   The :dbcommand:`listCommands` command generates a list of all
   database commands implemented for the current :program:`mongod`
   instance.

   .. slave-ok
=============
listDatabases
=============

.. default-domain:: mongodb

.. dbcommand:: listDatabases

   The :dbcommand:`listDatabases` command provides a list of existing
   databases along with basic statistics about them:

   .. code-block:: javascript

      { listDatabases: 1 }

   The value (e.g. ``1``) does not affect the output of the
   command. :dbcommand:`listDatabases` returns a document for each
   database. Each document contains a ``name`` field with the database
   name, a ``sizeOnDisk`` field with the total size of the database
   file on disk in bytes, and an ``empty`` field specifying whether
   the database has any data.

.. example::

   The following operation returns a list of all databases:

   .. code-block:: javascript

      db.runCommand( { listDatabases: 1 } )

.. seealso:: :doc:`/tutorial/use-database-commands`.
==========
listShards
==========

.. default-domain:: mongodb

.. dbcommand:: listShards

   Use the :dbcommand:`listShards` command to return a list of
   configured shards. The command takes the following form:

   .. code-block:: javascript

        { listShards: 1 }
=====================
logApplicationMessage
=====================

.. default-domain:: mongodb

.. dbcommand:: logApplicationMessage

   The :dbcommand:`logApplicationMessage` command allows users to post
   a custom message to the audit log. If running with authorization,
   users must have :authrole:`clusterAdmin` role, or roles that inherit
   from :authrole:`clusterAdmin`, to run the command.

   .. include:: /includes/note-audit-in-enterprise-only.rst

   The :dbcommand:`logApplicationMessage` has the following syntax:

   .. code-block:: javascript

      { logApplicationMessage: <string> }

   MongoDB associates these custom messages with the :ref:`audit
   operation <audit-action-details-results>` ``applicationMessage``,
   and the messages are subject to any :ref:`filtering <audit-filter>`.
======
logout
======

.. default-domain:: mongodb

.. dbcommand:: logout

   The :dbcommand:`logout` command terminates the current
   authenticated session:

   .. code-block:: javascript

      { logout: 1 }

   .. |operation-name| replace:: :dbcommand:`logout`
   .. include:: /includes/note-logout-namespace.rst

   .. example::

      .. include:: /includes/fact-change-database-context.rst

      When you have set the database context and ``db`` object, you
      can use the |operation-name| to log out of database as in the
      following operation:

      .. code-block:: javascript

         db.runCommand( { logout: 1 } )
=========
logRotate
=========

.. default-domain:: mongodb

.. dbcommand:: logRotate

   The :dbcommand:`logRotate` command is an administrative command that
   allows you to rotate
   the MongoDB logs to prevent a single logfile from consuming too
   much disk space.
   You must issue the
   :dbcommand:`logRotate`
   command against the :term:`admin database` in the form:

   .. code-block:: javascript

        { logRotate: 1 }

   .. note::

      Your :program:`mongod` instance needs to be running with the
      :option:`--logpath [file] <mongod --logpath>` option.

   You may also rotate the logs by sending a ``SIGUSR1`` signal to the
   :program:`mongod` process.
   If your :program:`mongod` has a process ID of 2200, here's how to
   send the signal on Linux:

   .. code-block:: sh

       kill -SIGUSR1 2200

   :dbcommand:`logRotate` renames the existing log file by
   appending the current timestamp to the filename.  The appended
   timestamp has the following form:

   .. code-block:: none

      <YYYY>-<mm>-<DD>T<HH>-<MM>-<SS>

   Then :dbcommand:`logRotate` creates a new log file with the same
   name as originally specified by the :setting:`systemLog.path` setting to
   :program:`mongod` or :program:`mongos`.

   .. note::

      .. versionadded:: 2.0.3
         The :dbcommand:`logRotate` command is available to
         :program:`mongod` instances running on Windows systems with
         MongoDB release 2.0.3 and higher.

.. :error:`16175` is thrown if logRotate fails
=======================
mapreduce.shardedfinish
=======================

.. default-domain:: mongodb

.. dbcommand:: mapreduce.shardedfinish

   Provides internal functionality to support :term:`map-reduce` in
   :term:`sharded <sharded cluster>` environments.

   .. seealso:: ":dbcommand:`mapReduce`"

   .. slave-ok
=========
mapReduce
=========

.. default-domain:: mongodb

.. dbcommand:: mapReduce

   The :dbcommand:`mapReduce` command allows you to run
   :term:`map-reduce` aggregation operations over a collection. The
   :dbcommand:`mapReduce` command has the following prototype
   form:

   .. code-block:: javascript

      db.runCommand(
                     {
                       mapReduce: <collection>,
                       map: <function>,
                       reduce: <function>,
                       out: <output>,
                       query: <document>,
                       sort: <document>,
                       limit: <number>,
                       finalize: <function>,
                       scope: <document>,
                       jsMode: <boolean>,
                       verbose: <boolean>
                     }
                   )

   Pass the name of the collection to the ``mapReduce`` command
   (i.e. ``<collection>``) to use as the source documents to perform
   the map reduce operation. The command also accepts the following
   parameters:

   .. include:: /reference/command/mapReduce-field.rst

   The following is a prototype usage of the :dbcommand:`mapReduce`
   command:

   .. code-block:: javascript

      var mapFunction = function() { ... };
      var reduceFunction = function(key, values) { ... };

      db.runCommand(
                     {
                       mapReduce: 'orders',
                       map: mapFunction,
                       reduce: reduceFunction,
                       out: { merge: 'map_reduce_results', db: 'test' },
                       query: { ord_date: { $gt: new Date('01/01/2012') } }
                     }
                   )

   .. |javascript-using-operation| replace:: :dbcommand:`mapReduce` uses
   .. include:: /includes/admonition-javascript-prevalence.rst

.. note::

   .. versionchanged:: 2.4

   .. include:: /includes/fact-group-map-reduce-where-limitations-in-24.rst

.. _mapreduce-map-cmd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-map
   :end-before: end-map

.. _mapreduce-reduce-cmd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-reduce
   :end-before: end-reduce

.. _mapreduce-out-cmd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-out
   :end-before: end-out

.. _mapreduce-finalize-cmd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-finalize
   :end-before: end-finalize

Examples
--------

In the :program:`mongo` shell, the :method:`db.collection.mapReduce()`
method is a wrapper around the :dbcommand:`mapReduce` command. The
following examples use the :method:`db.collection.mapReduce()` method:

.. include:: /includes/examples-map-reduce.rst
   :start-after: map-reduce-document-prototype-begin

For more information and examples, see the
:doc:`Map-Reduce </core/map-reduce>` page and
:doc:`/tutorial/perform-incremental-map-reduce`.

.. seealso::

   - :doc:`/tutorial/troubleshoot-map-function`

   - :doc:`/tutorial/troubleshoot-reduce-function`

   - :method:`db.collection.mapReduce()`

   - :doc:`/core/aggregation`

.. slave-ok
=========
medianKey
=========

.. default-domain:: mongodb

.. dbcommand:: medianKey

   :dbcommand:`medianKey` is an internal command.

   .. slave-ok, read-lock
===========
mergeChunks
===========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: mergeChunks

   For a sharded collection, :dbcommand:`mergeChunks` combines two
   contiguous :term:`chunk` ranges the same shard into a single
   chunk. At least one of chunk must not have any documents. Issue the
   :dbcommand:`mergeChunks` command from a :program:`mongos` instance.

   :dbcommand:`mergeChunks` has the following form:

   .. code-block:: javascript

      db.runCommand( { mergeChunks : <namespace> ,
                       bounds : [ { <shardKeyField>: <minFieldValue> },
                                  { <shardKeyField>: <maxFieldValue> } ] } )

   For compound shard keys, you must include the full shard key in the
   ``bounds`` specification. If the shard key is ``{ x: 1, y: 1 }``,
   :dbcommand:`mergeChunks` has the following form:

   .. code-block:: javascript

      db.runCommand( { mergeChunks : <namespace> ,
                       bounds : [ { x: <minValue>, y: <minValue> },
                                  { x: <minValue>, y: <maxValue> } ] } )

   The :dbcommand:`mergeChunks` command has the following fields:

   .. include:: /reference/command/mergeChunks-field.rst

Behavior
--------

.. note::

   Use the :dbcommand:`mergeChunks` only in special circumstances
   such as cleaning up your :term:`sharded cluster` after removing
   many documents.

In order to successfully merge chunks, the following *must* be true

- In the ``bounds`` field, ``<minkey>`` and ``<maxkey>`` must correspond to
  the lower and upper bounds of the :term:`chunks <chunk>` to merge.

- The two chunks must reside on the same shard.

- The two chunks must be contiguous.

- One or both chunks must be empty.

:dbcommand:`mergeChunks` returns an error if these conditions are not
satisfied.

Return Messages
---------------

On success, :dbcommand:`mergeChunks` returns to following document:

.. code-block:: none

   { "ok" : 1 }

Another Operation in Progress
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:dbcommand:`mergeChunks` returns the following error message if another
metadata operation is in progress on the :data:`~config.chunks` collection:

.. code-block:: none

   errmsg: "The collection's metadata lock is already taken."

If another process, such as balancer process, changes metadata while
:dbcommand:`mergeChunks` is running, you may see this error. You can
retry the :dbcommand:`mergeChunks` operation without side effects.

Chunks on Different Shards
~~~~~~~~~~~~~~~~~~~~~~~~~~

If the input :term:`chunks <chunk>` are not on the same :term:`shard`,
:dbcommand:`mergeChunks` returns an error similar to the following:

.. code-block:: javascript

   {
      "ok" : 0,
      "errmsg" : "could not merge chunks, collection test.users does not contain a chunk ending at { username: \"user63169\" }"
   }

Noncontiguous Chunks
~~~~~~~~~~~~~~~~~~~~

If the input :term:`chunks <chunk>` are not contiguous,
:dbcommand:`mergeChunks` returns an error similar to the following:

.. code-block:: javascript

   {
      "ok" : 0,
      "errmsg" : "could not merge chunks, collection test.users has more than 2 chunks between [{ username: \"user29937\" }, { username: \"user49877\" })"
   }

Documents in Both Chunks
~~~~~~~~~~~~~~~~~~~~~~~~

If neither input :term:`chunk` is empty, :dbcommand:`mergeChunks`
returns an error similar to the following:

.. code-block:: javascript

   {
      "ok" : 0,
      "errmsg" : "could not merge chunks, collection test.users has more than one non-empty chunk between [{ username: \"user36583\" }, { username: \"user49877\" })"
   }

.. admin-only
============
migrateClone
============

.. default-domain:: mongodb

.. dbcommand:: _migrateClone

   :dbcommand:`_migrateClone` is an internal command. Do not call
   directly.

   .. admin-only
=========
moveChunk
=========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: moveChunk

   Internal administrative command. Moves :term:`chunks <chunk>` between
   :term:`shards <shard>`. Issue the :dbcommand:`moveChunk` command via
   a :program:`mongos` instance while using the :term:`admin database`.
   Use the following forms:

   .. code-block:: javascript

      db.runCommand( { moveChunk : <namespace> ,
                       find : <query> ,
                       to : <string>,
                       _secondaryThrottle : <boolean>,
                       _waitForDelete : <boolean> } )

   Alternately:

   .. code-block:: javascript

      db.runCommand( { moveChunk : <namespace> ,
                       bounds : <array> ,
                       to : <string>,
                       _secondaryThrottle : <boolean>,
                       _waitForDelete : <boolean> } )

   The :dbcommand:`moveChunk` command has the following fields:

   .. include:: /reference/command/moveChunk-field.rst

   The value of ``bounds`` takes the form:

   .. code-block:: javascript

      [ { hashedField : <minValue> } ,
        { hashedField : <maxValue> } ]

   The :ref:`chunk migration <sharding-chunk-migration>` section
   describes how chunks move between shards on MongoDB.

.. seealso:: :dbcommand:`split`, :method:`sh.moveChunk()`,
   :method:`sh.splitAt()`, and :method:`sh.splitFind()`.

Return Messages
---------------

:dbcommand:`moveChunk` returns the following error message if another
metadata operation is in progress on the :data:`~config.chunks` collection:

.. code-block:: none

   errmsg: "The collection's metadata lock is already taken."

If another process, such as a balancer process, changes meta data
while :dbcommand:`moveChunk` is running, you may see this
error. You may retry the :dbcommand:`moveChunk` operation without
side effects.

.. note::

   Only use the :dbcommand:`moveChunk` in special circumstances
   such as preparing your :term:`sharded cluster` for an initial
   ingestion of data, or a large bulk import operation.
   In most cases allow the balancer to create and balance chunks
   in sharded clusters.
   See
   :doc:`/tutorial/create-chunks-in-sharded-cluster` for more information.

.. admin-only
===========
movePrimary
===========

.. default-domain:: mongodb

.. dbcommand:: movePrimary

   In a :term:`sharded cluster`, this command reassigns the database's
   :term:`primary shard`, which holds all un-sharded collections in
   the database. :dbcommand:`movePrimary` is an administrative command
   that is only available for :program:`mongos` instances. Only use
   :dbcommand:`movePrimary` when removing a shard from a sharded
   cluster.

   .. important:: Only use :dbcommand:`movePrimary` when:

      - the database does not contain any collections with data, *or*

      - you have drained all sharded collections using the
        :dbcommand:`removeShard` command.

      See :doc:`/tutorial/remove-shards-from-cluster` for a complete
      procedure.

   :dbcommand:`movePrimary` changes the primary shard for this
   database in the cluster metadata, and migrates all un-sharded
   collections to the specified shard. Use the command with the
   following form:

   .. code-block:: javascript

      { movePrimary : "test", to : "shard0001" }

   When the command returns, the database's primary location will
   shift to the designated :term:`shard`. To fully decommission a
   shard, use the :dbcommand:`removeShard` command.
=======================
Administration Commands
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-administration.rst

.. include:: /includes/toc/command-administration.rst
====================
Aggregation Commands
====================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-aggregation.rst

.. include:: /includes/toc/command-aggregation.rst

For a detailed comparison of the different approaches, see
:doc:`/reference/aggregation-commands-comparison`.
===============================
System Events Auditing Commands
===============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-audit.rst

.. include:: /includes/toc/command-audit.rst
=======================
Authentication Commands
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-authentication.rst

.. include:: /includes/toc/command-authentication.rst
==================================
Query and Write Operation Commands
==================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-crud.rst

.. include:: /includes/toc/command-crud.rst
===================
Diagnostic Commands
===================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-diagnostic.rst

.. include:: /includes/toc/command-diagnostic.rst
===================
Geospatial Commands
===================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-geospatial.rst

.. include:: /includes/toc/command-geospatial.rst
=================
Internal Commands
=================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-internal.rst

.. include:: /includes/toc/command-internal.rst
=========================
Query Plan Cache Commands
=========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-plan-cache.rst

.. include:: /includes/toc/command-plan-cache.rst
====================
Replication Commands
====================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-replication.rst

.. include:: /includes/toc/command-replication.rst
========================
Role Management Commands
========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-role-management.rst

.. include:: /includes/toc/command-role-management.rst
=================
Sharding Commands
=================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-sharding.rst

.. include:: /includes/toc/command-sharding.rst
================
Testing Commands
================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-testing.rst

.. include:: /includes/toc/command-testing.rst
========================
User Management Commands
========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-command-user-management.rst

.. include:: /includes/toc/command-user-management.rst
=======
netstat
=======

.. default-domain:: mongodb

.. dbcommand:: netstat

   :dbcommand:`netstat` is an internal command that is only
   available on :program:`mongos` instances.
======================
parallelCollectionScan
======================

.. default-domain:: mongodb

.. dbcommand:: parallelCollectionScan

   .. versionadded:: 2.6

   Allows applications to use multiple parallel cursors when reading
   all the documents from a collection, thereby increasing throughput.
   The :dbcommand:`parallelCollectionScan` command returns a document
   that contains an array of cursor information.

   The command has the following syntax:

   .. code-block:: javascript

      {
        parallelCollectionScan: "<collection>",
        numCursors: <integer>
      }

   The :dbcommand:`parallelCollectionScan` command takes the following
   fields:

   .. include:: /reference/command/parallelCollectionScan-field.rst
====
ping
====

.. default-domain:: mongodb

.. dbcommand:: ping

   The :dbcommand:`ping` command is a no-op used to test whether a
   server is responding to commands. This command will return
   immediately even if the server is write-locked:

   .. code-block:: javascript

      { ping: 1 }

   The value (e.g. ``1`` above) does not impact the behavior of the
   command.
==============
planCacheClear
==============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheClear

   .. versionadded:: 2.6

   Removes cached query plans for a collection. Specify a :term:`query
   shape` to remove cached query plans for that shape. Omit the query
   shape to clear all cached query plans.

   The command has the following syntax:

   .. code-block:: javascript

      db.runCommand(
         {
            planCacheClear: <collection>,
            query: <query>,
            sort: <sort>,
            projection: <projection>
         }
      )

   The :dbcommand:`planCacheClear` command has the following field:

   .. include:: planCacheClear-field.rst

   To see the query shapes for which cached query plans exist, use the
   :dbcommand:`planCacheListQueryShapes` command.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheWrite` action.

Examples
--------

Clear Cached Plans for a Query Shape
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If a collection ``orders`` has the following query shape:

.. code-block:: javascript

     {
       "query" : { "qty" : { "$gt" : 10 } },
       "sort" : { "ord_date" : 1 },
       "projection" : { }
     }

The following operation clears the query plan cached for the shape:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheClear: "orders",
         query: { "qty" : { "$gt" : 10 } },
         sort: { "ord_date" : 1 }
      }
   )

Clear All Cached Plans for a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example clears all the cached query plans for the
``orders`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheClear: "orders"
      }
   )

.. seealso::
   - :method:`PlanCache.clearPlansByQuery()`
   - :method:`PlanCache.clear()`
=====================
planCacheClearFilters
=====================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheClearFilters

   .. versionadded:: 2.6

   Removes :ref:`index filters <index-filters>` on a collection.
   Although index filters only exist for the duration of the server
   process and do not persist after shutdown, you can also clear
   existing index filters with the :dbcommand:`planCacheClearFilters`
   command.

   Specify the :term:`query shape` to remove a specific index filter.
   Omit the query shape to clear all index filters on a collection.

   The command has the following syntax:

   .. code-block:: javascript

      db.runCommand(
         {
            planCacheClearFilters: <collection>,
            query: <query pattern>,
            sort: <sort specification>,
            projection: <projection specification>
         }
      )

   The :dbcommand:`planCacheClearFilters` command has the following field:

   .. include:: planCacheClearFilters-field.rst

Required Access
---------------

A user must have access that includes the
:authaction:`planCacheIndexFilter` action.

Examples
--------

Clear Specific Index Filter on Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``orders`` collection contains the following two filters:

.. code-block:: javascript

   {
     "query" : { "status" : "A" },
     "sort" : { "ord_date" : -1 },
     "projection" : { },
     "indexes" : [ { "status" : 1, "cust_id" : 1 } ]
   }

   {
     "query" : { "status" : "A" },
     "sort" : { },
     "projection" : { },
     "indexes" : [ { "status" : 1, "cust_id" : 1 } ]
   }

The following command removes the second index filter only:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheClearFilters: "orders",
         query: { "status" : "A" }
      }
   )

Because the values in the ``query`` predicate are insignificant in
determining the :term:`query shape`, the following command would also
remove the second index filter:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheClearFilters: "orders",
         query: { "status" : "P" }
      }
   )

Clear all Index Filters on a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example clears all index filters on the ``orders``
collection:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheClearFilters: "orders"
      }
   )

.. seealso::
   :dbcommand:`planCacheListFilters`,
   :dbcommand:`planCacheSetFilter`
====================
planCacheListFilters
====================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheListFilters

   .. versionadded:: 2.6

   Lists the :ref:`index filters <index-filters>` associated with
   :term:`query shapes <query shape>` for a collection.

   The command has the following syntax:

   .. code-block:: javascript

      db.runCommand( { planCacheListFilters: <collection> } )

   The :dbcommand:`planCacheListFilters` command has the following field:

   .. include:: planCacheListFilters-field.rst

   :returns: Document listing the index filters. See
      :ref:`planCacheListFilters-output`.

Required Access
---------------

A user must have access that includes the
:authaction:`planCacheIndexFilter` action.

.. _planCacheListFilters-output:

Output
------

The :dbcommand:`planCacheListFilters` command returns the document with
the following form:

.. code-block:: none

   {
      "filters" : [
         {
            "query" : <query>
            "sort" : <sort>,
            "projection" : <projection>,
            "indexes" : [
               <index1>,
               ...
            ]
         },
         ...
      ],
      "ok" : 1
   }

.. data:: planCacheListFilters.filters

   The array of documents that contain the index filter information.

   Each document contains the following fields:

   .. data:: planCacheListFilters.filters.query

      The query predicate associated with this filter. Although the
      :data:`~planCacheListFilters.filters.query` shows the specific
      values used to create the index filter, the values in the
      predicate are insignificant; i.e. query predicates cover similar
      queries that differ only in the values.

      For instance, a :data:`~planCacheListFilters.filters.query`
      predicate of ``{ "type": "electronics", "status" : "A" }`` covers
      the following query predicates:

      .. code-block:: javascript

         { type: "food", status: "A" }
         { type: "utensil", status: "D" }

      Together with the :data:`~planCacheListFilters.filters.sort` and
      the :data:`~planCacheListFilters.filters.projection`, the
      :data:`~planCacheListFilters.filters.query` make up the
      :term:`query shape` for the specified index filter.

   .. data:: planCacheListFilters.filters.sort

      The sort associated with this filter. Can be an empty document.

      Together with the :data:`~planCacheListFilters.filters.query` and
      the :data:`~planCacheListFilters.filters.projection`, the
      :data:`~planCacheListFilters.filters.sort` make up the
      :term:`query shape` for the specified index filter.

   .. data:: planCacheListFilters.filters.projection

      The projection associated with this filter. Can be an empty
      document.

      Together with the :data:`~planCacheListFilters.filters.query` and
      the :data:`~planCacheListFilters.filters.sort`, the
      :data:`~planCacheListFilters.filters.projection` make up the
      :term:`query shape` for the specified index filter.

   .. data:: planCacheListFilters.filters.indexes

      The array of indexes for this :term:`query shape`. To choose the
      optimal query plan, the query optimizer evaluates only the listed
      :data:`~planCacheListFilters.hints.indexes` *and* the collection
      scan.

.. data:: planCacheListFilters.ok

   The status of the command.

.. seealso::
   :dbcommand:`planCacheClearFilters`,
   :dbcommand:`planCacheSetFilter`
==================
planCacheListPlans
==================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheListPlans

   .. versionadded:: 2.6

   Displays the cached query plans for the specified :term:`query
   shape`.

   .. include:: /includes/fact-query-optimizer-cache-behavior.rst

   The :program:`mongo` shell provides the wrapper
   :method:`PlanCache.getPlansByQuery()` for this command.

   The :dbcommand:`planCacheListPlans` command has the following syntax:

   .. code-block:: javascript

      db.runCommand(
         {
            planCacheListPlans: <collection>,
            query: <query>,
            sort: <sort>,
            projection: <projection>
         }

   The :dbcommand:`planCacheListPlans` command has the following field:

   .. include:: planCacheListPlans-field.rst

   To see the query shapes for which cached query plans exist, use the
   :dbcommand:`planCacheListQueryShapes` command.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheRead` action.

Example
-------

If a collection ``orders`` has the following query shape:

.. code-block:: javascript

     {
       "query" : { "qty" : { "$gt" : 10 } },
       "sort" : { "ord_date" : 1 },
       "projection" : { }
     }

The following operation displays the query plan cached for the shape:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheListPlans: "orders",
         query: { "qty" : { "$gt" : 10 } },
         sort: { "ord_date" : 1 }
      }
   )

.. seealso::

   - :dbcommand:`planCacheListQueryShapes`
   - :method:`PlanCache.getPlansByQuery()`
   - :method:`PlanCache.listQueryShapes()`
========================
planCacheListQueryShapes
========================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheListQueryShapes

   .. versionadded:: 2.6

   Displays the :term:`query shapes <query shape>` for which cached
   query plans exist for a collection.

   .. include:: /includes/fact-query-optimizer-cache-behavior.rst

   The :program:`mongo` shell provides the wrapper
   :method:`PlanCache.listQueryShapes()` for this command.

   The command has the following syntax:

   .. code-block:: javascript

      db.runCommand(
         {
            planCacheListQueryShapes: <collection>
         }

   The :dbcommand:`planCacheListQueryShapes` command has the following field:

   .. include:: planCacheListQueryShapes-field.rst

   :returns: A document that contains an array of :term:`query shapes
       <query shape>` for which cached query plans exist.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheRead` action.

Example
-------

The following returns the :term:`query shapes <query shape>` that have
cached plans for the ``orders`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheListQueryShapes: "orders"
      }
   )

The command returns a document that contains the field ``shapes`` that
contains an array of the :term:`query shapes <query shape>` currently
in the cache. In the example, the ``orders`` collection had cached
query plans associated with the following shapes:

.. code-block:: javascript

   {
     "shapes" : [
        {
          "query" : { "qty" : { "$gt" : 10 } },
           "sort" : { "ord_date" : 1 },
           "projection" : { }
        },
        {
           "query" : { "$or" : [ { "qty" : { "$gt" : 15 } }, { "status" : "A" } ] },
           "sort" : { },
           "projection" : { }
        },
        {
          "query" : { "$or" :
             [
               { "qty" : { "$gt" : 15 }, "item" : "xyz123" },
               { "status" : "A" }
             ]
           },
           "sort" : { },
           "projection" : { }
         }
      ],
      "ok" : 1
   }

.. seealso::
   - :method:`PlanCache.listQueryShapes()`
==================
planCacheSetFilter
==================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: planCacheSetFilter

   .. versionadded:: 2.6

   Set an :ref:`index filter <index-filters>` for a collection. If
   an index filter already exists for the :term:`query shape`, the
   command overrides the previous index filter.

   The command has the following syntax:

   .. code-block:: javascript

      db.runCommand(
         {
            planCacheSetFilter: <collection>,
            query: <query>,
            sort: <sort>,
            projection: <projection>,
            indexes: [ <index1>, <index2>, ...]
         }
      )

   The :dbcommand:`planCacheSetFilter` command has the following field:

   .. include:: planCacheSetFilter-field.rst

   Index filters only exist for the duration of the server process and
   do not persist after shutdown; however, you can also clear existing
   index filters using the :dbcommand:`planCacheClearFilters` command.

Required Access
---------------

A user must have access that includes the
:authaction:`planCacheIndexFilter` action.

.. _planCacheSetFilter-output:

Examples
--------

The following example creates an index filter on the ``orders``
collection such that for queries that consists only of an equality
match on the ``status`` field without any projection and sort, the
query optimizer evaluates only the two specified indexes and the
collection scan for the winning plan:

.. code-block:: javascript

   db.runCommand(
      {
         planCacheSetFilter: "orders",
         query: { status: "A" },
         indexes: [
            { cust_id: 1, status: 1 },
            { status: 1, order_date: -1 }
         ]
      }
   )

In the query predicate, only the structure of the predicate, including
the field names, are significant; the values are insignificant. As
such, the created filter applies to the following operations:

.. code-block:: javascript

   db.orders.find( { status: "D" } )
   db.orders.find( { status: "P" } )

To see whether MongoDB applied an index filter for a query, check the
:data:`explain.filterSet` field of the :method:`~cursor.explain()`
output.

.. seealso::
   :dbcommand:`planCacheClearFilters`,
   :dbcommand:`planCacheListFilters`
=======
profile
=======

.. default-domain:: mongodb

.. dbcommand:: profile

   Use the :dbcommand:`profile` command to enable, disable, or change
   the query profiling level. This allows administrators to capture
   data regarding performance. The database profiling system can
   impact performance and can allow the server to write the contents
   of queries to the log.  Your deployment should carefully consider the
   security implications of this.
   Consider the following prototype syntax:

   .. code-block:: javascript

      { profile: <level> }

   The following profiling levels are available:

   =========  ==============================================
   **Level**  **Setting**
   ---------  ----------------------------------------------
     -1       No change. Returns the current profile level.
      0       Off. No profiling.
      1       On. Only includes slow operations.
      2       On. Includes all operations.
   =========  ==============================================

   You may optionally set a threshold in milliseconds for profiling using
   the ``slowms`` option, as follows:

   .. code-block:: javascript

      { profile: 1, slowms: 200 }

   :program:`mongod` writes the output of the database profiler to the
   ``system.profile`` collection.

   :program:`mongod` records queries that take longer than
   the :setting:`~operationProfiling.slowOpThresholdMs` to the server log even when the database profiler is
   not active.

   .. seealso:: Additional documentation regarding :ref:`Database Profiling <database-profiling>`.

   .. seealso:: ":method:`db.getProfilingStatus()`" and
                ":method:`db.setProfilingLevel()`" provide wrappers
                around this functionality in the :program:`mongo`
                shell.

   .. include:: /includes/note-disable-profiling-fsynclock.rst

   .. note::

      This command obtains a write lock on the affected database and will
      block other operations until it has completed.  However, the write
      lock is only held while enabling or disabling the
      profiler.  This is typically a short operation.
==============
recvChunkAbort
==============

.. default-domain:: mongodb

.. dbcommand:: _recvChunkAbort

   :dbcommand:`_recvChunkAbort` is an internal command. Do not call
   directly.

   .. admin-only
===============
recvChunkCommit
===============

.. default-domain:: mongodb

.. dbcommand:: _recvChunkCommit

   :dbcommand:`_recvChunkCommit` is an internal command. Do not call
   directly.

   .. admin-only
==============
recvChunkStart
==============

.. default-domain:: mongodb

.. dbcommand:: _recvChunkStart

   :dbcommand:`_recvChunkStart` is an internal command. Do not call
   directly.

   .. warning::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed.

   .. admin-only, write-lock
===============
recvChunkStatus
===============

.. default-domain:: mongodb

.. dbcommand:: _recvChunkStatus

   :dbcommand:`_recvChunkStatus` is an internal command. Do not call
   directly.

   .. admin-only
=======
reIndex
=======

.. default-domain:: mongodb

.. dbcommand:: reIndex

   The :dbcommand:`reIndex` command drops all indexes on a
   collection and recreates them. This operation may be expensive for
   collections that have a large amount of data and/or a large number
   of indexes. Use the following syntax:

   .. code-block:: javascript

      { reIndex: "collection" }

   Normally, MongoDB compacts indexes during routine updates. For most
   users, the :dbcommand:`reIndex` command is unnecessary. However, it
   may be worth running if the collection size has changed significantly
   or if the indexes are consuming a disproportionate amount of disk space.

   Call :dbcommand:`reIndex` using the following form:

   .. code-block:: javascript

      db.collection.reIndex();

   .. |cmd-name| replace:: :dbcommand:`reIndex`
   .. include:: /includes/note-reindex-impact-on-replica-sets.rst

   .. include:: /includes/important-reindex-locking.rst

.. see:: :doc:`/core/index-creation` for more information on the
   behavior of indexing operations in MongoDB.
===========
removeShard
===========

.. default-domain:: mongodb

.. dbcommand:: removeShard

   Removes a shard from a :term:`sharded cluster`. When you run
   :dbcommand:`removeShard`, MongoDB first moves the shard's chunks to
   other shards in the cluster. Then MongoDB removes the shard.

Behavior
--------

Access Requirements
~~~~~~~~~~~~~~~~~~~

You *must* run :dbcommand:`removeShard` while connected to a
:program:`mongos`. Issue the command against the ``admin`` database or
use the :method:`sh._adminCommand()` helper.

If you have :setting:`~security.authentication` enabled, you must have the
:authrole:`clusterManager` role or any role that
includes the :authaction:`removeShard` action.

Database Migration Requirements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Each database in a sharded cluster has a primary shard. If the shard you
want to remove is also the primary of one of the cluster's databases, then
you must manually move the databases to a new shard after migrating
all data from the shard. See the :dbcommand:`movePrimary` command and
the :doc:`/tutorial/remove-shards-from-cluster` for more information.

Example
-------

From the :program:`mongo` shell, the :dbcommand:`removeShard`
operation resembles the following:

.. code-block:: javascript

   use admin
   db.runCommand( { removeShard : "bristol01" } )

Replace ``bristol01`` with the name of the shard to remove. When you
run :dbcommand:`removeShard`, the command returns immediately,
with the following message:

.. code-block:: javascript

   {
       "msg" : "draining started successfully",
       "state" : "started",
       "shard" : "bristol01",
       "ok" : 1
   }

The balancer begins migrating chunks from the shard named
``bristol01`` to other shards in the cluster. These migrations happens
slowly to avoid placing undue load on the overall cluster.

If you run the command again, :dbcommand:`removeShard` returns the
following progress output:

.. code-block:: javascript

   {
       "msg" : "draining ongoing",
       "state" : "ongoing",
       "remaining" : {
           "chunks" : 23,
           "dbs" : 1
       },
       "ok" : 1
   }

The ``remaining`` :term:`document` specifies how many chunks and databases
remain on the shard. Use :method:`db.printShardingStatus()` to list the
databases that you must move from the shard. Use the
:dbcommand:`movePrimary` to move databases.

After removing all chunks and databases from the shard, you can issue
:dbcommand:`removeShard` again see the following:

.. code-block:: javascript

   {
       "msg" : "removeshard completed successfully",
       "state" : "completed",
       "shard" : "bristol01",
       "ok" : 1
   }
================
renameCollection
================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: renameCollection

   Changes the name of an existing collection. Specify collections
   to :dbcommand:`renameCollection` in the form of a complete
   :term:`namespace`, which includes the database name. Issue the
   :dbcommand:`renameCollection` command against the :term:`admin
   database`. The command takes the following form:

   .. code-block:: javascript

      { renameCollection: "<source_namespace>", to: "<target_namespace>", dropTarget: <true|false> }

   The command contains the following fields:

   .. include:: /reference/command/renameCollection-field.rst

   :dbcommand:`renameCollection` is suitable for production
   environments; *however*:

   - :dbcommand:`renameCollection` blocks all database activity
     for the duration of the operation.

   - :dbcommand:`renameCollection` is **not** compatible with sharded
     collections.

   .. warning::

      :dbcommand:`renameCollection` fails if ``target`` is the name of
      an existing collection *and* you do not specify ``dropTarget:
      true``.

      If the :dbcommand:`renameCollection` operation does not complete
      the ``target`` collection and indexes will not be usable and
      will require manual intervention to clean up.

Exceptions
----------

:exception 10026:
   Raised if the ``source`` namespace does not exist.

:exception 10027:
   Raised if the ``target`` namespace exists and ``dropTarget`` is
   either ``false`` or unspecified.

:exception 15967:
   Raised if the ``target`` namespace is an invalid collection
   name.

Shell Helper
------------

The shell helper :method:`db.collection.renameCollection()` provides a
simpler interface to using this command within a database.
The following is equivalent to the previous example:

.. code-block:: javascript

   db.source-namespace.renameCollection( "target" )

.. warning:: You cannot use :dbcommand:`renameCollection` with
   sharded collections.

.. include:: /includes/warning-blocking-global.rst
==============
repairDatabase
==============

.. default-domain:: mongodb

.. contents::
   :backlinks: none
   :local:

Definition
----------

.. dbcommand:: repairDatabase

   Checks and repairs errors and inconsistencies in data storage.
   :dbcommand:`repairDatabase` is analogous to a ``fsck`` command for
   file systems. Run the :dbcommand:`repairDatabase` command to ensure
   data integrity after the system experiences an unexpected system
   restart or crash, if:

   #. The :program:`mongod` instance is not running with
      :term:`journaling <journal>` enabled.

      .. include:: /includes/note-repair.rst

   #. There are *no* other intact :term:`replica set` members with a complete
      data set.

      .. include:: /includes/warning-repair.rst

   :dbcommand:`repairDatabase` takes the following form:

   .. code-block:: javascript

      { repairDatabase: 1 }

   :dbcommand:`repairDatabase` has the following fields:

   .. include:: /reference/command/repairDatabase-field.rst

   You can explicitly set the options as follows:

   .. code-block:: javascript

      { repairDatabase: 1,
        preserveClonedFilesOnFailure: <boolean>,
        backupOriginalFiles: <boolean> }

   .. include:: /includes/warning-blocking-global.rst

   .. note:: :dbcommand:`repairDatabase` requires free disk space equal to the size
      of your current data set plus 2 gigabytes. If the volume that holds dbpath
      lacks sufficient space, you can mount a separate volume
      and use that for the repair. When mounting a separate volume for
      :dbcommand:`repairDatabase` you must run :dbcommand:`repairDatabase`
      from the command line and use the
      :option:`--repairpath <mongod --repairpath>`
      switch to specify the folder in which to store
      temporary repair files.

   See :option:`mongod --repair` and :option:`mongodump --repair` for
   information on these related options.

Behavior
--------

The :dbcommand:`repairDatabase` command compacts all collections in the
database. It is identical to running the :dbcommand:`compact` command on
each collection individually.

:dbcommand:`repairDatabase` reduces the total size of the data files on
disk. It also recreates all indexes in the database.

The time requirement for :dbcommand:`repairDatabase` depends on the size of the
data set.

You may invoke :dbcommand:`repairDatabase` from multiple contexts:

- Use the :program:`mongo` shell to run the command, as above.

- Use the :method:`db.repairDatabase()` in the :program:`mongo`
  shell.

- Run :program:`mongod` directly from your system's shell. Make sure
  that :program:`mongod` isn't already running, and that you invoke
  :program:`mongod` as a user that has access to MongoDB's data files. Run
  as:

  .. code-block:: sh

     mongod --repair

  To add a repair path:

  .. code-block:: sh

     mongod --repair --repairpath /opt/vol2/data

  .. note::

     :option:`mongod --repair <mongod --repair>` will fail if your database
     is not a master or primary. In most cases, you should recover a
     corrupt secondary using the data from an existing intact node. To
     run repair on a secondary/slave restart the instance in standalone
     mode without the :option:`--replSet <mongod --replSet>` or
     :option:`--slave <mongod --slave>` options.

Example
-------

.. code-block:: javascript

   { repairDatabase: 1 }

Using ``repairDatabase`` to Reclaim Disk Space
----------------------------------------------

You should not use :dbcommand:`repairDatabase` for data recovery
unless you have no other option.

However, if you trust that there is no corruption and you have enough
free space, then :dbcommand:`repairDatabase` is the appropriate and
the only way to reclaim disk space.
============
replSetElect
============

.. default-domain:: mongodb

.. dbcommand:: replSetElect

   :dbcommand:`replSetElect` is an internal command that support replica set
   functionality.

   .. slave-ok, admin-only
=============
replSetFreeze
=============

.. default-domain:: mongodb

.. dbcommand:: replSetFreeze

   The :dbcommand:`replSetFreeze` command prevents a replica set
   member from seeking election for the specified number of
   seconds. Use this command in conjunction with the
   :dbcommand:`replSetStepDown` command to make a different node in
   the replica set a primary.

   The :dbcommand:`replSetFreeze` command uses the following syntax:

   .. code-block:: javascript

      { replSetFreeze: <seconds> }

   If you want to unfreeze a replica set member before the specified number
   of seconds has elapsed, you can issue the command with a seconds
   value of ``0``:

   .. code-block:: javascript

      { replSetFreeze: 0 }

   Restarting the :program:`mongod` process also unfreezes a replica
   set member.

   :dbcommand:`replSetFreeze` is an administrative command, and you
   must issue it against the :term:`admin database`.

   .. slave-ok, admin-only
============
replSetFresh
============

.. default-domain:: mongodb

.. dbcommand:: replSetFresh

   :dbcommand:`replSetFresh` is an internal command that supports replica set
   functionality.

   .. slave-ok, admin-only
==============
replSetGetRBID
==============

.. default-domain:: mongodb

.. dbcommand:: replSetGetRBID

   :dbcommand:`replSetGetRBID` is an internal command that supports replica set
   functionality.

   .. slave-ok, admin-only
================
replSetGetStatus
================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: replSetGetStatus

   The ``replSetGetStatus`` command returns the status of the replica
   set from the point of view of the current server. You must run the
   command against the :term:`admin database`. The command has the
   following prototype format:

   .. code-block:: javascript

      { replSetGetStatus: 1 }

   The value specified does not affect the output of the command. Data
   provided by this command derives from data included in heartbeats
   sent to the current instance by other members of the replica set.
   Because of the frequency of heartbeats, these data can be several
   seconds out of date.

   You can also access this functionality through the
   :method:`rs.status()` helper in the :program:`mongo` shell.

   The :program:`mongod` must have replication enabled and be a member
   of a replica set for the for :dbcommand:`replSetGetStatus` to return
   successfully.

.. _rs-status-output:

Output
------

.. data:: replSetGetStatus.set

   The ``set`` value is the name of the replica set, configured in the
   :setting:`~replication.replSetName` setting. This is the same value as
   :data:`~local.system.replset._id` in :method:`rs.conf()`.

.. data:: replSetGetStatus.date

   The value of the ``date`` field is an :term:`ISODate` of the
   current time, according to the current server. Compare this to the
   value of the :data:`~replSetGetStatus.members.lastHeartbeat` to find the
   operational lag between the current host and the other hosts in the
   set.

.. data:: replSetGetStatus.myState

   The value of
   :data:`~replSetGetStatus.myState` is an integer between ``0`` and
   ``10`` that represents the :doc:`replica state </reference/replica-states>` of the current member.

.. data:: replSetGetStatus.members

   The ``members`` field holds an array that contains a document for
   every member in the replica set.

   .. data:: replSetGetStatus.members.name

      The ``name`` field holds the name of the server.

   .. data:: replSetGetStatus.members.self

      The ``self`` field is only included in the document for the
      current ``mongod`` instance in the members array. It's value is
      ``true``.

   .. data:: replSetGetStatus.members.health

      The ``health`` value is only present for the other members of the
      replica set (i.e. not the member that returns
      :method:`rs.status`.) This field conveys if the member is up (i.e.
      ``1``) or down (i.e. ``0``.)

   .. data:: replSetGetStatus.members.state

      The value of :data:`~replSetGetStatus.members.state` is an
      integer between ``0`` and ``10`` that represents the
      :doc:`replica state </reference/replica-states>` of the member.

   .. data:: replSetGetStatus.members.stateStr

      A string that describes :data:`~replSetGetStatus.members.state`.

   .. data:: replSetGetStatus.members.uptime

      The :data:`~replSetGetStatus.members.uptime` field holds a value
      that reflects the number of seconds that this member has been
      online.

      This value does not appear for the member that returns the
      :method:`rs.status()` data.

   .. data:: replSetGetStatus.members.optime

      A document that contains information regarding the last operation
      from the operation log that this member has applied.

      .. data:: replSetGetStatus.members.optime.t

         A 32-bit timestamp of the last operation applied to this member
         of the replica set from the :term:`oplog`.

      .. data:: replSetGetStatus.members.optime.i

         An incremented field, which reflects the number of operations
         in since the last time stamp. This value only increases if
         there is more than one operation per second.

   .. data:: replSetGetStatus.members.optimeDate

      An :term:`ISODate` formatted date string that reflects the last
      entry from the :term:`oplog` that this member applied. If this
      differs significantly from
      :data:`~replSetGetStatus.members.lastHeartbeat` this member is
      either experiencing "replication lag" *or* there have not been any
      new operations since the last update. Compare
      ``members.optimeDate`` between all of the members of the set.

   .. data:: replSetGetStatus.members.lastHeartbeat

      The ``lastHeartbeat`` value provides an :term:`ISODate` formatted
      date and time of the transmission time of last heartbeat
      received from this member. Compare this value to the value of the
      :data:`~replSetGetStatus.date` and
      :data:`~replSetGetStatus.lastHeartBeatRecv` field to track
      latency between these members.

      This value does not appear for the member that returns the
      :method:`rs.status()` data.

   .. data:: replSetGetStatus.members.lastHeartbeatRecv

      The ``lastHeartbeatRecv`` value provides an :term:`ISODate` formatted
      date and time that the last heartbeat was received from this
      member. Compare this value to the value of the
      :data:`~replSetGetStatus.date` and
      :data:`~replSetGetStatus.lastHeartBeat` field to track
      latency between these members.

   .. data:: replSetGetStatus.members.lastHeartbeatMessage

      When the last heartbeat included an extra message, the
      :data:`~replSetGetStatus.members.lastHeartbeatMessage` contains
      a string representation of that message.

   .. data:: replSetGetStatus.members.pingMs

      The ``pingMs`` represents the number of milliseconds (ms) that a
      round-trip packet takes to travel between the remote member and
      the local instance.

      This value does not appear for the member that returns the
      :method:`rs.status()` data.

.. data:: replSetGetStatus.syncingTo

   The ``syncingTo`` field is only present on the output of
   :method:`rs.status()` on :term:`secondary` and recovering members,
   and holds the hostname of the member from which this instance is
   syncing.
================
replSetHeartbeat
================

.. default-domain:: mongodb

.. dbcommand:: replSetHeartbeat

   :dbcommand:`replSetHeartbeat` is an internal command that supports
   replica set functionality.

   .. slave-OK
===============
replSetInitiate
===============

.. default-domain:: mongodb

.. dbcommand:: replSetInitiate

   The :dbcommand:`replSetInitiate` command initializes a new replica set. Use the
   following syntax:

   .. code-block:: javascript

      { replSetInitiate : <config_document> }

   The ``<config_document>`` is a :term:`document` that specifies
   the replica set's configuration. For instance, here's a config document
   for creating a simple 3-member replica set:

   .. code-block:: javascript

      {
          _id : <setname>,
           members : [
               {_id : 0, host : <host0>},
               {_id : 1, host : <host1>},
               {_id : 2, host : <host2>},
           ]
      }

   A typical way of running this command is to assign the config document to
   a variable and then to pass the document to the
   :method:`rs.initiate()` helper:

   .. code-block:: javascript

      config = {
          _id : "my_replica_set",
           members : [
               {_id : 0, host : "rs1.example.net:27017"},
               {_id : 1, host : "rs2.example.net:27017"},
               {_id : 2, host : "rs3.example.net", arbiterOnly: true},
           ]
      }

      rs.initiate(config)

   Notice that omitting the port cause the host to use the default port
   of 27017. Notice also that you can specify other options in the config
   documents such as the ``arbiterOnly`` setting in this example.

   .. slave-ok, admin-only

   .. seealso:: :doc:`/reference/replica-configuration`,
      :doc:`/administration/replica-sets`, and :ref:`Replica Set
      Reconfiguration <replica-set-reconfiguration-usage>`.
==================
replSetMaintenance
==================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: replSetMaintenance

   The :dbcommand:`replSetMaintenance` admin command enables or disables the
   maintenance mode for a :term:`secondary` member of a :term:`replica
   set`.

   The command has the following prototype form:

   .. code-block:: javascript

      { replSetMaintenance: <boolean> }

Behavior
--------

Consider the following behavior when running the
:dbcommand:`replSetMaintenance` command:

- You cannot run the command on the Primary.

- You must run the command against the ``admin`` database.

- When enabled ``replSetMaintenance: true``, the member enters the
  ``RECOVERING`` state. While the secondary is ``RECOVERING``:

  - The member is not accessible for read operations.

  - The member continues to sync its :term:`oplog` from the Primary.

.. |cmd-name| replace:: :dbcommand:`compact`
.. include:: /includes/fact-command-puts-secondary-into-recovering.rst

.. admin-only
===============
replSetReconfig
===============

.. default-domain:: mongodb

.. dbcommand:: replSetReconfig

   The :dbcommand:`replSetReconfig` command modifies the configuration
   of an existing replica set. You can use this command to add and
   remove members, and to alter the options set on existing
   members. Use the following syntax:

   .. code-block:: javascript

      { replSetReconfig: <new_config_document>, force: false }

   You may also run :dbcommand:`replSetReconfig` with the shell's
   :method:`rs.reconfig()` method.

.. slave-ok, admin-only

Behaviors
---------

Be aware of the following :dbcommand:`replSetReconfig` behaviors:

- You must issue this command against the :term:`admin database` of the current
  primary member of the replica set.

- You can optionally force the replica set to accept the new
  configuration by specifying ``force: true``. Use this option if
  the current member is not primary or if a majority of the members
  of the set are not accessible.

  .. warning::

     Forcing the :dbcommand:`replSetReconfig` command can lead to a
     :term:`rollback` situation. Use with caution.

  Use the force option to restore a replica set to new servers with
  different hostnames. This works even if the set members already
  have a copy of the data.

- A majority of the set's members must be operational for the
  changes to propagate properly.

- This command can cause downtime as the set renegotiates
  primary-status. Typically this is 10-20 seconds, but could
  be as long as a minute or more. Therefore, you should attempt
  to reconfigure only during scheduled maintenance periods.

- In some cases, :dbcommand:`replSetReconfig` forces the current
  primary to step down, initiating an election for primary among
  the members of the replica set. When this happens, the set will
  drop all current connections.

:dbcommand:`replSetReconfig` obtains a special mutually
exclusive lock to prevent more than one
:dbcommand:`replSetReconfig` operation from occurring at the same
time.

Additional Information
----------------------

:doc:`/reference/replica-configuration`, :method:`rs.reconfig()`, and
:method:`rs.conf()`.
===============
replSetStepDown
===============

.. default-domain:: mongodb

Description
-----------

.. dbcommand:: replSetStepDown

   Forces the :term:`primary` of the replica set to become a
   :term:`secondary`. This initiates an :ref:`election for primary
   <replica-set-election-internals>`.

   :dbcommand:`replSetStepDown` has the following prototype form:

   .. code-block:: javascript

      db.runCommand( { replSetStepDown: <seconds> , force: <true|false> } )


   :dbcommand:`replSetStepDown` has the following fields:

   .. include:: /reference/command/replSetStepDown-field.rst

   .. warning:: :dbcommand:`replSetStepDown` forces all clients
      currently connected to the database to disconnect. This helps
      ensure that clients maintain an accurate view of the replica
      set.

   .. versionadded:: 2.0
      If there is no :term:`secondary` within 10 seconds of the
      primary, :dbcommand:`replSetStepDown` will not succeed to
      prevent long running elections.

Example
-------

The following example specifies that the former primary avoids
reelection to primary for 120 seconds:

.. code-block:: javascript

   db.runCommand( { replSetStepDown: 120 } )

.. slave-ok, admin-only
===============
replSetSyncFrom
===============

.. default-domain:: mongodb

Description
-----------

.. dbcommand:: replSetSyncFrom

   .. versionadded:: 2.2

   Explicitly configures
   which host the current :program:`mongod` pulls :term:`oplog`
   entries from. This operation is useful for testing different
   patterns and in situations where a set member is not replicating from
   the desired host.

   The :dbcommand:`replSetSyncFrom` command has the following form:

   .. code-block:: javascript

      { replSetSyncFrom: "hostname<:port>" }

   The :dbcommand:`replSetSyncFrom` command has the following field:

   .. include:: /reference/command/replSetSyncFrom-field.rst

The Target Member
-----------------

  The member to replicate from must be a valid source for data in the
  set. The member cannot be:

   - The same as the :program:`mongod` on which you run
     :dbcommand:`replSetSyncFrom`. In other words, a member cannot
     replicate from itself.
   - An arbiter, because arbiters do not hold data.
   - A member that does not build indexes.
   - An unreachable member.
   - A :program:`mongod` instance that is not a member of the same
     replica set.

   If you attempt to replicate from a member that is more than 10 seconds
   behind the current member, :program:`mongod` will log a
   warning but will still replicate from the lagging member.

   If you run :dbcommand:`replSetSyncFrom` during initial sync, MongoDB
   produces no error messages, but the sync target will not change
   until after the initial sync operation.

Run from the ``mongo`` Shell
----------------------------

   To run the command in the :program:`mongo` shell, use the following
   invocation:

   .. code-block:: javascript

      db.adminCommand( { replSetSyncFrom: "hostname<:port>" } )

   You may also use the :method:`rs.syncFrom()` helper in the
   :program:`mongo` shell in an operation with the following form:

   .. code-block:: javascript

      rs.syncFrom("hostname<:port>")

   .. note::

      .. include:: /includes/fact-replica-set-sync-from-is-temporary.rst

   .. slave-ok, admin-only
===========
replSetTest
===========

.. default-domain:: mongodb

.. dbcommand:: replSetTest

   :dbcommand:`replSetTest` is internal diagnostic command used for regression
   tests that supports replica set functionality.

   .. |dbcommand| replace:: :dbcommand:`replSetTest`
   .. include:: /includes/note-enabletestcommands.rst


   .. slave-ok, admin-only
==========
resetError
==========

.. default-domain:: mongodb

.. dbcommand:: resetError

   The :dbcommand:`resetError` command resets the last error status.

   .. seealso:: :method:`db.resetError()`
======
resync
======

.. default-domain:: mongodb

.. dbcommand:: resync

   The :dbcommand:`resync` command forces an out-of-date slave
   :program:`mongod` instance to re-synchronize itself. Note
   that this command is relevant to master-slave replication only. It does
   not apply to replica sets.

   .. include:: /includes/warning-blocking-global.rst

   .. write-lock, slave-ok, admin-only.
========================
revokePrivilegesFromRole
========================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: revokePrivilegesFromRole

   Removes the specified privileges from the :ref:`user-defined
   <user-defined-roles>` role on the database where the
   command is run. The :dbcommand:`revokePrivilegesFromRole` command
   has the following syntax:

   .. code-block:: javascript

      {
        revokePrivilegesFromRole: "<role>",
        privileges:
            [
              { resource: { <resource> }, actions: [ "<action>", ... ] },
              ...
            ],
        writeConcern: <write concern document>
      }

   The :dbcommand:`revokePrivilegesFromRole` command has the following fields:

   .. include:: /reference/command/revokePrivilegesFromRole-field.rst

Behavior
--------

To revoke a privilege, the :doc:`resource document
</reference/resource-document>` pattern must match **exactly** the
``resource`` field of that privilege. The ``actions`` field can be a
subset or match exactly.

For example, consider the role ``accountRole`` in the ``products``
database with the following privilege that specifies the ``products``
database as the resource:

.. code-block:: javascript

   {
     "resource" : {
         "db" : "products",
         "collection" : ""
     },
     "actions" : [
         "find",
         "update"
     ]
   }

You *cannot* revoke ``find`` and/or ``update`` from just *one*
collection in the ``products`` database. The following operations
result in no change to the role:

.. code-block:: javascript

   use products
   db.runCommand(
       {
         revokePrivilegesFromRole: "accountRole",
         privileges:
           [
             {
               resource : {
                   db : "products",
                   collection : "gadgets"
               },
               actions : [
                   "find",
                   "update"
               ]
             }
           ]
       }
   )

   db.runCommand(
       {
         revokePrivilegesFromRole: "accountRole",
         privileges:
           [
             {
               resource : {
                   db : "products",
                   collection : "gadgets"
               },
               actions : [
                   "find"
               ]
             }
           ]
       }
   )

To revoke the ``"find"`` and/or the ``"update"`` action from the role
``accountRole``, you must match the resource document exactly. For
example, the following operation revokes just the ``"find"`` action
from the existing privilege.

.. code-block:: javascript

   use products
   db.runCommand(
       {
         revokePrivilegesFromRole: "accountRole",
         privileges:
           [
             {
               resource : {
                   db : "products",
                   collection : ""
               },
               actions : [
                   "find"
               ]
             }
           ]
       }
   )

Required Access
---------------

.. include:: /includes/access-revoke-privileges.rst

Example
-------

The following operation removes multiple privileges from the
``associates`` role in the ``products`` database:

.. code-block:: javascript

   use products
   db.runCommand(
      {
        revokePrivilegesFromRole: "associate",
        privileges:
         [
           {
             resource: { db: "products", collection: "" },
             actions: [ "createCollection", "createIndex", "find" ]
           },
           {
             resource: { db: "products", collection: "orders" },
             actions: [ "insert" ]
           }
         ],
        writeConcern: { w: "majority" }
      }
   )
===================
revokeRolesFromRole
===================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: revokeRolesFromRole

   Removes the specified inherited roles from a role. The
   :dbcommand:`revokeRolesFromRole` command has the following syntax:

   .. code-block:: javascript

      { revokeRolesFromRole: "<role>",
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: { <write concern> }
      }

   The command has the following fields:

   .. include:: /reference/command/revokeRolesFromRole-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`revokeRolesFromRole`
   .. include:: /includes/fact-roles-array-contents.rst

Required Access
---------------

.. include:: /includes/access-revoke-roles.rst

Example
-------

The ``purchaseAgents`` role in the ``emea`` database inherits privileges
from several other roles, as listed in the ``roles`` array:

.. code-block:: javascript

   {
      "_id" : "emea.purchaseAgents",
      "role" : "purchaseAgents",
      "db" : "emea",
      "privileges" : [],
      "roles" : [
         {
            "role" : "readOrdersCollection",
            "db" : "emea"
         },
         {
            "role" : "readAccountsCollection",
            "db" : "emea"
         },
         {
            "role" : "writeOrdersCollection",
            "db" : "emea"
         }
      ]
   }

The following :dbcommand:`revokeRolesFromRole` operation on the ``emea``
database removes two roles from the ``purchaseAgents`` role:

.. code-block:: javascript

   use emea
   db.runCommand( { revokeRolesFromRole: "purchaseAgents",
                    roles: [
                             "writeOrdersCollection",
                             "readOrdersCollection"
                           ],
                     writeConcern: { w: "majority" , wtimeout: 5000 }
                } )

The ``purchaseAgents`` role now contains just one role:

.. code-block:: javascript

   {
      "_id" : "emea.purchaseAgents",
      "role" : "purchaseAgents",
      "db" : "emea",
      "privileges" : [],
      "roles" : [
         {
            "role" : "readAccountsCollection",
            "db" : "emea"
         }
      ]
   }
===================
revokeRolesFromUser
===================

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: revokeRolesFromUser

   Removes a one or more roles from a user on the database where the
   roles exist. The :dbcommand:`revokeRolesFromUser` command uses the
   following syntax:

   .. code-block:: javascript

      { revokeRolesFromUser: "<user>",
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: { <write concern> }
      }

   The command has the following fields:

   .. include:: /reference/command/revokeRolesFromUser-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`revokeRolesFromUser`
   .. include:: /includes/fact-roles-array-contents.rst

Required Access
---------------

.. include:: /includes/access-revoke-roles.rst

Example
-------

The ``accountUser01`` user in the ``products`` database has the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       },
       { "role" : "read",
         "db" : "stock"
       },
       { "role" : "readWrite",
         "db" : "products"
       }
   ]

The following :dbcommand:`revokeRolesFromUser` command removes the two of
the user's roles: the :authrole:`read` role on the ``stock`` database and
the :authrole:`readWrite` role on the ``products`` database, which is also
the database on which the command runs:

.. code-block:: javascript

   use products
   db.runCommand( { revokeRolesFromUser: "accountUser01",
                    roles: [
                             { role: "read", db: "stock" },
                             "readWrite"
                    ],
                    writeConcern: { w: "majority" }
                } )

The user ``accountUser01`` in the ``products`` database now has only one
remaining role:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       }
   ]
=========
rolesInfo
=========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: rolesInfo

   Returns inheritance and privilege information for specified roles,
   including both :ref:`user-defined roles <user-defined-roles>` and
   :ref:`built-in roles <built-in-roles>`.

   The :dbcommand:`rolesInfo` command can also retrieve all roles
   scoped to a database.

   The command has the following fields:

   .. include:: /reference/command/rolesInfo-field.rst

.. _rolesinfo-behavior:

Behavior
--------

When specifying roles, use the syntax described here.

To specify a role from the current database, specify the role by its name:

.. code-block:: javascript

   rolesInfo: "<rolename>"

To specify a role from another database, specify the role by a document that
specifies the role and database:

.. code-block:: javascript

   rolesInfo: { role: "<rolename>", db: "<database>" }

To specify multiple roles, use an array. Specify each role in the array as a
document or string. Use a string only if the role exists on the database on
which the command runs:

.. code-block:: javascript

   rolesInfo:
     [
       "<rolename>",
       { role: "<rolename>", db: "<database>" },
         ...
     ]

To specify all roles in the database on which the command runs, specify
``rolesInfo: 1``. By default MongoDB displays all the :ref:`user-defined roles
<user-defined-roles>` in the database. To include :ref:`built-in roles
<built-in-roles>` as well, include the parameter-value pair
``showBuiltinRoles: true``:

.. code-block:: javascript

   rolesInfo: 1, showBuiltinRoles: true

Required Access
---------------

.. include:: /includes/access-roles-info.rst

.. _rolesinfo-output:

Output
------

.. data:: rolesInfo.role

   The name of the role.

.. data:: rolesInfo.db

   The database on which the role is defined. Every database has :ref:`built-in
   roles <built-in-roles>`. A database might also have :ref:`user-defined
   roles <user-defined-roles>`.

.. data:: rolesInfo.roles

   The roles that directly provide privileges to this role and the databases
   on which the roles are defined.

.. data:: rolesInfo.indirectRoles

   .. todo:: change this to "inheritedRoles" when the software changes

   All roles from which this role inherits privileges. This includes the roles
   in the :data:`rolesInfo.roles` array as well as the roles from which the
   roles in the :data:`rolesInfo.roles` array inherit privileges. All
   privileges apply to the current role. The documents in this field list the
   roles and the databases on which they are defined.

.. data:: rolesInfo.privileges

   All the privileges granted by this role. By default the output does not
   include this array. To include it, specify ``showPrivileges: true`` when
   running the :dbcommand:`rolesInfo` command.

   The array includes privileges defined directly in the role as well as
   privileges inherited from other roles.

   Each set of privileges in the array is contained in its own document. Each
   document specifies the :ref:`resources <resource-document>` the privilege
   accesses and the :doc:`actions </reference/privilege-actions>` allowed.

.. data:: rolesInfo.isBuiltin

   A value of ``true`` indicates the role is a :ref:`built-in role
   <built-in-roles>`. A value of ``false`` indicates the role is a
   :ref:`user-defined role <user-defined-roles>`.

Examples
--------

View Information for a Single Role
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following command returns the role inheritance information for the
role ``associate`` defined in the ``products`` database:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo: { role: "associate", db: "products" }
       }
   )

The following command returns the role inheritance information for the role
``siteManager`` on the database on which the command runs:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo: "siteManager"
       }
   )

The following command returns *both* the role inheritance and the privileges
for the role ``associate`` defined on the ``products`` database:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo:
           { role: "associate", db: "products" },
           showPrivileges: true
       }
   )

View Information for Several Roles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following command returns information for two roles on two different
databases:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo:
           [
             { role: "associate", db: "products" },
             { role: "manager", db: "resources" },
           ]
       }
   )

The following returns *both* the role inheritance and the privileges:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo:
           [
             { role: "associate", db: "products" },
             { role: "manager", db: "resources" },
           ],
           showPrivileges: true
       }
   )

View All User-Defined Roles for a Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns all :ref:`user-defined roles
<user-defined-roles>` on the database on which the command runs and includes
privileges:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo: 1,
         showPrivileges: true
       }
   )

View All User-Defined and Built-In Roles for a Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns all roles on the database on which the command
runs, including both built-in and user-defined roles:

.. code-block:: javascript

   db.runCommand(
       {
         rolesInfo: 1,
         showBuiltinRoles: true
       }
   )
============
serverStatus
============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: serverStatus

   The :dbcommand:`serverStatus` command returns a document that
   provides an overview of the database process's state. Most
   monitoring applications run this command at a regular interval to
   collection statistics about the instance:

   .. code-block:: javascript

      { serverStatus: 1 }

   The value (i.e. ``1`` above), does not affect the operation of the
   command.

   .. versionchanged:: 2.4
      In 2.4 you can dynamically suppress portions of the
      :dbcommand:`serverStatus` output, or include suppressed sections
      by adding fields to the command document as in the following
      examples:

   .. code-block:: javascript

      db.runCommand( { serverStatus: 1, repl: 0, indexCounters: 0 } )
      db.runCommand( { serverStatus: 1, workingSet: 1, metrics: 0, locks: 0 } )

   .. |operation-name| replace:: :dbcommand:`serverStatus`
   .. include:: /includes/example-server-status-projection.rst

   .. seealso:: :method:`db.serverStatus()` and :doc:`/reference/server-status`

.. When adding status fields to this document, make sure that the
   ``docs/source/reference/server-status.txt`` document also
   reflects those changes.

Output
------

The :dbcommand:`serverStatus` command returns a collection of information that
reflects the database's status. These data are useful for diagnosing
and assessing the performance of your MongoDB instance. This reference
catalogs each datum included in the output of this command and
provides context for using this data to more effectively administer
your database.

.. seealso:: Much of the output of :dbcommand:`serverStatus` is also
   displayed dynamically by :program:`mongostat`. See the
   :doc:`/reference/program/mongostat` command for more information.

   For examples of the :dbcommand:`serverStatus` output, see
   :doc:`/reference/server-status`.

.. _server-status-instance-information:

Instance Information
~~~~~~~~~~~~~~~~~~~~

For an example of the instance information, see the :ref:`Instance
Information section <server-status-example-instance-information>` of
the :doc:`/reference/server-status` page.

.. data:: serverStatus.host

   The :data:`~serverStatus.host` field contains the system's hostname. In Unix/Linux
   systems, this should be the same as the output of the ``hostname``
   command.

.. data:: serverStatus.version

   The :data:`~serverStatus.version` field contains the version of MongoDB running on
   the current :program:`mongod` or :program:`mongos` instance.

.. data:: serverStatus.process

   The :data:`~serverStatus.process` field identifies which kind of MongoDB instance is
   running. Possible values are:

   - :program:`mongos`
   - :program:`mongod`

.. data:: serverStatus.uptime

   The value of the :data:`~serverStatus.uptime` field corresponds to the number of
   seconds that the :program:`mongos` or :program:`mongod` process has
   been active.

.. data:: serverStatus.uptimeEstimate

   :data:`~serverStatus.uptimeEstimate` provides the uptime as calculated from MongoDB's
   internal course-grained time keeping system.

.. data:: serverStatus.localTime

   The :data:`~serverStatus.localTime` value is the current time, according to the
   server, in UTC specified in an ISODate format.

.. _locks:
.. _server-status-locks:

locks
~~~~~

.. versionadded:: 2.1.2
   All :data:`~serverStatus.locks` statuses first appeared in the 2.1.2
   development release for the 2.2 series.

For an example of the ``locks`` output, see the :ref:`locks section
<server-status-example-locks>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.locks

   The :data:`~serverStatus.locks` document contains sub-documents that provides a
   granular report on MongoDB database-level lock use. All values are
   of the ``NumberLong()`` type.

   Generally, fields named:

   - ``R`` refer to the global read lock,
   - ``W`` refer to the global write lock,
   - ``r`` refer to the database specific read lock, and
   - ``w`` refer to the database specific write lock.

   If a document does not have any fields, it means that no locks have
   existed with this context since the last time the :program:`mongod`
   started.

.. data:: serverStatus.locks..

   A field named ``.`` holds the first document in :data:`~serverStatus.locks`
   that contains information about the global lock.

.. data:: serverStatus.locks...timeLockedMicros

   The :data:`~serverStatus.locks...timeLockedMicros` document reports the amount of
   time in microseconds that a lock has existed in all databases in
   this :program:`mongod` instance.

.. data:: serverStatus.locks...timeLockedMicros.R

   The ``R`` field reports the amount of time in microseconds that any
   database has held the global read lock.

.. data:: serverStatus.locks...timeLockedMicros.W

   The ``W`` field reports the amount of time in microseconds that any
   database has held the global write lock.

.. data:: serverStatus.locks...timeLockedMicros.r

   The ``r`` field reports the amount of time in microseconds that any
   database has held the local read lock.

.. data:: serverStatus.locks...timeLockedMicros.w

   The ``w`` field reports the amount of time in microseconds that any
   database has held the local write lock.

.. data:: serverStatus.locks...timeAcquiringMicros

   The :data:`~serverStatus.locks...timeAcquiringMicros` document reports the amount of
   time in microseconds that operations have spent waiting to acquire
   a lock in all databases in this :program:`mongod` instance.

.. data:: serverStatus.locks...timeAcquiringMicros.R

   The ``R`` field reports the amount of time in microseconds that any
   database has spent waiting for the global read lock.

.. data:: serverStatus.locks...timeAcquiringMicros.W

   The ``W`` field reports the amount of time in microseconds that any
   database has spent waiting for the global write lock.

.. data:: serverStatus.locks.admin

   The :data:`~serverStatus.locks.admin` document contains two sub-documents that
   report data regarding lock use in the :term:`admin database`.

.. data:: serverStatus.locks.admin.timeLockedMicros

   The :data:`~serverStatus.locks.admin.timeLockedMicros` document reports the amount of
   time in microseconds that locks have existed in the context of the
   :term:`admin database`.

.. data:: serverStatus.locks.admin.timeLockedMicros.r

   The ``r`` field reports the amount of time in microseconds that the
   :term:`admin database` has held the read lock.

.. data:: serverStatus.locks.admin.timeLockedMicros.w

   The ``w`` field reports the amount of time in microseconds that the
   :term:`admin database` has held the write lock.

.. data:: serverStatus.locks.admin.timeAcquiringMicros

   The :data:`~serverStatus.locks.admin.timeAcquiringMicros` document reports on the
   amount of field time in microseconds that operations have spent
   waiting to acquire a lock for the :term:`admin database`.

.. data:: serverStatus.locks.admin.timeAcquiringMicros.r

   The ``r`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a read lock on the
   :term:`admin database`.

.. data:: serverStatus.locks.admin.timeAcquiringMicros.w

   The ``w`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a write lock on the
   :term:`admin database`.

.. data:: serverStatus.locks.local

   The :data:`~serverStatus.locks.local` document contains two sub-documents that
   report data regarding lock use in the ``local`` database. The
   local database contains a number of instance specific data,
   including the :term:`oplog` for replication.

.. data:: serverStatus.locks.local.timeLockedMicros

   The :data:`~serverStatus.locks.local.timeLockedMicros` document reports on the amount
   of time in microseconds that locks have existed in the context of
   the ``local`` database.

.. data:: serverStatus.locks.local.timeLockedMicros.r

   The ``r`` field reports the amount of time in microseconds that the
   ``local`` database has held the read lock.

.. data:: serverStatus.locks.local.timeLockedMicros.w

   The ``w`` field reports the amount of time in microseconds that the
   ``local`` database has held the write lock.

.. data:: serverStatus.locks.local.timeAcquiringMicros

   The :data:`~serverStatus.locks.local.timeAcquiringMicros` document reports on the
   amount of time in microseconds that operations have spent waiting
   to acquire a lock for the ``local`` database.

.. data:: serverStatus.locks.local.timeAcquiringMicros.r

   The ``r`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a read lock on the ``local``
   database.

.. data:: serverStatus.locks.local.timeAcquiringMicros.w

   The ``w`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a write lock on the ``local``
   database.

.. data:: serverStatus.locks.<database>

   For each additional database :data:`~serverStatus.locks` includes a document
   that reports on the lock use for this database. The names of these
   documents reflect the database name itself.

.. data:: serverStatus.locks.<database>.timeLockedMicros

   The :data:`~serverStatus.locks.<database>.timeLockedMicros` document reports on the amount
   of time in microseconds that locks have existed in the context of
   the ``<database>`` database.

.. data:: serverStatus.locks.<database>.timeLockedMicros.r

   The ``r`` field reports the amount of time in microseconds that the
   ``<database>`` database has held the read lock.

.. data:: serverStatus.locks.<database>.timeLockedMicros.w

   The ``w`` field reports the amount of time in microseconds that the
   ``<database>`` database has held the write lock.

.. data:: serverStatus.locks.<database>.timeAcquiringMicros

   The :data:`~serverStatus.locks.<database>.timeAcquiringMicros` document reports on the
   amount of time in microseconds that operations have spent waiting
   to acquire a lock for the ``<database>`` database.

.. data:: serverStatus.locks.<database>.timeAcquiringMicros.r

   The ``r`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a read lock on the ``<database>``
   database.

.. data:: serverStatus.locks.<database>.timeAcquiringMicros.w

   The ``w`` field reports the amount of time in microseconds that
   operations have spent waiting to acquire a write lock on the ``<database>``
   database.

.. _globallock:
.. _global-lock:
.. _server-status-globallock:
.. _server-status-global-lock:

globalLock
~~~~~~~~~~

For an example of the ``globalLock`` output, see the :ref:`globalLock
section <server-status-example-globallock>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.globalLock

   The :data:`~serverStatus.globalLock` data structure contains information regarding
   the database's current lock state, historical lock status, current
   operation queue, and the number of active clients.

.. data:: serverStatus.globalLock.totalTime

   The value of :data:`~serverStatus.globalLock.totalTime` represents the time, in
   microseconds, since the database last started and creation of the
   :data:`~serverStatus.globalLock`. This is roughly equivalent to total server
   uptime.

.. data:: serverStatus.globalLock.lockTime

   The value of :data:`~serverStatus.globalLock.lockTime` represents the time, in
   microseconds, since the database last started, that the
   :data:`~serverStatus.globalLock` has been *held*.

   Consider this value in combination with the value of
   :data:`~serverStatus.globalLock.totalTime`. MongoDB aggregates these values in
   the :data:`~serverStatus.globalLock.ratio` value. If the
   :data:`~serverStatus.globalLock.ratio` value is small but
   :data:`~serverStatus.globalLock.totalTime` is high the :data:`~serverStatus.globalLock` has
   typically been held frequently for shorter periods of time, which
   may be indicative of a more normal use pattern. If the
   :data:`~serverStatus.globalLock.lockTime` is higher and the
   :data:`~serverStatus.globalLock.totalTime` is smaller (relatively) then fewer
   operations are responsible for a greater portion of server's use
   (relatively).

.. data:: serverStatus.globalLock.ratio

   .. versionchanged:: 2.2
      :data:`~serverStatus.globalLock.ratio` was removed. See :data:`~serverStatus.locks`.

   The value of :data:`~serverStatus.globalLock.ratio` displays the relationship between
   :data:`~serverStatus.globalLock.lockTime` and :data:`~serverStatus.globalLock.totalTime`.

   Low values indicate that operations have held the :data:`~serverStatus.globalLock`
   frequently for shorter periods of time. High values indicate that
   operations have held :data:`~serverStatus.globalLock` infrequently for longer periods of
   time.

.. data:: serverStatus.globalLock.currentQueue

   The :data:`~serverStatus.globalLock.currentQueue` data structure value provides more
   granular information concerning the number of operations queued
   because of a lock.

.. data:: serverStatus.globalLock.currentQueue.total

   The value of :data:`~serverStatus.globalLock.currentQueue.total` provides a combined
   total of operations queued waiting for the lock.

   A consistently small queue, particularly of shorter operations
   should cause no concern. Also, consider this value in light of the
   size of queue waiting for the read lock
   (e.g. :data:`~serverStatus.globalLock.currentQueue.readers`) and write lock
   (e.g. :data:`~serverStatus.globalLock.currentQueue.writers`) individually.

.. data:: serverStatus.globalLock.currentQueue.readers

   The value of :data:`~serverStatus.globalLock.currentQueue.readers` is the number of
   operations that are currently queued and waiting for the
   read lock. A consistently small read-queue, particularly of
   shorter operations should cause no concern.

.. data:: serverStatus.globalLock.currentQueue.writers

   The value of :data:`~serverStatus.globalLock.currentQueue.writers` is the number of
   operations that are currently queued and waiting for the
   write lock. A consistently small write-queue, particularly of
   shorter operations is no cause for concern.

globalLock.activeClients
========================

.. data:: serverStatus.globalLock.activeClients

   The :data:`~serverStatus.globalLock.activeClients` data structure provides more
   granular information about the number of connected clients and the
   operation types (e.g. read or write) performed by these clients.

   Use this data to provide context for the
   :data:`~serverStatus.globalLock.currentQueue` data.

.. data:: serverStatus.globalLock.activeClients.total

   The value of :data:`~serverStatus.globalLock.activeClients.total` is the total number
   of active client connections to the database. This combines clients
   that are performing read operations
   (e.g. :data:`~serverStatus.globalLock.activeClients.readers`) and clients that
   are performing write operations (e.g. :data:`~serverStatus.globalLock.activeClients.writers`).

.. data:: serverStatus.globalLock.activeClients.readers

   The value of :data:`~serverStatus.globalLock.activeClients.readers` contains a count
   of the active client connections performing read operations.

.. data:: serverStatus.globalLock.activeClients.writers

   The value of :data:`~serverStatus.globalLock.activeClients.writers` contains a count
   of active client connections performing write operations.

.. _memory-status:
.. _server-status-memory:

mem
~~~

For an example of the ``mem`` output, see the :ref:`mem section
<server-status-example-memory>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.mem

   The :data:`~serverStatus.mem` data structure holds information regarding the target
   system architecture of :program:`mongod` and current memory use.

.. data:: serverStatus.mem.bits

   The value of :data:`~serverStatus.mem.bits` is either ``64`` or ``32``,
   depending on which target architecture specified during the
   :program:`mongod` compilation process. In most instances this is
   ``64``, and this value does not change over time.

.. data:: serverStatus.mem.resident

   The value of :data:`~serverStatus.mem.resident` is roughly equivalent to the amount
   of RAM, in megabytes (MB), currently used by the database process. In normal
   use this value tends to grow. In dedicated database servers this
   number tends to approach the total amount of system memory.

.. data:: serverStatus.mem.virtual

   :data:`~serverStatus.mem.virtual` displays the quantity, in
   megabytes (MB), of virtual memory used by the :program:`mongod`
   process. With :term:`journaling <journal>` enabled, the value of
   :data:`~serverStatus.mem.virtual` is at least twice the value of
   :data:`~serverStatus.mem.mapped`.

   If :data:`~serverStatus.mem.virtual` value is significantly larger
   than :data:`~serverStatus.mem.mapped` (e.g. 3 or more times), this
   may indicate a memory leak.

.. data:: serverStatus.mem.supported

   :data:`~serverStatus.mem.supported` is true when the underlying system supports
   extended memory information. If this value is false and the system
   does not support extended memory information, then other
   :data:`~serverStatus.mem` values may not be accessible to the database server.

.. data:: serverStatus.mem.mapped

   The value of :data:`~serverStatus.mem.mapped` provides the amount of mapped memory,
   in megabytes (MB), by
   the database. Because MongoDB uses memory-mapped files, this value
   is likely to be to be roughly equivalent to the total size of your
   database or databases.

.. data:: serverStatus.mem.mappedWithJournal

   :data:`~serverStatus.mem.mappedWithJournal` provides the amount of
   mapped memory, in megabytes (MB), including the memory used for
   journaling.
   This value will always be twice the value of :data:`~serverStatus.mem.mapped`.
   This field is only included if journaling is enabled.

.. _server-status-connections:

connections
~~~~~~~~~~~

For an example of the ``connections`` output, see the :ref:`connections
section <server-status-example-connections>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.connections

   The :data:`~serverStatus.connections` sub document data regarding the
   current connection status and availability of the database
   server. Use these values to asses the current load and capacity
   requirements of the server.

.. data:: serverStatus.connections.current

   The value of :data:`~serverStatus.connections.current` corresponds to the number of
   connections to the database server from clients. This number
   includes the current shell session. Consider the value of
   :data:`~serverStatus.connections.available` to add more context to this
   datum.

   This figure will include the current shell connection as well as
   any inter-node connections to support a :term:`replica set` or
   :term:`sharded cluster`.

.. data:: serverStatus.connections.available

   :data:`~serverStatus.connections.available` provides a count of the number of unused
   available connections that the database can provide. Consider this
   value in combination with the value of
   :data:`~serverStatus.connections.current` to understand the connection load on
   the database, and the :doc:`/reference/ulimit` document for
   more information about system thresholds on available connections.

.. data:: serverStatus.connections.totalCreated

   :data:`~serverStatus.connections.totalCreated` provides a count of
   **all** connections created to the server. This number includes
   connections that have since closed.

.. _server-status-extra-info:
.. _server-status-extra_info:
.. _server-status-extrainfo:

extra_info
~~~~~~~~~~

For an example of the ``extra_info`` output, see the :ref:`extra_info
section <server-status-example-extrainfo>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.extra_info

   The :data:`~serverStatus.extra_info` data structure holds data collected by the
   :program:`mongod` instance about the underlying system. Your system may
   only report a subset of these fields.

.. data:: serverStatus.extra_info.note

   The field :data:`~serverStatus.extra_info.note` reports that the data in this
   structure depend on the underlying platform, and has the text:
   "fields vary by platform."

.. data:: serverStatus.extra_info.heap_usage_bytes

   The :data:`~serverStatus.extra_info.heap_usage_bytes` field is only available on
   Unix/Linux systems, and reports the total size in bytes of heap space
   used by the database process.

.. data:: serverStatus.extra_info.page_faults

   The :data:`~serverStatus.extra_info.page_faults`
   Reports the total number of page faults that require
   disk operations. Page faults refer to operations that require the
   database server to access data which isn't available in active
   memory. The :data:`~serverStatus.extra_info.page_faults` counter may
   increase dramatically during
   moments of poor performance and may correlate with limited
   memory environments and larger data sets. Limited and sporadic page
   faults do not necessarily indicate an issue.

.. _server-status-indexcounters:

indexCounters
~~~~~~~~~~~~~

For an example of the ``indexCounters`` output, see the
:ref:`indexCounters section <server-status-example-indexcounters>` of
the :doc:`/reference/server-status` page.

.. data:: serverStatus.indexCounters

   .. versionchanged:: 2.2
      Previously, data in the :data:`~serverStatus.indexCounters` document
      reported sampled data, and were only useful in relative
      comparison to each other, because they could not reflect
      absolute index use. In 2.2 and later, these data reflect actual
      index use.

   .. versionchanged:: 2.4
      Fields previously in the ``btree`` sub-document of
      :data:`~serverStatus.indexCounters` are now fields in the
      :data:`~serverStatus.indexCounters` document.

   The :data:`~serverStatus.indexCounters` data structure reports information
   regarding the state and use of indexes in MongoDB.

.. data:: serverStatus.indexCounters.accesses

   :data:`~serverStatus.indexCounters.accesses` reports the number of times
   that operations have accessed indexes. This value is the
   combination of the :data:`~serverStatus.indexCounters.hits` and
   :data:`~serverStatus.indexCounters.misses`. Higher values indicate that
   your database has indexes and that queries are taking advantage of
   these indexes. If this number does not grow over time, this might
   indicate that your indexes do not effectively support your use.

.. data:: serverStatus.indexCounters.hits

   The :data:`~serverStatus.indexCounters.hits` value reflects the number of
   times that an index has been accessed and :program:`mongod` is able
   to return the index from memory.

   A higher value indicates effective index
   use. :data:`~serverStatus.indexCounters.hits` values that represent a
   greater proportion of the :data:`~serverStatus.indexCounters.accesses`
   value, tend to indicate more effective index configuration.

.. data:: serverStatus.indexCounters.misses

   The :data:`~serverStatus.indexCounters.misses` value represents the
   number of times that an operation attempted to access an index that
   was not in memory. These "misses," do not indicate a failed query
   or operation, but rather an inefficient use of the index. Lower
   values in this field indicate better index use and likely overall
   performance as well.

.. data:: serverStatus.indexCounters.resets

   The :data:`~serverStatus.indexCounters.resets` value reflects the number
   of times that the index counters have been reset since the database
   last restarted. Typically this value is ``0``, but use this value
   to provide context for the data specified by other
   :data:`~serverStatus.indexCounters` values.

.. data:: serverStatus.indexCounters.missRatio

   The :data:`~serverStatus.indexCounters.missRatio` value is the ratio of
   :data:`~serverStatus.indexCounters.hits` to
   :data:`~serverStatus.indexCounters.misses`. This value is
   typically ``0`` or approaching ``0``.

.. _server-status-backgroundflushing:
.. _server-status-background-flushing:

backgroundFlushing
~~~~~~~~~~~~~~~~~~

For an example of the ``backgroundFlushing`` output, see the
:ref:`backgroundFlushing section
<server-status-example-backgroundflushing>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.backgroundFlushing

   :program:`mongod` periodically flushes writes to disk. In the default
   configuration, this happens every 60 seconds. The
   :data:`~serverStatus.backgroundFlushing` data structure contains data regarding
   these operations. Consider these values if you have concerns about
   write performance and :ref:`journaling <journaling-status>`.

.. data:: serverStatus.backgroundFlushing.flushes

   :data:`~serverStatus.backgroundFlushing.flushes` is a counter that collects the
   number of times the database has flushed all writes to disk. This
   value will grow as database runs for longer periods of time.

.. data:: serverStatus.backgroundFlushing.total_ms

   The :data:`~serverStatus.backgroundFlushing.total_ms` value provides the total number
   of milliseconds (ms) that the :program:`mongod` processes have spent
   writing (i.e. flushing) data to disk. Because this is an absolute
   value, consider the value of :data:`~serverStatus.backgroundFlushing.flushes`
   and :data:`~serverStatus.backgroundFlushing.average_ms` to provide better
   context for this datum.

.. data:: serverStatus.backgroundFlushing.average_ms

   The :data:`~serverStatus.backgroundFlushing.average_ms` value describes the
   relationship between the number of flushes and the total amount of
   time that the database has spent writing data to disk. The larger
   :data:`~serverStatus.backgroundFlushing.flushes` is, the more likely this value
   is likely to represent a "normal," time; however, abnormal data can
   skew this value.

   Use the :data:`~serverStatus.backgroundFlushing.last_ms` to ensure that a high
   average is not skewed by transient historical issue or a
   random write distribution.

.. data:: serverStatus.backgroundFlushing.last_ms

   The value of the :data:`~serverStatus.backgroundFlushing.last_ms` field is the amount
   of time, in milliseconds, that the last flush operation took to
   complete. Use this value to verify that the current performance of
   the server and is in line with the historical data provided by
   :data:`~serverStatus.backgroundFlushing.average_ms` and
   :data:`~serverStatus.backgroundFlushing.total_ms`.

.. data:: serverStatus.backgroundFlushing.last_finished

   The :data:`~serverStatus.backgroundFlushing.last_finished` field provides a timestamp
   of the last completed flush operation in the :term:`ISODate`
   format. If this value is more than a few minutes old relative to
   your server's current time and accounting for differences in time
   zone, restarting the database may result in some data loss.

   Also consider ongoing operations that might skew this value by
   routinely block write operations.

.. _server-status-cursors:

cursors
~~~~~~~

For an example of the ``cursors`` output, see the :ref:`cursors section
<server-status-example-cursors>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.cursors

   The :data:`~serverStatus.cursors` data structure contains data regarding cursor state
   and use.

.. data:: serverStatus.cursors.totalOpen

   :data:`~serverStatus.cursors.totalOpen` provides the number of cursors that
   MongoDB is maintaining for clients. Because MongoDB exhausts unused
   cursors, typically this value small or zero. However, if there is a
   queue, stale tailable cursors, or a large number of operations this
   value may rise.

.. data:: serverStatus.cursors.clientCursors_size

   .. deprecated:: 1.x
      See :data:`~serverStatus.cursors.totalOpen` for this datum.

.. data:: serverStatus.cursors.timedOut

   :data:`~serverStatus.cursors.timedOut` provides a counter of the total number
   of cursors that have timed out since the server process started. If
   this number is large or growing at a regular rate, this may
   indicate an application error.

.. _server-status-network:

network
~~~~~~~

For an example of the ``network`` output, see the :ref:`network section
<server-status-example-network>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.network

   The :data:`~serverStatus.network` data structure contains data regarding MongoDB's
   network use.

.. data:: serverStatus.network.bytesIn

   The value of the :data:`~serverStatus.network.bytesIn` field reflects the
   amount of network traffic, in bytes, received *by* this
   database. Use this value to ensure that network traffic sent to the
   :program:`mongod` process is consistent with expectations and
   overall inter-application traffic.

.. data:: serverStatus.network.bytesOut

   The value of the :data:`~serverStatus.network.bytesOut` field reflects the amount of
   network traffic, in bytes, sent *from* this database. Use this
   value to ensure that network traffic sent by the :program:`mongod` process
   is consistent with expectations and overall inter-application
   traffic.

.. data:: serverStatus.network.numRequests

   The :data:`~serverStatus.network.numRequests` field is a counter of the total number
   of distinct requests that the server has received. Use this value
   to provide context for the :data:`~serverStatus.network.bytesIn` and
   :data:`~serverStatus.network.bytesOut` values to ensure that MongoDB's network
   utilization is consistent with expectations and application use.

.. _server-status-repl:

repl
~~~~

For an example of the ``repl`` output, see the :ref:`repl section
<server-status-example-repl>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.repl

   The :data:`~serverStatus.repl` data structure contains status
   information for MongoDB's replication (i.e. "replica set")
   configuration. These values only appear when the current host has
   replication enabled.

   See :doc:`/replication` for more information on replication.

.. data:: serverStatus.repl.setName

   The :data:`~serverStatus.repl.setName` field contains a string with
   the name of the current replica set. This value reflects the
   :option:`--replSet <mongod --replSet>` command line argument, or
   :setting:`~replication.replSetName` value in the configuration file.

   See :doc:`/replication` for more information on replication.

.. data:: serverStatus.repl.ismaster

   The value of the :data:`~serverStatus.repl.ismaster` field is
   either ``true`` or ``false`` and reflects whether the current node
   is the master or primary node in the replica set.

   See :doc:`/replication` for more information on replication.

.. data:: serverStatus.repl.secondary

   The value of the :data:`~serverStatus.repl.secondary` field is
   either ``true`` or ``false`` and reflects whether the current node
   is a secondary node in the replica set.

   See :doc:`/replication` for more information on replication.

.. data:: serverStatus.repl.hosts

   :data:`~serverStatus.repl.hosts` is an array that lists the other
   nodes in the current replica set. Each member of the replica set
   appears in the form of ``hostname:port``.

   See :doc:`/replication` for more information on replication.

.. _server-status-opcountersrepl:
.. _server-status-opcounters-repl:

opcountersRepl
~~~~~~~~~~~~~~

For an example of the ``opcountersRepl`` output, see the
:ref:`opcountersRepl section <server-status-example-opcountersrepl>` of
the :doc:`/reference/server-status` page.

.. data:: serverStatus.opcountersRepl

   The :data:`~serverStatus.opcountersRepl` data structure, similar to the
   :data:`~serverStatus.opcounters` data structure, provides an overview of
   database replication operations by type and makes it possible to
   analyze the load on the replica in more granular manner.  These
   values only appear when the current host has replication enabled.

   These values will differ from the :data:`~serverStatus.opcounters` values
   because of how MongoDB serializes operations during replication.
   See :doc:`/replication` for more information on replication.

   These numbers will grow over time in response to database
   use. Analyze these values over time to track database utilization.

.. data:: serverStatus.opcountersRepl.insert

   :data:`~serverStatus.opcountersRepl.insert` provides a counter of the total number
   of replicated insert operations since the :program:`mongod` instance
   last started.

.. data:: serverStatus.opcountersRepl.query

   :data:`~serverStatus.opcountersRepl.query` provides a counter of the total number
   of replicated queries since the :program:`mongod` instance last
   started.

.. data:: serverStatus.opcountersRepl.update

   :data:`~serverStatus.opcountersRepl.update` provides a counter of the total number
   of replicated update operations since the :program:`mongod` instance
   last started.

.. data:: serverStatus.opcountersRepl.delete

   :data:`~serverStatus.opcountersRepl.delete` provides a counter of the total number
   of replicated delete operations since the :program:`mongod` instance
   last started.

.. data:: serverStatus.opcountersRepl.getmore

   :data:`~serverStatus.opcountersRepl.getmore` provides a counter of the total number
   of "getmore" operations since the :program:`mongod` instance last
   started. This counter can be high even if the query count is low.
   Secondary nodes send ``getMore`` operations as part of the replication
   process.

.. data:: serverStatus.opcountersRepl.command

   :data:`~serverStatus.opcountersRepl.command` provides a counter of the total number
   of replicated commands issued to the database since the
   :program:`mongod` instance last started.

.. _server-status-opcounters:

opcounters
~~~~~~~~~~

For an example of the ``opcounters`` output, see the :ref:`opcounters
section <server-status-example-opcounters>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.opcounters

   The :data:`~serverStatus.opcounters` data structure provides an overview of
   database operations by type and makes it possible to analyze the
   load on the database in more granular manner.

   These numbers will grow over time and in response to database
   use. Analyze these values over time to track database utilization.

   .. note::

      The data in :data:`~serverStatus.opcounters` treats operations
      that affect multiple documents, such as bulk insert or
      multi-update operations, as a single operation. See
      :data:`~serverStatus.metrics.document` for more granular
      document-level operation tracking.

.. data:: serverStatus.opcounters.insert

   :data:`~serverStatus.opcounters.insert` provides a counter of the total number
   of insert operations since the :program:`mongod` instance last
   started.

.. data:: serverStatus.opcounters.query

   :data:`~serverStatus.opcounters.query` provides a counter of the total number
   of queries since the :program:`mongod` instance last started.

.. data:: serverStatus.opcounters.update

   :data:`~serverStatus.opcounters.update` provides a counter of the total number
   of update operations since the :program:`mongod` instance last
   started.

.. data:: serverStatus.opcounters.delete

   :data:`~serverStatus.opcounters.delete` provides a counter of the total number
   of delete operations since the :program:`mongod` instance last
   started.

.. data:: serverStatus.opcounters.getmore

   :data:`~serverStatus.opcounters.getmore` provides a counter of the total number
   of "getmore" operations since the :program:`mongod` instance last
   started. This counter can be high even if the query count is low.
   Secondary nodes send ``getMore`` operations as part of the replication
   process.

.. data:: serverStatus.opcounters.command

   :data:`~serverStatus.opcounters.command` provides a counter of the total number
   of commands issued to the database since the :program:`mongod`
   instance last started.

.. _server-status-asserts:

asserts
~~~~~~~

For an example of the ``asserts`` output, see the :ref:`asserts section
<server-status-example-asserts>` of the :doc:`/reference/server-status`
page.

.. data:: serverStatus.asserts

   The :data:`~serverStatus.asserts` document reports the number of asserts on the
   database. While assert errors are typically uncommon, if there are
   non-zero values for the :data:`~serverStatus.asserts`, you should check the log
   file for the :program:`mongod` process for more information. In
   many cases these errors are trivial, but are worth investigating.

.. data:: serverStatus.asserts.regular

   The :data:`~serverStatus.asserts.regular` counter tracks the number of regular
   assertions raised since the server process started. Check the log
   file for more information about these messages.

.. data:: serverStatus.asserts.warning

   The :data:`~serverStatus.asserts.warning` counter tracks the number of warnings
   raised since the server process started. Check the log file for
   more information about these warnings.

.. data:: serverStatus.asserts.msg

   The :data:`~serverStatus.asserts.msg` counter tracks the number of message
   assertions raised since the server process started. Check the log
   file for more information about these messages.

.. data:: serverStatus.asserts.user

   The :data:`~serverStatus.asserts.user` counter reports the number of "user asserts"
   that have occurred since the last time the server process
   started. These are errors that user may generate, such as out of
   disk space or duplicate key. You can prevent these assertions by
   fixing a problem with your application or deployment. Check the
   MongoDB log for more information.

.. data:: serverStatus.asserts.rollovers

   The :data:`~serverStatus.asserts.rollovers` counter displays the number of
   times that the rollover counters have rolled over since the last
   time the server process started. The counters will rollover to zero
   after 2\ :superscript:`30` assertions. Use this value to provide
   context to the other values in the :data:`~serverStatus.asserts` data structure.

.. _server-status-writebacksqueued:
.. _server-status-write-backs-queued:

writeBacksQueued
~~~~~~~~~~~~~~~~

For an example of the ``writeBacksQueued`` output, see the
:ref:`writeBacksQueued section
<server-status-example-writebacksqueued>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.writeBacksQueued

   The value of :data:`~serverStatus.writeBacksQueued` is ``true`` when there
   are operations from a :program:`mongos` instance queued for
   retrying. Typically this option is false.

   .. seealso:: :term:`writeBacks`

.. _durability-status:
.. _journaling-status:
.. _server-status-journaling:

Journaling (dur)
~~~~~~~~~~~~~~~~

.. versionadded:: 1.8

For an example of the ``Journaling (dur)`` output, see the
:ref:`journaling section <server-status-example-journaling>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.dur

   The :data:`~serverStatus.dur` (for "durability") document contains data
   regarding the :program:`mongod`'s journaling-related operations and
   performance. :program:`mongod` must be running with journaling for
   these data to appear in the output of ":dbcommand:`serverStatus`".

   MongoDB reports the data in :data:`~serverStatus.dur` based on 3
   second intervals of data, collected between 3 and 6 seconds in the
   past.

   .. seealso:: :doc:`/core/journaling` for more information about journaling operations.

.. data:: serverStatus.dur.commits

   The :data:`~serverStatus.dur.commits` provides the number of transactions
   written to the :term:`journal` during the last :ref:`journal group
   commit interval <journaling-journal-commit-interval>`.

.. data:: serverStatus.dur.journaledMB

   The :data:`~serverStatus.dur.journaledMB` provides the amount of data in
   megabytes (MB) written to :term:`journal` during the last
   :ref:`journal group commit interval
   <journaling-record-write-operation>`.

.. data:: serverStatus.dur.writeToDataFilesMB

   The :data:`~serverStatus.dur.writeToDataFilesMB` provides the amount of data in
   megabytes (MB) written from :term:`journal` to the data files during the
   last :ref:`journal group commit interval <journaling-record-write-operation>`.

.. data:: serverStatus.dur.compression

   .. versionadded:: 2.0

   The :data:`~serverStatus.dur.compression` represents the compression ratio of
   the data written to the :term:`journal`:

   .. code-block:: javascript

      ( journaled_size_of_data / uncompressed_size_of_data )

.. data:: serverStatus.dur.commitsInWriteLock

   The :data:`~serverStatus.dur.commitsInWriteLock` provides a count of the commits
   that occurred while a write lock was held. Commits in a write lock
   indicate a MongoDB node under a heavy write load and call for
   further diagnosis.

.. data:: serverStatus.dur.earlyCommits

   The :data:`~serverStatus.dur.earlyCommits` value reflects the number of times
   MongoDB requested a commit before the scheduled :ref:`journal group
   commit interval <journaling-record-write-operation>`. Use this
   value to ensure that your :ref:`journal group commit interval
   <journaling-journal-commit-interval>` is not too long for your
   deployment.

.. data:: serverStatus.dur.timeMS

   The :data:`~serverStatus.dur.timeMS` document provides information about the
   performance of the :program:`mongod` instance during the various
   phases of journaling in the last :ref:`journal group commit
   interval <journaling-journal-commit-interval>`.

.. data:: serverStatus.dur.timeMS.dt

   The :data:`~serverStatus.dur.timeMS.dt` value provides, in milliseconds, the
   amount of time over which MongoDB collected the :data:`~serverStatus.dur.timeMS`
   data. Use this field to provide context to the other
   :data:`~serverStatus.dur.timeMS` field values.

.. data:: serverStatus.dur.timeMS.prepLogBuffer

   The :data:`~serverStatus.dur.timeMS.prepLogBuffer` value provides, in milliseconds,
   the amount of time spent preparing to write to the journal. Smaller
   values indicate better journal performance.

.. data:: serverStatus.dur.timeMS.writeToJournal

   The :data:`~serverStatus.dur.timeMS.writeToJournal` value provides, in milliseconds,
   the amount of time spent actually writing to the journal. File
   system speeds and device interfaces can affect performance.

.. data:: serverStatus.dur.timeMS.writeToDataFiles

   The :data:`~serverStatus.dur.timeMS.writeToDataFiles` value provides, in
   milliseconds, the amount of time spent writing to data files after
   journaling. File system speeds and device interfaces can affect
   performance.

.. data:: serverStatus.dur.timeMS.remapPrivateView

   The :data:`~serverStatus.dur.timeMS.remapPrivateView` value provides, in
   milliseconds, the amount of time spent remapping copy-on-write memory
   mapped views. Smaller values indicate better journal performance.

.. _server-status-recordstats:
.. _server-status-record-stats:

recordStats
~~~~~~~~~~~

For an example of the ``recordStats`` output, see the :ref:`recordStats
section <server-status-example-recordstats>` of the
:doc:`/reference/server-status` page.

.. versionadded::2.1.2
   All :data:`~serverStatus.recordStats` statuses first appeared in the 2.1.2
   development release for the 2.2 series.

.. data:: serverStatus.recordStats

   The :data:`~serverStatus.recordStats` document provides fine grained reporting
   on page faults on a per database level.

   MongoDB uses a read lock on each database to return
   :data:`~serverStatus.recordStats`. To minimize this overhead, you
   can disable this section, as in the following operation:

   .. code-block:: javascript

      db.serverStatus( { recordStats: 0 } )

.. data:: serverStatus.recordStats.accessesNotInMemory

   :data:`~serverStatus.recordStats.accessesNotInMemory` reflects the number of
   times :program:`mongod` needed to access a memory page that was
   *not* resident in memory for *all* databases managed by this
   :program:`mongod` instance.

.. data:: serverStatus.recordStats.pageFaultExceptionsThrown

   :data:`~serverStatus.recordStats.pageFaultExceptionsThrown` reflects the
   number of page fault exceptions thrown by :program:`mongod` when
   accessing data for *all* databases managed by this
   :program:`mongod` instance.

.. data:: serverStatus.recordStats.local.accessesNotInMemory

   :data:`~serverStatus.recordStats.local.accessesNotInMemory` reflects the number of
   times :program:`mongod` needed to access a memory page that was
   *not* resident in memory for the ``local`` database.

.. data:: serverStatus.recordStats.local.pageFaultExceptionsThrown

   :data:`~serverStatus.recordStats.local.pageFaultExceptionsThrown` reflects the
   number of page fault exceptions thrown by :program:`mongod` when
   accessing data for the ``local`` database.

.. data:: serverStatus.recordStats.admin.accessesNotInMemory

   :data:`~serverStatus.recordStats.admin.accessesNotInMemory` reflects the number of
   times :program:`mongod` needed to access a memory page that was
   *not* resident in memory for the :term:`admin database`.

.. data:: serverStatus.recordStats.admin.pageFaultExceptionsThrown

   :data:`~serverStatus.recordStats.admin.pageFaultExceptionsThrown` reflects the
   number of page fault exceptions thrown by :program:`mongod` when
   accessing data for the :term:`admin database`.

.. data:: serverStatus.recordStats.<database>.accessesNotInMemory

   :data:`~serverStatus.recordStats.<database>.accessesNotInMemory` reflects the number of
   times :program:`mongod` needed to access a memory page that was
   *not* resident in memory for the ``<database>`` database.

.. data:: serverStatus.recordStats.<database>.pageFaultExceptionsThrown

   :data:`~serverStatus.recordStats.<database>.pageFaultExceptionsThrown` reflects the
   number of page fault exceptions thrown by :program:`mongod` when
   accessing data for the ``<database>`` database.

.. _server-status-workingset:
.. _server-status-working-set:

workingSet
~~~~~~~~~~

.. versionadded:: 2.4

.. note:: The :data:`~serverStatus.workingSet` data is only included in
   the output of :dbcommand:`serverStatus` if explicitly enabled. To
   return the :data:`~serverStatus.workingSet`, use one of the
   following commands:

   .. code-block:: javascript

      db.serverStatus( { workingSet: 1 } )
      db.runCommand( { serverStatus: 1, workingSet: 1 } )

For an example of the ``workingSet`` output, see the :ref:`workingSet
section <server-status-example-workingset>` of the
:doc:`/reference/server-status` page.

.. data:: serverStatus.workingSet

   :data:`~serverStatus.workingSet` is a document that contains values
   useful for estimating the size of the working set, which is the
   amount of data that MongoDB uses
   actively. :data:`~serverStatus.workingSet` uses an internal data
   structure that tracks pages accessed by :program:`mongod`.

.. data:: serverStatus.workingSet.note

   :data:`~serverStatus.workingSet.note` is a field that holds a
   string warning that the :data:`~serverStatus.workingSet`
   document is an estimate.

.. data:: serverStatus.workingSet.pagesInMemory

   :data:`~serverStatus.workingSet.pagesInMemory` contains a count of
   the total number of pages accessed by :program:`mongod` over the
   period displayed in
   :data:`~serverStatus.workingSet.overSeconds`. The default page size
   is 4 kilobytes: to convert this value to the amount of data in
   memory multiply this value by 4 kilobytes.

   If your total working set is less than the size of physical
   memory, over time the value of
   :data:`~serverStatus.workingSet.pagesInMemory` will reflect your data
   size.

   Use :data:`~serverStatus.workingSet.pagesInMemory` in conjunction
   with :data:`~serverStatus.workingSet.overSeconds` to help estimate
   the actual size of the working set.

.. data:: serverStatus.workingSet.computationTimeMicros

   :data:`~serverStatus.workingSet.computationTimeMicros` reports the
   amount of time the :program:`mongod` instance used to compute the other
   fields in the :data:`~serverStatus.workingSet` section.

   Reporting on :data:`~serverStatus.workingSet` may impact the
   performance of other operations on the :program:`mongod` instance
   because MongoDB must collect some data within the context of a
   lock. Ensure that automated monitoring tools consider this metric
   when determining the frequency of collection for
   :data:`~serverStatus.workingSet`.

.. data:: serverStatus.workingSet.overSeconds

   :data:`~serverStatus.workingSet.overSeconds` returns the amount of
   time elapsed between the newest and oldest pages tracked in the
   :data:`~serverStatus.workingSet.pagesInMemory` data point.

   If :data:`~serverStatus.workingSet.overSeconds` is decreasing, or
   if :data:`~serverStatus.workingSet.pagesInMemory` equals physical
   RAM *and* :data:`~serverStatus.workingSet.overSeconds` is very
   small, the working set may be much *larger* than physical RAM.

   When :data:`~serverStatus.workingSet.overSeconds` is large,
   MongoDB's data set is equal to or *smaller* than physical RAM.

.. _server-status-metrics:

metrics
~~~~~~~

For an example of the metrics output, see the :ref:`metrics section
<server-status-example-metrics>` of the :doc:`/reference/server-status`
page.

.. versionadded:: 2.4

.. data:: serverStatus.metrics

   The :data:`~serverStatus.metrics` document holds a number of
   statistics that reflect the current use and state of a running
   :program:`mongod` instance.

.. data:: serverStatus.metrics.document

   The :data:`~serverStatus.metrics.document` holds a document of that
   reflect document access and modification patterns and data
   use. Compare these values to the data in the
   :data:`~serverStatus.opcounters` document, which track total number
   of operations.

.. data:: serverStatus.metrics.document.deleted

   :data:`~serverStatus.metrics.document.deleted` reports the total
   number of documents deleted.

.. data:: serverStatus.metrics.document.inserted

   :data:`~serverStatus.metrics.document.inserted` reports the total
   number of documents inserted.

.. data:: serverStatus.metrics.document.returned

   :data:`~serverStatus.metrics.document.returned` reports the total
   number of documents returned by queries.

.. data:: serverStatus.metrics.document.updated

   :data:`~serverStatus.metrics.document.updated` reports the total
   number of documents updated.

.. data:: serverStatus.metrics.getLastError

   :data:`~serverStatus.metrics.getLastError` is a document that
   reports on :dbcommand:`getLastError` use.

.. data:: serverStatus.metrics.getLastError.wtime

   :data:`~serverStatus.metrics.getLastError.wtime` is a sub-document
   that reports :dbcommand:`getLastError` operation counts with a
   ``w`` argument greater than ``1``.

.. data:: serverStatus.metrics.getLastError.wtime.num

   :data:`~serverStatus.metrics.getLastError.wtime.num` reports the
   total number of :dbcommand:`getLastError` operations with a
   specified write concern (i.e. ``w``) that wait for one or more
   members of a replica set to acknowledge the write operation (i.e. a
   ``w`` value greater than ``1``.)

.. data:: serverStatus.metrics.getLastError.wtime.totalMillis

   :data:`~serverStatus.metrics.getLastError.wtime.totalMillis`
   reports the total amount of time in milliseconds that the
   :program:`mongod` has spent performing :dbcommand:`getLastError`
   operations with write concern (i.e.  ``w``) that wait for one or
   more members of a replica set to acknowledge the write operation
   (i.e. a ``w`` value greater than ``1``.)

.. data:: serverStatus.metrics.getLastError.wtimeouts

   :data:`~serverStatus.metrics.getLastError.wtimeouts` reports the
   number of times that :term:`write concern` operations have timed out
   as a result of the ``wtimeout`` threshold to
   :dbcommand:`getLastError`.

.. data:: serverStatus.metrics.operation

   :data:`~serverStatus.metrics.operation` is a sub-document that
   holds counters for several types of update and query operations
   that MongoDB handles using special operation types.

.. data:: serverStatus.metrics.operation.fastmod

   :data:`~serverStatus.metrics.operation.fastmod` reports the number
   of :doc:`update </core/write-operations>` operations that neither
   cause documents to grow nor require updates to the index. For
   example, this counter would record an update operation that use the
   :update:`$inc` operator to increment the value of a field that is
   not indexed.

.. data:: serverStatus.metrics.operation.idhack

   :data:`~serverStatus.metrics.operation.idhack` reports the number
   of queries that contain the ``_id`` field. For these queries,
   MongoDB will use default index on the ``_id`` field and skip all
   query plan analysis.

.. data:: serverStatus.metrics.operation.scanAndOrder

   :data:`~serverStatus.metrics.operation.scanAndOrder` reports the
   total number of queries that return sorted numbers that cannot
   perform the sort operation using an index.

.. data:: serverStatus.metrics.queryExecutor

   :data:`~serverStatus.metrics.queryExecutor` is a document that
   reports data from the query execution system.

.. data:: serverStatus.metrics.queryExecutor.scanned

   :data:`~serverStatus.metrics.queryExecutor.scanned` reports the total
   number of index items scanned during queries and query-plan
   evaluation. This counter is the same as :data:`~explain.nscanned`
   in the output of :method:`~cursor.explain()`.

.. data:: serverStatus.metrics.record

   :data:`~serverStatus.metrics.record` is a document that reports
   data related to record allocation in the on-disk memory files.

.. data:: serverStatus.metrics.record.moves

   :data:`~serverStatus.metrics.record.moves` reports the total number
   of times documents move within the on-disk representation of the
   MongoDB data set. Documents move as a result of operations that
   increase the size of the document beyond their allocated record
   size.

.. _server-status-replnetworkqueue:
.. _server-status-repl-network-queue:

.. data:: serverStatus.metrics.repl

   :data:`~serverStatus.metrics.repl` holds a sub-document that
   reports metrics related to the replication process.
   :data:`~serverStatus.metrics.repl` document appears on all
   :program:`mongod` instances, even those that aren't members of
   :term:`replica sets <replica set>`.

.. data:: serverStatus.metrics.repl.apply

   :data:`~serverStatus.metrics.repl.apply` holds a sub-document that
   reports on the application of operations from the replication
   :term:`oplog`.

.. data:: serverStatus.metrics.repl.apply.batches

   :data:`~serverStatus.metrics.repl.apply.batches` reports on the
   oplog application process on :term:`secondaries <secondary>`
   members of replica sets. See
   :ref:`replica-set-internals-multi-threaded-replication` for more
   information on the oplog application processes

.. data:: serverStatus.metrics.repl.apply.batches.num

   :data:`~serverStatus.metrics.repl.apply.batches.num` reports the
   total number of batches applied across all databases.

.. data:: serverStatus.metrics.repl.apply.batches.totalMillis

   :data:`~serverStatus.metrics.repl.apply.batches.totalMillis` reports
   the total amount of time the :program:`mongod` has spent applying
   operations from the oplog.

.. data:: serverStatus.metrics.repl.apply.ops

   :data:`~serverStatus.metrics.repl.apply.ops` reports the total
   number of :term:`oplog` operations applied.

.. data:: serverStatus.metrics.repl.buffer

   MongoDB buffers oplog operations from the replication sync source
   buffer before applying oplog entries in a
   batch. :data:`~serverStatus.metrics.repl.buffer` provides a way to
   track the oplog buffer. See
   :ref:`replica-set-internals-multi-threaded-replication` for more
   information on the oplog application process.

.. data:: serverStatus.metrics.repl.buffer.count

   :data:`~serverStatus.metrics.repl.buffer.count` reports the current
   number of operations in the oplog buffer.

.. data:: serverStatus.metrics.repl.buffer.maxSizeBytes

   :data:`~serverStatus.metrics.repl.buffer.maxSizeBytes` reports the
   maximum size of the buffer. This value is a constant setting in
   the :program:`mongod`, and is not configurable.

.. data:: serverStatus.metrics.repl.buffer.sizeBytes

   :data:`~serverStatus.metrics.repl.buffer.sizeBytes` reports the current
   size of the contents of the oplog buffer.

.. data:: serverStatus.metrics.repl.network

   :data:`~serverStatus.metrics.repl.network` reports network use by
   the replication process.

.. data:: serverStatus.metrics.repl.network.bytes

   :data:`~serverStatus.metrics.repl.network.bytes` reports the total
   amount of data read from the replication sync source.

.. data:: serverStatus.metrics.repl.network.getmores

   :data:`~serverStatus.metrics.repl.network.getmores` reports on the
   ``getmore`` operations, which are requests for additional results
   from the oplog :term:`cursor` as part of the oplog replication
   process.

   .. TODO move the documentation of getmore into some central place

.. data:: serverStatus.metrics.repl.network.getmores.num

   :data:`~serverStatus.metrics.repl.network.getmores.num` reports the
   total number of ``getmore`` operations, which are operations that
   request an additional set of operations from the replication sync
   source.

.. data:: serverStatus.metrics.repl.network.getmores.totalMillis

   :data:`~serverStatus.metrics.repl.network.getmores.totalMillis`
   reports the total amount of time required to collect data from
   ``getmore`` operations.

   .. note::

      This number can be quite large, as MongoDB will wait for more
      data even if the ``getmore`` operation does not initial return
      data.

.. data:: serverStatus.metrics.repl.network.ops

   :data:`~serverStatus.metrics.repl.network.ops` reports the total
   number of operations read from the replication source.

.. data:: serverStatus.metrics.repl.network.readersCreated

   :data:`~serverStatus.metrics.repl.network.readersCreated` reports
   the total number of oplog query processes created. MongoDB will
   create a new oplog query any time an error occurs in the
   connection, including a timeout, or a network
   operation. Furthermore,
   :data:`~serverStatus.metrics.repl.network.readersCreated` will
   increment every time MongoDB selects a new source fore replication.

.. data:: serverStatus.metrics.repl.oplog

   :data:`~serverStatus.metrics.repl.oplog` is a document that reports
   on the size and use of the :term:`oplog` by this :program:`mongod`
   instance.

.. data:: serverStatus.metrics.repl.oplog.insert

   :data:`~serverStatus.metrics.repl.oplog.insert` is a document that
   reports insert operations into the :term:`oplog`.

.. data:: serverStatus.metrics.repl.oplog.insert.num

   :data:`~serverStatus.metrics.repl.oplog.insert.num` reports the total
   number of items inserted into the :term:`oplog`.

.. data:: serverStatus.metrics.repl.oplog.insert.totalMillis

   :data:`~serverStatus.metrics.repl.oplog.insert.totalMillis` reports the
   total amount of time spent for the :program:`mongod` to insert data
   into the :term:`oplog`.

.. data:: serverStatus.metrics.repl.oplog.insertBytes

   :data:`~serverStatus.metrics.repl.oplog.insertBytes` the total size
   of documents inserted into the oplog.

.. data:: serverStatus.metrics.repl.preload

   :data:`~serverStatus.metrics.repl.preload` reports on the
   "pre-fetch" stage, where MongoDB loads documents and indexes into
   RAM to improve replication throughput.

   See :ref:`replica-set-internals-multi-threaded-replication` for
   more information about the *pre-fetch* stage of the replication
   process.

.. data:: serverStatus.metrics.repl.preload.docs

   :data:`~serverStatus.metrics.repl.preload.docs` is a sub-document
   that reports on the documents loaded into memory during the
   *pre-fetch* stage.

.. data:: serverStatus.metrics.repl.preload.docs.num

   :data:`~serverStatus.metrics.repl.preload.docs.num` reports the
   total number of documents loaded during the *pre-fetch* stage of
   replication.

.. data:: serverStatus.metrics.repl.preload.docs.totalMillis

   :data:`~serverStatus.metrics.repl.preload.docs.totalMillis` reports
   the total amount of time spent loading documents as part of
   the *pre-fetch* stage of replication.

.. data:: serverStatus.metrics.repl.preload.indexes

   :data:`~serverStatus.metrics.repl.preload.indexes` is a
   sub-document that reports on the index items loaded into
   memory during the *pre-fetch* stage of replication.

   See :ref:`replica-set-internals-multi-threaded-replication` for
   more information about the *pre-fetch* stage of replication.

.. data:: serverStatus.metrics.repl.preload.indexes.num

   :data:`~serverStatus.metrics.repl.preload.indexes.num` reports the
   total number of index entries loaded by members before updating
   documents as part of the *pre-fetch* stage of replication.

.. data:: serverStatus.metrics.repl.preload.indexes.totalMillis

   :data:`~serverStatus.metrics.repl.preload.indexes.totalMillis`
   reports the total amount of time spent loading index entries as
   part of the *pre-fetch* stage of replication.

.. data:: serverStatus.metrics.ttl

   :data:`~serverStatus.metrics.ttl` is a sub-document that reports on
   the operation of the resource use of the :doc:`ttl index
   </tutorial/expire-data/>` process.

.. data:: serverStatus.metrics.ttl.deletedDocuments

   :data:`~serverStatus.metrics.ttl.deletedDocuments` reports the
   total number of documents deleted from collections with a :doc:`ttl
   index </tutorial/expire-data/>`.

.. data:: serverStatus.metrics.ttl.passes

   :data:`~serverStatus.metrics.ttl.passes` reports the number of
   times the background process removes documents from collections with a
   :doc:`ttl index </tutorial/expire-data/>`.
============
setParameter
============

.. default-domain:: mongodb

.. dbcommand:: setParameter

   :dbcommand:`setParameter` is an administrative command for
   modifying options normally set on the command line. You must issue
   the :dbcommand:`setParameter` command against the :term:`admin database`
   in the form:

   .. code-block:: javascript

      { setParameter: 1, <option>: <value> }

   Replace the ``<option>`` with one of the supported
   :dbcommand:`setParameter` options:

   - :parameter:`journalCommitInterval`
   - :parameter:`logLevel`
   - :parameter:`logUserIds`
   - :parameter:`notablescan`
   - :parameter:`quiet`
   - :parameter:`replApplyBatchSize`
   - :parameter:`replIndexPrefetch`
   - :parameter:`syncdelay`
   - :parameter:`traceExceptions`
   - :parameter:`textSearchEnabled`
   - :parameter:`sslMode`
   - :parameter:`clusterAuthMode`
   - :parameter:`userCacheInvalidationIntervalSecs`

   .. slave-ok, admin-only
===============
setShardVersion
===============

.. default-domain:: mongodb

.. dbcommand:: setShardVersion

   :dbcommand:`setShardVersion` is an internal command that supports
   sharding functionality.

   .. admin-only
===============
shardCollection
===============

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: shardCollection

   Enables a collection for sharding and allows MongoDB to begin
   distributing data among shards. You must run
   :dbcommand:`enableSharding` on a database before running the
   :dbcommand:`shardCollection` command. :dbcommand:`shardCollection`
   has the following form:

   .. code-block:: javascript

      { shardCollection: "<database>.<collection>", key: <shardkey> }

   :dbcommand:`shardCollection` has the following fields:

   .. include:: /reference/command/shardCollection-field.rst

Considerations
--------------

Use
~~~

Do **not** run more than one :dbcommand:`shardCollection` command on
the same collection at the same time.

.. include:: /includes/fact-cannot-unshard-collection.rst

Shard Keys
~~~~~~~~~~

Choosing the best shard key to effectively distribute load among your
shards requires some planning. Review :ref:`sharding-shard-key`
regarding choosing a shard key.

Hashed Shard Keys
~~~~~~~~~~~~~~~~~

.. versionadded:: 2.4

:ref:`Hashed shard keys <sharding-hashed-sharding>` use a
hashed index of a single field as the shard key.

Example
-------

The following operation enables sharding for the ``people`` collection
in the ``records`` database and uses the ``zipcode`` field as the
:ref:`shard key <shard-key>`:

.. code-block:: javascript

   db.runCommand( { shardCollection: "records.people", key: { zipcode: 1 } } )

Additional Information
----------------------

:doc:`/sharding`, :doc:`/core/sharding`, and
:doc:`/tutorial/deploy-shard-cluster`.
==================
shardConnPoolStats
==================

.. default-domain:: mongodb

.. avail in 2.2.4 (backported), 2.4.2, and 2.5

Definition
----------

.. dbcommand:: shardConnPoolStats

   Returns information on the pooled and cached connections in the sharded
   connection pool. The command also returns information on the per-thread
   connection cache in the connection pool.

   The :dbcommand:`shardConnPoolStats` command uses the following syntax:

   .. code-block:: javascript

      { shardConnPoolStats: 1 }

   The sharded connection pool is specific to connections between members
   in a sharded cluster. The :program:`mongos` instances in a cluster use
   the connection pool to execute client reads and writes. The
   :program:`mongod` instances in a cluster use the pool when issuing
   :dbcommand:`mapReduce` to query temporary collections on other shards.

   When the cluster requires a connection, MongoDB pulls a connection from
   the sharded connection pool into the per-thread connection cache.
   MongoDB returns the connection to the connection pool after every
   operation.

Output
------

.. data:: shardConnPoolStats.hosts

   Displays connection status for each :term:`config server`,
   :term:`replica set`, and :term:`standalone instance <standalone>` in
   the cluster.

   .. data:: shardConnPoolStats.hosts.<host>.available

      The number of connections available for this host to connect to the
      :program:`mongos`.

   .. data:: shardConnPoolStats.hosts.<host>.created

      The number of connections the host has ever created to connect to
      the :program:`mongos`.

.. data:: shardConnPoolStats.replicaSets

   Displays information specific to replica sets.

   .. data:: shardConnPoolStats.replicaSets.<name>.host

      Holds an array of documents that report on each replica set member.
      These values derive from the :doc:`replica set status
      </reference/command/replSetGetStatus>` values.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].addr

         The host address in the format ``[hostname]:[port]``.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].ok

         This field is for internal use. Reports ``false`` when the
         :program:`mongos` either cannot connect to instance or received a
         connection exception or error.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].ismaster

         The host is the replica set's :term:`primary` if this is set to ``true``.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].hidden

         The host is a :term:`hidden member` of the replica set if this is
         set to ``true``.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].secondary

         The host is a :term:`hidden member` of the replica set if this is
         set to ``true``.

         The host is a :term:`secondary` member of the replica set if this
         is set to ``true``.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].pingTimeMillis

         The latency, in milliseconds, from the :program:`mongos` to this member.

      .. data:: shardConnPoolStats.replicaSets.<name>.host[n].tags

         The member has tags configured.

.. data:: shardConnPoolStats.createdByType

   The number connections in the cluster's connection pool.

   .. data:: shardConnPoolStats.createdByType.master

      The number of connections to a shard.

   .. data:: shardConnPoolStats.createdByType.set

      The number of connections to a replica set.

   .. data:: shardConnPoolStats.createdByType.sync

      The number of connections to the config database.

.. data:: shardConnPoolStats.totalAvailable

   The number of connections available from the :program:`mongos` to
   the config servers, replica sets, and standalone :program:`mongod`
   instances in the cluster.

.. data:: shardConnPoolStats.totalCreated

   The number of connections the :program:`mongos` has ever created to
   other members of the cluster.

.. data:: shardConnPoolStats.threads

   Displays information on the per-thread connection cache.

   .. data:: shardConnPoolStats.threads.hosts

      Displays each incoming client connection. For a :program:`mongos`,
      this array field displays one document per incoming client thread.
      For a :program:`mongod`, the array displays one entry per incoming
      sharded :dbcommand:`mapReduce` client thread.

      .. data:: shardConnPoolStats.threads.hosts.host

         The host using the connection. The host can be a :term:`config
         server`, :term:`replica set`, or :term:`standalone instance
         <standalone>`.

      .. data:: shardConnPoolStats.threads.hosts.created

         The number of times the host pulled a connection from the pool.

      .. data:: shardConnPoolStats.threads.hosts.avail

         The thread's availability.

   .. data:: shardConnPoolStats.threads.seenNS

      The namespaces used on this connection thus far.
=============
shardingState
=============

.. default-domain:: mongodb

.. dbcommand:: shardingState

   :dbcommand:`shardingState` is an admin command that reports if
   :program:`mongod` is a member of a :term:`sharded cluster`.
   :dbcommand:`shardingState` has the following prototype form:

   .. code-block:: javascript

      { shardingState: 1 }

   For :dbcommand:`shardingState` to detect that a :program:`mongod`
   is a member of a sharded cluster, the :program:`mongod` must
   satisfy the following conditions:

   #. the :program:`mongod` is a primary member of a replica set, and

   #. the :program:`mongod` instance is a member of a sharded
      cluster.

   If :dbcommand:`shardingState` detects that a :program:`mongod` is a
   member of a sharded cluster, :dbcommand:`shardingState` returns a
   document that resembles the following prototype:

   .. code-block:: javascript

      {
        "enabled" : true,
        "configServer" : "<configdb-string>",
        "shardName" : "<string>",
        "shardHost" : "string:",
        "versions" : {
             "<database>.<collection>" : Timestamp(<...>),
             "<database>.<collection>" : Timestamp(<...>)
        },
        "ok" : 1
      }

   Otherwise, :dbcommand:`shardingState` will return the following
   document:

   .. code-block:: javascript

      { "note" : "from execCommand", "ok" : 0, "errmsg" : "not master" }

   The response from :dbcommand:`shardingState` when used with a
   :term:`config server <config database>` is:

   .. code-block:: javascript

      { "enabled": false, "ok": 1 }


   .. note::

      :program:`mongos` instances do not provide the
      :dbcommand:`shardingState`.

   .. warning::

      This command obtains a write lock on the affected database and
      will block other operations until it has completed; however, the
      operation is typically short lived.

   .. admin-only
========
shutdown
========

.. default-domain:: mongodb

.. dbcommand:: shutdown

   The :dbcommand:`shutdown` command cleans up all database resources
   and then terminates the process.  You must issue
   the :dbcommand:`shutdown` command against the :term:`admin database`
   in the form:

   .. code-block:: javascript

      { shutdown: 1 }

   .. note::

      Run the :dbcommand:`shutdown` against the :term:`admin database`. When
      using :dbcommand:`shutdown`, the connection must originate from
      localhost **or** use an authenticated connection.

   If the node you're trying to shut down is a :doc:`replica set </replication>`
   primary, then the command will succeed only if there exists a secondary node
   whose oplog data is within 10 seconds of the primary. You can override this protection
   using the ``force`` option:

   .. code-block:: javascript

      { shutdown: 1, force: true }

   Alternatively, the :dbcommand:`shutdown` command also supports a ``timeoutSecs`` argument
   which allows you to specify a number of seconds to wait for other
   members of the replica set to catch up:

   .. code-block:: javascript

      { shutdown: 1, timeoutSecs: 60 }

   The equivalent :program:`mongo` shell helper syntax looks like this:

   .. code-block:: javascript

      db.shutdownServer({timeoutSecs: 60});
================
skewClockCommand
================

.. default-domain:: mongodb

.. dbcommand:: _skewClockCommand

   :dbcommand:`_skewClockCommand` is an internal command. Do not call
   directly.

   .. |dbcommand| replace:: :dbcommand:`_skewClockCommand`
   .. include:: /includes/note-enabletestcommands.rst

   .. admin-only
=====
sleep
=====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: sleep

   Forces the database to block all operations. This is an internal
   command for testing purposes.

   The :dbcommand:`sleep` command takes the following prototype form:

   .. code-block:: javascript

      { sleep: 1, w: <true|false>, secs: <seconds> }

   The :dbcommand:`sleep` command has the following fields:

   .. include:: /reference/command/sleep-field.rst

Behavior
--------

The command places the :program:`mongod` instance in a :term:`write lock`
state for ``100`` seconds. Without arguments,
:dbcommand:`sleep` causes a "read lock" for 100 seconds.

.. warning::

   :dbcommand:`sleep` claims the lock specified in the ``w``
   argument and blocks *all* operations on the :program:`mongod`
   instance for the specified amount of time.

.. read lock, write lock, global-lock

.. |dbcommand| replace:: :dbcommand:`sleep`
.. include:: /includes/note-enabletestcommands.rst
=====
split
=====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: split

   Splits a :term:`chunk` in a :term:`sharded cluster` into two
   chunks. The :program:`mongos` instance splits and manages
   chunks automatically, but for exceptional circumstances the
   :dbcommand:`split` command does allow administrators to manually
   create splits. See :doc:`/tutorial/split-chunks-in-sharded-cluster` for
   information on these circumstances, and on the MongoDB shell commands
   that wrap :dbcommand:`split`.

   The :dbcommand:`split` command uses the following form:

   .. code-block:: javascript

      db.adminCommand( { split: <database>.<collection>,
                         <find|middle|bounds> } )

   The :dbcommand:`split` command takes a document with the following
   fields:

   .. include:: /reference/command/split-field.rst

Considerations
--------------

:dbcommand:`split` does *not* support splitting an chunk that does not
contain documents. Use :dbcommand:`splitAt` to create splits in chunks
that do not contain documents.

Command Formats
---------------

To create a chunk split, connect to a :program:`mongos` instance, and
issue the following command to the ``admin`` database:

.. code-block:: javascript

   db.adminCommand( { split: <database>.<collection>,
                      find: <document> } )

Or:

.. code-block:: javascript

   db.adminCommand( { split: <database>.<collection>,
                      middle: <document> } )

Or:

.. code-block:: javascript

   db.adminCommand( { split: <database>.<collection>,
                      bounds: [ <lower>, <upper> ] } )

To create a split for a collection that uses a :term:`hashed shard key`,
use the ``bounds`` parameter. Do *not* use the ``middle`` parameter for
this purpose.

.. include:: /includes/warning-splitting-chunks.rst

.. seealso:: :dbcommand:`moveChunk`, :method:`sh.moveChunk()`,
   :method:`sh.splitAt()`, and :method:`sh.splitFind()`, which wrap the
   functionality of :dbcommand:`split`.

Examples
--------

The following sections provide examples of the :dbcommand:`split` command.

Split a Chunk in Half
~~~~~~~~~~~~~~~~~~~~~

.. code-block:: javascript

   db.runCommand( { split : "test.people", find : { _id : 99 } } )

The :dbcommand:`split` command identifies the chunk in the ``people``
collection of the ``test`` database, that holds documents that match ``{
_id : 99 }``. :dbcommand:`split` does not require that a match exist, in order
to identify the appropriate chunk. Then the command splits it into two
chunks of equal size.

.. note:: :dbcommand:`split` creates two equal chunks by range as
   opposed to size, and does not use the selected point as a boundary for
   the new chunks

Define an Arbitrary Split Point
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To define an arbitrary split point, use the following form:

.. code-block:: javascript

   db.runCommand( { split : "test.people", middle : { _id : 99 } } )

The :dbcommand:`split` command identifies the chunk in the ``people``
collection of the ``test`` database, that would hold documents
matching the query ``{ _id : 99 }``. :dbcommand:`split` does not
require that a match exist, in order to identify the appropriate
chunk. Then the command splits it into two chunks, with the matching
document as the lower bound of one of the split chunks.

This form is typically used when :term:`pre-splitting` data in a
collection.

Split a Chunk Using Values of a Hashed Shard Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This example uses the :term:`hashed shard key` ``userid`` in a
``people`` collection of a ``test`` database. The following command
uses an array holding two single-field documents to represent the
minimum and maximum values of the hashed shard key to split the chunk:

.. code-block:: javascript

   db.runCommand( { split: "test.people",
                     bounds : [ { userid: NumberLong("-5838464104018346494") },
                                { userid: NumberLong("-5557153028469814163") }
                ] } )

.. note:: MongoDB uses the 64-bit :ref:`NumberLong <shell-type-long>`
   type to represent the hashed value.

Use :method:`sh.status()` to see the existing bounds of the shard keys.

Metadata Lock Error
-------------------

If another process in the :program:`mongos`, such as a balancer
process, changes metadata while :dbcommand:`split` is running, you may
see a ``metadata lock error``.

.. code-block:: none

   errmsg: "The collection's metadata lock is already taken."

This message indicates that the split has failed with no side
effects. Retry the :dbcommand:`split` command.
==========
splitChunk
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: splitChunk

   An internal administrative command. To split chunks, use
   the :method:`sh.splitFind()` and :method:`sh.splitAt()` functions
   in the :program:`mongo` shell.

   .. include:: /includes/warning-splitting-chunks.rst

   .. admin-only.

   .. seealso:: :dbcommand:`moveChunk` and :method:`sh.moveChunk()`.

   The :dbcommand:`splitChunk` command takes a document with the following fields:

   .. include:: /reference/command/splitChunk-field.rst
===========
splitVector
===========

.. default-domain:: mongodb

.. dbcommand:: splitVector

   Is an internal command that supports meta-data operations in sharded
   clusters.
====================
testDistLockWithSkew
====================

.. default-domain:: mongodb

.. dbcommand:: _testDistLockWithSkew

   :dbcommand:`_testDistLockWithSkew` is an internal command. Do not call
   directly.

   .. admin-only

   .. |dbcommand| replace:: :dbcommand:`_testDistLockWithSkew`
   .. include:: /includes/note-enabletestcommands.rst
===========================
testDistLockWithSyncCluster
===========================

.. default-domain:: mongodb

.. dbcommand:: _testDistLockWithSyncCluster

   :dbcommand:`_testDistLockWithSyncCluster` is an internal command. Do not call
   directly.

   .. admin-only

   .. |dbcommand| replace:: :dbcommand:`_testDistLockWithSyncCluster`
   .. include:: /includes/note-enabletestcommands.rst
====
text
====

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: text

   .. deprecated:: 2.6
      Use :query:`$text` query operator instead.

      For document on the :dbcommand:`text`, refer to the 2.4 Manual
      :v2.4:`2.4 Manual </reference/command/text>`.
===
top
===

.. default-domain:: mongodb

.. dbcommand:: top

   :dbcommand:`top` is an administrative command that
   returns usage statistics for each collection. :dbcommand:`top`
   provides amount of time, in microseconds, used and a count of
   operations for the following event types:

   - total
   - readLock
   - writeLock
   - queries
   - getmore
   - insert
   - update
   - remove
   - commands

   Issue the :dbcommand:`top` command against the :term:`admin
   database` in the form:

   .. code-block:: javascript

      { top: 1 }


Example
-------

At the :program:`mongo` shell prompt, use :dbcommand:`top` with the
following evocation:

.. code-block:: javascript

   db.adminCommand("top")

Alternately you can use :dbcommand:`top` as follows:

.. code-block:: javascript

   use admin
   db.runCommand( { top: 1 } )

The output of the top command would resemble the following
output:

.. code-block:: javascript

   {
     "totals" : {
        "records.users" : {
                     "total" : {
                             "time" : 305277,
                             "count" : 2825
                     },
                     "readLock" : {
                             "time" : 305264,
                             "count" : 2824
                     },
                     "writeLock" : {
                             "time" : 13,
                             "count" : 1
                     },
                     "queries" : {
                             "time" : 305264,
                             "count" : 2824
                     },
                     "getmore" : {
                             "time" : 0,
                             "count" : 0
                     },
                     "insert" : {
                             "time" : 0,
                             "count" : 0
                     },
                     "update" : {
                             "time" : 0,
                             "count" : 0
                     },
                     "remove" : {
                             "time" : 0,
                             "count" : 0
                     },
                     "commands" : {
                             "time" : 0,
                             "count" : 0
                     }
             }
   }
=====
touch
=====

.. default-domain:: mongodb

.. dbcommand:: touch

   .. versionadded:: 2.2

   The :dbcommand:`touch` command loads data from the data storage
   layer into memory. :dbcommand:`touch` can load the data
   (i.e. documents) indexes or both documents and indexes. Use this
   command to ensure that a collection, and/or its indexes, are in
   memory before another operation. By loading the collection or
   indexes into memory, :program:`mongod` will ideally be able to
   perform subsequent operations more efficiently. The
   :dbcommand:`touch` command has the following prototypical form:

   .. code-block:: javascript

      { touch: [collection], data: [boolean], index: [boolean] }

   By default, ``data`` and ``index`` are false, and
   :dbcommand:`touch` will perform no operation. For example, to load
   both the data and the index for a collection named ``records``, you
   would use the following command in the :program:`mongo` shell:

   .. code-block:: javascript

      db.runCommand({ touch: "records", data: true, index: true })

   :dbcommand:`touch` will not block read and write operations on a
   :program:`mongod`, and can run on :term:`secondary` members of
   replica sets.

   .. note::

      Using :dbcommand:`touch` to control or tweak what a
      :program:`mongod` stores in memory may displace other records
      data in memory and hinder performance. Use with caution in
      production systems.

   .. warning::

      If you run :dbcommand:`touch` on a secondary, the secondary will
      enter a ``RECOVERING`` state to prevent clients from sending
      read operations during the :dbcommand:`touch` operation. When
      :dbcommand:`touch` finishes the secondary will automatically
      return to ``SECONDARY`` state.  See
      :data:`~replSetGetStatus.members.state` for more information on
      replica set member states.
============
transferMods
============

.. default-domain:: mongodb

.. dbcommand:: _transferMods

   :dbcommand:`_transferMods` is an internal command. Do not call
   directly.

   .. admin-only
=============
unsetSharding
=============

.. default-domain:: mongodb

.. dbcommand:: unsetSharding

   :dbcommand:`unsetSharding` is an internal command that supports sharding
   functionality.

   .. slave-ok, admin-only
======
update
======

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: update

   .. versionadded:: 2.6

   The :dbcommand:`update` command modifies documents in a collection.
   A single :dbcommand:`update` command can contain multiple update
   statements. The update methods provided by the MongoDB drivers use
   this command internally.

   The :dbcommand:`update` command has the following syntax:

   .. code-block:: javascript

      {
         update: <collection>,
         updates:
            [
               { q: <query>, u: <update>, upsert: <boolean>, multi: <boolean> },
               { q: <query>, u: <update>, upsert: <boolean>, multi: <boolean> },
               { q: <query>, u: <update>, upsert: <boolean>, multi: <boolean> },
               ...
            ],
         ordered: <boolean>,
         writeConcern: { <write concern> }
      }

   The command takes the following fields:

   .. include:: update-field.rst

   Each element of the ``updates`` array contains the following sub-fields:

   .. include:: update-field-updates.rst

   :return:

      A document that contains the status of the operation.
      See :ref:`update-command-output` for details.

.. _update-command-behaviors:

Behaviors
---------

The ``<update>`` document can contain either all :ref:`update operator
<update-operators>` expressions or all ``field:value`` expressions.

Update Operator Expressions
~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the ``<update>`` document contains all :ref:`update operator
<update-operators>` expressions, as in:

.. code-block:: javascript

   {
     $set: { status: "D" },
     $inc: { quantity: 2 }
   }

Then, the :dbcommand:`update` command updates only the corresponding
fields in the document.

``Field: Value`` Expressions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the ``<update>`` document contains *only* ``field:value``
expressions, as in:

.. code-block:: javascript

   {
     status: "D",
     quantity: 4
   }

Then the :dbcommand:`update` command *replaces* the matching document
with the update document. The :dbcommand:`update` command can only
replace a *single* matching document; i.e. the ``multi`` field cannot
be ``true``. The :dbcommand:`update` command *does not* replace the
``_id`` value.

Limits
~~~~~~

For each update element in the ``updates`` array, the sum of the query
and the update sizes (i.e. ``q`` and ``u`` ) must be less than or equal
to the :limit:`maximum BSON document size <BSON Document Size>`.

The total number of update statements in the ``updates`` array must be
less than or equal to the :limit:`maximum bulk size
<Bulk Operation Size>`.

Examples
--------

Update Specific Fields of One Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use :ref:`update operators <update-operators>` to update only the
specified fields of a document.

For example, given a ``users`` collection, the following command uses
the :update:`$set` and :update:`$inc` operators to modify the ``status``
and the ``points`` fields respectively of a document where the ``user``
equals ``"abc123"``:

.. code-block:: javascript

   db.runCommand(
      {
         update: "users",
         updates: [
            {
              q: { user: "abc123" }, u: { $set: { status: "A" }, $inc: { points: 1 } }
            }
         ],
         ordered: false,
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

Because ``<update>`` document does not specify the optional ``multi``
field, the update only modifies one document, even if more than one
document matches the ``q`` match condition.

The returned document shows that the command found and updated a single
document. See :ref:`update-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "nModified" : 1, "n" : 1 }

Update Specific Fields of Multiple Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use :ref:`update operators <update-operators>` to update only the
specified fields of a document, and include the ``multi`` field set to
``true`` in the update statement.

For example, given a ``users`` collection, the following command uses
the :update:`$set` and :update:`$inc` operators to modify the
``status`` and the ``points`` fields respectively of all documents in
the collection:

.. code-block:: javascript

   db.runCommand(
      {
         update: "users",
         updates: [
            { q: { }, u: { $set: { status: "A" }, $inc: { points: 1 } }, multi: true }
         ],
         ordered: false,
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

The update modifies all documents that match the query specified in the
``q`` field, namely the empty query which matches all documents in the
collection.

The returned document shows that the command found and updated multiple
documents. See :ref:`update-command-output` for details.

.. code-block:: javascript

   { "ok" : 1, "nModified" : 100, "n" : 100 }

Bulk Update
~~~~~~~~~~~

The following example performs multiple update operations on the
``users`` collection:

.. code-block:: javascript

   db.runCommand(
      {
         update: "users",
         updates: [
            { q: { status: "P" }, u: { $set: { status: "D" } }, multi: true },
            { q: { _id: 5 }, u: { _id: 5, name: "abc123", status: "A" }, upsert: true }
         ],
         ordered: false,
         writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

The returned document shows that the command modified ``10`` documents
and upserted a document with the ``_id`` value ``5``. See
:ref:`update-command-output` for details.

.. code-block:: javascript

   {
      "ok" : 1,
      "nModified" : 10,
      "n" : 11,
      "upserted" : [
         {
            "index" : 1,
            "_id" : 5
         }
      ]
   }

.. _update-command-output:

Output
------

The returned document contains a subset of the following fields:

.. data:: update.ok

   The status of the command.

.. data:: update.n

   The number of documents selected for update. If the update operation
   results in no change to the document, e.g. :update:`$set` expression
   updates the value to the current value, :data:`~update.n` can be
   greater than :data:`~update.nModified`.

.. data:: update.nModified

   The number of documents updated. If the update operation results in
   no change to the document, such as setting the value of the field to
   its current value, :data:`~update.nModified` can be less than
   :data:`~update.n`.

.. data:: update.upserted

   An array of documents that contains information for each upserted
   documents.

   Each document contains the following information:

   .. data:: update.upserted.index

      An integer that identifies the upsert statement in the
      ``updates`` array, which uses a zero-based index.

   .. data:: update.upserted._id

      The ``_id`` value of the upserted document.

.. data:: update.writeErrors

   An array of documents that contains information regarding any error
   encountered during the update operation. The
   :data:`~update.writeErrors` array contains an error document for
   each update statement that errors.

   Each error document contains the following fields:

   .. data:: update.writeErrors.index

      An integer that identifies the update statement in the
      ``updates`` array, which uses a zero-based index.

   .. data:: update.writeErrors.code

      An integer value identifying the error.

   .. data:: update.writeErrors.errmsg

      A description of the error.

.. data:: update.writeConcernError

   Document that describe error related to write concern and contains
   the field:

   .. data:: update.writeConcernError.code

      An integer value identifying the cause of the write concern error.

   .. data:: update.writeConcernError.errmsg

      A description of the cause of the write concern error.

The following is an example document returned for a successful
:dbcommand:`update` command that performed an upsert:

.. code-block:: javascript

   {
      "ok" : 1,
      "nModified" : 0,
      "n" : 1,
      "upserted" : [
         {
            "index" : 0,
            "_id" : ObjectId("52ccb2118908ccd753d65882")
         }
      ]
   }

The following is an example document returned for a bulk update
involving three update statements, where one update statement was
successful and two other update statements encountered errors:

.. code-block:: javascript

   {
      "ok" : 1,
      "nModified" : 1,
      "n" : 1,
      "writeErrors" : [
         {
            "index" : 1,
            "code" : 16837,
            "errmsg" : "The _id field cannot be changed from {_id: 1.0} to {_id: 5.0}."
         },
         {
            "index" : 2,
            "code" : 16837,
            "errmsg" : "The _id field cannot be changed from {_id: 2.0} to {_id: 6.0}."
         },
      ]
   }
==========
updateRole
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: updateRole

   Updates a :ref:`user-defined role <user-defined-roles>`. The
   :dbcommand:`updateRole` command must run on the role's database.

   An update to a field **completely replaces** the previous field's values.
   To grant or remove roles or :ref:`privileges <privileges>` without
   replacing all values, use one or more of the following commands:

   - :dbcommand:`grantRolesToRole`
   - :dbcommand:`grantPrivilegesToRole`
   - :dbcommand:`revokeRolesFromRole`
   - :dbcommand:`revokePrivilegesFromRole`

   .. warning::

      An update to the ``privileges`` or ``roles`` array
      completely replaces the previous array's values.

   The :dbcommand:`updateRole` command uses the following syntax. To
   update a role, you must provide the ``privileges`` array, ``roles``
   array, or both:

   .. code-block:: javascript

      {
        updateRole: "<role>",
        privileges:
            [
              { resource: { <resource> }, actions: [ "<action>", ... ] },
              ...
            ],
        roles:
            [
              { role: "<role>", db: "<database>" } | "<role>",
              ...
            ],
        writeConcern: <write concern document>
      }

   The :dbcommand:`updateRole` command has the following fields:

   .. include:: /reference/command/updateRole-field.rst

   .. |local-cmd-name| replace:: :dbcommand:`updateRole`
   .. include:: /includes/fact-roles-array-contents.rst

Behavior
--------

A role's privileges apply to the database where the role is created. The
role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>` and can inherit
privileges from roles in other databases.

Required Access
---------------

.. include:: /includes/access-update-role.rst

Example
-------

The following is an example of the :dbcommand:`updateRole` command that
updates the ``myClusterwideAdmin`` role on the ``admin`` database.
While the :data:`~admin.system.roles.privileges` and the
:data:`~admin.system.roles.roles` arrays are both optional, at least
one of the two is required:

.. code-block:: javascript

   use admin
   db.runCommand(
      {
        updateRole: "myClusterwideAdmin",
        privileges:
            [
              {
                resource: { db: "", collection: "" },
                actions: [ "find" , "update", "insert", "remove" ]
              }
            ],
        roles:
            [
              { role: "dbAdminAnyDatabase", db: "admin" }
            ],
        writeConcern: { w: "majority" }
      }
   )

To view a role's privileges, use the :dbcommand:`rolesInfo` command.
==========
updateUser
==========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: updateUser

   .. Defines a |local-cmd-name| replacement:
   .. |local-cmd-name| replace:: :dbcommand:`updateUser`

   Updates the user's profile on the database on which you run the
   command. An update to a field **completely replaces** the previous
   field's values, including updates to the user's ``roles`` array.

   .. warning::

      When you update the ``roles`` array, you completely replace the
      previous array's values. To add or remove roles without replacing all
      the user's existing roles, use the :dbcommand:`grantRolesToUser` or
      :dbcommand:`revokeRolesFromUser` commands.

   The :dbcommand:`updateUser` command uses the following syntax. To
   update a user, you must specify the ``updateUser`` field and at least
   one other field, other than ``writeConcern``:

   .. code-block:: javascript

      { updateUser: "<username>",
        pwd: "<cleartext password>",
        customData: { <any information> },
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: { <write concern> }
      }

   The command has the following fields:

   .. include:: /reference/command/updateUser-field.rst

   .. include:: /includes/fact-roles-array-contents.rst

Behavior
--------

:dbcommand:`updateUser` sends the password to the MongoDB instance in
cleartext. To encrypt the password in transit, use :doc:`SSL
</tutorial/configure-ssl>`.

Required Access
---------------

.. include:: /includes/access-update-user.rst

.. include:: /includes/access-change-own-password-and-custom-data.rst

Example
-------

Given a user ``appClient01`` in the ``products`` database with the following
user info:

.. code-block:: javascript

   {
      "_id" : "products.appClient01",
      "user" : "appClient01",
      "db" : "products",
      "customData" : { "empID" : "12345", "badge" : "9156" },
      "roles" : [
          { "role" : "readWrite",
            "db" : "products"
          },
          { "role" : "read",
            "db" : "inventory"
          }
      ]
   }

The following :dbcommand:`updateUser` command **completely** replaces the
user's ``customData`` and ``roles`` data:

.. code-block:: javascript

   use products
   db.runCommand( { updateUser : "appClient01",
                    customData : { employeeId : "0x3039" },
                    roles : [
                              { role : "read", db : "assets"  }
                            ]
                } )

The user ``appClient01`` in the ``products`` database now has the following
user information:

.. code-block:: javascript

   {
      "_id" : "products.appClient01",
      "user" : "appClient01",
      "db" : "products",
      "customData" : { "employeeId" : "0x3039" },
      "roles" : [
          { "role" : "read",
            "db" : "assets"
          }
      ]
   }
=========
usersInfo
=========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: usersInfo

   Returns information about one or more users. To match a single user
   on the database, use the following form:

   .. code-block:: javascript

      { usersInfo: { user: <name>, db: <db> },
        showCredentials: <Boolean>,
        showPrivileges: <Boolean>
      }

   The command has the following fields:

   .. include:: /reference/command/usersInfo-field.rst

Required Access
---------------

Users can always view their own information.

To view another user's information, the user running the command must
have privileges that include the :authaction:`viewUser` action on the
other user's database.

.. _usersInfo-field-specification:

Behavior
--------

The argument to the :dbcommand:`usersInfo` command has multiple forms
depending on the requested information:

Specify a Single User
~~~~~~~~~~~~~~~~~~~~~

In the ``usersInfo`` field, specify a document with the user's name and
database:

.. code-block:: javascript

   { usersInfo: { user: <name>, db: <db> } }

Alternatively, for a user that exists on the same database where the
command runs, you can specify the user by its name only.

.. code-block:: javascript

   { usersInfo: <name> }

Specify Multiple Users
~~~~~~~~~~~~~~~~~~~~~~

In the ``usersInfo`` field, specify an array of documents:

.. code-block:: javascript

   { usersInfo: [ { user: <name>, db: <db> },  { user: <name>, db: <db> }, ... ] }

Specify All Users for a Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the ``usersInfo`` field, specify ``1``:

.. code-block:: javascript

   { usersInfo: 1 }

Examples
--------

View Specific Users
~~~~~~~~~~~~~~~~~~~

To see information and privileges, but not the credentials, for the
user``"Kari"`` defined in ``"home"`` database,
run the following command:

.. code-block:: javascript

   db.runCommand(
                  {
                    usersInfo:  { user: "Kari", db: "home" },
                    showPrivileges: true
                  }
   )

To view a user that exists in the *current* database, you can specify
the user by name only. For example, if you are in the ``home``
database and a user named ``"Kari"`` exists in the ``home`` database,
you can run the following command:

.. code-block:: javascript

   db.getSiblingDB("home").runCommand(
                                       {
                                         usersInfo:  "Kari",
                                         showPrivileges: true
                                       }
   )

View Multiple Users
~~~~~~~~~~~~~~~~~~~

To view info for several users, use an array, with or without the
optional fields ``showPrivileges`` and ``showCredentials``. For example:

.. code-block:: javascript

   db.runCommand( { usersInfo: [ { user: "Kari", db: "home" }, { user: "Li", db: "myApp" } ],
                    showPrivileges: true
                } )

View All Users for a Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To view all users on the database the command is run, use a command document
that resembles the following:

.. code-block:: javascript

    db.runCommand( { usersInfo: 1 } )

When viewing all users, you can specify the ``showCredentials``
field but not the ``showPrivileges`` field.
========
validate
========

.. default-domain:: mongodb

Definition
----------

.. dbcommand:: validate

   The :dbcommand:`validate` command checks the structures within a
   namespace for correctness by scanning the collection's data and
   indexes. The command returns information regarding the on-disk
   representation of the collection.

   The ``validate`` command can be slow, particularly on larger data sets.

   The following example validates the contents of the collection named
   ``users``.

   .. code-block:: javascript

      { validate: "users" }

   You may also specify one of the following options:

   - ``full: true`` provides a more thorough scan of the data.

   - ``scandata: false`` skips the scan of the base collection
      without skipping the scan of the index.

   The :program:`mongo` shell also provides a wrapper:

   .. code-block:: javascript

      db.collection.validate();

   Use one of the following forms to perform the full collection
   validation:

   .. code-block:: javascript

      db.collection.validate(true)
      db.runCommand( { validate: "collection", full: true } )

   .. warning:: This command is resource intensive and may have an
      impact on the performance of your MongoDB instance.

   .. todo:: link to the document with these statistics

   .. read-lock

Output
------

.. data:: validate.ns

   The full namespace name of the collection. Namespaces include the
   database name and the collection name in the form
   ``database.collection``.

.. data:: validate.firstExtent

   The disk location of the first extent in the collection. The value
   of this field also includes the namespace.

.. data:: validate.lastExtent

   The disk location of the last extent in the collection. The value
   of this field also includes the namespace.

.. data:: validate.extentCount

   The number of extents in the collection.

.. data:: validate.extents

   :dbcommand:`validate` returns one instance of this document for
   every extent in the collection. This sub-document is only returned
   when you specify the ``full`` option to the command.

   .. data:: validate.extents.loc

      The disk location for the beginning of this extent.

   .. data:: validate.extents.xnext

      The disk location for the extent following this one. "null" if
      this is the end of the linked list of extents.

   .. data:: validate.extents.xprev

      The disk location for the extent preceding this one. "null" if
      this is the head of the linked list of extents.

   .. data:: validate.extents.nsdiag

      The namespace this extent belongs to (should be the same as the
      namespace shown at the beginning of the validate listing).

   .. data:: validate.extents.size

      The number of bytes in this extent.

   .. data:: validate.extents.firstRecord

      The disk location of the first record in this extent.

   .. data:: validate.extents.lastRecord

      The disk location of the last record in this extent.

.. data:: validate.datasize

   The number of bytes in all data records.  This value does not
   include deleted records, nor does it include extent headers, nor
   record headers, nor space in a file unallocated to any
   extent. :data:`~validate.datasize` includes record :term:`padding`.

.. data:: validate.nrecords

   The number of :term:`documents <document>` in the collection.

.. data:: validate.lastExtentSize

   The size of the last new extent created in this collection. This
   value determines the size of the *next* extent created.

.. data:: validate.padding

   A floating point value between 1 and 2.

   When MongoDB creates a new record it uses the :term:`padding
   factor` to determine how much additional space to add to the
   record. The padding factor is automatically adjusted by mongo when
   it notices that update operations are triggering record moves.

.. data:: validate.firstExtentDetails

   The size of the first extent created in this collection. This data
   is similar to the data provided by the :data:`~validate.extents`
   sub-document; however, the data reflects only the first extent in
   the collection and is always returned.

   .. data:: validate.firstExtentDetails.loc

      The disk location for the beginning of this extent.

   .. data:: validate.firstExtentDetails.xnext

      The disk location for the extent following this one. "null" if
      this is the end of the linked list of extents, which should only
      be the case if there is only one extent.

   .. data:: validate.firstExtentDetails.xprev

      The disk location for the extent preceding this one. This should
      always be "null."

   .. data:: validate.firstExtentDetails.nsdiag

      The namespace this extent belongs to (should be the same as the
      namespace shown at the beginning of the validate listing).

   .. data:: validate.firstExtentDetails.size

      The number of bytes in this extent.

   .. data:: validate.firstExtentDetails.firstRecord

      The disk location of the first record in this extent.

   .. data:: validate.firstExtentDetails.lastRecord

      The disk location of the last record in this extent.

.. data:: validate.objectsFound

   The number of records actually encountered in a scan of the
   collection. This field should have the same value as the
   :data:`~validate.nrecords` field.

.. data:: validate.invalidObjects

   The number of records containing BSON documents that do not pass a
   validation check.

   .. note::

      This field is only included in the validation output when you
      specify the ``full`` option.

.. data:: validate.bytesWithHeaders

   This is similar to datasize, except that :data:`~validate.bytesWithHeaders`
   includes the record headers. In version 2.0, record headers are 16
   bytes per document.

   .. note::

      This field is only included in the validation output when you
      specify the ``full`` option.

.. data:: validate.bytesWithoutHeaders

   :data:`~validate.bytesWithoutHeaders` returns data collected from a scan of
   all records. The value should be the same as :data:`~validate.datasize`.

   .. note::

      This field is only included in the validation output when you
      specify the ``full`` option.

.. data:: validate.deletedCount

   The number of deleted or "free" records in the collection.

.. data:: validate.deletedSize

   The size of all deleted or "free" records in the collection.

.. data:: validate.nIndexes

   The number of indexes on the data in the collection.

.. data:: validate.keysPerIndex

   A document containing a field for each index, named after the
   index's name, that contains the number of keys, or documents
   referenced, included in the index.

.. data:: validate.valid

   Boolean. ``true``, unless :dbcommand:`validate` determines that an
   aspect of the collection is not valid. When ``false``, see the
   :data:`~validate.errors` field for more information.

.. data:: validate.errors

   Typically empty; however, if the collection is not valid (i.e
   :data:`~validate.valid` is false), this field will contain a message
   describing the validation error.

.. data:: validate.ok

   Set to ``1`` when the command succeeds. If the command fails
   the :data:`~validate.ok` field has a value of ``0``.
==========
whatsmyuri
==========

.. default-domain:: mongodb

.. dbcommand:: whatsmyuri

   :dbcommand:`whatsmyuri` is an internal command.

   .. slave-ok
===============
writebacklisten
===============

.. default-domain:: mongodb

.. dbcommand:: writebacklisten

   :dbcommand:`writebacklisten` is an internal command.

   .. slave-ok, admin-only
================
writeBacksQueued
================

.. default-domain:: mongodb

.. dbcommand:: writeBacksQueued

   :dbcommand:`writeBacksQueued` is an internal command that returns
   a document reporting there are operations in the write back queue
   for the given :program:`mongos` and information about the queues.

   .. data:: writeBacksQueued.hasOpsQueued

      Boolean.

      :data:`~writeBacksQueued.hasOpsQueued` is ``true`` if there are ``write Back``
      operations queued.

   .. data:: writeBacksQueued.totalOpsQueued

      Integer.

      :data:`~writeBacksQueued.totalOpsQueued` reflects the number of operations queued.

   .. data:: writeBacksQueued.queues

      Document.

      :data:`~writeBacksQueued.queues` holds a sub-document where the fields are all
      write back queues. These field hold a document with two fields
      that reports on the state of the queue. The fields in these
      documents are:

      .. data:: writeBacksQueued.queues.n

         :data:`~writeBacksQueued.queues.n` reflects the size, by number of items, in
         the queues.

      .. data:: writeBacksQueued.queues.minutesSinceLastCall

         The number of minutes since the last time the
         :program:`mongos` touched this queue.

   The command document has the following prototype form:

   .. code-block:: javascript

      {writeBacksQueued: 1}

   To call :dbcommand:`writeBacksQueued` from the :program:`mongo`
   shell, use the following :method:`db.runCommand()` form:

   .. code-block:: javascript

      db.runCommand({writeBacksQueued: 1})

   Consider the following example output:

   .. code-block:: javascript

      {
      	"hasOpsQueued" : true,
      	"totalOpsQueued" : 7,
      	"queues" : {
      		"50b4f09f6671b11ff1944089" : { "n" : 0, "minutesSinceLastCall" : 1 },
      		"50b4f09fc332bf1c5aeaaf59" : { "n" : 0, "minutesSinceLastCall" : 0 },
      		"50b4f09f6671b1d51df98cb6" : { "n" : 0, "minutesSinceLastCall" : 0 },
      		"50b4f0c67ccf1e5c6effb72e" : { "n" : 0, "minutesSinceLastCall" : 0 },
      		"50b4faf12319f193cfdec0d1" : { "n" : 0, "minutesSinceLastCall" : 4 },
      		"50b4f013d2c1f8d62453017e" : { "n" : 0, "minutesSinceLastCall" : 0 },
      		"50b4f0f12319f193cfdec0d1" : { "n" : 0, "minutesSinceLastCall" : 1 }
      	},
      	"ok" : 1
      }


.. slave-ok, admin-only

.. The queue ids are BSON ObjectIds, they correspond to the
   mongos that has talked to this shard. This is derived from the
   "serverID" field of the setShardVersion command the mongos
   sends to the shard
=================
Database Commands
=================

.. default-domain:: mongodb

All command documentation outlined below describes a command and
its available parameters and provides a document template or prototype
for each command. Some command documentation also includes the relevant
:program:`mongo` shell helpers.

User Commands
-------------

Aggregation Commands
~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-aggregation.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-aggregation

Geospatial Commands
~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-geospatial.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-geospatial

.. _collection-commands:

Query and Write Operation Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-crud.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-crud

.. _query-plan-commands:

Query Plan Cache Commands
~~~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-plan-cache.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-plan-cache


Database Operations
-------------------

Authentication Commands
~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-authentication.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-authentication

.. _user-management-commands:

User Management Commands
~~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-user-management.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-user-management

.. _role-management-commands:

Role Management Commands
~~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-role-management.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-role-management

Replication Commands
~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-replication.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-replication

.. seealso:: :doc:`/replication` for more information regarding
   replication.

Sharding Commands
~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-sharding.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-sharding

.. seealso:: :doc:`/sharding` for more information about MongoDB's
   sharding functionality.

.. _admin-commands:

Instance Administration Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-administration.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-administration

Diagnostic Commands
~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-command-diagnostic.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-diagnostic

Internal Commands
-----------------

.. only:: website

   .. include:: /includes/toc/table-command-internal.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-internal

Testing Commands
----------------

.. only:: website

   .. include:: /includes/toc/table-command-testing.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-testing

Auditing Commands
-----------------

.. only:: website

   .. include:: /includes/toc/table-command-audit.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/command/nav-auditing
.. index:: internals; config database
.. index:: sharding; config database
.. _config-database:

===============
Config Database
===============

.. default-domain:: mongodb

The ``config`` database supports :term:`sharded cluster`
operation. See the :doc:`/sharding` section of this manual for full
documentation of sharded clusters.

.. important:: Consider the schema of the ``config`` database
   *internal* and may change between releases of MongoDB. The
   ``config`` database is not a dependable API, and users should not
   write data to the ``config`` database in the course of normal
   operation or maintenance.

.. warning:: Modification of the ``config`` database on a functioning
   system may lead to instability or inconsistent data sets. If you
   must modify the ``config`` database, use :program:`mongodump` to
   create a full backup of the ``config`` database.

To access the ``config`` database, connect to a :program:`mongos`
instance in a sharded cluster, and use the following helper:

.. code-block:: javascript

   use config

You can return a list of the collections, with the following helper:

.. code-block:: javascript

   show collections

Collections
-----------

.. if we add a collection to the config database here, also update the
   list on the sharding-internals page.

.. data:: config

.. data:: config.changelog

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.changelog` collection stores a document for each change to
   the metadata of a sharded collection.

   .. example::

      The following example displays a single record of a chunk split
      from a :data:`~config.changelog` collection:

      .. code-block:: javascript

         {
          "_id" : "<hostname>-<timestamp>-<increment>",
          "server" : "<hostname><:port>",
          "clientAddr" : "127.0.0.1:63381",
          "time" : ISODate("2012-12-11T14:09:21.039Z"),
          "what" : "split",
          "ns" : "<database>.<collection>",
          "details" : {
             "before" : {
                "min" : {
                   "<database>" : { $minKey : 1 }
                },
                "max" : {
                   "<database>" : { $maxKey : 1 }
                },
                "lastmod" : Timestamp(1000, 0),
                "lastmodEpoch" : ObjectId("000000000000000000000000")
             },
             "left" : {
                "min" : {
                   "<database>" : { $minKey : 1 }
                },
                "max" : {
                   "<database>" : "<value>"
                },
                "lastmod" : Timestamp(1000, 1),
                "lastmodEpoch" : ObjectId(<...>)
             },
             "right" : {
                "min" : {
                   "<database>" : "<value>"
                },
                "max" : {
                   "<database>" : { $maxKey : 1 }
                },
                "lastmod" : Timestamp(1000, 2),
                "lastmodEpoch" : ObjectId("<...>")
             }
          }
         }

   Each document in the :data:`~config.changelog` collection contains the
   following fields:

   .. data:: config.changelog._id

      The value of ``changelog._id`` is:
      ``<hostname>-<timestamp>-<increment>``.

   .. data:: config.changelog.server

      The hostname of the server that holds this data.

   .. data:: config.changelog.clientAddr

      A string that holds the address of the client, a
      :program:`mongos` instance that initiates this change.

   .. data:: config.changelog.time

      A :term:`ISODate` timestamp that reflects when the change
      occurred.

   .. data:: config.changelog.what

      Reflects the type of change recorded. Possible values are:

      - ``dropCollection``
      - ``dropCollection.start``
      - ``dropDatabase``
      - ``dropDatabase.start``
      - ``moveChunk.start``
      - ``moveChunk.commit``
      - ``split``
      - ``multi-split``

   .. data:: config.changelog.ns

      Namespace where the change occurred.

   .. data:: config.changelog.details

      A :term:`document` that contains  additional details regarding
      the change. The structure of the :data:`~config.changelog.details`
      document depends on the type of change.

.. data:: config.chunks

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.chunks` collection stores a document for each chunk in
   the cluster.  Consider the following example of a document for a
   chunk named ``records.pets-animal_\"cat\"``:

   .. code-block:: javascript

      {
         "_id" : "mydb.foo-a_\"cat\"",
         "lastmod" : Timestamp(1000, 3),
         "lastmodEpoch" : ObjectId("5078407bd58b175c5c225fdc"),
         "ns" : "mydb.foo",
         "min" : {
               "animal" : "cat"
         },
         "max" : {
               "animal" : "dog"
         },
         "shard" : "shard0004"
      }

   These documents store the range of values for the shard key that
   describe the chunk in the ``min`` and ``max`` fields. Additionally
   the ``shard`` field identifies the shard in the cluster that "owns"
   the chunk.

.. data:: config.collections

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.collections` collection stores a document for each sharded collection
   in the cluster. Given a collection named ``pets``
   in the ``records`` database, a document in the :data:`~config.collections`
   collection would resemble the following:

   .. code-block:: javascript

      {
         "_id" : "records.pets",
         "lastmod" : ISODate("1970-01-16T15:00:58.107Z"),
         "dropped" : false,
         "key" : {
               "a" : 1
         },
         "unique" : false,
         "lastmodEpoch" : ObjectId("5078407bd58b175c5c225fdc")
      }

.. data:: config.databases

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.databases` collection stores a document for each
   database in the cluster, and tracks if the database has sharding
   enabled. :data:`~config.databases` represents each database in a
   distinct document. When a databases have sharding enabled, the
   ``primary`` field holds the name of the :term:`primary shard`.

   .. code-block:: javascript

      { "_id" : "admin", "partitioned" : false, "primary" : "config" }
      { "_id" : "mydb", "partitioned" : true, "primary" : "shard0000" }

.. data:: config.lockpings

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.lockpings` collection keeps track of the active components
   in the sharded cluster. Given a cluster with a :program:`mongos`
   running on ``example.com:30000``, the document in the
   :data:`~config.lockpings` collection would resemble:

   .. code-block:: javascript

      { "_id" : "example.com:30000:1350047994:16807", "ping" : ISODate("2012-10-12T18:32:54.892Z") }

.. data:: config.locks

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.locks` collection stores a distributed lock. This
   ensures that only one :program:`mongos` instance can perform
   administrative tasks on the cluster at once. The :program:`mongos`
   acting as :term:`balancer` takes a lock by inserting a document
   resembling the following into the ``locks`` collection.

   .. code-block:: javascript

      {
          "_id" : "balancer",
          "process" : "example.net:40000:1350402818:16807",
          "state" : 2,
          "ts" : ObjectId("507daeedf40e1879df62e5f3"),
          "when" : ISODate("2012-10-16T19:01:01.593Z"),
          "who" : "example.net:40000:1350402818:16807:Balancer:282475249",
          "why" : "doing balance round"
      }

   If a :program:`mongos` holds the balancer lock, the ``state`` field
   has a value of ``2``, which means that balancer is active. The ``when`` field
   indicates when the balancer began the current operation.

   .. versionchanged:: 2.0
      The value of the ``state`` field was ``1`` before MongoDB 2.0.

.. data:: config.mongos

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.mongos` collection stores a document for each
   :program:`mongos` instance affiliated with the
   cluster. :program:`mongos` instances send pings to all members of
   the cluster every 30 seconds so the cluster can verify that the
   :program:`mongos` is active. The ``ping`` field shows the time of
   the last ping, while the ``up`` field reports the uptime of the
   :program:`mongos` as of the last ping. The cluster maintains this
   collection for reporting purposes.

   The following document shows the status of the :program:`mongos`
   running on ``example.com:30000``.

   .. code-block:: javascript

      { "_id" : "example.com:30000", "ping" : ISODate("2012-10-12T17:08:13.538Z"), "up" : 13699, "waiting" : true }

.. data:: config.settings

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.settings` collection holds the following
   sharding configuration settings:

   - Chunk size. To change chunk size,
     see :doc:`/tutorial/modify-chunk-size-in-sharded-cluster`.

   - Balancer status. To change status,
     see :ref:`sharding-balancing-disable-temporarily`.

   The following is an example ``settings`` collection:

   .. code-block:: javascript

      { "_id" : "chunksize", "value" : 64 }
      { "_id" : "balancer", "stopped" : false }

.. data:: config.shards

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.shards` collection represents each shard in the cluster
   in a separate document. If the shard is a replica set, the
   ``host`` field displays the name of the replica set, then a slash, then
   the hostname, as in the following example:

   .. code-block:: javascript

      { "_id" : "shard0000", "host" : "shard1/localhost:30000" }

   If the shard has :ref:`tags <tag-aware-sharding>` assigned, this
   document has a ``tags`` field, that holds an array of the tags, as
   in the following example:

   .. code-block:: javascript

      { "_id" : "shard0001", "host" : "localhost:30001", "tags": [ "NYC" ] }

.. data:: config.tags

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.tags` collection holds documents for each tagged
   shard key range in the cluster. The documents in the
   :data:`~config.tags` collection resemble the following:

   .. code-block:: javascript

      {
          "_id" : { "ns" : "records.users", "min" : { "zipcode" : "10001" } },
          "ns" : "records.users",
          "min" : { "zipcode" : "10001" },
          "max" : { "zipcode" : "10281" },
          "tag" : "NYC"
      }

.. data:: config.version

   .. include:: /includes/admonition-config-db-is-internal.rst

   The :data:`~config.version` collection holds the current metadata version number. This
   collection contains only one document:

   To access the :data:`~config.version` collection you must use the
   :method:`db.getCollection()` method. For example, to display the
   collection's document:

   .. code-block:: javascript

      mongos> db.getCollection("version").find()
      { "_id" : 1, "version" : 3 }

.. note::

   Like all databases in MongoDB, the ``config`` database contains a
   :data:`system.indexes <<database>.system.indexes>` collection
   contains metadata for all indexes in the database for information
   on indexes, see :doc:`/indexes`.
==========================
Configuration File Options
==========================

.. default-domain:: mongodb

.. versionchanged:: 2.6
   MongoDB introduces a YAML-based configuration file format; however
   the :v2.4:`2.4 configuration file format
   </reference/configuration-options>` remains for backward
   compatibility.

Synopsis
--------

Administrators and users can control :program:`mongod` or
:program:`mongos` instances at runtime either directly from
:doc:`mongod's command line arguments </reference/program/mongod>` or using a
configuration file.

While both methods are functionally equivalent and all settings are
similar, the configuration file method is preferable. If you
installed from a package and have started MongoDB using your system's
:term:`control script`, you're already using a configuration file.

To start :program:`mongod` or :program:`mongos` using a config file,
use one of the following forms:

.. code-block:: sh

   mongod --config /etc/mongodb.conf
   mongod -f /etc/mongodb.conf
   mongos --config /srv/mongodb/mongos.conf
   mongos -f /srv/mongodb/mongos.conf

The configuration file is in `YAML <http://www.yaml.org>`_ format:
specify a single YAML document or record with options describe
below. Consider the following configuration file fragment:

.. code-block:: yaml

   systemLog:
      destination: file
      path: "/var/log/mongodb/mongodb.log"
      quiet: true
      logAppend: true
   storage:
      journal:
         enabled: true
   processManagement:
      fork: true
   net:
      bindIp: 127.0.0.1
      port: 27017
   ...

Settings
--------

Core Options
~~~~~~~~~~~~

.. setting:: systemLog

.. include:: /includes/option/setting-conf-systemLog.verbosity.rst

.. include:: /includes/option/setting-conf-systemLog.quiet.rst

.. include:: /includes/option/setting-conf-systemLog.traceAllExceptions.rst

.. include:: /includes/option/setting-conf-systemLog.syslogFacility.rst

.. include:: /includes/option/setting-conf-systemLog.path.rst

.. include:: /includes/option/setting-conf-systemLog.logAppend.rst

.. include:: /includes/option/setting-conf-systemLog.destination.rst

.. include:: /includes/option/setting-conf-systemLog.timeStampFormat.rst

.. setting:: processManagement

.. include:: /includes/option/setting-conf-processManagement.pidFilePath.rst

.. include:: /includes/option/setting-conf-processManagement.fork.rst

.. setting:: net

.. include:: /includes/option/setting-conf-net.port.rst

.. include:: /includes/option/setting-conf-net.bindIp.rst

.. include:: /includes/option/setting-conf-net.maxIncomingConnections.rst

.. include:: /includes/option/setting-conf-net.wireObjectCheck.rst

.. include:: /includes/option/setting-conf-net.http.enabled.rst

.. PENDING SERVER-13097

   .. include:: /includes/option/setting-conf-net.http.port.rst

.. include:: /includes/option/setting-conf-net.unixDomainSocket.enabled.rst

.. include:: /includes/option/setting-conf-net.unixDomainSocket.pathPrefix.rst

.. include:: /includes/option/setting-conf-net.ipv6.rst

.. include:: /includes/option/setting-conf-net.http.JSONPEnabled.rst

.. include:: /includes/option/setting-conf-net.http.RESTInterfaceEnabled.rst

.. include:: /includes/option/setting-conf-net.ssl.mode.rst

.. include:: /includes/option/setting-conf-net.ssl.PEMKeyFile.rst

.. include:: /includes/option/setting-conf-net.ssl.PEMKeyPassword.rst

.. include:: /includes/option/setting-conf-net.ssl.clusterFile.rst

.. include:: /includes/option/setting-conf-net.ssl.clusterPassword.rst

.. include:: /includes/option/setting-conf-net.ssl.CAFile.rst

.. include:: /includes/option/setting-conf-net.ssl.CRLFile.rst

.. include:: /includes/option/setting-conf-net.ssl.weakCertificateValidation.rst

.. include:: /includes/option/setting-conf-net.ssl.allowInvalidCertificates.rst

.. include:: /includes/option/setting-conf-net.ssl.FIPSMode.rst

.. include:: /includes/option/setting-conf-setParameter.rst

.. setting:: security

.. include:: /includes/option/setting-conf-security.keyFile.rst

.. include:: /includes/option/setting-conf-security.clusterAuthMode.rst

.. include:: /includes/option/setting-conf-security.authentication.rst

.. include:: /includes/option/setting-conf-security.authenticationMechanisms.rst

.. include:: /includes/option/setting-conf-security.enableLocalhostAuthBypass.rst

.. include:: /includes/option/setting-conf-security.supportCompatibilityFormPrivilegeDocuments.rst

.. include:: /includes/option/setting-conf-security.authSchemaVersion.rst

.. include:: /includes/option/setting-conf-security.sasl.hostName.rst

.. include:: /includes/option/setting-conf-security.sasl.serviceName.rst

.. include:: /includes/option/setting-conf-security.sasl.saslauthdSocketPath.rst

.. setting:: operationProfiling

.. include:: /includes/option/setting-conf-operationProfiling.slowOpThresholdMs.rst

.. include:: /includes/option/setting-conf-operationProfiling.mode.rst

.. setting:: storage

.. include:: /includes/option/setting-conf-storage.dbPath.rst

.. include:: /includes/option/setting-conf-storage.directoryPerDB.rst

.. include:: /includes/option/setting-conf-storage.indexBuildRetry.rst

.. include:: /includes/option/setting-conf-storage.preallocDataFiles.rst

.. include:: /includes/option/setting-conf-storage.nsSize.rst

.. include:: /includes/option/setting-conf-storage.quota.enforced.rst

.. include:: /includes/option/setting-conf-storage.quota.maxFilesPerDB.rst

.. include:: /includes/option/setting-conf-storage.smallFiles.rst

.. include:: /includes/option/setting-conf-storage.syncPeriodSecs.rst

.. include:: /includes/option/setting-conf-storage.repairPath.rst

.. include:: /includes/option/setting-conf-storage.journal.enabled.rst

.. include:: /includes/option/setting-conf-storage.journal.debugFlags.rst

.. include:: /includes/option/setting-conf-storage.journal.commitIntervalMs.rst

.. setting:: replication

.. include:: /includes/option/setting-conf-replication.oplogSizeMB.rst

.. include:: /includes/option/setting-conf-replication.replSetName.rst

.. include:: /includes/option/setting-conf-replication.secondaryIndexPrefetch.rst

.. setting:: sharding

.. include:: /includes/option/setting-conf-sharding.clusterRole.rst

.. include:: /includes/option/setting-conf-sharding.archiveMovedChunks.rst

.. setting:: auditLog

.. include:: /includes/option/setting-conf-auditLog.destination.rst

.. include:: /includes/option/setting-conf-auditLog.format.rst

.. include:: /includes/option/setting-conf-auditLog.path.rst

.. include:: /includes/option/setting-conf-auditLog.filter.rst

.. setting:: snmp

.. include:: /includes/option/setting-conf-snmp.subagent.rst

.. include:: /includes/option/setting-conf-snmp.master.rst

``mongos``\ -only Options
~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/option/setting-conf-replication.localPingThresholdMs.rst

.. include:: /includes/option/setting-conf-sharding.autoSplit.rst

.. include:: /includes/option/setting-conf-sharding.configDB.rst

.. include:: /includes/option/setting-conf-sharding.chunkSize.rst
.. index:: connections

============================
Connection String URI Format
============================

.. default-domain:: mongodb

This document describes the URI format for defining connections between
applications and MongoDB instances in the official MongoDB :doc:`drivers
</applications/drivers>`.

.. index:: connections; connection string format
.. _connections-standard-connection-string-format:

Standard Connection String Format
---------------------------------

This section describes the standard format of the MongoDB connection URI
used to connect to a MongoDB database server. The format is the same for
all official MongoDB drivers. For a list of drivers and links to driver
documentation, see :doc:`/applications/drivers`.

The following is the standard URI connection scheme:

.. code-block:: none

   mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]

The components of this string are:

#. ``mongodb://``

   A required prefix to identify that this is a string in the standard
   connection format.

#. ``username:password@``

   Optional. If specified, the client will attempt to log in to the
   specific database using these credentials after connecting to the
   :program:`mongod` instance.

#. ``host1``

   This the only required part of the URI. It identifies a server
   address to connect to. It identifies either a hostname, IP address,
   or UNIX domain socket.

#. ``:port1``

   Optional. The default value is ``:27017`` if not specified.

#. ``hostX``

   Optional. You can specify as many hosts as necessary. You would
   specify multiple hosts, for example, for connections to replica
   sets.

#. ``:portX``

   Optional. The default value is ``:27017`` if not specified.

#. ``/database``

   Optional. The name of the database to authenticate if the
   connection string includes authentication credentials in the form
   of ``username:password@``. If ``/database`` is not specified and
   the connection string includes credentials, the driver will
   authenticate to the ``admin`` database.

#. ``?options``

   Connection specific options. See
   :ref:`connections-connection-options` for a full description of
   these options.

   If the connection string does not specify a database/ you must
   specify a slash (i.e.  ``/``) between the last ``hostN`` and the
   question mark that begins the string of options.

.. example:: To describe a connection to a replica set named ``test``,
   with the following :program:`mongod` hosts:

   - ``db1.example.net`` on port ``27017`` and
   - ``db2.example.net`` on port ``2500``.

   You would use a connection string that resembles the following:

   .. code-block:: none

      mongodb://db1.example.net,db2.example.net:2500/?replicaSet=test

.. index:: connections; options
.. _connections-connection-options:

Connection String Options
-------------------------

This section lists all connection options used in the
:ref:`connections-standard-connection-string-format`.The options are
not case-sensitive.

Connection options are pairs in the following form:
``name=value``. Separate options with the ampersand (i.e. ``&``)
character. In the following example, a connection uses the
``replicaSet`` and ``connectTimeoutMS`` options:

.. code-block:: none

   mongodb://db1.example.net,db2.example.net:2500/?replicaSet=test&connectTimeoutMS=300000

.. admonition:: Semi-colon separator for connection string arguments

   To provide backwards compatibility, drivers currently accept
   semi-colons (i.e. ``;``) as option separators.

.. _replica-set-options:
.. _replica-set-option:

Replica Set Option
~~~~~~~~~~~~~~~~~~

.. data:: uri.replicaSet

   Specifies the name of the :term:`replica set`, if the
   :program:`mongod` is a member of a replica set.

   When connecting to a replica set it is important to give a seed
   list of at least two :program:`mongod` instances. If you only
   provide the connection point of a single :program:`mongod`
   instance, and omit the :data:`~uri.replicaSet`, the client will create a
   :term:`standalone` connection.

Connection Options
~~~~~~~~~~~~~~~~~~

.. data:: uri.ssl

   ``true``: Initiate the connection with SSL.

   ``false``: Initiate the connection without SSL.

   .. todo Determine whether ``prefer`` option exists. Check with server
      team whether it's possible to do rolling restarts of a cluster to
      turn on SSL. Here is the option
      ``prefer``: Initiate the connection with SSL, but if that fails initiate without SSL.

   The default value is ``false``.

   .. note:: The :data:`~uri.ssl` option is not supported by all
      drivers. See your :doc:`driver </applications/drivers>`
      documentation and the :doc:`/tutorial/configure-ssl`
      document.

.. data:: uri.connectTimeoutMS

   The time in milliseconds to attempt a connection before timing out.
   The default is never to timeout, though different drivers might
   vary.  See the :doc:`driver </applications/drivers>` documentation.

.. data:: uri.socketTimeoutMS

   The time in milliseconds to attempt a send or receive on a socket
   before the attempt times out. The default is never to timeout,
   though different drivers might vary. See the :doc:`driver
   </applications/drivers>` documentation.

Connection Pool Options
~~~~~~~~~~~~~~~~~~~~~~~

Most drivers implement some kind of connection pooling handle this for
you behind the scenes. Some drivers do not support connection
pools. See your :doc:`driver </applications/drivers>` documentation
for more information on the connection pooling implementation. These
options allow applications to configure the connection pool when
connecting to the MongoDB deployment.

.. data:: uri.maxPoolSize

   The maximum number of connections in the connection pool. The default
   value is ``100``.

.. data:: uri.minPoolSize

   The minimum number of connections in the connection pool. The default
   value is ``0``.

   .. note:: The :data:`~uri.minPoolSize` option is not supported by all
      drivers. For information on your driver, see the :doc:`drivers
      </applications/drivers>` documentation.

.. data:: uri.maxIdleTimeMS

   The maximum number of milliseconds that a connection can remain
   idle in the pool before being removed and closed.

   This option is not supported by all drivers.

.. data:: uri.waitQueueMultiple

   A number that the driver multiples the :data:`~uri.maxPoolSize` value
   to, to provide the maximum number of threads allowed to wait for a
   connection to become available from the pool. For default values,
   see the :doc:`/applications/drivers` documentation.

.. data:: uri.waitQueueTimeoutMS

   The maximum time in milliseconds that a thread can wait for a
   connection to become available. For default values, see the
   :doc:`/applications/drivers` documentation.

.. _connections-write-concern:

Write Concern Options
~~~~~~~~~~~~~~~~~~~~~

:ref:`Write concern <write-concern>` describes the kind of assurances
that the program:`mongod` and the driver provide to the application
regarding the success and durability of the write operation. For a
full explanation of write concern and write operations in general see
the: :doc:`/core/write-operations`:

.. data:: uri.w

   Defines the level and kind of write concern, that the driver uses
   when calling :dbcommand:`getLastError`. This option can take either
   a number or a string as a value.

   .. include:: /reference/connection-string-w-param.rst

.. data:: uri.wtimeoutMS

   The time in milliseconds to wait for replication to succeed, as
   specified in the :data:`~uri.w` option, before timing out. When ``wtimeoutMS`` is ``0``,
   write operations will never time out.

.. data:: uri.journal

   Controls whether write operations will wait until the
   :program:`mongod` acknowledges the write operations and commits the
   data to the on disk :term:`journal`.

   .. include:: /reference/connection-string-journal-param.rst

   If you set :data:`~uri.journal` to ``true``, and specify a :data:`~uri.w`
   value less than 1, :data:`~uri.journal` prevails.

   If you set :data:`~uri.journal` to true, and the :program:`mongod` does
   not have journaling enabled, as with :setting:`nojournal`, then
   :dbcommand:`getLastError` will provide basic receipt
   acknowledgment (i.e. ``w:1``), and will include a ``jnote`` field
   in its return document.

Read Preference Options
~~~~~~~~~~~~~~~~~~~~~~~

:doc:`Read preferences </core/read-preference>` describe the
behavior of read operations with regards to :term:`replica sets
<replica set>`. These parameters allow you to specify read preferences
on a per-connection basis in the connection string:

.. data:: uri.readPreference

   Specifies the :term:`replica set` read preference for this
   connection. This setting overrides any ``slaveOk`` value. The read
   preference values are the following:

   - :readmode:`primary`
   - :readmode:`primaryPreferred`
   - :readmode:`secondary`
   - :readmode:`secondaryPreferred`
   - :readmode:`nearest`

   For descriptions of each value, see
   :ref:`replica-set-read-preference-modes`.

   The default value is :readmode:`primary`, which sends all read
   operations to the replica set's :term:`primary`.

.. data:: uri.readPreferenceTags

   Specifies a tag set as a comma-separated list of
   colon-separated key-value pairs. For example:

   .. code-block:: none

      dc:ny,rack:1

   To specify a *list* of tag sets, use multiple ``readPreferenceTags``.
   The following specifies two tag sets and an empty tag set:

   .. code-block:: none

      readPreferenceTags=dc:ny,rack:1&readPreferenceTags=dc:ny&readPreferenceTags=

   Order matters when using multiple ``readPreferenceTags``.

Authentication Options
~~~~~~~~~~~~~~~~~~~~~~

.. data:: uri.authSource

   .. versionadded:: 2.4

   Specify the database name associated with the user's
   credentials, if the users collection do not exist in the database
   where the client is connecting. :data:`~uri.authSource` defaults to
   the database specified in the connection string.

   For authentication mechanisms that delegate credential storage to
   other services, the :data:`~uri.authSource` value should be
   ``$external`` as with the ``PLAIN`` (LDAP) and ``GSSAPI``
   (Kerberos) authentication mechanisms.

   MongoDB will ignore :data:`~uri.authSource` values if the
   connection string specifies no  user name.

.. data:: uri.authMechanism

   .. versionadded:: 2.4

   .. versionchanged:: 2.6
      Support for the ``PLAIN`` and ``MONGODB-X509`` authentication mechanisms.

   Specify the authentication mechanism that MongoDB will use to
   authenticate the connection. Possible values include:

   .. these options should link to a reference page for authentication
      mechanisms. DOCS-2940

   - MONGODB-CR
   - MONGODB-X509
   - GSSAPI
   - PLAIN

   Only MongoDB Enterprise :program:`mongod` and :program:`mongos`
   instances provide ``GSSAPI`` (Kerberos) and ``PLAIN`` (LDAP)
   mechanisms. To use ``MONGODB-X509``, you must have SSL Enabled.

   See :doc:`/core/authentication` for more information about the
   authentication system in MongoDB. Also consider
   :doc:`/tutorial/configure-x509` for more information on x509
   authentication.

.. data:: uri.gssapiServiceName

   Set the Kerberos service name when connecting to Kerberized MongoDB
   instances. This value must match the service name set on MongoDB
   instances.

   :data:`~uri.gssapiServiceName` defaults to ``mongodb`` for all
   clients and for MongoDB instance. If you change
   :parameter:`saslServiceName` setting on a MongoDB instance, you
   will need to set :data:`~uri.gssapiServiceName` to the same value.

Miscellaneous Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. data:: uri.uuidRepresentation

   .. include:: /reference/connection-string-uuidRepresentation-param.rst

   For the default, see the :doc:`drivers </applications/drivers>`
   documentation for your driver.

   .. note:: Not all drivers support the :data:`~uri.uuidRepresentation`
      option. For information on your driver, see the :doc:`drivers
      </applications/drivers>` documentation.

.. _connections-connection-examples:

Examples
--------

The following provide example URI strings for common connection targets.

Database Server Running Locally
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following connects to a database server running locally on the
default port:

.. code-block:: none

   mongodb://localhost

``admin`` Database
~~~~~~~~~~~~~~~~~~

The following connects and logs in to the ``admin`` database as user
``sysop`` with the password ``moon``:

.. code-block:: none

   mongodb://sysop:moon@localhost

``records`` Database
~~~~~~~~~~~~~~~~~~~~

The following connects and logs in to the ``records`` database as user
``sysop`` with the password ``moon``:

.. code-block:: none

   mongodb://sysop:moon@localhost/records

UNIX Domain Socket
~~~~~~~~~~~~~~~~~~

The following connects to a UNIX domain socket:

.. code-block:: none

   mongodb:///tmp/mongodb-27017.sock

.. note:: Not all drivers support UNIX domain sockets. For information
   on your driver, see the :doc:`drivers </applications/drivers>`
   documentation.

Replica Set with Members on Different Machines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following connects to a :term:`replica set` with two members, one on
``db1.example.net`` and the other on ``db2.example.net``:

.. code-block:: none

   mongodb://db1.example.net,db2.example.com

Replica Set with Members on ``localhost``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following connects to a replica set with three members running on ``localhost`` on
ports ``27017``, ``27018``, and ``27019``:

.. code-block:: none

   mongodb://localhost,localhost:27018,localhost:27019

Replica Set with Read Distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following connects to a replica set with three members and
distributes reads to the :term:`secondaries <secondary>`:

.. code-block:: none

   mongodb://example1.com,example2.com,example3.com/?readPreference=secondary

Replica Set with a High Level of Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following connects to a replica set with write concern configured to wait for
replication to succeed on at least two members, with a two-second
timeout.

.. code-block:: none

   mongodb://example1.com,example2.com,example3.com/?w=2&wtimeoutMS=2000
======================
MongoDB CRUD Reference
======================

.. default-domain:: mongodb

Query Cursor Methods
--------------------

.. include:: /includes/toc/table-spec-crud-cursor-methods.rst

Query and Data Manipulation Collection Methods
----------------------------------------------

.. include:: /includes/toc/table-spec-crud-collection-methods.rst

MongoDB CRUD Reference Documentation
------------------------------------

.. include:: /includes/toc/dfn-list-crud-reference.rst

.. include:: /includes/toc/crud-reference.rst
====================
Data Model Reference
====================

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-data-modeling-reference.rst

.. include:: /includes/toc/data-modeling-reference.rst
.. _profiler:

========================
Database Profiler Output
========================

.. default-domain:: mongodb

The database profiler captures data information about read and write
operations, cursor operations, and database commands. To configure the
database profile and set the thresholds for capturing profile data,
see the :doc:`/tutorial/manage-the-database-profiler` section.

The database profiler writes data in the :data:`system.profile
<<database>.system.profile>` collection,
which is a :term:`capped collection`. To view the profiler's output,
use normal MongoDB queries on the :data:`system.profile
<<database>.system.profile>` collection.

.. note::

   Because the database profiler writes data to the
   :data:`system.profile <<database>.system.profile>` collection in a
   database, the profiler will profile some write activity, even for
   databases that are otherwise read-only.

Example ``system.profile`` Document
-----------------------------------

The documents in the :data:`system.profile
<<database>.system.profile>` collection have the following form. This
example document reflects an update operation:

.. code-block:: javascript

   {
       "ts" : ISODate("2012-12-10T19:31:28.977Z"),
       "op" : "update",
       "ns" : "social.users",
       "query" : {
           "name" : "j.r."
       },
       "updateobj" : {
           "$set" : {
               "likes" : [
                   "basketball",
                   "trekking"
               ]
           }
       },
       "nscanned" : 8,
       "scanAndOrder" : true,
       "moved" : true,
       "nmoved" : 1,
       "nupdated" : 1,
       "keyUpdates" : 0,
       "numYield" : 0,
       "lockStats" : {
           "timeLockedMicros" : {
               "r" : NumberLong(0),
               "w" : NumberLong(258)
           },
           "timeAcquiringMicros" : {
               "r" : NumberLong(0),
               "w" : NumberLong(7)
           }
       },
       "millis" : 0,
       "client" : "127.0.0.1",
       "user" : ""
   }

Output Reference
----------------

For any single operation, the documents created by the database
profiler will include a subset of the following fields. The precise
selection of fields in these documents depends on the type of
operation.

.. data:: system.profile.ts

   The timestamp of the operation.

.. data:: system.profile.op

   The type of operation. The possible values are:

   - ``insert``
   - ``query``
   - ``update``
   - ``remove``
   - ``getmore``
   - ``command``

.. data:: system.profile.ns

   The :term:`namespace` the operation targets. Namespaces in MongoDB
   take the form of the :term:`database`, followed by a dot (``.``),
   followed by the name of
   the :term:`collection`.

.. data:: system.profile.query

   The :ref:`query document <read-operations-query-document>` used.

.. data:: system.profile.command

   The command operation.

.. data:: system.profile.updateobj

   The ``<update>`` document passed in
   during an :doc:`update </core/write-operations>` operation.

.. data:: system.profile.cursorid

   The ID of the cursor accessed by a ``getmore`` operation.

.. data:: system.profile.ntoreturn

   .. versionchanged:: 2.2
      In 2.0, MongoDB includes this field for ``query`` and
      ``command`` operations. In 2.2, this information MongoDB also
      includes this field for ``getmore`` operations.

   The number of documents the operation specified to return. For
   example, the :dbcommand:`profile` command would return one document
   (a results document) so the :data:`~system.profile.ntoreturn` value would be ``1``. The
   :method:`limit(5) <cursor.limit()>` command would return five
   documents so the :data:`~system.profile.ntoreturn` value would be ``5``.

   If the :data:`~system.profile.ntoreturn` value is ``0``, the command did not specify a
   number of documents to return, as would be the case with a simple
   :method:`~db.collection.find()` command with no limit
   specified.

.. data:: system.profile.ntoskip

   .. versionadded:: 2.2

   The number of documents the :method:`~cursor.skip()` method
   specified to skip.

.. data:: system.profile.nscanned

   The number of documents that MongoDB scans in the :doc:`index </indexes>` in order to
   carry out the operation.

   In general, if :data:`~system.profile.nscanned` is much higher than :data:`~system.profile.nreturned`, the
   database is scanning many objects to find the target objects.
   Consider creating an index to improve this.

.. data:: system.profile.scanAndOrder

   :data:`~system.profile.scanAndOrder` is a boolean that is ``true`` when
   a query **cannot** use the order of documents in the index for
   returning sorted results: MongoDB must sort the documents after it
   receives the documents from a cursor.

   If :data:`~system.profile.scanAndOrder` is ``false``, MongoDB *can* use
   the order of the documents in an index to return sorted results.

   .. Based on informal tests, it looks like:data:`~system.profile.scanAndOrder`  is
      only included in the profiler data when scanAndOrder is true.

.. data:: system.profile.moved

   This field appears with a value of ``true`` when an update operation
   moved one or more documents to a new location on disk. If the operation
   did not result in a move, this field does not appear. Operations that
   result in a move take more time than in-place updates and typically
   occur as a result of document growth.

   .. todo put a link to document growth in schema design/data
      modeling docs.

.. data:: system.profile.nmoved

   .. versionadded:: 2.2

   The number of documents the operation moved on disk. This field appears
   only if the operation resulted in a move. The field's implicit value is
   zero, and the field is present only when non-zero.

.. data:: system.profile.nupdated

   .. versionadded:: 2.2

   The number of documents updated by the operation.

.. data:: system.profile.keyUpdates

   .. versionadded:: 2.2

   The number of :doc:`index </indexes>` keys the update changed in
   the operation. Changing an index key
   carries a small performance cost because the database must remove the old
   key and inserts a new key into the B-tree index.

.. data:: system.profile.numYield

   .. versionadded:: 2.2

   The number of times the operation yielded to allow other operations
   to complete. Typically, operations yield when they need access to
   data that MongoDB has not yet fully read into memory. This allows
   other operations that have data in memory to complete while MongoDB
   reads in data for the yielding operation. For more information,
   see :ref:`the FAQ on when operations yield <faq-concurrency-yielding>`.

.. data:: system.profile.lockStats

   .. versionadded:: 2.2

   The time in microseconds the operation spent acquiring and holding
   locks. This field reports data for the following lock types:

   - ``R`` - global read lock
   - ``W`` - global write lock
   - ``r`` - database-specific read lock
   - ``w`` - database-specific write lock

   .. data:: system.profile.lockStats.timeLockedMicros

      The time in microseconds the operation held a specific lock. For
      operations that require more than one lock, like those
      that lock the ``local`` database to update the :term:`oplog`,
      this value may be longer than the total length of the
      operation (i.e. :data:`~system.profile.millis`.)

   .. data:: system.profile.lockStats.timeAcquiringMicros

      The time in microseconds the operation spent waiting to acquire a
      specific lock.

.. data:: system.profile.nreturned

   The number of documents returned by the operation.

.. data:: system.profile.responseLength

   The length in bytes of the operation's result document. A large
   :data:`~system.profile.responseLength` can affect performance.
   To limit the size of the
   result document for a query operation, you can use any of the
   following:

   - :ref:`Projections <read-operations-projection>`
   - :method:`The limit() method <cursor.limit()>`
   - :method:`The batchSize() method <cursor.batchSize()>`

   .. note:: When MongoDB writes query profile information to the log,
      the :data:`~system.profile.responseLength` value is in a field
      named ``reslen``.

.. data:: system.profile.millis

   The time in milliseconds from the perspective of the
   :program:`mongod` from the beginning of the operation to the end of
   the operation.

.. data:: system.profile.client

   The IP address or hostname of the client connection where the
   operation originates.

   For some operations, such as :method:`db.eval()`, the client is
   ``0.0.0.0:0`` instead of an actual client.

.. data:: system.profile.user

   The authenticated user who ran the operation.

.. todo Test what values appear for authenticated users and add that info.
   When we do, we can add back this: "If the operation was not run by an
   authenticated user, this field's value is an empty string."

.. todo Consider adding documentation to our manual on the
   "internal user" used by replication

.. todo add versionchanged stats for version 2.0 in all the fields.
.. index:: DBRef
.. index:: database references
.. index:: references
.. _database-references:

===================
Database References
===================

.. default-domain:: mongodb

MongoDB does not support joins. In MongoDB some data is
*denormalized*, or stored with related data in :term:`documents
<document>` to remove the need for joins. However, in some cases it
makes sense to store related information in separate documents,
typically in different collections or databases.

MongoDB applications use one of two methods for relating documents:

#. :ref:`Manual references <document-references>` where you save the
   ``_id`` field of one document in another document as a reference.
   Then your application can run a second query to return the related
   data. These references are simple and sufficient for most use
   cases.

#. :ref:`DBRefs <dbref>` are references from one document to another
   using the value of the first document's ``_id`` field, collection name,
   and, optionally, its database name. By including these names, DBRefs
   allow documents located in multiple collections to be more easily linked
   with documents from a single collection.

   To resolve DBRefs, your application
   must perform additional queries to return the referenced
   documents. Many :doc:`drivers </applications/drivers>` have helper
   methods that form the query for the DBRef automatically. The
   drivers [#official-driver]_ do not *automatically* resolve DBRefs
   into documents.

   DBRefs provide a common format and type to represent relationships among
   documents. The DBRef format also provides common semantics for representing
   links between documents if your database must interact with
   multiple frameworks and tools.

Unless you have a compelling reason to use DBRefs, use manual
references instead.

.. [#official-driver] Some community supported drivers may have
   alternate behavior and may resolve a DBRef into a document
   automatically.

.. _document-references:

Manual References
-----------------

Background
~~~~~~~~~~

Using manual references is the practice of including one
:term:`document's <document>` ``_id`` field in another document. The
application can then issue a second query to resolve the referenced
fields as needed.

Process
~~~~~~~

Consider the following operation to insert two documents, using the
``_id`` field of the first document as a reference in the second
document:

.. code-block:: javascript

   original_id = ObjectId()

   db.places.insert({
       "_id": original_id,
       "name": "Broadway Center",
       "url": "bc.example.net"
   })

   db.people.insert({
       "name": "Erin",
       "places_id": original_id,
       "url":  "bc.example.net/Erin"
   })

Then, when a query returns the document from the ``people`` collection
you can, if needed, make a second query for the document referenced by
the ``places_id`` field in the ``places`` collection.

Use
~~~

For nearly every case where you want to store a relationship between
two documents, use :ref:`manual references <document-references>`. The
references are simple to create and your application can resolve
references as needed.

The only limitation of manual linking is that these references do not
convey the database and collection names. If you have documents in a
single collection that relate to documents in more than one
collection, you may need to consider using :ref:`DBRefs <dbref>`.

.. _dbref:

DBRefs
------

Background
~~~~~~~~~~

DBRefs are a convention for representing a :term:`document`, rather
than a specific reference type. They include the name of the
collection, and in some cases the database name, in addition to the
value from the ``_id`` field.

Format
~~~~~~

DBRefs have the following fields:

.. describe:: $ref

   The ``$ref`` field holds the name of the collection where the
   referenced document resides.

.. describe:: $id

   The ``$id`` field contains the value of the ``_id`` field in the
   referenced document.

.. describe:: $db

   *Optional.*

   Contains the name of the database where the referenced document
   resides.

   Only some drivers support ``$db`` references.

.. example::

   DBRef documents resemble the following document:

   .. code-block:: javascript

      { "$ref" : <value>, "$id" : <value>, "$db" : <value> }

   Consider a document from a collection that stored a DBRef in a
   ``creator`` field:

   .. code-block:: javascript

      {
        "_id" : ObjectId("5126bbf64aed4daf9e2ab771"),
        // .. application fields
        "creator" : {
                        "$ref" : "creators",
                        "$id" : ObjectId("5126bc054aed4daf9e2ab772"),
                        "$db" : "users"
                     }
      }

   The DBRef in this example points to a document in the ``creators``
   collection of the ``users`` database that has
   ``ObjectId("5126bc054aed4daf9e2ab772")`` in its ``_id`` field.

.. note::

   The order of fields in the DBRef matters, and you must use the above
   sequence when using a DBRef.

Support
~~~~~~~

**C++**
   The C++ driver contains no support for DBRefs. You can transverse
   references manually.

**C#**
   The C# driver provides access to DBRef objects with the
   :api:`MongoDBRef Class <csharp/current/html/46c356d3-ed06-a6f8-42fa-e0909ab64ce2.htm>`
   and supplies the :api:`FetchDBRef Method <csharp/current/html/1b0b8f48-ba98-1367-0a7d-6e01c8df436f.htm>`
   for accessing these objects.

**Java**
   The :api:`DBRef <java/current/com/mongodb/DBRef.html>` class
   provides supports for DBRefs from Java.

**JavaScript**
   The :program:`mongo` shell's :doc:`JavaScript </reference/method>`
   interface provides a DBRef.

**Perl**
   The Perl driver contains no support for DBRefs.  You can transverse
   references manually or use the `MongoDBx::AutoDeref
   <http://search.cpan.org/dist/MongoDBx-AutoDeref/>`_ CPAN module.

**PHP**
   The PHP driver supports DBRefs, including the optional ``$db`` reference, through
   `The MongoDBRef class <http://www.php.net/manual/en/class.mongodbref.php/>`_.

**Python**
   The Python driver provides the :api:`DBRef class <python/current/api/bson/dbref.html>`,
   and the :api:`dereference method </python/current/api/pymongo/database.html#pymongo.database.Database.dereference>`
   for interacting with DBRefs.

**Ruby**
   The Ruby Driver supports DBRefs using the :api:`DBRef class </ruby/current/BSON/DBRef.html>`
   and the :api:`deference method </ruby/current/Mongo/DB.html#dereference-instance_method>`.

Use
~~~

In most cases you should use the :ref:`manual reference
<document-references>` method for connecting two or more related
documents. However, if you need to reference documents from multiple
collections, consider using DBRefs.
====================
Default MongoDB Port
====================

.. default-domain:: mongodb

The following table lists the default ports used by MongoDB:

.. include:: /includes/table/default-mongodb-port.rst
=======================
Exit Codes and Statuses
=======================

.. default-domain:: mongodb

MongoDB will return one of the following codes and statuses when
exiting. Use this guide to interpret logs and when troubleshooting
issues with :program:`mongod` and :program:`mongos` instances.

.. error:: 0

   Returned by MongoDB applications upon successful exit.

   .. symbol: EXIT_CLEAN

.. error:: 2

   The specified options are in error or are incompatible
   with other options.

   .. symbol: EXIT_BADOPTIONS 2

.. error:: 3

   Returned by :program:`mongod` if there is a mismatch between hostnames
   specified on the command line and in the :data:`local.sources`
   collection. :program:`mongod` may also return this status if
   :term:`oplog` collection in the ``local`` database is not readable.

   .. symbol: EXIT_REPLICATION_ERROR = 3

.. error:: 4

   The version of the database is different from the version supported
   by the :program:`mongod` (or :program:`mongod.exe`) instance.  The
   instance exits cleanly.  Restart :program:`mongod` with the
   :option:`--upgrade <mongod --upgrade>` option to upgrade the
   database to the version supported by this :program:`mongod`
   instance.

   .. symbol: EXIT_NEED_UPGRADE = 4

.. error:: 5

   Returned by :program:`mongod` if a :dbcommand:`moveChunk` operation
   fails to confirm a commit.

   .. symbol: EXIT_SHARDING_ERROR = 5

.. error:: 12

   Returned by the :program:`mongod.exe` process on Windows when it
   receives a Control-C, Close, Break or Shutdown event.

   .. symbol: EXIT_KILL = 12

.. error:: 14

   Returned by MongoDB applications which encounter an unrecoverable
   error, an uncaught exception or uncaught signal.  The system exits
   without performing a clean shut down.

   .. symbol: EXIT_ABRUPT = 14

.. error:: 20

   *Message:* ``ERROR: wsastartup failed <reason>``

   Returned by MongoDB applications on Windows following an error in the
   WSAStartup function.

   *Message:* ``NT Service Error``

   Returned by MongoDB applications for Windows due to failures installing,
   starting or removing the NT Service for the application.

   .. symbol: EXIT_NTSERVICE_ERROR = 20

.. error:: 45

   Returned when a MongoDB application cannot open a file or cannot
   obtain a lock on a file.

   .. Symbol: EXIT_FS

.. error:: 47

   MongoDB applications exit cleanly following a large clock skew (32768
   milliseconds) event.

   .. symbol: EXIT_CLOCK_SKEW

.. error:: 48

   :program:`mongod` exits cleanly if the server socket closes. The
   server socket is on port ``27017`` by default, or as specified to
   the :option:`--port <mongod --port>` run-time option.

    .. symbol: EXIT_NET_ERROR = 48

.. error:: 49

   Returned by :program:`mongod.exe` or :program:`mongos.exe` on Windows
   when either receives a shutdown message from the
   :guilabel:`Windows Service Control Manager`.

   .. symbol: EXIT_WINDOWS_SERVICE_STOP 49

.. error:: 100

   Returned by :program:`mongod` when the process throws an uncaught exception.

   .. symbol: EXIT_UNCAUGHT = 100
========
Glossary
========

.. Link to other glossary terms when possible.
   Reserve the "See" text at the end of the entry to link to topics in the manual.

.. default-domain:: mongodb

.. glossary::
   :sorted:

   $cmd
      A special virtual :term:`collection` that exposes MongoDB's
      :term:`database commands <database command>`.
      To use database commands, see :ref:`issue-commands`.

   _id
      A field required in every MongoDB :term:`document`. The
      :ref:`_id <document-id-field>` field must have a unique value. You can
      think of the ``_id`` field as the document's :term:`primary key`.
      If you create a new document without an ``_id`` field, MongoDB
      automatically creates the field and assigns a unique
      BSON :term:`ObjectId`.

   accumulator
      An :term:`expression` in the :term:`aggregation framework` that
      maintains state between documents in the aggregation
      :term:`pipeline`. For a list of accumulator operations, see
      :pipeline:`$group`.

   admin database
      A privileged database. Users
      must have access to the ``admin`` database to run certain
      administrative commands. For a list of administrative commands,
      see :ref:`admin-commands`.

   aggregation
      Any of a variety of operations that reduces and summarizes large
      sets of data. MongoDB's :method:`~db.collection.aggregate()` and
      :method:`~db.collection.mapReduce()` methods are two
      examples of aggregation operations. For more information, see
      :doc:`/core/aggregation`.

   aggregation framework
      The set of MongoDB operators that let you calculate aggregate
      values without having to use :term:`map-reduce`. For a list of
      operators, see :doc:`/reference/aggregation`.

   arbiter
      A member of a :term:`replica set` that exists solely to vote in
      :term:`elections <election>`. Arbiters do not replicate data. See
      :ref:`replica-set-arbiter-configuration`.

   authentication
      Verification of the user identity. See
      :doc:`/core/authentication`.

   authorization
      Provisioning of access to databases and operations. See
      :doc:`/core/authorization`.

   balancer
      An internal MongoDB process that runs in the context of a
      :term:`sharded cluster` and manages the migration of :term:`chunks
      <chunk>`. Administrators must disable the balancer for all
      maintenance operations on a sharded cluster. See
      :ref:`sharding-balancing`.

   BSON
      A serialization format used to store :term:`documents <document>` and make
      remote procedure calls in MongoDB. "BSON" is a portmanteau of the words
      "binary" and "JSON". Think of BSON as a binary representation
      of JSON (JavaScript Object Notation) documents. See
      :doc:`/reference/bson-types` and
      :doc:`/reference/mongodb-extended-json`.

   BSON types
      The set of types supported by the :term:`BSON` serialization
      format. For a list of BSON types, see :doc:`/reference/bson-types`.

   B-tree
      A data structure commonly used by database management systems to
      store indexes. MongoDB uses B-trees for its indexes.

   CAP Theorem
      Given three properties of computing systems, consistency,
      availability, and partition tolerance, a distributed computing
      system can provide any two of these features, but never all
      three.

   capped collection
      A fixed-sized :term:`collection <collection>` that automatically
      overwrites its oldest entries when it reaches its maximum size.
      The MongoDB :term:`oplog` that is used in :term:`replication` is a
      capped collection. See :doc:`/core/capped-collections`.

   checksum
      A calculated value used to ensure data integrity.
      The :term:`md5` algorithm is sometimes used as a checksum.

   chunk
      A contiguous range of :term:`shard key` values within a particular
      :term:`shard`. Chunk ranges are inclusive of the lower boundary
      and exclusive of the upper boundary. MongoDB splits chunks when
      they grow beyond the configured chunk size, which by default is
      64 megabytes. MongoDB migrates chunks when a shard contains too
      many chunks of a collection relative to other shards. See
      :ref:`sharding-data-partitioning` and :doc:`/core/sharded-cluster-mechanics`.

   client
      The application layer that uses a database for data persistence
      and storage. :term:`Drivers <driver>` provide the interface
      level between the application layer and the database server.

   cluster
      See :term:`sharded cluster`.

   collection
      A grouping of MongoDB :term:`documents <document>`. A collection
      is the equivalent of an :term:`RDBMS` table. A collection exists
      within a single :term:`database`. Collections do not enforce a
      schema. Documents within a collection can have different fields.
      Typically, all documents in a collection have a similar or related
      purpose. See :ref:`faq-dev-namespace`.

   compound index
      An :term:`index` consisting of two or more keys. See
      :ref:`index-type-compound`.

   config database
      An internal database that holds the metadata associated with a
      :term:`sharded cluster`. Applications and administrators should
      not modify the ``config`` database in the course of normal
      operation. See :doc:`/reference/config-database`.

   config server
      A :program:`mongod` instance that stores all the metadata
      associated with a :term:`sharded cluster`. A production sharded
      cluster requires three config servers, each on a separate machine.
      See :ref:`sharding-config-server`.

   control script
      A simple shell script, typically located in the ``/etc/rc.d`` or
      ``/etc/init.d`` directory, and used by the system's initialization
      process to start, restart or stop a :term:`daemon` process.

   CRUD
      An acronym for the fundamental operations of a database: Create,
      Read, Update, and Delete. See :doc:`/crud`.

   CSV
      A text-based data format consisting of comma-separated values.
      This format is commonly used to exchange data between relational
      databases since the format is well-suited to tabular data. You can
      import CSV files using :program:`mongoimport`.

   cursor
      A pointer to the result set of a :term:`query`. Clients can
      iterate through a cursor to retrieve results. By default, cursors
      timeout after 10 minutes of inactivity. See
      :ref:`read-operations-cursors`.

   daemon
      The conventional name for a background, non-interactive
      process.

   data directory
      The file-system location where the :program:`mongod` stores data
      files. The :setting:`~storage.dbPath` option specifies the data directory.

   data-center awareness
      A property that allows clients to address members in a system
      based on their locations. :term:`Replica sets <replica set>`
      implement data-center awareness using :term:`tagging <tag>`. See
      :doc:`/data-center-awareness`.

   database
      A physical container for :term:`collections <collection>`.
      Each database gets its own set of files on the file
      system. A single MongoDB server typically has multiple
      databases.

   database command
      A MongoDB operation, other than an insert, update, remove, or
      query. For a list of database commands, see
      :doc:`/reference/command`. To use database commands, see
      :ref:`issue-commands`.

   database profiler
      A tool that, when enabled, keeps a record on all long-running
      operations in a database's ``system.profile`` collection. The
      profiler is most often used to diagnose slow queries. See
      :ref:`database-profiling`.

   datum
      A set of values used to define measurements on the earth. MongoDB
      uses the :term:`WGS84` datum in certain :term:`geospatial`
      calculations. See :doc:`/applications/geospatial-indexes`.

   dbpath
      The location of MongoDB's data file storage. See
      :setting:`~storage.dbPath`.

   delayed member
      A :term:`replica set` member that cannot become primary and
      applies operations at a specified delay. The delay is useful for
      protecting data from human error (i.e. unintentionally deleted
      databases) or updates that have unforeseen effects on the
      production database. See :ref:`replica-set-delayed-members`.

   diagnostic log
      A verbose log of operations stored in the :term:`dbpath`.
      See :setting:`diaglog`.

   document
      A record in a MongoDB :term:`collection` and the basic unit of
      data in MongoDB. Documents are analogous to :term:`JSON` objects
      but exist in the database in a more type-rich format known as
      :term:`BSON`. See :doc:`/core/document`.

   dot notation
      MongoDB uses the dot notation to access the elements of an array
      and to access the fields of a subdocument. See
      :ref:`document-dot-notation`.

   draining
      The process of removing or "shedding" :term:`chunks <chunk>` from
      one :term:`shard` to another. Administrators must drain shards
      before removing them from the cluster. See
      :doc:`/tutorial/remove-shards-from-cluster`.

   driver
      A client library for interacting with MongoDB in a particular
      language. See :doc:`/applications/drivers`.

   election
      The process by which members of a :term:`replica set` select a
      :term:`primary` on startup and in the event of a failure. See
      :ref:`replica-set-elections`.

   eventual consistency
      A property of a distributed system that allows changes to the
      system to propagate gradually. In a database system, this means
      that readable members are not required to reflect the latest
      writes at all times. In MongoDB, reads to a primary have
      :term:`strict consistency`; reads to secondaries have *eventual
      consistency*.

   expression
      In the context of :term:`aggregation framework`, expressions are
      the stateless transformations that operate on the data that passes
      through a :term:`pipeline`. See :doc:`/core/aggregation`.

   failover
      The process that allows a :term:`secondary` member of a
      :term:`replica set` to become :term:`primary` in the event of a
      failure. See :ref:`replica-set-failover`.

   field
      A name-value pair in a :term:`document <document>`. A document has
      zero or more fields. Fields are analogous to columns in relational
      databases. See :ref:`document-structure`.

   firewall
      A system level networking filter that restricts access based on,
      among other things, IP address. Firewalls form a part of an
      effective network security strategy. See
      :ref:`security-firewalls`.

   fsync
      A system call that flushes all dirty, in-memory pages to
      disk. MongoDB calls ``fsync()`` on its database files at least
      every 60 seconds. See :dbcommand:`fsync`.

   geohash
      A geohash value is a binary representation of the location on a
      coordinate grid. See :ref:`geospatial-indexes-geohash`.

   GeoJSON
      A :term:`geospatial` data interchange format based on JavaScript
      Object Notation (:term:`JSON`). GeoJSON is used in
      :doc:`geospatial queries </applications/geospatial-indexes>`. For
      supported GeoJSON objects, see :ref:`geo-overview-location-data`.
      For the GeoJSON format specification, see
      `<http://geojson.org/geojson-spec.html>`_.

   geospatial
      Data that relates to geographical location. In MongoDB, you may
      store, index, and query data according to geographical parameters.
      See :doc:`/applications/geospatial-indexes`.

   GridFS
      A convention for storing large files in a MongoDB database. All of
      the official MongoDB drivers support this convention, as does the
      :program:`mongofiles` program. See :doc:`/core/gridfs` and
      :doc:`/reference/gridfs`.

   hashed shard key
      A special type of :term:`shard key` that uses a hash of the value
      in the shard key field to distribute documents among members of
      the :term:`sharded cluster`. See :ref:`index-type-hashed`.

   haystack index
      A :term:`geospatial` index that enhances searches by creating
      "buckets" of objects grouped by a second criterion. See
      :doc:`/core/geohaystack`.

   hidden member
      A :term:`replica set` member that cannot become :term:`primary`
      and are invisible to client applications. See
      :ref:`replica-set-hidden-members`.

   idempotent
      The quality of an operation to produce the same result given the
      same input, whether run once or run multiple times.

   index
      A data structure that optimizes queries. See :doc:`/core/indexes`.

   initial sync
      The :term:`replica set` operation that replicates data from an
      existing replica set member to a new or restored replica set
      member. See :ref:`replica-set-initial-sync`.

   IPv6
      A revision to the IP (Internet Protocol) standard that
      provides a significantly larger address space to more effectively
      support the number of hosts on the contemporary Internet.

   ISODate
     The international date format used by :program:`mongo`
     to display dates. The format is: ``YYYY-MM-DD HH:MM.SS.millis``.

   interrupt point
     A point in an operation's lifecycle when it can
     safely abort. MongoDB only terminates an operation
     at designated interrupt points. See
     :doc:`/tutorial/terminate-running-operations`.

   JavaScript
      A popular scripting language originally designed for web
      browsers. The MongoDB shell and certain server-side functions use
      a JavaScript interpreter. See
      :doc:`/core/server-side-javascript` for more information.

   journal
      A sequential, binary transaction log used to bring the database
      into a valid state in the event of a hard shutdown.
      Journaling writes data first to the journal and then to the core
      data files. MongoDB enables journaling by default for 64-bit
      builds of MongoDB version 2.0 and newer. Journal files are
      pre-allocated and exist as files in the :term:`data directory`.
      See :doc:`/core/journaling/`.

   JSON
      JavaScript Object Notation. A human-readable, plain text format
      for expressing structured data with support in many programming
      languages. For more information, see `<http://www.json.org>`_.
      Certain MongoDB tools render an approximation of MongoDB
      :term:`BSON` documents in JSON format. See
      :doc:`/reference/mongodb-extended-json`.

   JSON document
      A :term:`JSON` document is a collection of fields and values in a
      structured format. For sample JSON documents, see
      `<http://json.org/example.html>`_.

   JSONP
      :term:`JSON` with Padding. Refers to a method of injecting JSON
      into applications. **Presents potential security concerns**.

   least privilege
      An authorization policy that gives a user only the amount of access
      that is essential to that user's work and no more.

   legacy coordinate pairs
      The format used for :term:`geospatial` data prior to MongoDB
      version 2.4. This format stores geospatial data as points on a
      planar coordinate system (e.g. ``[ x, y ]``). See
      :doc:`/applications/geospatial-indexes`.

   LineString
      A LineString is defined by an array of two or more positions. A
      closed LineString with four or more positions is called a
      LinearRing, as described in the GeoJSON LineString specification:
      `<http://geojson.org/geojson-spec.html#linestring>`_. To use a
      LineString in MongoDB, see
      :ref:`geospatial-indexes-store-geojson`.

   LVM
      Logical volume manager. LVM is a program that abstracts disk
      images from physical devices and provides a number of raw disk
      manipulation and snapshot capabilities useful for system
      management. For information on LVM and MongoDB, see
      :ref:`lvm-backup-and-restore`.

   mapping type
      A Structure in programming languages that associate keys with
      values, where keys may nest other pairs of keys and values
      (e.g. dictionaries, hashes, maps, and associative arrays).
      The properties of these structures depend on the language
      specification and implementation. Generally the order of keys in
      mapping types is arbitrary and not guaranteed.

   map-reduce
      A data processing and aggregation paradigm consisting of a "map"
      phase that selects data and a "reduce" phase that transforms the
      data. In MongoDB, you can run arbitrary aggregations over data
      using map-reduce. For map-reduce implementation, see
      :doc:`/core/map-reduce`. For all approaches to aggregation,
      see :doc:`/core/aggregation`.

   master
      The database that receives all writes in a conventional
      master-:term:`slave` replication. In MongoDB, :term:`replica
      sets <replica set>` replace master-slave replication for most use
      cases. For more information on master-slave replication, see
      :doc:`/core/master-slave`.

   md5
      A hashing algorithm used to efficiently provide
      reproducible unique strings to identify and :term:`checksum`
      data. MongoDB uses md5 to identify chunks of data for
      :term:`GridFS`. See :doc:`/reference/command/filemd5`.

   MIB
      Management Information Base. MongoDB uses MIB files to define the type of
      data tracked by SNMP in the MongoDB Enterprise edition.

   MIME
      Multipurpose Internet Mail Extensions. A standard set of type and
      encoding definitions used to declare the encoding and type of data
      in multiple data storage, transmission, and email contexts. The
      :program:`mongofiles` tool provides an option to specify a MIME
      type to describe a file inserted into :term:`GridFS` storage.

   mongo
      The MongoDB shell. The :program:`mongo` process starts the MongoDB
      shell as a daemon connected to either a :program:`mongod` or
      :program:`mongos` instance. The shell has a JavaScript interface.
      See :doc:`/reference/program/mongo` and :doc:`/reference/method`.

   mongod
      The MongoDB database server. The :program:`mongod` process starts
      the MongoDB server as a daemon. The MongoDB server manages data
      requests and formats and manages background operations. See
      :doc:`/reference/program/mongod`.

   MongoDB
      An open-source document-based database system. "MongoDB" derives
      from the word "humongous" because of the database's ability to
      scale up with ease and hold very large amounts of data. MongoDB
      stores :term:`documents <document>` in :term:`collections
      <collection>` within databases.

   MongoDB Enterprise
      A commercial edition of MongoDB that includes additional features. For
      more information, see `MongoDB Subscriptions
      <https://www.mongodb.com/products/mongodb-subscriptions>`_.

   mongos
      The routing and load balancing process that acts an interface
      between an application and a MongoDB :term:`sharded cluster`. See
      :doc:`/reference/program/mongos`.

   namespace
      The canonical name for a collection or index in MongoDB.
      The namespace is a combination of the database name and
      the name of the collection or index, like so:
      ``[database-name].[collection-or-index-name]``. All documents
      belong to a namespace. See :ref:`faq-dev-namespace`.

   natural order
      The order that a database stores documents on disk. Typically,
      the order of documents on disks reflects insertion order, except
      when a document moves internally because an update operation
      increases its size. In :term:`capped collections <capped
      collection>`, insertion order and natural order are identical
      because documents do not move internally. MongoDB returns
      documents in forward natural order for a
      :method:`~db.collection.find()` query with no parameters.
      MongoDB returns documents in reverse natural order for a
      :method:`~db.collection.find()` query :method:`sorted
      <cursor.sort()>` with a parameter of ``$natural:-1``. See
      :operator:`$natural`.

   ObjectId
      A special 12-byte :term:`BSON` type that guarantees uniqueness
      within the :term:`collection`. The ObjectId is generated based on
      timestamp, machine ID, process ID, and a process-local incremental
      counter. MongoDB uses ObjectId values as the default values for
      :term:`_id` fields.

   operator
      A keyword beginning with a ``$`` used to express an update,
      complex query, or data transformation. For example, ``$gt`` is the
      query language's "greater than" operator. For available operators,
      see :doc:`/reference/operator`.

   oplog
      A :term:`capped collection` that stores an ordered history of
      logical writes to a MongoDB database. The oplog is the
      basic mechanism enabling :term:`replication` in MongoDB.
      See :doc:`/core/replica-set-oplog`.

   ordered query plan
      A query plan that returns results in the order consistent with the
      :method:`~cursor.sort()` order. See
      :ref:`read-operations-query-optimization`.

   orphaned document
      In a sharded cluster, orphaned documents are those documents on a
      shard that also exist in chunks on other shards as a result of
      failed migrations or incomplete migration cleanup due to abnormal
      shutdown. Delete orphaned documents using
      :dbcommand:`cleanupOrphaned` to reclaim disk space and reduce
      confusion.

   padding
      The extra space allocated to document on the disk to prevent
      moving a document when it grows as the result of
      :method:`~db.collection.update()`
      operations. See :ref:`write-operations-padding-factor`.

   padding factor
      An automatically-calibrated constant used to determine how much
      extra space MongoDB should allocate per document container on disk.
      A padding factor of 1 means that MongoDB will allocate only the
      amount of space needed for the document. A padding factor of 2
      means that MongoDB will allocate twice the amount of space
      required by the document. See
      :ref:`write-operations-padding-factor`.

   page fault
      The event that occurs when a process requests stored data
      (i.e. a page) from memory that the operating system has moved to
      disk. See :ref:`faq-storage-page-faults`.

   partition
      A distributed system architecture that splits data into ranges.
      :term:`Sharding` uses partitioning. See
      :ref:`sharding-data-partitioning`.

   passive member
      A member of a :term:`replica set` that cannot become primary
      because its :data:`~local.system.replset.members[n].priority` is
      ``0``. See :doc:`/core/replica-set-priority-0-member`.

   pcap
      A packet-capture format used by :program:`mongosniff` to record
      packets captured from network interfaces and display them as
      human-readable MongoDB operations. See :ref:`mongosniff-options`.

   PID
      A process identifier. UNIX-like systems assign a unique-integer
      PID to each running process. You can use a PID to inspect a
      running process and send signals to it. See
      :ref:`proc-file-system`.

   pipe
      A communication channel in UNIX-like systems allowing independent
      processes to send and receive data. In the UNIX shell, piped
      operations allow users to direct the output of one command into
      the input of another.

   pipeline
      A series of operations in an :term:`aggregation` process.
      See :doc:`/core/aggregation`.

   Point
      A single coordinate pair as described in the GeoJSON Point
      specification: `<http://geojson.org/geojson-spec.html#point>`_. To
      use a Point in MongoDB, see
      :ref:`geospatial-indexes-store-geojson`.

   Polygon
      An array of :term:`LinearRing <LineString>` coordinate arrays, as
      described in the GeoJSON Polygon specification:
      `<http://geojson.org/geojson-spec.html#polygon>`_. For Polygons
      with multiple rings, the first must be the exterior ring and
      any others must be interior rings or holes.

      MongoDB does not permit the exterior ring to self-intersect.
      Interior rings must be fully contained within the outer loop and
      cannot intersect or overlap with each other. See
      :ref:`geospatial-indexes-store-geojson`.

   powerOf2Sizes
      A per-collection setting that changes and normalizes the way
      MongoDB allocates space for each :term:`document`, in an effort to
      maximize storage reuse and to reduce fragmentation. This is the
      default for :doc:`TTL Collections </tutorial/expire-data>`. See
      :doc:`/reference/command/collMod` and
      :collflag:`usePowerOf2Sizes`.

   pre-splitting
      An operation performed before inserting data that divides the
      range of possible shard key values into chunks to facilitate easy
      insertion and high write throughput. In some cases pre-splitting
      expedites the initial distribution of documents in :term:`sharded
      cluster` by manually dividing the collection rather than waiting
      for the MongoDB :term:`balancer` to do so. See
      :doc:`/tutorial/create-chunks-in-sharded-cluster`.

   primary
      In a :term:`replica set`, the primary member is the current
      :term:`master` instance, which receives all write operations.
      See :ref:`replica-set-primary-member`.

   primary key
      A record's unique immutable identifier. In an :term:`RDBMS`, the primary
      key is typically an integer stored in each row's ``id`` field.
      In MongoDB, the :term:`_id` field holds a document's primary
      key which is usually a BSON :term:`ObjectId`.

   primary shard
      The :term:`shard` that holds all the un-sharded collections. See
      :ref:`primary-shard`.

   priority
      A configurable value that helps determine which members in
      a :term:`replica set` are most likely to become :term:`primary`.
      See :data:`~local.system.replset.members[n].priority`.

   projection
      A document given to a :term:`query` that specifies which fields
      MongoDB returns in the result set. See :ref:`projection`. For a
      list of projection operators, see
      :doc:`/reference/operator/projection`.

   query
      A read request. MongoDB uses a :term:`JSON`-like query language
      that includes a variety of :term:`query operators <operator>` with
      names that begin with a ``$`` character. In the :program:`mongo`
      shell, you can issue queries using the
      :method:`~db.collection.find()` and
      :method:`~db.collection.findOne()` methods. See
      :ref:`read-operations-queries`.

   query optimizer
      A process that generates query plans. For each query, the
      optimizer generates a plan that matches the query to the index
      that will return results as efficiently as possible. The
      optimizer reuses the query plan each time the query runs. If a
      collection changes significantly, the optimizer creates a new
      query plan. See :ref:`read-operations-query-optimization`.

   query shape
      A combination of query predicate, sort, and projection
      specifications.

      For the query predicate, only the structure of the predicate,
      including the field names, are significant; the values in the
      query predicate are insignificant. As such, a query predicate ``{
      type: 'food' }`` is equivalent to the query predicate ``{ type:
      'utensil' }`` for a query shape.

   read preference
      A setting that determines how clients direct read operations. Read
      preference affects all replica sets, including shards. By default,
      MongoDB directs reads to :term:`primaries <primary>` for
      :term:`strict consistency`. However, you may also direct reads to
      secondaries for :term:`eventually consistent <eventual
      consistency>` reads. See :doc:`Read Preference
      </core/read-preference>`.

   read lock
      In the context of a reader-writer lock, a lock that while held
      allows concurrent readers but no writers. See
      :ref:`faq-concurrency-locking`.

   RDBMS
      Relational Database Management System. A database management
      system based on the relational model, typically using
      :term:`SQL` as the query language.

   record size
      The space allocated for a document including the padding. For more
      information on padding, see :ref:`write-operations-padding-factor`
      and :doc:`/reference/command/compact`.

   recovering
      A :term:`replica set` member status indicating that a member
      is not ready to begin normal activities of a secondary or primary.
      Recovering members are unavailable for reads.

   replica pairs
      The precursor to the MongoDB :term:`replica sets <replica set>`.

      .. deprecated:: 1.6

   replica set
      A cluster of MongoDB servers that implements master-slave
      replication and automated failover. MongoDB's recommended
      replication strategy. See :doc:`/replication`.

   replication
      A feature allowing multiple database servers to share the same
      data, thereby ensuring redundancy and facilitating load balancing.
      See :doc:`/replication`.

   replication lag
      The length of time between the last operation in the
      :term:`primary's <primary>` :term:`oplog` and the last operation
      applied to a particular :term:`secondary`. In general, you want to
      keep replication lag as small as possible. See :ref:`Replication
      Lag <replica-set-replication-lag>`.

   resident memory
      The subset of an application's memory currently stored in
      physical RAM. Resident memory is a subset of :term:`virtual memory`,
      which includes memory mapped to physical RAM and to disk.

   REST
     An API design pattern centered around the idea of resources and the
     :term:`CRUD` operations that apply to them. Typically REST is
     implemented over HTTP. MongoDB provides a simple HTTP REST
     interface that allows HTTP clients to run commands against the
     server. See :ref:`rest-interface` and :ref:`rest-api`.

   rollback
      A process that reverts writes operations to ensure the consistency
      of all replica set members. See :ref:`replica-set-rollback`.

   secondary
      A :term:`replica set` member that replicates the contents of the
      master database. Secondary members may handle read requests, but
      only the :term:`primary` members can handle write operations. See
      :ref:`replica-set-secondary-members`.

   secondary index
      A database :term:`index` that improves query performance by
      minimizing the amount of work that the query engine must perform
      to fulfill a query. See :doc:`/indexes`.

   set name
      The arbitrary name given to a replica set. All members of a
      replica set must have the same name specified with the
      :setting:`~replication.replSetName` setting or the :option:`--replSet <mongod --replSet>` option.

   shard
      A single :program:`mongod` instance or :term:`replica set` that
      stores some portion of a :term:`sharded cluster's <sharded
      cluster>` total data set. In production, all shards should be
      replica sets. See :doc:`/core/sharded-cluster-shards`.

   shard key
      The field MongoDB uses to distribute documents among members of a
      :term:`sharded cluster`. See :ref:`shard-key`.

   sharded cluster
      The set of nodes comprising a :term:`sharded <sharding>` MongoDB
      deployment. A sharded cluster consists of three config processes,
      one or more replica sets, and one or more :program:`mongos`
      routing processes. See :doc:`/core/sharded-cluster-components`.

   sharding
      A database architecture that partitions data by key ranges and
      distributes the data among two or more database instances.
      Sharding enables horizontal scaling. See :doc:`/sharding`.

   shell helper
      A method in the ``mongo`` shell that provides a more concise
      syntax for a :doc:`database command <command>`. Shell helpers
      improve the general interactive experience. See
      :doc:`/reference/method`.

   single-master replication
      A :term:`replication` topology where only a single database
      instance accepts writes. Single-master replication ensures
      consistency and is the replication topology employed by MongoDB.
      See :doc:`/core/replica-set-primary`.

   slave
      A read-only database that replicates operations from a
      :term:`master` database in conventional master/slave replication.
      In MongoDB, :term:`replica sets <replica set>` replace
      master/slave replication for most use cases. However, for
      information on master/slave replication, see
      :doc:`/core/master-slave`.

   split
      The division between :term:`chunks <chunk>` in a :term:`sharded
      cluster`. See :doc:`/core/sharding-chunk-splitting`.

   SQL
      Structured Query Language (SQL) is a common special-purpose
      programming language used for interaction with a relational
      database, including access control, insertions,
      updates, queries, and deletions. There are some similar
      elements in the basic SQL syntax supported by different database
      vendors, but most implementations have their own dialects, data
      types, and interpretations of proposed SQL standards. Complex
      SQL is generally not directly portable between major
      :term:`RDBMS` products. ``SQL`` is often used as
      metonym for relational databases.

   SSD
      Solid State Disk. A high-performance disk drive that uses solid
      state electronics for persistence, as opposed to the rotating platters
      and movable read/write heads used by traditional mechanical hard drives.

   standalone
      An instance of :program:`mongod` that is running as a single
      server and not as part of a :term:`replica set`. To convert a
      standalone into a replica set, see
      :doc:`/tutorial/convert-standalone-to-replica-set`.

   strict consistency
      A property of a distributed system requiring that all members
      always reflect the latest changes to the system. In a database
      system, this means that any system that can provide data must
      reflect the latest writes at all times. In MongoDB, reads from a
      primary have :term:`strict consistency`; reads from secondary
      members have :term:`eventual consistency`.

   stale
      Refers to the amount of time a :term:`secondary` member of a
      :term:`replica set` trails behind the current state of the
      :term:`primary's <primary>`\ :term:`oplog`. If a secondary
      becomes too stale, it can no longer use replication to catch up
      to the current state of the primary. See
      :doc:`/core/replica-set-oplog` and :doc:`/core/replica-set-sync`
      for more information.

   sync
      The :term:`replica set` operation where members replicate data
      from the :term:`primary`. Sync first occurs when MongoDB creates
      or restores a member, which is called :term:`initial sync`. Sync
      then occurs continually to keep the member updated with changes to
      the replica set's data. See :doc:`/core/replica-set-sync`.

   syslog
      On UNIX-like systems, a logging process that provides a uniform
      standard for servers and processes to submit logging information.
      MongoDB provides an option to send output to the host’s syslog
      system. See :setting:`syslog`.

   tag
     A label applied to a replica set member or shard and used by
     clients to issue data-center-aware operations. For more information
     on using tags with replica sets and with shards, see the following
     sections of this manual: :ref:`replica-set-read-preference-tag-sets`
     and :ref:`shards-tag-sets`.

   TSV
      A text-based data format consisting of tab-separated values.
      This format is commonly used to exchange data between relational
      databases, since the format is well-suited to tabular data. You can
      import TSV files using :program:`mongoimport`.

   TTL
      Stands for "time to live" and represents an expiration time or
      period for a given piece of information to remain in a cache or
      other temporary storage before the system deletes it or ages it
      out. MongoDB has a TTL collection feature. See
      :doc:`/tutorial/expire-data`.

   unique index
      An index that enforces uniqueness for a particular field across
      a single collection. See :ref:`index-type-unique`.

   unordered query plan
      A query plan that returns results in an order inconsistent with the
      :method:`~cursor.sort()` order.
      See :ref:`read-operations-query-optimization`.

   upsert
      An operation that will either update the first document matched by
      a query or insert a new document if none matches. The new document
      will have the fields implied by the operation. You perform upserts
      with the :method:`~db.collection.update()` operation. See
      :ref:`upsert-parameter`.

   virtual memory
      An application's working memory, typically residing on both
      disk an in physical RAM.

   WGS84
      The default :term:`datum` MongoDB uses to calculate geometry over
      an Earth-like sphere. MongoDB uses the WGS84 datum for
      :term:`geospatial` queries on :term:`GeoJSON` objects. See
      the "EPSG:4326: WGS 84" specification:
      `<http://spatialreference.org/ref/epsg/4326/>`_.

   working set
      The data that MongoDB uses most often. This data is preferably
      held in RAM, solid-state drive (SSD), or other fast media. See
      :ref:`faq-working-set`.

   write concern
      Specifies whether a write operation has succeeded. Write concern
      allows your application to detect insertion errors or unavailable
      :program:`mongod` instances. For :term:`replica sets <replica
      set>`, you can configure write concern to confirm replication to a
      specified number of members. See :doc:`/core/write-concern`.

   write lock
      A lock on the database for a given writer. When a process writes
      to the database, it takes an exclusive write lock to prevent other
      processes from writing or reading. For more information on locks,
      see :doc:`/faq/concurrency`.

   writeBacks
      The process within the sharding system that ensures that writes
      issued to a :term:`shard` that *is not* responsible for the
      relevant chunk get applied to the proper shard. For related
      information, see :ref:`faq-writebacklisten` and
      :ref:`server-status-writebacksqueued`.
.. index:: GridFS

================
GridFS Reference
================

.. default-domain:: mongodb

.. index:: GridFS; collections
.. _gridfs-collections:

:term:`GridFS` stores files in two collections:

- ``chunks`` stores the binary chunks. For details, see
  :ref:`gridfs-chunks-collection`.

- ``files`` stores the file's metadata. For details, see
  :ref:`gridfs-files-collection`.

GridFS places the collections in a common bucket by prefixing each
with the bucket name. By default, GridFS uses two collections with
names prefixed by ``fs`` bucket:

- ``fs.files``
- ``fs.chunks``

You can choose a different bucket name than ``fs``, and create
multiple buckets in a single database.

.. seealso:: :doc:`/core/gridfs` for more information about GridFS.

.. index:: GridFS; chunks collection
.. _gridfs-chunks-collection:

The ``chunks`` Collection
-------------------------

Each document in the ``chunks`` collection represents a distinct chunk
of a file as represented in the :term:`GridFS` store. The following is a
prototype document from the ``chunks`` collection.:

.. code-block:: javascript

   {
     "_id" : <ObjectId>,
     "files_id" : <ObjectId>,
     "n" : <num>,
     "data" : <binary>
   }

A document from the ``chunks`` collection contains the following fields:

.. data:: chunks._id

   The unique :term:`ObjectId` of the chunk.

.. data:: chunks.files_id

   The ``_id`` of the "parent" document, as specified in the ``files``
   collection.

.. data:: chunks.n

   The sequence number of the chunk. GridFS numbers all chunks,
   starting with 0.

.. data:: chunks.data

   The chunk's payload as a :term:`BSON` binary type.

The ``chunks`` collection uses a :term:`compound index` on
``files_id`` and ``n``, as described in :ref:`gridfs-index`.

.. index:: GridFS; files collection
.. _gridfs-files-collection:

The ``files`` Collection
------------------------

Each document in the ``files`` collection represents a file in the
:term:`GridFS` store. Consider the following prototype of a document in
the ``files`` collection:

.. code-block:: javascript

   {
     "_id" : <ObjectId>,
     "length" : <num>,
     "chunkSize" : <num>
     "uploadDate" : <timestamp>
     "md5" : <hash>

     "filename" : <string>,
     "contentType" : <string>,
     "aliases" : <string array>,
     "metadata" : <dataObject>,
   }

Documents in the ``files`` collection contain some or all of the
following fields. Applications may create additional arbitrary fields:

.. data:: files._id

   The unique ID for this document. The ``_id`` is of the data type you
   chose for the original document. The default type for MongoDB
   documents is :term:`BSON` :term:`ObjectId`.

.. data:: files.length

   The size of the document in bytes.

.. data:: files.chunkSize

   The size of each chunk. GridFS divides the document into chunks of
   the size specified here. The default size is 255 kilobytes.

   .. versionchanged:: 2.4.10
      The default chunk size changed from 256k to 255k.

.. data:: files.uploadDate

   The date the document was first stored by GridFS. This value has the
   ``Date`` type.

.. data:: files.md5

   An MD5 hash returned from the filemd5 API. This value has the ``String``
   type.

.. data:: files.filename

   Optional. A human-readable name for the document.

.. data:: files.contentType

   Optional. A valid MIME type for the document.

.. data:: files.aliases

   Optional. An array of alias strings.

.. data:: files.metadata

   Optional. Any additional information you want to store.
==================
Indexing Reference
==================

.. default-domain:: mongodb

Indexing Methods in the ``mongo`` Shell
---------------------------------------

.. include:: /includes/toc/table-spec-indexes-methods.rst

Indexing Database Commands
--------------------------

.. include:: /includes/toc/table-spec-indexes-commands.rst

Geospatial Query Selectors
--------------------------

.. include:: /includes/toc/table-spec-indexes-query-selectors.rst

Indexing Query Modifiers
------------------------

.. include:: /includes/toc/table-spec-indexes-query-modifiers.rst

Other Index References
----------------------

.. include:: /includes/toc/dfn-list-indexes-reference.rst

.. include:: /includes/toc/indexes-reference.rst
=============================
MongoDB Limits and Thresholds
=============================

.. default-domain:: mongodb

This document provides a collection of hard and soft limitations of
the MongoDB system.

BSON Documents
--------------

.. _limit-bson-document-size:
.. limit:: BSON Document Size

   .. include:: /includes/fact-document-max-size.rst

.. _limit-nested-depth:
.. limit:: Nested Depth for BSON Documents

   .. versionchanged:: 2.2

   MongoDB supports no more than 100 levels of nesting for :term:`BSON
   documents <document>`.

Namespaces
----------

.. _limit-namespace-length:
.. limit:: Namespace Length

   Each namespace, including database and collection name, must be
   shorter than 123 bytes.

   .. fix when we know what the actual limit is.

.. _limit-number-of-namespaces:
.. limit:: Number of Namespaces

   The limitation on the number of namespaces is the size of the
   namespace file divided by 628.

   A 16 megabyte namespace file can support approximately 24,000
   namespaces. Each collection and index is a namespace.

.. _limit-size-of-namespace-file:
.. limit:: Size of Namespace File

   Namespace files can be no larger than 2047 megabytes.

   By default namespace files are 16 megabytes. You can configure the
   size using the :setting:`nssize` option.

.. _index-limitations:

Indexes
-------

.. _limit-index-size:
.. limit:: Index Key

   The *total size* of an index key entry, which can include structural
   overhead depending on the BSON type, must be *less than* 1024 bytes.

   .. COMMENT refer to src/mongo/db/structure/btree/key.cpp (KeyV1Owned::KeyV1Owned)

   .. |limit| replace:: :limit:`index key limit <Index Key>`

   .. versionchanged:: 2.6

      .. include:: /includes/list-index-field-limit-behaviors.rst

.. _limit-number-of-indexes-per-collection:
.. limit:: Number of Indexes per Collection

   A single collection can have *no more* than 64 indexes.

.. _limit-index-name-length:
.. limit:: Index Name Length

   The names of indexes, including their namespace (i.e database and
   collection name), cannot be longer than 125 characters. The default
   index name is the concatenation of the field names and index
   directions.

   You can explicitly specify an index name to the
   :method:`~db.collection.ensureIndex()` helper if the default index
   name is too long.

   .. The actual maximum length of an index is 128 characters,
      however, there are three characters of required punctuation
      hence the 125 figure.

.. limit:: Number of Indexed Fields in a Compound Index

   There can be no more than 31 fields in a compound index.

.. limit:: Queries cannot use both text and Geospatial Indexes

   .. |operation| replace:: :dbcommand:`text` command

   .. include:: /includes/fact-special-indexes-and-text.rst

   .. TODO remove in the 2.6 version of the manual

.. seealso:: The unique indexes limit in :ref:`limits-sharding-operations`.

Data
----

.. limit:: Maximum Number of Documents in a Capped Collection

   .. versionchanged:: 2.4

   If you specify a maximum number of documents for a capped
   collection using the ``max`` parameter to
   :dbcommand:`create`, the limit must be less than 2\ :sup:`32`
   documents. If you do not specify a maximum number of documents when
   creating a capped collection, there is no limit on the number of
   documents.

.. _limit-data-size:
.. limit:: Data Size

   A single :program:`mongod` instance cannot manage a data set that
   exceeds maximum virtual memory address space provided by the
   underlying operating system.

   .. list-table:: Virtual Memory Limitations
      :widths: 15 10 10
      :header-rows: 1

      * - Operating System
        - Journaled
        - Not Journaled
      * - Linux
        - 64 terabytes
        - 128 terabytes
      * - Windows
        - 4 terabytes
        - 8 terabytes

.. limit:: Number of Collections in a Database

   The maximum number of collections in a database is a function of
   the size of the namespace file and the number of indexes of
   collections in the database.

   See :limit:`Number of Namespaces` for more information.

Replica Sets
------------

.. limit:: Number of Members of a Replica Set

   Replica sets can have no more than 12 members.

.. limit:: Number of Voting Members of a Replica Set

   Only 7 members of a replica set can have votes at any given
   time. See can vote :ref:`replica-set-non-voting-members` for more information

.. _limits-sharding:

Sharded Clusters
----------------

Sharded clusters have the restrictions and thresholds described here.

.. _limits-sharding-operations:

Sharding Operational Restrictions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. limit:: Operations Unavailable in Sharded Environments

   The :dbcommand:`group` does not work with sharding. Use
   :dbcommand:`mapReduce` or :dbcommand:`aggregate` instead.

   :method:`db.eval()` is incompatible with sharded collections. You may
   use :method:`db.eval()` with un-sharded collections in a shard
   cluster.

   :query:`$where` does not permit references to the ``db`` object
   from the :query:`$where` function. This is uncommon in
   un-sharded collections.

   The :update:`$isolated` update modifier does not work in sharded
   environments.

   :operator:`$snapshot` queries do not work in sharded environments.

   The :dbcommand:`geoSearch` command is not supported in sharded
   environments.

.. limit:: Covered Queries in Sharded Clusters

   MongoDB does not support :ref:`covered queries <covered-queries>`
   from sharded collections.

.. limit:: Sharding Existing Collection Data Size

   For existing collections that hold documents, MongoDB supports
   enabling sharding on *any* collections that contains less than 256
   gigabytes of data. MongoDB *may* be able to shard collections with as
   many as 400 gigabytes depending on the distribution of document
   sizes. The precise size of the limitation is a function of the
   chunk size and the data size.

   .. important:: Sharded collections may have *any* size, after
      successfully enabling sharding.

.. limit:: Single Document Modification Operations in Sharded Collections

   .. |single-modification-operation-names| replace:: :method:`~db.collection.update()` and :method:`~db.collection.remove()`
   .. |single-modification-operation-option| replace:: ``justOne`` or ``multi: false``

   .. include:: /includes/fact-single-modification-in-sharded-collections.rst

.. _limit-sharding-unique-indexes:

.. limit:: Unique Indexes in Sharded Collections

   MongoDB does not support unique indexes across shards, except when
   the unique index contains the full shard key as a prefix of the
   index. In these situations MongoDB will enforce uniqueness across
   the full key, not a single field.

   .. see:: :doc:`/tutorial/enforce-unique-keys-for-sharded-collections`
      for an alternate approach.

.. _limits-shard-keys:

Shard Key Limitations
~~~~~~~~~~~~~~~~~~~~~

.. limit:: Shard Key Size

   A shard key cannot exceed 512 bytes.

.. limit:: Shard Key is Immutable

   You cannot change a shard key after sharding the collection. If
   you must change a shard key:

   - Dump all data from MongoDB into an external format.

   - Drop the original sharded collection.

   - Configure sharding using the new shard key.

   - :doc:`Pre-split </tutorial/create-chunks-in-sharded-cluster>` the shard
     key range to ensure initial even distribution.

   - Restore the dumped data into MongoDB.

.. limit:: Shard Key Value in a Document is Immutable

   After you insert a document into a sharded collection, you cannot
   change the document's value for the field or fields that comprise
   the shard key. The :method:`~db.collection.update()` operation will
   not modify the value of a shard key in an existing document.

.. limit:: Monotonically Increasing Shard Keys Can Limit Insert Throughput

   For clusters with high insert volumes, a shard keys with
   monotonically increasing and decreasing keys can affect insert
   throughput. If your shard key is the ``_id`` field, be aware that
   the default values of the ``_id`` fields are :term:`ObjectIds
   <ObjectId>` which have generally increasing values.

   When inserting documents with monotonically increasing shard keys, all
   inserts belong to the same :term:`chunk` on a single
   :term:`shard`. The system will eventually divide the
   chunk range that receives all write operations
   and migrate its contents to distribute data more evenly. However, at any
   moment the cluster can direct insert operations only to a single
   shard, which creates an insert throughput bottleneck.

   If the operations on the cluster are predominately read operations
   and updates, this limitation may not affect the cluster.

   To avoid this constraint, use a :ref:`hashed shard key
   <sharding-hashed-sharding>`  or select a field that does not
   increase or decrease monotonically.

   .. versionchanged:: 2.4
      :ref:`Hashed shard keys <sharding-hashed-sharding>` and
      :ref:`hashed indexes <index-type-hashed>` store hashes of keys
      with ascending values.

Operations
----------

.. _limit-sort:
.. limit:: Sorted Documents

   MongoDB will only return sorted results on fields without an index
   *if* the sort operation uses less than 32 megabytes of memory.

.. _limit-agg-sort:

.. limit:: Aggregation Pipeline Operation

   .. include:: /includes/fact-agg-memory-limit.rst

.. limit:: 2d Geospatial queries cannot use the $or operator

   .. see:: :query:`$or` and :doc:`/core/geospatial-indexes`.

.. limit:: Spherical Polygons must fit within a hemisphere.

   .. |geo-operator-method| replace:: :query:`$geoIntersects` or :query:`$geoWithin`

   .. include:: /includes/fact-geometry-hemisphere-limitation.rst

.. limit:: Bulk Operation Size

   A bulk operation can have at most 1000 operations.

Naming Restrictions
-------------------

.. limit:: Database Name Case Sensitivity

   Database names are case sensitive even if the underlying file
   system is case insensitive. MongoDB does not permit database names
   that differ only by the case of the characters.

.. limit:: Restrictions on Database Names for Windows

   .. versionchanged:: 2.2
      :ref:`rn-2.2-database-name-restriction-windows`.

   For MongoDB deployments running on Windows, MongoDB will not permit
   database names that include any of the following characters:

   .. code-block:: none

         /\. "*<>:|?

   Also, database names cannot contain the null character.

.. limit:: Restrictions on Database Names for Unix and Linux Systems

   For MongoDB deployments running on Unix and Linux systems, MongoDB
   will not permit database names that include any of the following
   characters:

   .. code-block:: none

      /\. "

   Also, database names cannot contain the null character.

.. limit:: Length of Database Names

   Database names cannot be empty and must have fewer than 64
   characters.

.. limit:: Restriction on Collection Names

   .. versionadded:: 2.2

   Collection names should begin with an underscore or a letter
   character, and *cannot*:

   - contain the ``$``.

   - be an empty string (e.g. ``""``).

   - contain the null character.

   - begin with the ``system.`` prefix. (Reserved for internal use.)

   In the :program:`mongo` shell, use :method:`db.getCollection()` to
   specify collection names that might interact with the shell or are
   not valid JavaScript.

.. _limit-restrictions-on-field-names:

.. limit:: Restrictions on Field Names

   Field names cannot contain dots (i.e. ``.``), dollar signs
   (i.e. ``$``), or null characters. See
   :ref:`faq-dollar-sign-escaping` for an alternate approach.
.. _replica-set-local-database:

======================
The ``local`` Database
======================

.. default-domain:: mongodb

.. index:: replica set; local database
.. index:: local database
.. index:: database; local
.. index:: namespace; local

Overview
--------

Every :program:`mongod` instance has its own ``local`` database, which
stores data used in the replication process, and other
instance-specific data. The ``local`` database is invisible to
replication: collections in the ``local`` database are not replicated.

In replication, the ``local`` database store stores internal replication
data for each member of a :term:`replica set`. The ``local`` stores the
following collections:

.. versionchanged:: 2.4
   When running with authentication (i.e. :setting:`~security.authentication`),
   authenticating to the ``local`` database is **not** equivalent to
   authenticating to the ``admin`` database. In previous versions,
   authenticating to the ``local`` database provided access to all databases.

Collection on all ``mongod`` Instances
--------------------------------------

.. data:: local.startup_log

   On startup, each :program:`mongod` instance inserts a document into
   :data:`~local.startup_log` with diagnostic information about the
   :program:`mongod` instance itself and host
   information. :data:`~local.startup_log` is a capped
   collection. This information is primarily useful for diagnostic
   purposes.

   .. example::

      Consider the following prototype of a document from the
      :data:`~local.startup_log` collection:

      .. code-block:: javascript

         {
           "_id" : "<string>",
           "hostname" : "<string>",
           "startTime" : ISODate("<date>"),
           "startTimeLocal" : "<string>",
           "cmdLine" : {
                 "dbpath" : "<path>",
                 "<option>" : <value>
           },
           "pid" : <number>,
           "buildinfo" : {
                 "version" : "<string>",
                 "gitVersion" : "<string>",
                 "sysInfo" : "<string>",
                 "loaderFlags" : "<string>",
                 "compilerFlags" : "<string>",
                 "allocator" : "<string>",
                 "versionArray" : [ <num>, <num>, <...> ],
                 "javascriptEngine" : "<string>",
                 "bits" : <number>,
                 "debug" : <boolean>,
                 "maxBsonObjectSize" : <number>
           }
         }

    Documents in the :data:`~local.startup_log` collection contain the
    following fields:

    .. data:: local.startup_log._id

       Includes the system hostname and a millisecond epoch value.

    .. data:: local.startup_log.hostname

       The system's hostname.

    .. data:: local.startup_log.startTime

       A UTC :term:`ISODate` value that reflects when the server started.

    .. data:: local.startup_log.startTimeLocal

       A string that reports the :data:`~local.startup_log.startTime`
       in the system's local time zone.

    .. data:: local.startup_log.cmdLine

       A sub-document that reports the :program:`mongod` runtime
       options and their values.

    .. data:: local.startup_log.pid

       The process identifier for this process.

    .. data:: local.startup_log.buildinfo

       A sub-document that reports information about the build
       environment and settings used to compile this
       :program:`mongod`. This is the same output as
       :dbcommand:`buildInfo`. See :data:`buildInfo`.

Collections on Replica Set Members
----------------------------------

.. data:: local.system.replset

   :data:`local.system.replset` holds the replica set's configuration
   object as its single document. To view the object's configuration
   information, issue :method:`rs.conf()` from the :program:`mongo`
   shell. You can also query this collection directly.

.. data:: local.oplog.rs

   :data:`local.oplog.rs` is the capped collection that holds the
   :term:`oplog`. You set its size at creation using the
   :setting:`~replication.oplogSizeMB` setting. To resize the oplog after replica set
   initiation, use the :doc:`/tutorial/change-oplog-size`
   procedure. For additional information, see the
   :ref:`replica-set-oplog-sizing` section.

.. data:: local.replset.minvalid

   This contains an object used internally by replica sets to track replication
   status.

.. data:: local.slaves

   This contains information about each member of the set and the
   latest point in time that this member has synced to. If this
   collection becomes out of date, you can refresh it by dropping the
   collection and allowing MongoDB to automatically refresh it during
   normal replication:

   .. code-block:: javascript

      db.getSiblingDB("local").slaves.drop()

Collections used in Master/Slave Replication
--------------------------------------------

In :term:`master`\/:term:`slave` replication, the ``local`` database contains
the following collections:

- On the master:

  .. data:: local.oplog.$main

     This is the oplog for the master-slave configuration.

  .. data:: local.slaves

     This contains information about each slave.

- On each slave:

  .. data:: local.sources

     This contains information about the slave's master server.
==============
Bulk.execute()
==============

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.execute()

   Executes the list of operations built by the :method:`Bulk()`
   operations builder.

   :method:`Bulk.execute()` accepts the following parameter:

   .. include:: /reference/method/Bulk.execute-param.rst

   :returns: :ref:`bulk-write-result`

   After execution, you must reinitialize the :method:`Bulk()`
   operations builder. See :method:`db.collection.initializeUnorderedBulkOp()`
   and :method:`db.collection.initializeOrderedBulkOp()`.

Example
-------

The following initializes a :method:`Bulk()` operations builder on the
``items`` collection, adds a series of write operations, and executes
the operations:

.. code-block:: javascript

   var bulk = db.items.initializeOrderedBulkOp();
   bulk.insert( { items: "abc123", status: "A", defaultQty: 500, points: 5 } );
   bulk.insert( { items: "ijk123", status: "A", defaultQty: 100, points: 10 } );
   bulk.find( { status: "D" } ).removeOne();
   bulk.find( { status: "D" } ).update( { $set: { points: 0 } } );
   bulk.execute();

.. see:: :method:`Bulk()` for a listing of methods available for bulk
   operations.

.. _bulk-write-result:

BulkWriteResult
---------------

The ``BulkWriteResult`` contains a document with the following fields:

.. data:: BulkWriteResult.nInserted

   The number of documents inserted using the :method:`Bulk.insert()`
   method.

.. data:: BulkWriteResult.nUpserted

   The number of documents inserted through update or replacement
   operations with the :method:`Bulk.find.upsert()` option.

.. data:: BulkWriteResult.nMatched

   The number of documents selected for update or replacement. If the
   update/replacement operation results in no change to the document,
   e.g. :update:`$set` expression updates the value to the current
   value, :data:`~BulkWriteResult.nMatched` can be greater than
   :data:`~BulkWriteResult.nModified`.

.. data:: BulkWriteResult.nModified

   The number of documents updated or replaced. If the
   update/replacement operation results in no change to the document,
   such as setting the value of the field to its current value,
   :data:`~BulkWriteResult.nModified` can be less than
   :data:`~BulkWriteResult.nMatched`.

.. data:: BulkWriteResult.nRemoved

   The number of documents removed.

.. data:: BulkWriteResult.upserted

   An array of documents that contains information for each upserted
   documents.

   Each document contains the following information:

   .. data:: BulkWriteResult.upserted.index

      An integer that identifies the operation in the
      bulk operations list, which uses a zero-based index.

   .. data:: BulkWriteResult.upserted._id

      The ``_id`` value of the upserted document.

.. data:: BulkWriteResult.writeErrors

   An array of documents that contains information regarding any error,
   unrelated to write concerns, encountered during the update
   operation. The :data:`~BulkWriteResult.writeErrors` array contains
   an error document for each write operation that errors.

   Each error document contains the following fields:

   .. data:: BulkWriteResult.writeErrors.index

      An integer that identifies the write operation in the bulk
      operations list, which uses a zero-based index.

   .. data:: BulkWriteResult.writeErrors.code

      An integer value identifying the error.

   .. data:: BulkWriteResult.writeErrors.errmsg

      A description of the error.

   .. data:: BulkWriteResult.writeErrors.op

      A document identifying the operation that failed. For instance,
      an update/replace operation error will return a document
      specifying the query, the update, the ``multi`` and the
      ``upsert`` options; an insert operation will return the document
      the operation tried to insert.

.. data:: BulkWriteResult.writeConcernError

   Document that describe error related to write concern and contains
   the field:

   .. data:: BulkWriteResult.writeConcernError.code

      An integer value identifying the cause of the write concern error.

   .. data:: BulkWriteResult.writeConcernError.errInfo

      A document identifying the write concern setting related to the
      error.

   .. data:: BulkWriteResult.writeConcernError.errmsg

      A description of the cause of the write concern error.
==================
Bulk.find.remove()
==================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.remove()

   .. versionadded:: 2.6

   Adds a remove operation to a bulk operations list. Use the
   :method:`Bulk.find()` method to specify the condition that
   determines which documents to remove. The
   :method:`Bulk.find.remove()` method removes all matching documents
   in the collection. To limit the remove to a single document, see
   :method:`Bulk.find.removeOne()`.

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection and adds a remove operation to the list of
operations. The remove operation removes all documents in the
collection where the ``status`` equals ``"D"``:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "D" } ).remove();
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.find.removeOne()`

   - :method:`Bulk.execute()`
=====================
Bulk.find.removeOne()
=====================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.removeOne()

   .. versionadded:: 2.6

   Adds a single document remove operation to a bulk operations list.
   Use the :method:`Bulk.find()` method to specify the condition that
   determines which document to remove. The
   :method:`Bulk.find.removeOne()` limits the removal to one document.
   To remove multiple documents, see :method:`Bulk.find.remove()`.

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection and adds two
:method:`Bulk.find.removeOne()` operations to the list of operations.

Each remove operation removes just one document: one document with the
``status`` equal to ``"D"`` and another document with the ``status``
equal to ``"P"``.

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "D" } ).removeOne();
   bulk.find( { status: "P" } ).removeOne();
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.find.remove()`

   - :method:`Bulk.execute()`

   - :ref:`All Bulk Methods <bulk-methods>`
======================
Bulk.find.replaceOne()
======================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.replaceOne(<document>)

   .. versionadded:: 2.6

   Adds a single document replacement operation to a bulk operations
   list. Use the :method:`Bulk.find()` method to specify the condition
   that determines which document to replace. The
   :method:`Bulk.find.replaceOne()` method limits the replacement to a
   single document.

   :method:`Bulk.find.replaceOne()` accepts the following parameter:

   .. include:: /reference/method/Bulk.find.replaceOne-param.rst

   To specify an :term:`upsert` for this operation, see
   :method:`Bulk.find.upsert()`.

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection, and adds various
:method:`~Bulk.find.replaceOne` operations to the list of operations.

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { item: "abc123" } ).replaceOne( { item: "abc123", status: "P", points: 100 } );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.execute()`

   - :ref:`All Bulk Methods <bulk-methods>`
===========
Bulk.find()
===========

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find(<query>)

   .. versionadded:: 2.6

   Specifies a query condition for an update or a remove operation.

   :method:`Bulk.find()` accepts the following parameter:

   .. include:: /reference/method/Bulk.find-param.rst

   Use :method:`Bulk.find()` with the following write operations:

   - :method:`Bulk.find.removeOne()`

   - :method:`Bulk.find.remove()`

   - :method:`Bulk.find.replaceOne()`

   - :method:`Bulk.find.updateOne()`

   - :method:`Bulk.find.update()`

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection and adds a remove operation and an update
operation to the list of operations. The remove operation and the
update operation use the :method:`Bulk.find()` method to specify a
condition for their respective actions:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "D" } ).remove();
   bulk.find( { status: "P" } ).update( { $set: { points: 0 } } )
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.execute()`
==================
Bulk.find.update()
==================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.update(<update>)

   .. versionadded:: 2.6

   Adds a ``multi`` update operation to a bulk operations list. The
   method updates specific fields in existing documents.

   Use the :method:`Bulk.find()` method to specify the condition that
   determines which documents to update. The
   :method:`Bulk.find.update()` method updates all matching documents.
   To specify a single document update, see
   :method:`Bulk.find.updateOne()`.

   :method:`Bulk.find.update()` accepts the following parameter:

   .. include:: /reference/method/Bulk.find.update-param.rst

   To specify an :term:`upsert` for this operation, see
   :method:`Bulk.find.upsert()`. With :method:`Bulk.find.upsert()`, if
   no documents match the :method:`Bulk.find()` query condition, the
   update operation inserts only a single document.

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection, and adds various ``multi`` update
operations to the list of operations.

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "D" } ).update( { $set: { status: "I", points: "0" } } );
   bulk.find( { item: null } ).update( { $set: { item: "TBD" } } );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.find.updateOne()`

   - :method:`Bulk.execute()`

   - :ref:`All Bulk Methods <bulk-methods>`
=====================
Bulk.find.updateOne()
=====================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.updateOne(<update>)

   .. versionadded:: 2.6

   Adds a single document update operation to a bulk operations list.
   The operation can either replace an existing document or update
   specific fields in an existing document.

   Use the :method:`Bulk.find()` method to specify the condition that
   determines which document to update. The
   :method:`Bulk.find.updateOne()` method limits the update or
   replacement to a single document. To update multiple documents, see
   :method:`Bulk.find.update()`.

   :method:`Bulk.find.updateOne()` accepts the following parameter:

   .. include:: /reference/method/Bulk.find.updateOne-param.rst

   To specify an :term:`upsert` for this operation, see
   :method:`Bulk.find.upsert()`.

Behavior
--------

Update Specific Fields
~~~~~~~~~~~~~~~~~~~~~~

If the ``<update>`` document contains only :ref:`update operator
<update-operators>` expressions, as in:

.. code-block:: javascript

   {
     $set: { status: "D" },
     points: { $inc: 2 }
   }

Then, :method:`Bulk.find.updateOne()` updates only the corresponding
fields, ``status`` and ``points``, in the document.

Replace a Document
~~~~~~~~~~~~~~~~~~

If the ``<update>`` document contains only ``field:value``
expressions, as in:

.. code-block:: javascript

   {
     item: "TBD",
     points: 0,
     inStock: true,
     status: "I"
   }

Then, :method:`Bulk.find.updateOne()` *replaces* the matching document
with the ``<update>`` document with the exception of the ``_id`` field.
The :method:`Bulk.find.updateOne()` method *does not* replace the
``_id`` value.

Example
-------

The following example initializes a :method:`Bulk()` operations builder
for the ``items`` collection, and adds various
:method:`~Bulk.find.updateOne` operations to the list of operations.

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "D" } ).updateOne( { $set: { status: "I", points: "0" } } );
   bulk.find( { item: null } ).updateOne(
      {
         item: "TBD",
         points: 0,
         inStock: true,
         status: "I"
      }
   );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.find.update()`

   - :method:`Bulk.execute()`

   - :ref:`All Bulk Methods <bulk-methods>`
==================
Bulk.find.upsert()
==================

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.find.upsert()

   .. versionadded:: 2.6

   Sets the optional :term:`upsert` flag for an update or a replacement
   operation and has the following syntax:

   .. code-block:: javascript

      Bulk.find(<query>).upsert().update(<update>);
      Bulk.find(<query>).upsert().updateOne(<update>);
      Bulk.find(<query>).upsert().replaceOne(<replacement>);

   With the :term:`upsert` flag, if no matching documents exist for the
   :method:`Bulk.find()` condition, then the update or the replacement
   operation performs an insert. If a matching document does exist,
   then the update or replacement operation performs the specified
   update or replacement.

   Use :method:`Bulk.find.upsert()` with the following write operations:

   - :method:`Bulk.find.replaceOne()`

   - :method:`Bulk.find.updateOne()`

   - :method:`Bulk.find.update()`

Behavior
--------

The following describe the insert behavior of various write operations
when used in conjunction with :method:`Bulk.find.upsert()`.

Insert for ``Bulk.find.replaceOne()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :method:`Bulk.find.replaceOne()` method accepts, as its parameter,
a replacement document that only contains field and value pairs:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { item: "abc123" } ).upsert().replaceOne(
      {
        item: "abc123",
        status: "P",
        points: 100,
      }
   );
   bulk.execute();

If the replacement operation with the :method:`Bulk.find.upsert()`
option performs an insert, the inserted document is the replacement
document. If the replacement document does not specify an ``_id``
field, MongoDB adds the ``_id`` field:

.. code-block:: javascript

   {
     "_id" : ObjectId("52ded3b398ca567f5c97ac9e"),
     "item" : "abc123",
     "status" : "P",
     "points" : 100
   }

Insert for ``Bulk.find.updateOne()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :method:`Bulk.find.updateOne()` method accepts, as its parameter,
an ``<update>`` document that contains only field and value pairs or
only :ref:`update operator <update-operators>` expressions.

Field and Value Pairs
`````````````````````

If the ``<update>`` document contains only field and value pairs:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "P" } ).upsert().updateOne(
      {
        item: "TBD",
        points: 0,
        inStock: true,
        status: "I"
      }
   );
   bulk.execute();

Then, if the update operation with the :method:`Bulk.find.upsert()`
option performs an insert, the inserted document is the ``<update>``
document. If the update document does not specify an ``_id`` field,
MongoDB adds the ``_id`` field:

.. code-block:: javascript

   {
     "_id" : ObjectId("52ded5a898ca567f5c97ac9f"),
     "item" : "TBD",
     "points" : 0,
     "inStock" : true,
     "status" : "I"
   }

Update Operator Expressions
```````````````````````````

If the ``<update>`` document contains contains only :ref:`update
operator <update-operators>` expressions:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "P", item: null } ).upsert().updateOne(
      {
        $setOnInsert: { defaultQty: 0, inStock: true },
        $currentDate: { lastModified: true },
        $set: { points: "0" }
      }
   );
   bulk.execute();

Then, if the update operation with the :method:`Bulk.find.upsert()`
option performs an insert, the update operation inserts a document with
field and values from the ``<query>`` document of the
:method:`Bulk.find()` method and then applies the specified update from
the ``<update>`` document:

.. code-block:: javascript

   {
      "_id" : ObjectId("52ded68c98ca567f5c97aca0"),
      "item" : null,
      "status" : "P",
      "defaultQty" : 0,
      "inStock" : true,
      "lastModified" : ISODate("2014-01-21T20:20:28.786Z"),
      "points" : "0"
   }

If neither the ``<query>`` document nor the ``<update>`` document
specifies an ``_id`` field, MongoDB adds the ``_id`` field.

Insert for ``Bulk.find.update()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When using :method:`~Bulk.find.upsert()` with the multiple document
update method :method:`Bulk.find.update()`, if no documents match the
query condition, the update operation inserts a *single* document.

The :method:`Bulk.find.update()` method accepts, as its parameter, an
``<update>`` document that contains *only* :ref:`update operator
<update-operators>` expressions:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.find( { status: "P" } ).upsert().update(
      {
        $setOnInsert: { defaultQty: 0, inStock: true },
        $currentDate: { lastModified: true },
        $set: { status: "I", points: "0" }
      }
   );
   bulk.execute();

Then, if the update operation with the :method:`Bulk.find.upsert()`
option performs an insert, the update operation inserts a single
document with the fields and values from the ``<query>`` document of
the :method:`Bulk.find()` method and then applies the specified update
from the ``<update>`` document:

.. code-block:: javascript

   {
      "_id": ObjectId("52ded81a98ca567f5c97aca1"),
      "status": "I",
      "defaultQty": 0,
      "inStock": true,
      "lastModified": ISODate("2014-01-21T20:27:06.691Z"),
      "points": "0"
   }

If neither the ``<query>`` document nor the ``<update>`` document
specifies an ``_id`` field, MongoDB adds the ``_id`` field.

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.execute()`

   - :ref:`All Bulk Methods <bulk-methods>`
=============
Bulk.insert()
=============

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk.insert(<document>)

   .. versionadded:: 2.6

   Adds an insert operation to a bulk operations list.

   :method:`Bulk.insert()` accepts the following parameter:

   .. include:: /reference/method/Bulk.insert-param.rst

Example
-------

The following initializes a :method:`Bulk()` operations builder for the
``items`` collection and adds a series of insert operations to add
multiple documents:

.. code-block:: javascript

   var bulk = db.items.initializeUnorderedBulkOp();
   bulk.insert( { item: "abc123", defaultQty: 100, status: "A", points: 100 } );
   bulk.insert( { item: "ijk123", defaultQty: 200, status: "A", points: 200 } );
   bulk.insert( { item: "mop123", defaultQty: 0, status: "P", points: 0 } );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk.execute()`
======
Bulk()
======

.. default-domain:: mongodb

Description
-----------

.. method:: Bulk()

   .. versionadded:: 2.6

   Bulk operations builder used to construct a list of write operations
   to perform in bulk for a single collection. To instantiate the
   builder, use either the
   :method:`db.collection.initializeOrderedBulkOp()` or the
   :method:`db.collection.initializeUnorderedBulkOp()` method.

Ordered and Unordered Bulk Operations
-------------------------------------

The builder can construct the list of operations as *ordered* or
*unordered*.

Ordered Operations
~~~~~~~~~~~~~~~~~~

With an *ordered* operations list, MongoDB executes the write
operations in the list serially. If an error occurs during the
processing of one of the write operations, MongoDB will return without
processing any remaining write operations in the list.

Use :method:`db.collection.initializeOrderedBulkOp()` to create a
builder for an ordered list of write commands.

Unordered Operations
~~~~~~~~~~~~~~~~~~~~

With an *unordered* operations list, MongoDB can execute in parallel,
as well as in a nondeterministic order, the write operations in the
list. If an error occurs during the processing of one of the write
operations, MongoDB will continue to process remaining write operations
in the list.

Use :method:`db.collection.initializeUnorderedBulkOp()` to create a
builder for an unordered list of write commands.

.. _bulk-methods:

Methods
-------

The :method:`Bulk()` builder has the following methods:

.. include:: /includes/toc/table-spec-bulk-methods.rst
=====
cat()
=====

.. default-domain:: mongodb

Definition
----------

.. method:: cat(filename)

   Returns the contents of the specified file. The method returns with
   output relative to the current shell session and does not impact the
   server.

   .. include:: /reference/method/cat-param.rst
====
cd()
====

.. default-domain:: mongodb

Definition
----------

.. method:: cd(path)

   .. include:: /reference/method/cd-param.rst

   :method:`cd()` changes the directory context of the
   :program:`mongo` shell and has no effect on the MongoDB server.
============================
clearRawMongoProgramOutput()
============================

.. default-domain:: mongodb

.. method:: clearRawMongoProgramOutput()

   For internal use.
=========
connect()
=========

.. default-domain:: mongodb

.. method:: connect( <hostname><:port>/<database> )

   The :method:`connect()` method creates a connection to a MongoDB instance.
   However, use the :method:`Mongo()` object and its
   :method:`~Mongo.getDB()` method in most cases.

   :method:`connect()` accepts a string ``<hostname><:port>/<database>``
   parameter to connect to the MongoDB instance on the
   ``<hostname><:port>`` and return the reference to the database
   ``<database>``.

   The following example instantiates a new connection to the MongoDB
   instance running on the localhost interface and returns a reference
   to ``myDatabase``:

   .. code-block:: javascript

      db = connect("localhost:27017/myDatabase")

   .. seealso:: :method:`Mongo.getDB()`
============
copyDbpath()
============

.. default-domain:: mongodb

.. method:: copyDbpath()

   For internal use.
==================
cursor.addOption()
==================

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.addOption(flag)

   Adds ``OP_QUERY`` wire protocol flags, such as the ``tailable``
   flag, to change the behavior of queries.

   The :method:`cursor.addOption()` method has the following parameter:

   .. include:: /reference/method/cursor.addOption-param.rst

.. _cursor-flags:

Flags
-----

The :program:`mongo` shell provides several additional cursor flags to
modify the behavior of the cursor.

.. data:: DBQuery.Option.tailable

.. data:: DBQuery.Option.slaveOk

.. data:: DBQuery.Option.oplogReplay

.. data:: DBQuery.Option.noTimeout

.. data:: DBQuery.Option.awaitData

.. data:: DBQuery.Option.exhaust

.. data:: DBQuery.Option.partial

For a description of the flags, see
:meta-driver:`MongoDB wire protocol </legacy/mongodb-wire-protocol/?pageVersion=106#op-query>`.

Example
-------

The following example adds the ``DBQuery.Option.tailable`` flag and the
``DBQuery.Option.awaitData`` flag to ensure that the query returns a
tailable cursor. The sequence creates a cursor that will wait for few
seconds after returning the full result set so that it can capture and
return additional data added during the query:

.. code-block:: javascript

   var t = db.myCappedCollection;
   var cursor = t.find().addOption(DBQuery.Option.tailable).
                         addOption(DBQuery.Option.awaitData)

.. warning::

   Adding incorrect wire protocol flags can cause problems and/or
   extra server load.
==================
cursor.batchSize()
==================

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.batchSize(size)

   Specifies the
   number of documents to return in each batch of the response from the
   MongoDB instance. In most cases, modifying the batch size will
   not affect the user or the application, as the
   :program:`mongo` shell and most :doc:`drivers
   </applications/drivers>` return results as if MongoDB returned a
   single batch.

   The :method:`~cursor.batchSize()` method takes the
   following parameter:

   .. include:: /reference/method/cursor.batchSize-param.rst

   .. note::

      Specifying ``1`` or a negative number is analogous to using the
      :method:`~cursor.limit()` method.

Example
-------

The following example sets the batch size for the results of a query
(i.e. :method:`~db.collection.find()`) to ``10``. The
:method:`~cursor.batchSize()` method does not change the
output in the :program:`mongo` shell, which, by default, iterates over the
first 20 documents.

.. code-block:: javascript

   db.inventory.find().batchSize(10)
==============
cursor.count()
==============

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.count()

   Counts the number of documents referenced by a cursor. Append the
   :method:`~cursor.count()` method to a
   :method:`~db.collection.find()` query to return the number of
   matching documents. The operation does not perform the query but
   instead counts the results that would be returned by the query.

   .. versionchanged:: 2.6
      MongoDB supports the use of :method:`~cursor.hint()` with
      :method:`~cursor.count()`. See :ref:`count-method-hint` for an
      example.

   The :method:`~cursor.count()` method has the following
   prototype form:

   .. code-block:: javascript

      db.collection.find().count()

   The :method:`~cursor.count()` method has the following
   parameter:

   .. include:: /reference/method/cursor.count-param.rst

   MongoDB also provides the shell wrapper
   :method:`db.collection.count()` for the
   ``db.collection.find().count()`` construct.

   .. seealso:: :method:`cursor.size()`

Examples
--------

The following are examples of the :method:`~cursor.count()` method.

Count All Documents
~~~~~~~~~~~~~~~~~~~

The following operation counts the number of all documents in the
``orders`` collection:

.. code-block:: javascript

   db.orders.find().count()

Count Documents That Match a Query
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation counts the number of the documents in the
``orders`` collection with the field ``ord_dt`` greater than ``new
Date('01/01/2012')``:

.. code-block:: javascript

   db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).count()

Limit Documents in Count
~~~~~~~~~~~~~~~~~~~~~~~~

The following operation counts the number of the documents in the
``orders`` collection with the field ``ord_dt`` greater than ``new
Date('01/01/2012')`` *taking into account* the effect of the
``limit(5)``:

.. code-block:: javascript

   db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).limit(5).count(true)

.. _count-method-hint:

Specify the Index to Use
~~~~~~~~~~~~~~~~~~~~~~~~

The following operation uses the index ``{ status: 1 }`` to return a
count of the documents in the ``orders`` collection with the field
``ord_dt`` greater than ``new Date('01/01/2012')`` and the ``status``
field is equal to ``"D"``:

.. code-block:: javascript

   db.orders.find(
      { ord_dt: { $gt: new Date('01/01/2012') }, status: "D" }
   ).hint( { status: 1 } ).count()
================
cursor.explain()
================

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.explain(verbose)

   Provides information on the
   query plan. The query plan is the plan the server uses to find the
   matches for a query. This information may be useful when optimizing
   a query. The :method:`~cursor.explain()` method returns a document
   that describes the process used to return the query results.

   The :method:`~cursor.explain()` method has the following form:

   .. code-block:: javascript

      db.collection.find().explain()

   The :method:`~cursor.explain()` method has the following parameter:

   .. include:: /reference/method/cursor.explain-param.rst

   For an explanation of output, see
   :ref:`explain-output-fields-sharded` and
   :ref:`explain-output-fields-core`.

Behavior
--------

The :method:`~cursor.explain()` method runs the actual query to
determine the result. Although there are some differences between
running the query with :method:`~cursor.explain()` and
running without, generally, the performance will be similar between
the two. So, if the query is slow, the :method:`~cursor.explain()` operation is also slow.

Additionally, the :method:`~cursor.explain()` operation reevaluates a set
of candidate query plans, which may cause the :method:`~cursor.explain()`
operation to perform differently than a normal query. As a result,
these operations generally provide an accurate account of *how*
MongoDB would perform the query, but do not reflect the length of
these queries.

.. seealso::

   - :operator:`$explain`

   - :doc:`/administration/optimization` page for information
     regarding optimization strategies.

   - :doc:`/tutorial/manage-the-database-profiler` tutorial for
     information regarding the database profile.

   - :doc:`Current Operation Reporting </reference/method/db.currentOp>`

.. _explain-results:

Explain Results
---------------

Explain on Queries on Unsharded Collections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For queries on unsharded collections, :method:`~cursor.explain()`
returns the following core information.

.. code-block:: javascript

   {
     "cursor" : "<Cursor Type and Index>",
     "isMultiKey" : <boolean>,
     "n" : <num>,
     "nscannedObjects" : <num>,
     "nscanned" : <num>,
     "nscannedObjectsAllPlans" : <num>,
     "nscannedAllPlans" : <num>,
     "scanAndOrder" : <boolean>,
     "indexOnly" : <boolean>,
     "nYields" : <num>,
     "nChunkSkips" : <num>,
     "millis" : <num>,
     "indexBounds" : { <index bounds> },
     "allPlans" : [
                    { "cursor" : "<Cursor Type and Index>",
                      "n" : <num>,
                      "nscannedObjects" : <num>,
                      "nscanned" : <num>,
                      "indexBounds" : { <index bounds> }
                    },
                     ...
                  ],
     "oldPlan" : {
                   "cursor" : "<Cursor Type and Index>",
                   "indexBounds" : { <index bounds> }
                 }
     "server" : "<host:port>",
     "filterSet" : <boolean>
   }

For details on the fields, see :ref:`explain-output-fields-core`.

Explain on ``$or`` Queries
~~~~~~~~~~~~~~~~~~~~~~~~~~

Queries with :query:`$or` operator execute each clause of the
:query:`$or` expression in parallel and can use separate indexes on
the individual clauses. If the query uses indexes on any or all of the
query's clause, :method:`~cursor.explain()` contains
:ref:`output <explain-output-fields-core>` for each clause as well as
the cumulative data for the entire query:

.. code-block:: javascript

   {
      "clauses" : [
                     {
                        <core explain output>
                     },
                     {
                        <core explain output>
                     },
                     ...
                  ],
      "n" : <num>,
      "nscannedObjects" : <num>,
      "nscanned" : <num>,
      "nscannedObjectsAllPlans" : <num>,
      "nscannedAllPlans" : <num>,
      "millis" : <num>,
      "server" : "<host:port>"
   }

For details on the fields, see :ref:`explain-output-field-or-clauses`
and :ref:`explain-output-fields-core`.

.. _explain-output-fields-sharded:

Explain on Queries on Sharded Collections
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For queries on sharded collections, :method:`~cursor.explain()` returns
information for each shard the query accesses. For queries on
unsharded collections, see :ref:`explain-output-fields-core`.

For queries on a sharded collection, the output contains the
:ref:`explain-output-fields-core` for each accessed shard and
:ref:`cumulative shard information <explain-output-fields-sharded-collection>`:

.. code-block:: javascript

   {
      "clusteredType" : "<Shard Access Type>",
      "shards" : {

                   "<shard1>" : [
                                  {
                                    <core explain output>
                                  }
                                ],
                   "<shard2>" : [
                                  {
                                   <core explain output>
                                  }
                                ],
                   ...
                 },
      "millisShardTotal" : <num>,
      "millisShardAvg" : <num>,
      "numQueries" : <num>,
      "numShards" : <num>,
      "cursor" : "<Cursor Type and Index>",
      "n" : <num>,
      "nChunkSkips" : <num>,
      "nYields" : <num>,
      "nscanned" : <num>,
      "nscannedAllPlans" : <num>,
      "nscannedObjects" : <num>,
      "nscannedObjectsAllPlans" : <num>,
      "millis" : <num>
   }

For details on these fields, see :ref:`explain-output-fields-core` for
each accessed shard and :ref:`explain-output-fields-sharded-collection`.

Explain Output Fields
---------------------

.. _explain-output-fields-core:

Core Explain Output Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~

This section explains output for queries on collections that are *not
sharded*. For queries on sharded collections, see
:ref:`explain-output-fields-sharded`.

.. data:: explain.cursor

   :data:`~explain.cursor` is a string that reports the type of cursor
   used by the query operation:

   - ``BasicCursor`` indicates a full collection scan.

   - ``BtreeCursor`` indicates that the query used an index. The
     cursor includes name of the index. When a query uses an index,
     the output of :method:`~cursor.explain()` includes
     :data:`~explain.indexBounds` details.

   - ``GeoSearchCursor`` indicates that the query used a geospatial
     index.

   - ``Complex Plan`` indicates that MongoDB used :doc:`index
     intersection </core/index-intersection>`.

   For ``BtreeCursor`` cursors, MongoDB will append the name of the
   index to the cursor string. Additionally, depending on how the
   query uses an index, MongoDB may append one or both of the
   following strings to the cursor string:

   - ``reverse`` indicates that query transverses the index from the
     highest values to the lowest values (e.g. "right to left".)

   - ``multi`` indicates that the query performed multiple
     look-ups. Otherwise, the query uses the index to determine a
     range of possible matches.

.. data:: explain.isMultiKey

   :data:`~explain.isMultiKey` is a boolean. When ``true``, the query uses a
   :ref:`multikey index <index-type-multikey>`, where one of the
   fields in the index holds an array.

.. data:: explain.n

   :data:`~explain.n` is a number that reflects the number of documents
   that match the query selection criteria.

.. data:: explain.nscannedObjects

   Specifies the total number of documents scanned during the query.
   The :data:`~explain.nscannedObjects` may be lower than
   :data:`~explain.nscanned`, such as if the index :ref:`covers
   <indexes-covered-queries>` a query. See
   :data:`~explain.indexOnly`. Additionally, the
   :data:`~explain.nscannedObjects` may be lower than
   :data:`~explain.nscanned` in the case of multikey index on an array
   field with duplicate documents.

.. data:: explain.nscanned

   Specifies the total number of documents or index entries scanned
   during the database operation. You want :data:`~explain.n` and
   :data:`~explain.nscanned` to be close in value as possible. The
   :data:`~explain.nscanned` value may be higher than the
   :data:`~explain.nscannedObjects` value, such as if the index :ref:`covers
   <indexes-covered-queries>` a query. See :data:`~explain.indexOnly`.

.. data:: explain.nscannedObjectsAllPlans

   .. versionadded:: 2.2

   :data:`~explain.nscannedObjectsAllPlans` is a number that reflects the
   total number of documents scanned for all query plans during the
   database operation.

.. data:: explain.nscannedAllPlans

   .. versionadded:: 2.2

   :data:`~explain.nscannedAllPlans` is a number that reflects the total
   number of documents or index entries scanned for all query plans
   during the database operation.

.. data:: explain.scanAndOrder

   :data:`~explain.scanAndOrder` is a boolean that is ``true`` when
   the query **cannot** use the order of documents in the index for
   returning sorted results: MongoDB must sort the documents after it
   receives the documents from a cursor.

   If :data:`~explain.scanAndOrder` is ``false``, MongoDB *can* use
   the order of the documents in an index to return sorted results.

.. data:: explain.indexOnly

   :data:`~explain.indexOnly` is a boolean value that returns ``true``
   when the query is :ref:`covered <indexes-covered-queries>` by
   the index indicated in the :data:`~explain.cursor` field. When an
   index covers a query, MongoDB can both match the :ref:`query
   conditions <read-operations-query-document>` **and** return the
   results using only the index because:

   - all the fields in the :ref:`query
     <read-operations-query-document>` are part of that index, **and**

   - all the fields returned in the results set are in the same index.

.. data:: explain.nYields

   :data:`~explain.nYields` is a number that reflects the number of times
   this query yielded the read lock to allow waiting writes execute.

.. data:: explain.nChunkSkips

   :data:`~explain.nChunkSkips` is a number that reflects the number of
   documents skipped because of active chunk migrations in a sharded
   system. Typically this will be zero. A number greater than zero is
   ok, but indicates a little bit of inefficiency.

.. data:: explain.millis

   :data:`~explain.millis` is a number that reflects the time in
   milliseconds to complete the query.

.. data:: explain.indexBounds

   :data:`~explain.indexBounds` is a document that contains the lower and upper
   index key bounds. This field resembles one of the following:

   .. code-block:: javascript

      "indexBounds" : {
                          "start" : { <index key1> : <value>, ...  },
                          "end" : { <index key1> : <value>, ... }
                      },

   .. code-block:: javascript

      "indexBounds" : { "<field>" : [ [ <lower bound>, <upper bound> ] ],
                        ...
                      }

.. data:: explain.allPlans

   :data:`~explain.allPlans` is an array that holds the list of plans
   the query optimizer runs in order to select the index for the query.
   Displays only when the ``<verbose>`` parameter to :method:`~cursor.explain()` is ``true`` or ``1``.

.. data:: explain.oldPlan

   .. versionadded:: 2.2

   :data:`~explain.oldPlan` is a document value that contains the previous plan
   selected by the query optimizer for the query. Displays only when
   the ``<verbose>`` parameter to :method:`~cursor.explain()` is ``true`` or ``1``.

.. data:: explain.server

   .. versionadded:: 2.2

   :data:`~explain.server` is a string that reports the MongoDB server.

.. data:: explain.filterSet

   .. versionadded:: 2.6

   :data:`~explain.filterSet` is a boolean that indicates whether
   MongoDB applied an :ref:`index filter <index-filters>` for the query.

.. _explain-output-field-or-clauses:

``$or`` Query Output Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. data:: explain.clauses

   :data:`~explain.clauses` is an array that holds the
   :ref:`explain-output-fields-core` information for each clause of the
   :query:`$or` expression. :data:`~explain.clauses` is only included when
   the clauses in the :query:`$or` expression use indexes.

.. _explain-output-fields-sharded-collection:

Sharded Collections Output Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. data:: explain.clusteredType

   :data:`~explain.clusteredType` is a string that reports the access
   pattern for shards. The value is:

   - ``ParallelSort``, if the :program:`mongos` queries shards in parallel.

   - ``SerialServer``, if the :program:`mongos` queries shards sequentially.

.. data:: explain.shards

   :data:`~explain.shards` contains fields for each shard in the
   cluster accessed during the query. Each field holds the
   :ref:`explain-output-fields-core` for that shard.

.. data:: explain.millisShardTotal

   :data:`~explain.millisShardTotal` is a number that reports the total
   time in milliseconds for the query to run on the shards.

.. data:: explain.millisShardAvg

   :data:`~explain.millisShardAvg` is a number that reports the average
   time in millisecond for the query to run on each shard.

.. data:: explain.numQueries

   :data:`~explain.numQueries` is a number that reports the total number
   of queries executed.

.. data:: explain.numShards

   :data:`~explain.numShards` is a number that reports the total number of
   shards queried.
================
cursor.forEach()
================

.. default-domain:: mongodb

Description
-----------

.. method:: cursor.forEach(function)

   Iterates the cursor to apply a JavaScript ``function`` to each
   document from the cursor.

   The :method:`~cursor.forEach()` method has the following prototype
   form:

   .. code-block:: javascript

      db.collection.find().forEach(<function>)

   The :method:`~cursor.forEach()` method has the following parameter:

   .. include:: /reference/method/cursor.forEach-param.rst

Example
-------

The following example invokes the :method:`~cursor.forEach()` method
on the cursor returned by :method:`~db.collection.find()` to print
the name of each user in the collection:

.. code-block:: javascript

   db.users.find().forEach( function(myDoc) { print( "user: " + myDoc.name ); } );

.. seealso:: :method:`cursor.map()` for similar functionality.
================
cursor.hasNext()
================

.. default-domain:: mongodb

.. method:: cursor.hasNext()

   :returns: Boolean.

   :method:`cursor.hasNext()` returns ``true`` if the cursor returned by
   the :method:`db.collection.find()` query can iterate further to
   return more documents.
=============
cursor.hint()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.hint(index)

   Call this method on a query to override MongoDB's default index
   selection and :doc:`query optimization process </core/query-plans>`.
   Use :method:`db.collection.getIndexes()` to return the list of
   current indexes on a collection.

   The :method:`cursor.hint()` method has the following parameter:

   .. include:: /reference/method/cursor.hint-param.rst

Behavior
--------

When an :ref:`index filter <index-filters>` exists for the query shape,
MongoDB ignores the :method:`~cursor.hint()`. The
:data:`explain.filterSet` field of the :method:`~cursor.explain()`
output indicates whether MongoDB applied an index filter for the query.

.. include:: /includes/fact-hint-text-query-restriction.rst

Example
-------

The following example returns all documents in the collection named
``users`` using the index on the ``age`` field.

.. code-block:: javascript

   db.users.find().hint( { age: 1 } )

You can also specify the index using the index name:

.. code-block:: javascript

   db.users.find().hint( "age_1" )

.. seealso::

   - :doc:`/core/indexes-introduction`
   - :doc:`/administration/indexes`
   - :doc:`/core/query-plans`
   - :ref:`index-filters`
   - :operator:`$hint`
==============
cursor.limit()
==============

.. default-domain:: mongodb

.. method:: cursor.limit()

   Use the :method:`~cursor.limit()` method on a cursor to specify the maximum
   number of documents the cursor will return. :method:`~cursor.limit()` is
   analogous to the ``LIMIT`` statement in a SQL database.

   .. note::

      You must apply :method:`~cursor.limit()` to the cursor before
      retrieving any documents from the database.

   Use :method:`~cursor.limit()` to maximize performance and prevent
   MongoDB from returning more results than required for processing.

   A :method:`~cursor.limit()` value of 0 (i.e. :method:`.limit(0)
   <cursor.limit()>`) is equivalent to setting no limit. A negative
   limit is similar to a positive limit, but a negative limit prevents
   the creation of a cursor such that only a single batch of results is
   returned. As such, with a negative limit, if the limited result set
   does not fit into a single batch, the number of documents received
   will be less than the limit.
============
cursor.map()
============

.. default-domain:: mongodb

.. method:: cursor.map(function)

   Applies ``function`` to each document visited by the cursor and
   collects the return values from successive application into
   an array.

   The :method:`cursor.map()` method has the following parameter:

   .. include:: /reference/method/cursor.map-param.rst

Example
-------

.. code-block:: javascript

   db.users.find().map( function(u) { return u.name; } );

.. seealso:: :method:`cursor.forEach()` for similar functionality.
============
cursor.max()
============

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.max()

   Specifies the *exclusive* upper bound for a specific index in order
   to constrain the results of
   :method:`~db.collection.find()`. :method:`~cursor.max()` provides a
   way to specify an upper bound on compound key indexes.

   The :method:`~cursor.max()` method has the following parameter:

   .. include:: /reference/method/cursor.max-param.rst

   The ``indexBounds`` parameter has the following prototype form:

   .. code-block:: javascript

      { field1: <max value>, field2: <max value2> ... fieldN:<max valueN>}

   The fields correspond to *all* the keys of a particular index *in
   order*. You can explicitly specify the particular index with the
   :method:`~cursor.hint()` method. Otherwise, :program:`mongod`
   selects the index using the fields in the ``indexBounds``; however,
   if multiple indexes exist on same fields with different sort
   orders, the selection of the index may be ambiguous.

   .. seealso:: :method:`~cursor.min()`.

.. note:: :method:`~cursor.max()` is a shell wrapper around the query
   modifier :operator:`$max`.

Behavior
--------

- Because :method:`~cursor.max()` requires an index on a field,
  and forces the query to use this index, you may prefer the
  :query:`$lt` operator for the query if possible. Consider the
  following example:

  .. code-block:: javascript

     db.products.find( { _id: 7 } ).max( { price: 1.39 } )

  The query will use the index on the ``price`` field, even if
  the index on ``_id`` may be better.

- :method:`~cursor.max()` exists primarily to support the
  :program:`mongos` (sharding) process.

- If you use :method:`~cursor.max()` with :method:`~cursor.min()` to
  specify a range, the index bounds specified in
  :method:`~cursor.min()` and :method:`~cursor.max()`
  must both refer to the keys of the same index.


Example
-------

This example assumes a collection named ``products`` that holds the
following documents:

.. code-block:: javascript

   { "_id" : 6, "item" : "apple", "type" : "cortland", "price" : 1.29 }
   { "_id" : 2, "item" : "apple", "type" : "fuji", "price" : 1.99 }
   { "_id" : 1, "item" : "apple", "type" : "honey crisp", "price" : 1.99 }
   { "_id" : 3, "item" : "apple", "type" : "jonagold", "price" : 1.29 }
   { "_id" : 4, "item" : "apple", "type" : "jonathan", "price" : 1.29 }
   { "_id" : 5, "item" : "apple", "type" : "mcintosh", "price" : 1.29 }
   { "_id" : 7, "item" : "orange", "type" : "cara cara", "price" : 2.99 }
   { "_id" : 10, "item" : "orange", "type" : "navel", "price" : 1.39 }
   { "_id" : 9, "item" : "orange", "type" : "satsuma", "price" : 1.99 }
   { "_id" : 8, "item" : "orange", "type" : "valencia", "price" : 0.99 }

The collection has the following indexes:

.. code-block:: javascript

   { "_id" : 1 }
   { "item" : 1, "type" : 1 }
   { "item" : 1, "type" : -1 }
   { "price" : 1 }

- Using the ordering of ``{ item: 1, type: 1 }`` index,
  :method:`~cursor.max()` limits the query to the documents
  that are below the bound of ``item`` equal to ``apple`` and
  ``type`` equal to ``jonagold``:

  .. code-block:: javascript

     db.products.find().max( { item: 'apple', type: 'jonagold' } ).hint( { item: 1, type: 1 } )

  The query returns the following documents:

  .. code-block:: javascript

     { "_id" : 6, "item" : "apple", "type" : "cortland", "price" : 1.29 }
     { "_id" : 2, "item" : "apple", "type" : "fuji", "price" : 1.99 }
     { "_id" : 1, "item" : "apple", "type" : "honey crisp", "price" : 1.99 }

  If the query did not explicitly specify the index with the
  :method:`~cursor.hint()` method, it is ambiguous as to
  whether :program:`mongod` would select the ``{ item: 1, type: 1
  }`` index ordering or the ``{ item: 1, type: -1 }`` index ordering.

- Using the ordering of the index ``{ price: 1 }``, :method:`~cursor.max()` limits the query to the documents that are below
  the index key bound of ``price`` equal to ``1.99`` and
  :method:`~cursor.min()` limits the query to the documents
  that are at or above the index key bound of ``price`` equal to
  ``1.39``:

  .. code-block:: javascript

      db.products.find().min( { price: 1.39 } ).max( { price: 1.99 } ).hint( { price: 1 } )

  The query returns the following documents:

  .. code-block:: javascript

     { "_id" : 6, "item" : "apple", "type" : "cortland", "price" : 1.29 }
     { "_id" : 4, "item" : "apple", "type" : "jonathan", "price" : 1.29 }
     { "_id" : 5, "item" : "apple", "type" : "mcintosh", "price" : 1.29 }
     { "_id" : 3, "item" : "apple", "type" : "jonagold", "price" : 1.29 }
     { "_id" : 10, "item" : "orange", "type" : "navel", "price" : 1.39 }
==================
cursor.maxTimeMS()
==================

.. default-domain:: mongodb

Definition
----------

.. versionadded:: 2.6

.. method:: cursor.maxTimeMS(<time limit>)

   Specifies a cumulative time limit in milliseconds for processing
   operations on a cursor.

   The :method:`~cursor.maxTimeMS()` method has the following
   parameter:

   .. include:: /reference/method/cursor.maxTimeMS-param.rst

.. important::

   :method:`~cursor.maxTimeMS()` is not related to the
   ``NoCursorTimeout`` query flag. :method:`~cursor.maxTimeMS()`
   relates to processing time, while ``NoCursorTimeout`` relates
   to idle time. A cursor's idle time does not contribute towards its
   processing time.

Behaviors
---------

MongoDB targets operations for termination if the associated cursor
exceeds its allotted time limit. MongoDB terminates operations that
exceed their allotted time limit, using the same mechanism as
:method:`db.killOp()`. MongoDB only terminates an operation at one of
its designated interrupt points.

MongoDB does not count network latency towards a cursor's time limit.

Queries that generate multiple batches of results continue to return
batches until the cursor exceeds its allotted time limit.

Examples
--------

.. example:: The following query specifies a time limit of 50 milliseconds:

   .. code-block:: javascript

      db.collection.find({description: /August [0-9]+, 1969/}).maxTimeMS(50)
============
cursor.min()
============

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.min()

   Specifies the *inclusive* lower bound for a specific index in order
   to constrain the results of
   :method:`~db.collection.find()`. :method:`~cursor.min()` provides a
   way to specify lower bounds on compound key indexes.

   The :method:`~cursor.min()` has the following parameter:

   .. include:: /reference/method/cursor.min-param.rst

   The ``indexBounds`` parameter has the following prototype form:

   .. code-block:: javascript

      { field1: <min value>, field2: <min value2>, fieldN:<min valueN> }

   The fields correspond to *all* the keys of a particular index
   *in order*. You can explicitly specify the particular index
   with the :method:`~cursor.hint()` method. Otherwise,
   MongoDB selects the index using the fields in the
   ``indexBounds``; however, if multiple indexes exist on same
   fields with different sort orders, the selection of the index
   may be ambiguous.

   .. seealso:: :method:`~cursor.max()`.

.. note:: :method:`~cursor.min()` is a shell wrapper around the
   query modifier :operator:`$min`.

Behaviors
---------

- Because :method:`~cursor.min()` requires an index on a
  field, and forces the query to use this index, you may prefer
  the :query:`$gte` operator for the query if
  possible. Consider the following example:

  .. code-block:: javascript

     db.products.find( { _id: 7 } ).min( { price: 1.39 } )

  The query will use the index on the ``price`` field, even if
  the index on ``_id`` may be better.

- :method:`~cursor.min()` exists primarily to support the
  :program:`mongos` process.

- If you use :method:`~cursor.min()` with :method:`~cursor.max()` to
  specify a range, the index bounds specified in
  :method:`~cursor.min()` and :method:`~cursor.max()` must both refer
  to the keys of the same index.

Example
-------

This example assumes a collection named ``products`` that holds the
following documents:

.. code-block:: javascript

   { "_id" : 6, "item" : "apple", "type" : "cortland", "price" : 1.29 }
   { "_id" : 2, "item" : "apple", "type" : "fuji", "price" : 1.99 }
   { "_id" : 1, "item" : "apple", "type" : "honey crisp", "price" : 1.99 }
   { "_id" : 3, "item" : "apple", "type" : "jonagold", "price" : 1.29 }
   { "_id" : 4, "item" : "apple", "type" : "jonathan", "price" : 1.29 }
   { "_id" : 5, "item" : "apple", "type" : "mcintosh", "price" : 1.29 }
   { "_id" : 7, "item" : "orange", "type" : "cara cara", "price" : 2.99 }
   { "_id" : 10, "item" : "orange", "type" : "navel", "price" : 1.39 }
   { "_id" : 9, "item" : "orange", "type" : "satsuma", "price" : 1.99 }
   { "_id" : 8, "item" : "orange", "type" : "valencia", "price" : 0.99 }

The collection has the following indexes:

.. code-block:: javascript

   { "_id" : 1 }
   { "item" : 1, "type" : 1 }
   { "item" : 1, "type" : -1 }
   { "price" : 1 }

- Using the ordering of the ``{ item: 1, type: 1 }`` index,
  :method:`~cursor.min()` limits the query to the documents
  that are at or above the index key bound of ``item`` equal to
  ``apple`` and ``type`` equal to ``jonagold``, as in the following:

  .. code-block:: javascript

     db.products.find().min( { item: 'apple', type: 'jonagold' } ).hint( { item: 1, type: 1 } )

  The query returns the following documents:

  .. code-block:: javascript

     { "_id" : 3, "item" : "apple", "type" : "jonagold", "price" : 1.29 }
     { "_id" : 4, "item" : "apple", "type" : "jonathan", "price" : 1.29 }
     { "_id" : 5, "item" : "apple", "type" : "mcintosh", "price" : 1.29 }
     { "_id" : 7, "item" : "orange", "type" : "cara cara", "price" : 2.99 }
     { "_id" : 10, "item" : "orange", "type" : "navel", "price" : 1.39 }
     { "_id" : 9, "item" : "orange", "type" : "satsuma", "price" : 1.99 }
     { "_id" : 8, "item" : "orange", "type" : "valencia", "price" : 0.99 }

  If the query did not explicitly specify the index with the
  :method:`~cursor.hint()` method, it is ambiguous as to
  whether :program:`mongod` would select the ``{ item: 1, type: 1 }``
  index ordering or the ``{ item: 1, type: -1 }`` index ordering.

- Using the ordering of the index ``{ price: 1 }``, :method:`~cursor.min()` limits the query to the documents that are at or
  above the index key bound of ``price`` equal to ``1.39`` and
  :method:`~cursor.max()` limits the query to the documents
  that are below the index key bound of ``price`` equal to ``1.99``:

  .. code-block:: javascript

      db.products.find().min( { price: 1.39 } ).max( { price: 1.99 } ).hint( { price: 1 } )

  The query returns the following documents:

  .. code-block:: javascript

     { "_id" : 6, "item" : "apple", "type" : "cortland", "price" : 1.29 }
     { "_id" : 4, "item" : "apple", "type" : "jonathan", "price" : 1.29 }
     { "_id" : 5, "item" : "apple", "type" : "mcintosh", "price" : 1.29 }
     { "_id" : 3, "item" : "apple", "type" : "jonagold", "price" : 1.29 }
     { "_id" : 10, "item" : "orange", "type" : "navel", "price" : 1.39 }
=============
cursor.next()
=============

.. default-domain:: mongodb

.. method:: cursor.next()

   :returns: The next document in the cursor returned by the
             :method:`db.collection.find()` method. See
             :method:`cursor.hasNext()` related functionality.
========================
cursor.objsLeftInBatch()
========================

.. default-domain:: mongodb

.. method:: cursor.objsLeftInBatch()

   :method:`cursor.objsLeftInBatch()` returns the number of
   documents remaining in the current batch.

   The MongoDB instance returns response in batches. To retrieve
   all the documents from a cursor may require multiple batch
   responses from the MongoDB instance. When there are no more
   documents remaining in the current batch, the cursor will retrieve
   another batch to get more documents until the cursor exhausts.
=================
cursor.readPref()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.readPref(mode, tagSet)

   Append :method:`~cursor.readPref()` to a cursor to
   control how the client routes the query to members
   of the replica set.

   .. include:: /reference/method/cursor.readPref-param.rst

   .. note::

      You must apply :method:`~cursor.readPref()` to the cursor before retrieving
      any documents from the database.
====================
cursor.showDiskLoc()
====================

.. default-domain:: mongodb

.. method:: cursor.showDiskLoc()

   :returns: A modified cursor object that contains documents with
             appended information that describes the on-disk location
             of the document.

   .. seealso:: :operator:`$showDiskLoc` for related
      functionality.
=============
cursor.size()
=============

.. default-domain:: mongodb

.. method:: cursor.size()

   :returns: A count of the number of documents that match the
             :method:`db.collection.find()` query after applying any
             :method:`cursor.skip()` and :method:`cursor.limit()` methods.
=============
cursor.skip()
=============

.. default-domain:: mongodb

.. method:: cursor.skip()

   Call the :method:`cursor.skip()` method on a cursor to control where MongoDB
   begins returning results. This approach may be useful in
   implementing "paged" results.

   .. note::

      You must apply :method:`cursor.skip()` to the cursor before
      retrieving any documents from the database.

   Consider the following JavaScript function as an example of the
   skip function:

   .. code-block:: javascript

      function printStudents(pageNumber, nPerPage) {
         print("Page: " + pageNumber);
         db.students.find().skip((pageNumber-1)*nPerPage).limit(nPerPage).forEach( function(student) { print(student.name + "<p>"); } );
      }

   The :method:`cursor.skip()` method is often expensive because it requires
   the server to walk from the beginning of the collection or index to
   get the offset or skip position before beginning to return
   result. As offset (e.g. ``pageNumber`` above) increases,
   :method:`cursor.skip()` will become slower and more CPU intensive. With
   larger collections, :method:`cursor.skip()` may become IO bound.

   Consider using range-based pagination for these kinds of
   tasks. That is, query for a range of objects, using logic within
   the application to determine the pagination rather than the
   database itself. This approach features better index utilization,
   if you do not need to easily jump to a specific page.
=================
cursor.snapshot()
=================

.. default-domain:: mongodb

.. method:: cursor.snapshot()

   Append the :method:`~cursor.snapshot()` method to a cursor to toggle
   the "snapshot" mode. This ensures that the query will not return a
   document multiple times, even if intervening write operations result
   in a move of the document due to the growth in document size.

   .. warning::

      - You must apply :method:`~cursor.snapshot()` to the cursor before
        retrieving any documents from the database.

      - You can only use :method:`~cursor.snapshot()` with
        **unsharded** collections.

   The :method:`~cursor.snapshot()` does not guarantee
   isolation from insertion or deletions.

   The :method:`~cursor.snapshot()` traverses the index on the ``_id``
   field. As such, :method:`~cursor.snapshot()` **cannot**
   be used with :method:`~cursor.sort()` or :method:`~cursor.hint()`.



   Queries with results of less than 1 megabyte are effectively
   implicitly snapshotted.
=============
cursor.sort()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: cursor.sort(sort)

   Specifies the order in which the query returns matching documents.
   You must apply :method:`~cursor.sort()` to the cursor before
   retrieving any documents from the database.

   The :method:`~cursor.sort()` method has the following parameter:

   .. include:: /reference/method/cursor.sort-param.rst

   The ``sort`` parameter contains field and value pairs, in the
   following form:

   .. code-block:: javascript

      { field: value }

   The sort document can specify :ref:`ascending or descending sort on
   existing fields <sort-asc-desc>` or :ref:`sort on computed metadata
   <sort-metadata>`.

Behaviors
---------

.. _sort-asc-desc:

Ascending/Descending Sort
~~~~~~~~~~~~~~~~~~~~~~~~~

Specify in the sort parameter the field or fields to sort by and a
value of ``1`` or ``-1`` to specify an ascending or descending sort
respectively.

The following sample document specifies a descending sort by the
``age`` field and then an ascending sort by the ``posts`` field:

.. code-block:: javascript

   { age : -1, posts: 1 }

.. include:: /includes/fact-sort-order.rst

.. _sort-metadata:

Metadata Sort
~~~~~~~~~~~~~

Specify in the sort parameter a new field name for the
computed metadata and specify the :projection:`$meta` expression as its
value.

The following sample document specifies a descending sort by the
``"textScore"`` metadata:

.. code-block:: javascript

   { score: { $meta: "textScore" } }

The specified metadata determines the sort order. For example, the
``"textScore"`` metadata sorts in descending order. See
:projection:`$meta` for details.

Limit Results
~~~~~~~~~~~~~

The sort operation requires that the entire sort be able to complete
within 32 megabytes.

When the sort operation consumes more than 32 megabytes, MongoDB
returns an error. To avoid this error, either create an index to
support the sort operation or use :method:`~cursor.sort()` in
conjunction with :method:`~cursor.limit()`. The specified limit must
result in a number of documents that fall within the 32 megabyte limit.

For example, if the following sort operation ``stocks_quotes`` exceeds
the 32 megabyte limit:

.. code-block:: javascript

   db.stocks.find().sort( { ticker: 1, date: -1 } )

Either create an index to support the sort operation:

.. code-block:: javascript

   db.stocks.ensureIndex( { ticker: 1, date: -1 } )

Or use :method:`~cursor.sort()` in conjunction with
:method:`~cursor.limit()`:

.. code-block:: javascript

   db.stocks.find().sort( { ticker: 1, date: -1 } ).limit(100)

Examples
--------

A collection ``orders`` contain the following documents:

.. code-block:: javascript

   { _id: 1, item: { category: "cake", type: "chiffon" }, amount: 10 }
   { _id: 2, item: { category: "cookies", type: "chocolate chip" }, amount: 50 }
   { _id: 3, item: { category: "cookies", type: "chocolate chip" }, amount: 15 }
   { _id: 4, item: { category: "cake", type: "lemon" }, amount: 30 }
   { _id: 5, item: { category: "cake", type: "carrot" }, amount: 20 }
   { _id: 6, item: { category: "brownies", type: "blondie" }, amount: 10 }

The following query, which returns all documents from the ``orders``
collection, does not specify a sort order:

.. code-block:: javascript

   db.orders.find()

The query returns the documents in indeterminate order:

.. code-block:: none

   { "_id" : 1, "item" : { "category" : "cake", "type" : "chiffon" }, "amount" : 10 }
   { "_id" : 2, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 50 }
   { "_id" : 3, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 15 }
   { "_id" : 4, "item" : { "category" : "cake", "type" : "lemon" }, "amount" : 30 }
   { "_id" : 5, "item" : { "category" : "cake", "type" : "carrot" }, "amount" : 20 }
   { "_id" : 6, "item" : { "category" : "brownies", "type" : "blondie" }, "amount" : 10 }

The following query specifies a sort on the ``amount`` field in
descending order.

.. code-block:: javascript

   db.orders.find().sort( { amount: -1 } )

The query returns the following documents, in descending order of
``amount``:

.. code-block:: javascript

   { "_id" : 2, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 50 }
   { "_id" : 4, "item" : { "category" : "cake", "type" : "lemon" }, "amount" : 30 }
   { "_id" : 5, "item" : { "category" : "cake", "type" : "carrot" }, "amount" : 20 }
   { "_id" : 3, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 15 }
   { "_id" : 1, "item" : { "category" : "cake", "type" : "chiffon" }, "amount" : 10 }
   { "_id" : 6, "item" : { "category" : "brownies", "type" : "blondie" }, "amount" : 10 }

The following query specifies the sort order using the fields from a
sub-document ``item``. The query sorts first by the ``category`` field
in ascending order, and then within each ``category``, by the ``type``
field in ascending order.

.. code-block:: javascript

   db.orders.find().sort( { "item.category": 1, "item.type": 1 } )

The query returns the following documents, ordered first by the
``category`` field, and within each category, by the ``type`` field:

.. code-block:: javascript

   { "_id" : 6, "item" : { "category" : "brownies", "type" : "blondie" }, "amount" : 10 }
   { "_id" : 5, "item" : { "category" : "cake", "type" : "carrot" }, "amount" : 20 }
   { "_id" : 1, "item" : { "category" : "cake", "type" : "chiffon" }, "amount" : 10 }
   { "_id" : 4, "item" : { "category" : "cake", "type" : "lemon" }, "amount" : 30 }
   { "_id" : 2, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 50 }
   { "_id" : 3, "item" : { "category" : "cookies", "type" : "chocolate chip" }, "amount" : 15 }

Return in Storage Order
-----------------------

The :operator:`$natural` parameter returns items according to their
storage order within the collection level extents.

Typically, the storage order reflects insertion order, *except* when
documents relocate because of :ref:`document growth due to updates
<data-model-document-growth>` or remove operations free up space which
are then taken up by newly inserted documents.

Consider the sequence of insert operations to the ``trees`` collection:

.. code-block:: javascript

   db.trees.insert( { _id: 1, common_name: "oak", genus: "quercus" } )
   db.trees.insert( { _id: 2, common_name: "chestnut", genus: "castanea" } )
   db.trees.insert( { _id: 3, common_name: "maple", genus: "aceraceae" } )
   db.trees.insert( { _id: 4, common_name: "birch", genus: "betula" } )

The following query returns the documents in the storage order:

.. code-block:: javascript

   db.trees.find().sort( { $natural: 1 } )

The documents return in the following order:

.. code-block:: javascript

   { "_id" : 1, "common_name" : "oak", "genus" : "quercus" }
   { "_id" : 2, "common_name" : "chestnut", "genus" : "castanea" }
   { "_id" : 3, "common_name" : "maple", "genus" : "aceraceae" }
   { "_id" : 4, "common_name" : "birch", "genus" : "betula" }

Update a document such that the document outgrows its current allotted space:

.. code-block:: javascript

   db.trees.update(
      { _id: 1 },
      { $set: { famous_oaks: [ "Emancipation Oak", "Goethe Oak" ] } }
   )

Rerun the query to returns the documents in the storage order:

.. code-block:: javascript

   db.trees.find().sort( { $natural: 1 } )

The documents return in the following storage order:

.. code-block:: javascript

   { "_id" : 2, "common_name" : "chestnut", "genus" : "castanea" }
   { "_id" : 3, "common_name" : "maple", "genus" : "aceraceae" }
   { "_id" : 4, "common_name" : "birch", "genus" : "betula" }
   { "_id" : 1, "common_name" : "oak", "genus" : "quercus", "famous_oaks" : [ "Emancipation Oak", "Goethe Oak" ] }

.. seealso:: :operator:`$natural`
================
cursor.toArray()
================

.. default-domain:: mongodb

.. method:: cursor.toArray()

   The :method:`~cursor.toArray()` method returns an array that
   contains all the documents from a cursor. The method iterates
   completely the cursor, loading all the documents into RAM and
   exhausting the cursor.

   :returns: An array of documents.

Consider the following example that applies :method:`~cursor.toArray()`
to the cursor returned from the :method:`~db.collection.find()` method:

.. code-block:: javascript

   var allProductsArray = db.products.find().toArray();

   if (allProductsArray.length > 0) { printjson (allProductsArray[0]); }

The variable ``allProductsArray`` holds the array of documents returned by
:method:`~cursor.toArray()`.
======
Date()
======

.. default-domain:: mongodb

.. method:: Date()

   :returns: Current date, as a string.
============
db.addUser()
============

.. default-domain:: mongodb

.. deprecated:: 2.6
   Use :method:`db.createUser()` and :method:`db.updateUser()` instead
   of :method:`db.addUser()` to add users to MongoDB.

In 2.6, MongoDB introduced a new model for user
credentials and privileges, as described in
:doc:`/core/security-introduction`. To use :method:`db.addUser()` on MongoDB
2.4, see :v2.4:`db.addUser() in the version 2.4 of the MongoDB Manual
</reference/method/db.addUser>`.

Definition
----------

.. method:: db.addUser(document)

   Adds a new user on the database where you run the method. The
   :method:`db.addUser()` method takes a user document as its
   argument:

   .. code-block:: javascript

      db.addUser(<user document>)

   Specify a document that resembles the following as an argument to
   :method:`db.addUser()`:

   .. code-block:: javascript

      { user: "<name>",
        pwd: "<cleartext password>",
        customData: { <any information> },
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ],
        writeConcern: { <write concern> }
      }

   The :method:`db.addUser()` user document has the following fields:

   .. include:: /reference/method/db.addUser-param.rst

   Users created on the ``$external`` database should have credentials
   stored externally to MongoDB, as, for example, with :doc:`MongoDB
   Enterprise installations that use Kerberos
   </tutorial/control-access-to-mongodb-with-kerberos-authentication>`.

   .. |local-cmd-name| replace:: :method:`db.addUser()`
   .. include:: /includes/fact-roles-array-contents.rst

Considerations
--------------

The :method:`db.addUser()` method returns a *duplicate user* error if
the user exists.

When interacting with 2.6 and later MongoDB instances,
:method:`db.addUser()` sends unencrypted passwords. To encrypt
the password in transit use :doc:`SSL </tutorial/configure-ssl>`.

In the 2.6 version of the shell, :method:`db.addUser()` is backwards
compatible with both the :v2.4:`2.4 version of db.addUser()
</reference/method/db.addUser>` and the :v2.2:`2.2 version of db.addUser()
</reference/method/db.addUser>`. In 2.6, for backwards compatibility
:method:`db.addUser()` creates users that approximate the privileges
available in previous versions of MongoDB.

Example
-------

The following :method:`db.addUser()` method creates a user ``Carlos`` on the
database where the command runs. The command gives ``Carlos`` the
``clusterAdmin`` and ``readAnyDatabase`` roles on the ``admin`` database
and the ``readWrite`` role on the current database:

.. code-block:: javascript

   { user: "Carlos",
     pwd: "cleartext password",
     customData: { employeeId: 12345 },
     roles: [
       { role: "clusterAdmin", db: "admin" },
       { role: "readAnyDatabase", db: "admin" },
       "readWrite"
     ],
     writeConcern: { w: "majority" , wtimeout: 5000 }
   }
=========
db.auth()
=========

.. default-domain:: mongodb

Definition
----------

.. method:: db.auth(username, password)

   Allows a user to authenticate to the database from within the
   shell.

   .. include:: /reference/method/db.auth-param.rst

   Alternatively, you can use :option:`mongo --username` and
   :option:`--password <mongo --password>` to specify authentication
   credentials.

   .. |operation-name| replace:: :method:`db.auth()`
   .. include:: /includes/note-auth-methods-excluded-from-shell-history.rst

   :returns:

      :method:`db.auth()` returns ``0`` when authentication is
      **not** successful, and ``1`` when the operation is successful.
=======================
db.changeUserPassword()
=======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.changeUserPassword(username, password)

   Updates a user's password.

   .. include:: /reference/method/db.auth-param.rst

Example
-------

The following operation changes the ``reporting`` user's
password to ``SOh3TbYhx8ypJPxmt1oOfL``:

.. code-block:: javascript

   db.changeUserPassword("reporting", "SOh3TbYhx8ypJPxmt1oOfL")
====================
db.cloneCollection()
====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.cloneCollection(from, collection, query)

   Copies data directly between MongoDB instances. The
   :method:`db.cloneCollection()` wraps the
   :dbcommand:`cloneCollection` database command and accepts the
   following arguments:

   .. include:: /reference/method/db.cloneCollection-param.rst

   :method:`db.cloneCollection()` does not allow you to
   clone a collection through a :program:`mongos`. You must connect
   directly to the :program:`mongod` instance.
==================
db.cloneDatabase()
==================

.. default-domain:: mongodb

Definition
----------

.. method:: db.cloneDatabase("hostname")

   Copies a remote database to the current database. The command assumes
   that the remote database has the same name as the current database.

   .. include:: /reference/method/db.cloneDatabase-param.rst

   This method provides a wrapper around the MongoDB :term:`database
   command` ":dbcommand:`clone`." The :dbcommand:`copydb` database command
   provides related functionality.

Example
-------

To clone a database named ``importdb`` on a host named ``hostname``,
issue the following:

.. code-block:: javascript

   use importdb
   db.cloneDatabase("hostname")

New databases are implicitly created, so the current host does not
need to have a database named ``importdb`` for this command to
succeed.
=========================
db.collection.aggregate()
=========================

.. default-domain:: mongodb

.. versionadded:: 2.2

Definition
----------

.. method:: db.collection.aggregate(pipeline, options)

   Calculates aggregate values for the data in a collection.

   .. include:: /reference/method/db.collection.aggregate-param.rst

   The ``options`` document can contain the following fields and values:

   .. include:: /reference/method/db.collection.aggregate-options.rst

   :returns:
      A :term:`cursor` to the documents produced by the final stage of
      the aggregation pipeline operation, or if you include the
      ``explain`` option, the document that provides
      details on the processing of the aggregation operation.

      If the pipeline includes the :pipeline:`$out` operator,
      :method:`~db.collection.aggregate()` returns an empty cursor. See
      :pipeline:`$out` for more information.

      .. include:: /includes/fact-agg-helper-returns-cursor.rst

.. include:: /includes/fact-agg-helper-exception.rst

.. seealso:: For more information, see
   :doc:`/core/aggregation-pipeline`, :doc:`/reference/aggregation`,
   :doc:`/core/aggregation-pipeline-limits`, and :dbcommand:`aggregate`.

Cursor Behavior
---------------

In the :program:`mongo` shell, if the cursor returned from the
:method:`db.collection.aggregate()` is not assigned to a variable using
the ``var`` keyword, then the :program:`mongo` shell automatically
iterates the cursor up to 20 times. See :doc:`/core/cursors` for cursor
behavior in the :program:`mongo` shell and
:doc:`/tutorial/iterate-a-cursor` for handling cursors in the
:program:`mongo` shell.

Cursors returned from aggregation only supports cursor methods that
operate on evaluated cursors (i.e. cursors whose first batch has been
retrieved), such as the following methods:

.. hlist::
   :columns: 2

   * :method:`cursor.hasNext()`
   * :method:`cursor.next()`
   * :method:`cursor.toArray()`
   * :method:`cursor.forEach()`
   * :method:`cursor.map()`
   * :method:`cursor.objsLeftInBatch()`
   * :method:`cursor.itcount()`
   * :method:`cursor.pretty()`

Examples
--------

The examples in this section use the
:method:`db.collection.aggregate()` helper provided in the 2.6 version
of the :program:`mongo` shell.

The following examples use the collection ``orders`` that contains the
following documents:

.. code-block:: javascript

   { _id: 1, cust_id: "abc1", ord_date: ISODate("2012-11-02T17:04:11.102Z"), status: "A", amount: 50 }
   { _id: 2, cust_id: "xyz1", ord_date: ISODate("2013-10-01T17:04:11.102Z"), status: "A", amount: 100 }
   { _id: 3, cust_id: "xyz1", ord_date: ISODate("2013-10-12T17:04:11.102Z"), status: "D", amount: 25 }
   { _id: 4, cust_id: "xyz1", ord_date: ISODate("2013-10-11T17:04:11.102Z"), status: "D", amount: 125 }
   { _id: 5, cust_id: "abc1", ord_date: ISODate("2013-11-12T17:04:11.102Z"), status: "A", amount: 25 }

Group by and Calculate a Sum
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation operation selects documents with status equal
to ``"A"``, groups the matching documents by the ``cust_id`` field and
calculates the ``total`` for each ``cust_id`` field from the sum of the
``amount`` field, and sorts the results by the ``total`` field in
descending order:

.. code-block:: javascript

   db.orders.aggregate([
                        { $match: { status: "A" } },
                        { $group: { _id: "$cust_id", total: { $sum: "$amount" } } },
                        { $sort: { total: -1 } }
                      ])

The operation returns a cursor with the following documents:

.. code-block:: javascript

   { "_id" : "xyz1", "total" : 100 }
   { "_id" : "abc1", "total" : 75 }

.. include:: /includes/note-mongo-shell-automatically-iterates-cursor.rst

.. _example-aggregate-method-explain-option:

Return Information on Aggregation Pipeline Operation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation operation sets the option ``explain`` to
``true`` to return information about the aggregation operation.

.. code-block:: javascript

   db.orders.aggregate(
                        [
                          { $match: { status: "A" } },
                          { $group: { _id: "$cust_id", total: { $sum: "$amount" } } },
                          { $sort: { total: -1 } }
                        ],
                        {
                          explain: true
                        }
                      )

The operation returns a cursor with the document that contains detailed
information regarding the processing of the aggregation pipeline. For
example, the document may show, among other details, which index, if
any, the operation used. [#agg-index-filters]_ If the ``orders`` collection
is a sharded collection, the document would also show the division of
labor between the shards and the merge operation, and for targeted
queries, the targeted shards.

.. note:: The intended readers of the ``explain`` output document are humans, and
   not machines, and the output format is subject to change between
   releases.

.. include:: /includes/note-mongo-shell-automatically-iterates-cursor.rst

.. _example-aggregate-method-external-sort:

Perform Large Sort Operation with External Sort
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Aggregation pipeline stages have :ref:`maximum memory use limit
<agg-memory-restrictions>`. To handle large datasets, set
``allowDiskUse`` option to ``true`` to enable writing data to
temporary files, as in the following example:

.. code-block:: javascript

   var results = db.stocks.aggregate(
                                      [
                                        { $project : { cusip: 1, date: 1, price: 1, _id: 0 } },
                                        { $sort : { cusip : 1, date: 1 } }
                                      ],
                                      {
                                        allowDiskUse: true
                                      }
                                    )

.. _example-aggregate-method-initial-batch-size:

Specify an Initial Batch Size
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To specify an initial batch size for the cursor, use the following
syntax for the ``cursor`` option:

.. code-block:: javascript

   cursor: { batchSize: <int> }

For example, the following aggregation operation specifies the
*initial* batch size of ``0`` for the cursor:

.. code-block:: javascript

   db.orders.aggregate(
                        [
                          { $match: { status: "A" } },
                          { $group: { _id: "$cust_id", total: { $sum: "$amount" } } },
                          { $sort: { total: -1 } },
                          { $limit: 2 }
                        ],
                        {
                          cursor: { batchSize: 0 }
                        }
                      )

A ``batchSize`` of ``0`` means an empty
first batch and is useful for quickly returning a cursor or failure
message without doing significant server-side work. Specify subsequent
batch sizes to :ref:`OP_GET_MORE <wire-op-get-more>` operations as with
other MongoDB cursors.

.. include:: /includes/note-mongo-shell-automatically-iterates-cursor.rst

.. [#agg-index-filters] :ref:`index-filters` can affect the choice of index
   used. See :ref:`index-filters` for details.
======================
db.collection.copyTo()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.copyTo(newCollection)

   Copies all documents from ``collection`` into ``newCollection`` using
   server-side JavaScript. If ``newCollection`` does not exist, MongoDB
   creates it.

   .. |eval-object| replace:: :method:`db.collection.copyTo()`
   .. include:: /includes/access-eval.rst

   .. include:: /reference/method/db.collection.copyTo-param.rst

   .. include:: /includes/warning-copyto-loss-of-type-fidelity.rst

:method:`~db.collection.copyTo()` returns the number of documents
copied. If the copy fails, it throws an exception.

Behavior
--------

Because :method:`~db.collection.copyTo()` uses
:dbcommand:`eval` internally, the copy operations will block all
other operations on the :program:`mongod` instance.

Example
-------

The following operation copies all documents from the ``source``
collection into the ``target`` collection.

.. code-block:: javascript

   db.source.copyTo(target)
=====================
db.collection.count()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.count(<query>)

   Returns the count of documents that would match a
   :method:`~db.collection.find()` query. The
   :method:`db.collection.count()` method does not perform the
   :method:`~db.collection.find()` operation but instead counts and
   returns the number of results that match a query.

   The :method:`db.collection.count()` method has the following parameter:

   .. include:: /reference/method/db.collection.count-param.rst

.. seealso:: :method:`cursor.count()`

Examples
--------

Count all Documents in a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To count the number of all documents in the ``orders`` collection, use
the following operation:

.. code-block:: javascript

   db.orders.count()

This operation is equivalent to the following:

.. code-block:: javascript

   db.orders.find().count()

Count all Documents that Match a Query
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Count the number of the documents in the ``orders``
collection with the field ``ord_dt`` greater than ``new
Date('01/01/2012')``:

.. code-block:: javascript

   db.orders.count( { ord_dt: { $gt: new Date('01/01/2012') } } )

The query is equivalent to the following:

.. code-block:: javascript

   db.orders.find( { ord_dt: { $gt: new Date('01/01/2012') } } ).count()
===========================
db.collection.createIndex()
===========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.createIndex(keys, options)

   .. deprecated:: 1.8

   Creates indexes on collections.

   .. include:: /reference/method/db.collection.createIndex-param.rst

   .. seealso:: :doc:`/indexes`,
      :method:`db.collection.createIndex()`,
      :method:`db.collection.dropIndex()`,
      :method:`db.collection.dropIndexes()`,
      :method:`db.collection.getIndexes()`,
      :method:`db.collection.reIndex()`, and
      :method:`db.collection.totalIndexSize()`
========================
db.collection.dataSize()
========================

.. default-domain:: mongodb

.. method:: db.collection.dataSize()

   :returns: The size of the collection. This method provides a wrapper
             around the :data:`~collStats.size` output of the :dbcommand:`collStats`
             (i.e. :method:`db.collection.stats()`) command.
========================
db.collection.distinct()
========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.distinct(field, query)

   Finds the distinct
   values for a specified field across a single collection and returns
   the results in an array.

   .. include:: /reference/method/db.collection.distinct-param.rst

   The :method:`db.collection.distinct()` method provides a wrapper
   around the :dbcommand:`distinct` command. Results must not be larger
   than the maximum :ref:`BSON size <limit-bson-document-size>`.

   When possible to use covered indexes, the
   :method:`db.collection.distinct()` method will use an index to find
   the documents in the query as well as to return the data.

Examples
--------

The following are examples of the :method:`db.collection.distinct()`
method:

- Return an array of the distinct values of the field ``ord_dt`` from
  all documents in the ``orders`` collection:

  .. code-block:: javascript

     db.orders.distinct( 'ord_dt' )

- Return an array of the distinct values of the field ``sku`` in the
  subdocument ``item`` from all documents in the ``orders`` collection:

  .. code-block:: javascript

     db.orders.distinct( 'item.sku' )

- Return an array of the distinct values of the field ``ord_dt`` from
  the documents in the ``orders`` collection where the ``price`` is
  greater than ``10``:

  .. code-block:: javascript

     db.orders.distinct( 'ord_dt', { price: { $gt: 10 } } )
====================
db.collection.drop()
====================

.. COMMENT Be sure to synchronize with the corresponding command.

.. default-domain:: mongodb

.. method:: db.collection.drop()

   Call the :method:`db.collection.drop()` method on a collection to
   drop it from the database. The method provides a wrapper around the
   :dbcommand:`drop` command.

   :method:`db.collection.drop()` takes no arguments and will produce
   an error if called with any arguments.

   This method also removes any indexes associated with the dropped
   collection.

   .. warning::

      This method obtains a write lock on the affected database and
      will block other operations until it has completed.
=========================
db.collection.dropIndex()
=========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.dropIndex(index)

   Drops or removes the specified index from a collection. The
   :method:`db.collection.dropIndex()` method provides a wrapper around
   the :dbcommand:`dropIndexes` command.

   .. note::

      You cannot drop the default index on the ``_id`` field.

   The :method:`db.collection.dropIndex()` method takes the following
   parameter:

   .. include:: /reference/method/db.collection.dropIndex-param.rst

See :doc:`/administration/indexes` for information. To view all
indexes on a collection, use the
:method:`db.collection.getIndexes()` method.

Example
-------

   The following example uses the :method:`db.collection.dropIndex()`
   method on the collection ``pets`` that has the following indexes:

   .. code-block:: javascript

      > db.pets.getIndexes()
      [
         {  "v" : 1,
            "key" : { "_id" : 1 },
            "ns" : "test.pets",
            "name" : "_id_"
         },
         {
            "v" : 1,
            "key" : { "cat" : -1 },
            "ns" : "test.pets",
            "name" : "catIdx"
         },
         {
            "v" : 1,
            "key" : { "cat" : 1, "dog" : -1 },
            "ns" : "test.pets",
            "name" : "cat_1_dog_-1"
         }
      ]

   The index on the field ``cat`` has the
   user-specified name of ``catIdx`` [#index-name]_. To drop the index
   ``catIdx``, you can use either the index name:

   .. code-block:: javascript

      db.pets.dropIndex( "catIdx" )

   or the index specification document ``{ "cat" : 1 }``:

   .. code-block:: javascript

      db.pets.dropIndex( { "cat" : 1 } )

   .. [#version-changed] When using a :program:`mongo` shell version
      earlier than 2.2.2, if you specified a name during the index
      creation, you must use the name to drop the index.

   .. [#index-name] During index creation, if the user does **not**
      specify an index name, the system generates the name by
      concatenating the index key field and value with an underscore,
      e.g. ``cat_1``.
===========================
db.collection.dropIndexes()
===========================

.. default-domain:: mongodb

.. method:: db.collection.dropIndexes()

   Drops all indexes other than the required index on the ``_id``
   field. Only call :method:`~db.collection.dropIndexes()` as a method on a
   collection object.
===========================
db.collection.ensureIndex()
===========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.ensureIndex(keys, options)

   Creates an index on the specified field if the index does not
   already exist.

   The :method:`~db.collection.ensureIndex()` method has the following fields:

   .. include:: /reference/method/db.collection.ensureIndex-param.rst

   .. warning:: Index names, including their full namespace
      (i.e. ``database.collection``) cannot be longer than 128
      characters. See the :method:`~db.collection.getIndexes()` field
      :data:`~system.indexes.name` for the names of existing indexes.

   The ``options`` document has one or more of the following fields:

   .. include:: /reference/method/db.collection.ensureIndex-options-param.rst


Behaviors
---------

The :method:`~db.collection.ensureIndex()` method has the behaviors
described here.

- To add or change index options you must drop the index using the
  :method:`~db.collection.dropIndex()` method and issue another
  :method:`~db.collection.ensureIndex()` operation
  with the new options.

  If you create an index with one set of options, and then issue
  the :method:`~db.collection.ensureIndex()` method
  with the same index fields and different options without
  first dropping the index,
  :method:`~db.collection.ensureIndex()` will *not* rebuild the existing
  index with the new options.

- If you call multiple :method:`~db.collection.ensureIndex()`
  methods with the same index specification at the same time, only
  the first operation will succeed, all other operations will have
  no effect.

- Non-background indexing operations will block all other
  operations on a database.

-  .. include:: /includes/fact-index-key-length-operation-behaviors.rst
      :start-after: index-field-limit-ensureIndex
      :end-before: .. index-field-limit-reIndex

   .. versionchanged:: 2.6

   .. |limit| replace:: :limit:`Maximum Index Key Length <Index Key>`

Examples
--------

Create an Ascending Index on a Single Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example creates an ascending index on the field
``orderDate``.

.. code-block:: javascript

   db.collection.ensureIndex( { orderDate: 1 } )

If the ``keys`` document specifies more than one field, then
:method:`~db.collection.ensureIndex()` creates a :term:`compound
index`.

Create an Index on a Multiple Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example creates a compound index on the
``orderDate`` field (in ascending order) and the ``zipcode``
field (in descending order.)

.. code-block:: javascript

   db.collection.ensureIndex( { orderDate: 1, zipcode: -1 } )

A compound index cannot include a :ref:`hashed index <index-type-hashed>`
component.

.. note::

   The order of an index is important for supporting
   :method:`~cursor.sort()` operations using the index.

.. seealso::

   - The :doc:`/indexes` section of this manual for full
     documentation of indexes and indexing in MongoDB.

   - :doc:`/core/index-text` for details on creating ``text``
     indexes.

   - :ref:`index-feature-geospatial` and
     :ref:`index-geohaystack-index` for geospatial queries.

   - :ref:`index-feature-ttl` for expiration of data.
====================
db.collection.find()
====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.find( <criteria>, <projection>)

   Selects documents
   in a collection and returns a :term:`cursor` to the selected
   documents. [#formal-query-structure]_

   .. include:: /reference/method/db.collection.find-param.rst

   :returns:

      A :term:`cursor` to the documents that match the ``query``
      criteria. When the :method:`find() <db.collection.find()>` method
      "returns documents," the method is actually returning a cursor to
      the documents.

      If the ``projection`` argument is specified, the
      matching documents contain only the ``projection`` fields and
      the ``_id`` field. You can optionally exclude the ``_id``
      field.

      Executing :method:`find() <db.collection.find()>` directly in the
      :program:`mongo` shell automatically iterates the cursor to
      display up to the first 20 documents. Type ``it`` to continue
      iteration.

      To access the returned documents with a driver, use the
      appropriate cursor handling mechanism for the :doc:`driver
      language </applications/drivers>`.

The ``projection`` parameter takes a document of the following form:

.. code-block:: javascript

   { field1: <boolean>, field2: <boolean> ... }

The ``<boolean>`` value can be any of the following:

- ``1`` or ``true`` to include the field. The :method:`find()
  <db.collection.find()>` method always includes the :term:`_id` field
  even if the field is not explicitly stated to return in the
  :term:`projection` parameter.

- ``0`` or ``false`` to exclude the field.

A ``projection`` *cannot* contain *both* include and exclude
specifications, except for the exclusion of the ``_id`` field. In
projections that *explicitly include* fields, the ``_id`` field is the
only field that you can *explicitly exclude*.

.. [#formal-query-structure] :method:`db.collection.find()` is a
   wrapper for the more formal query structure that uses the
   :operator:`$query` operator.

Examples
--------

Find All Documents in a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :method:`find() <db.collection.find()>` method with no parameters
returns all documents from a collection and returns all fields for the
documents. For example, the following operation returns all documents in
the :doc:`bios collection </reference/bios-example-collection>`:

.. code-block:: javascript

   db.bios.find()

Find Documents that Match Query Criteria
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To find documents that match a set of selection criteria, call
:method:`~ db.collection.find()` with the ``<criteria>``
parameter. The following operation returns all the documents from the
collection ``products`` where ``qty`` is greater than ``25``:

.. code-block:: javascript

   db.products.find( { qty: { $gt: 25 } } )

Query for Equality
``````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where ``_id`` equals ``5``:

.. code-block:: javascript

   db.bios.find( { _id: 5 } )

Query Using Operators
`````````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where ``_id`` equals either ``5``
or ``ObjectId("507c35dd8fada716c89d0013")``:

.. code-block:: javascript

   db.bios.find(
      {
         _id: { $in: [ 5,  ObjectId("507c35dd8fada716c89d0013") ] }
      }
   )

Query for Ranges
````````````````

Combine comparison operators to specify ranges. The following operation
returns documents with ``field`` between ``value1`` and ``value2``:

.. code-block:: javascript

   db.collection.find( { field: { $gt: value1, $lt: value2 } } );

Query a Field that Contains an Array
````````````````````````````````````

If a field contains an array and your query has multiple conditional
operators, the field as a whole will match if either a single array
element meets the conditions or a combination of array elements meet the
conditions.

Given a collection ``students`` that contains the following documents:

.. code-block:: javascript

   { "_id" : 1, "score" : [ -1, 3 ] }
   { "_id" : 2, "score" : [ 1, 5 ] }
   { "_id" : 3, "score" : [ 5, 5 ] }

The following query:

.. code-block:: javascript

   db.students.find( { score: { $gt: 0, $lt: 2 } } )

Matches the following documents:

.. code-block:: javascript

   { "_id" : 1, "score" : [ -1, 3 ] }
   { "_id" : 2, "score" : [ 1, 5 ] }

In the document with ``_id`` equal to ``1``, the ``score: [ -1, 3 ]``
meets the conditions because the element ``-1`` meets the ``$lt: 2``
condition and the element ``3`` meets the ``$gt: 0`` condition.

In the document with ``_id`` equal to ``2``, the ``score: [ 1, 5 ]``
meets the conditions because the element ``1`` meets both the ``$lt: 2``
condition and the ``$gt: 0`` condition.

Query Arrays
~~~~~~~~~~~~

Query for an Array Element
``````````````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where the array field ``contribs``
contains the element ``"UNIX"``:

.. code-block:: javascript

   db.bios.find( { contribs: "UNIX" } )

Query an Array of Documents
```````````````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where ``awards`` array contains a
subdocument element that contains the ``award`` field equal to ``"Turing
Award"`` and the ``year`` field greater than 1980:

.. code-block:: javascript

   db.bios.find(
      {
         awards: {
                   $elemMatch: {
                        award: "Turing Award",
                        year: { $gt: 1980 }
                   }
         }
      }
   )

.. _query-subdocuments:

Query Subdocuments
~~~~~~~~~~~~~~~~~~

Query Exact Matches on Subdocuments
```````````````````````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where the subdocument ``name`` is
*exactly* ``{ first: "Yukihiro", last: "Matsumoto" }``, including the
order:

.. code-block:: javascript

   db.bios.find(
       {
         name: {
                 first: "Yukihiro",
                 last: "Matsumoto"
               }
       }
   )

The ``name`` field must match the sub-document exactly. The query does
**not** match documents with the following ``name`` fields:

.. code-block:: javascript

   {
      first: "Yukihiro",
      aka: "Matz",
      last: "Matsumoto"
   }

   {
      last: "Matsumoto",
      first: "Yukihiro"
   }

Query Fields of a Subdocument
`````````````````````````````

The following operation returns documents in the :doc:`bios collection
</reference/bios-example-collection>` where the subdocument ``name``
contains a field ``first`` with the value ``"Yukihiro"`` and a field
``last`` with the value ``"Matsumoto"``. The query uses :term:`dot
notation` to access fields in a subdocument:

.. code-block:: javascript

   db.bios.find(
      {
        "name.first": "Yukihiro",
        "name.last": "Matsumoto"
      }
   )

The query matches the document where the ``name`` field contains a
subdocument with the field ``first`` with the value ``"Yukihiro"`` and a
field ``last`` with the value ``"Matsumoto"``. For instance, the query
would match documents with ``name`` fields that held either of the
following values:

.. code-block:: javascript

   {
     first: "Yukihiro",
     aka: "Matz",
     last: "Matsumoto"
   }

   {
     last: "Matsumoto",
     first: "Yukihiro"
   }

Projections
~~~~~~~~~~~

The ``projection`` parameter specifies which fields to return. The
parameter contains either include or exclude specifications, not both,
unless the exclude is for the ``_id`` field.

Specify the Fields to Return
````````````````````````````

The following operation returns all the documents from the ``products``
collection where ``qty`` is greater than ``25`` and returns only the
``_id``, ``item`` and ``qty`` fields:

.. code-block:: javascript

   db.products.find( { qty: { $gt: 25 } }, { item: 1, qty: 1 } )

The operation returns the following:

.. code-block:: javascript

   { "_id" : 11, "item" : "pencil", "qty" : 50 }
   { "_id" : ObjectId("50634d86be4617f17bb159cd"), "item" : "bottle", "qty" : 30 }
   { "_id" : ObjectId("50634dbcbe4617f17bb159d0"), "item" : "paper", "qty" : 100 }

The following operation finds all documents in the :doc:`bios collection
</reference/bios-example-collection>` and returns only the ``name``
field, ``contribs`` field and ``_id`` field:

.. code-block:: javascript

   db.bios.find( { }, { name: 1, contribs: 1 } )

Explicitly Excluded Fields
``````````````````````````

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` and returns all fields *except*
the the ``first`` field in the ``name`` subdocument and the ``birth``
field:

.. code-block:: javascript

   db.bios.find(
      { contribs: 'OOP' },
      { 'name.first': 0, birth: 0 }
   )

Explicitly Exclude the ``_id`` Field
````````````````````````````````````

The following operation excludes the ``_id`` and ``qty`` fields from the
result set:

.. code-block:: javascript

   db.products.find( { qty: { $gt: 25 } }, { _id: 0, qty: 0 } )

The documents in the result set contain all fields *except* the ``_id``
and ``qty`` fields:

.. code-block:: javascript

   { "item" : "pencil", "type" : "no.2" }
   { "item" : "bottle", "type" : "blue" }
   { "item" : "paper" }

The following operation finds documents in the :doc:`bios collection
</reference/bios-example-collection>` and returns only the ``name``
field and the ``contribs`` field:

.. code-block:: javascript

   db.bios.find(
      { },
      { name: 1, contribs: 1, _id: 0 }
   )

On Arrays and Subdocuments
``````````````````````````

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` and returns the ``last`` field in
the ``name`` subdocument and the first two elements in the ``contribs``
array:

.. code-block:: javascript

   db.bios.find(
      { },
      {
        _id: 0,
        'name.last': 1,
        contribs: { $slice: 2 }
      }
   )

.. _crud-read-cursor:

Iterate the Returned Cursor
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :method:`~db.collection.find()` method returns a :term:`cursor` to
the results. In the :program:`mongo` shell, if the returned cursor is
not assigned to a variable using the ``var`` keyword, the cursor is
automatically iterated up to 20 times to access up to the first 20
documents that match the query. You can use the
``DBQuery.shellBatchSize`` to change the number of iterations. See
:ref:`cursor-flags` and :ref:`cursor-behaviors`. To iterate manually,
assign the returned cursor to a variable using the ``var`` keyword.

With Variable Name
``````````````````

The following example uses the variable ``myCursor`` to iterate over the
cursor and print the matching documents:

.. code-block:: javascript

   var myCursor = db.bios.find( );

   myCursor

With ``next()`` Method
``````````````````````

The following example uses the cursor method :method:`~cursor.next()` to
access the documents:

.. code-block:: javascript

   var myCursor = db.bios.find( );

   var myDocument = myCursor.hasNext() ? myCursor.next() : null;

   if (myDocument) {
       var myName = myDocument.name;
       print (tojson(myName));
   }

To print, you can also use the ``printjson()`` method instead of
``print(tojson())``:

.. code-block:: javascript

   if (myDocument) {
      var myName = myDocument.name;
      printjson(myName);
   }

With ``forEach()`` Method
`````````````````````````

The following example uses the cursor method :method:`~cursor.forEach()`
to iterate the cursor and access the documents:

.. code-block:: javascript

   var myCursor = db.bios.find( );

   myCursor.forEach(printjson);

Modify the Cursor Behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~

The :program:`mongo` shell and the :doc:`drivers
</applications/drivers>` provide several cursor methods that call on the
*cursor* returned by the :method:`~db.collection.find()` method to
modify its behavior.

Order Documents in the Result Set
`````````````````````````````````

The :method:`~cursor.sort()` method orders the documents in the result
set. The following operation returns documents in the :doc:`bios
collection </reference/bios-example-collection>` sorted in ascending
order by the ``name`` field:

.. code-block:: javascript

   db.bios.find().sort( { name: 1 } )

:method:`~cursor.sort()` corresponds to the ``ORDER BY``
statement in SQL.

Limit the Number of Documents to Return
```````````````````````````````````````

The :method:`~cursor.limit()` method limits the number of documents in
the result set. The following operation returns at most ``5`` documents
in the :doc:`bios collection </reference/bios-example-collection>`:

.. code-block:: javascript

   db.bios.find().limit( 5 )

:method:`~cursor.limit()` corresponds to the ``LIMIT``
statement in SQL.

Set the Starting Point of the Result Set
````````````````````````````````````````

The :method:`~cursor.skip()` method controls the starting point of the
results set. The following operation skips the first ``5`` documents in
the :doc:`bios collection </reference/bios-example-collection>` and
returns all remaining documents:

.. code-block:: javascript

   db.bios.find().skip( 5 )

Combine Cursor Methods
``````````````````````

The following example chains cursor methods:

.. code-block:: javascript

   db.bios.find().sort( { name: 1 } ).limit( 5 )
   db.bios.find().limit( 5 ).sort( { name: 1 } )

Regardless of the order you chain the :method:`~cursor.limit()` and the
:method:`~cursor.sort()`, the request to the server has the structure
that treats the query and the :method:`~cursor.sort()` modifier as a
single object. Therefore, the :method:`~cursor.limit()` operation method
is always applied after the :method:`~cursor.sort()` regardless of the
specified order of the operations in the chain. See the :doc:`meta query
operators </reference/operator/query-modifier>`.
=============================
db.collection.findAndModify()
=============================

.. default-domain:: mongodb

.. EDITS to db.collection.findAndModify.txt must be carried over
   (possibly w modifications) to the command findAndModify.txt and vice
   versa

Definition
----------

.. method:: db.collection.findAndModify( <document> )

   Atomically
   modifies and returns a single document. By default, the returned
   document does not include the modifications made on the update. To
   return the document with the modifications made on the update, use
   the ``new`` option. The :method:`~db.collection.findAndModify()` method is a shell
   helper around the :dbcommand:`findAndModify` command.

   The :method:`~db.collection.findAndModify()` method has the following
   form:

   .. code-block:: none

      db.collection.findAndModify({
          query: <document>,
          sort: <document>,
          remove: <boolean>,
          update: <document>,
          new: <boolean>,
          fields: <document>,
          upsert: <boolean>
      });

   The :method:`db.collection.findAndModify()` method takes a document
   parameter with the following subdocument fields:

   .. include:: /reference/method/db.collection.findAndModify-param.rst

Return Data
-----------

The :method:`~db.collection.findAndModify()` method returns either:
the pre-modification document or, if ``new: true`` is set, the
modified document.

.. note::

   - If the query finds no document for ``update`` or ``remove``
     operations, :method:`~db.collection.findAndModify()` returns
     ``null``.

   - If the query finds no document for an ``upsert``, operation,
     :method:`~db.collection.findAndModify()` performs an insert. If
     ``new`` is ``false``, **and** the ``sort`` option is **NOT**
     specified, the method returns ``null``.

     .. versionchanged:: 2.2
        Previously returned an empty document ``{}``. See :ref:`the
        2.2 release notes <2.2-findandmodify-returns-null>` for more
        information.

   - If the query finds no document for an ``upsert``,
     :method:`~db.collection.findAndModify()` performs an insert. If
     ``new`` is ``false``, **and** a ``sort`` option, the method
     returns an empty document ``{}``.

Behaviors
---------

.. _upsert-and-unique-index:

Upsert and Unique Index
~~~~~~~~~~~~~~~~~~~~~~~

When :method:`~db.collection.findAndModify()` includes the ``upsert:
true`` option **and** the query field(s) is not uniquely indexed, the
method could insert a document multiple times in certain circumstances.
For instance, if multiple clients each invoke the method with the same
``query`` condition and these methods complete the ``find`` phase
before any of methods perform the ``modify`` phase, these methods could
insert the same document.

In the following example, no document with the name ``Andy`` exists,
and multiple clients issue the following command:

.. code-block:: javascript

   db.people.findAndModify({
       query: { name: "Andy" },
       sort: { rating: 1 },
       update: { $inc: { score: 1 } },
       upsert: true
   })

Then, if these clients' :method:`~db.collection.findAndModify()`
methods finish the ``query`` phase before any command starts the
``modify`` phase, **and** there is no unique index on the ``name``
field, the commands may all perform an upsert. To prevent this
condition, create a :ref:`unique index <index-type-unique>` on the
``name`` field. With the unique index in place, the multiple methods
would observe one of the following behaviors:

- Exactly one :method:`~db.collection.findAndModify()` would
  successfully insert a new document.

- Zero or more :method:`~db.collection.findAndModify()` methods
  would update the newly inserted document.

- Zero or more :method:`~db.collection.findAndModify()` methods
  would fail when they attempted to insert a duplicate. If the
  method fails due to a unique index constraint violation, you can
  retry the method. Absent a delete of the document, the retry
  should not fail.

Sharded Collections
~~~~~~~~~~~~~~~~~~~

When using :dbcommand:`findAndModify` in a :term:`sharded <sharding>`
environment, the ``query`` **must** contain the :term:`shard key` for
all operations against the shard cluster for the *sharded* collections.

:dbcommand:`findAndModify` operations issued against :program:`mongos`
instances for *non-sharded* collections function normally.

Examples
--------

Update and Return
~~~~~~~~~~~~~~~~~

The following method updates and returns an existing document in the
people collection where the document matches the query criteria:

.. code-block:: javascript

   db.people.findAndModify({
       query: { name: "Tom", state: "active", rating: { $gt: 10 } },
       sort: { rating: 1 },
       update: { $inc: { score: 1 } }
   })

This method performs the following actions:

#. The ``query`` finds a document in the ``people`` collection
   where the ``name`` field has the value ``Tom``, the ``state``
   field has the value ``active`` and the ``rating`` field has a
   value :operator:`greater than <$gt>` 10.

#. The ``sort`` orders the results of the query in ascending order.
   If multiple documents meet the ``query`` condition, the method
   will select for modification the first document as ordered by
   this ``sort``.

#. The update :operator:`increments <$inc>` the value of the
   ``score`` field by 1.

#. The method returns the original (i.e. pre-modification) document
   selected for this update:

   .. code-block:: javascript

      {
        "_id" : ObjectId("50f1e2c99beb36a0f45c6453"),
        "name" : "Tom",
        "state" : "active",
        "rating" : 100,
        "score" : 5
      }

   To return the modified document, add the ``new:true`` option to
   the method.

   If no document matched the ``query`` condition, the method
   returns ``null``:

   .. code-block:: javascript

      null

Update and Insert
~~~~~~~~~~~~~~~~~

The following method includes the ``upsert: true`` option to
insert a new document if no document matches the ``query``
condition:

.. code-block:: javascript

   db.people.findAndModify({
       query: { name: "Gus", state: "active", rating: 100 },
       sort: { rating: 1 },
       update: { $inc: { score: 1 } },
       upsert: true
   })

If the method does **not** find a matching document, the method
performs an upsert. Because the method included the ``sort``
option, it returns an empty document ``{ }`` as the original
(pre-modification) document:

.. code-block:: javascript

   { }

If the method did **not** include a ``sort`` option, the method returns
``null``.

.. code-block:: javascript

   null

Update, Insert and Return New Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following method includes both the ``upsert: true`` option and
the ``new:true`` option to return the newly inserted document if a
document matching the ``query`` is not found:

.. code-block:: none

   db.people.findAndModify({
       query: { name: "Pascal", state: "active", rating: 25 },
       sort: { rating: 1 },
       update: { $inc: { score: 1 } },
       upsert: true,
       new: true
   })

The method returns the newly inserted document:

.. code-block:: javascript

   {
      "_id" : ObjectId("50f49ad6444c11ac2448a5d6"),
      "name" : "Pascal",
      "rating" : 25,
      "score" : 1,
      "state" : "active"
   }

.. _findAndModify-wrapper-sorted-remove:

Sort and Remove
~~~~~~~~~~~~~~~

By including a ``sort`` specification on the ``rating`` field, the
following example removes from the ``people`` collection a single
document with the ``state`` value of ``active`` and the lowest
``rating`` among the matching documents:

.. code-block:: javascript

   db.people.findAndModify(
      {
        query: { state: "active" },
        sort: { rating: 1 },
        remove: true
      }
   )

The method returns the deleted document:

.. code-block:: javascript

   {
      "_id" : ObjectId("52fba867ab5fdca1299674ad"),
      "name" : "XYZ123",
      "score" : 1,
      "state" : "active",
      "rating" : 3
   }
=======================
db.collection.findOne()
=======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.findOne( <criteria>, <projection> )

   Returns one document that satisfies the specified query
   criteria. If multiple documents satisfy the query, this method
   returns the first document according to the :term:`natural order`
   which reflects the order of documents on the disk. In :term:`capped
   collections <capped collection>`, natural order is the same as
   insertion order.

   .. include:: /reference/method/db.collection.findOne-param.rst

The ``projection`` parameter takes a document of the following form:

.. code-block:: javascript

   { field1: <boolean>, field2: <boolean> ... }

The ``<boolean>`` can be one of the following include or exclude values:

- ``1`` or ``true`` to include. The :method:`~db.collection.findOne()`
  method always includes the :term:`_id` field even if the field is not
  explicitly specified in the :term:`projection` parameter.

- ``0`` or ``false`` to exclude.

The projection argument cannot mix include and exclude
specifications, with the exception of excluding the ``_id`` field.

   :returns:

      One document that satisfies the criteria specified as the first
      argument to this method. If you specify a ``projection``
      parameter, :method:`~db.collection.findOne()` returns a document
      that only contains the ``projection`` fields. The ``_id``
      field is always included unless you explicitly exclude it.

      Although similar to the :method:`~db.collection.find()` method,
      the :method:`~db.collection.findOne()` method returns a document
      rather than a cursor.

Examples
--------

With Empty Query Specification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns a single document from
the :doc:`bios collection </reference/bios-example-collection>`:

.. code-block:: javascript

   db.bios.findOne()

With a Query Specification
~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation returns the first matching document from the
:doc:`bios collection </reference/bios-example-collection>` where either
the field ``first`` in the subdocument ``name`` starts with the letter
``G`` **or** where the field ``birth`` is less than ``new
Date('01/01/1945')``:

.. code-block:: javascript

   db.bios.findOne(
      {
        $or: [
               { 'name.first' : /^G/ },
               { birth: { $lt: new Date('01/01/1945') } }
             ]
      }
   )

With a Projection
~~~~~~~~~~~~~~~~~

The ``projection`` parameter specifies which fields to return. The
parameter contains either include or exclude specifications, not both,
unless the exclude is for the ``_id`` field.

Specify the Fields to Return
````````````````````````````

The following operation finds a document in the :doc:`bios collection
</reference/bios-example-collection>` and returns only the ``name``,
``contribs`` and ``_id`` fields:

.. code-block:: javascript

   db.bios.findOne(
       { },
       { name: 1, contribs: 1 }
   )

Return All but the Excluded Fields
``````````````````````````````````

The following operation returns a document in the :doc:`bios collection
</reference/bios-example-collection>` where the ``contribs`` field
contains the element ``OOP`` and returns all fields *except* the ``_id``
field, the ``first`` field in the ``name`` subdocument, and the
``birth`` field:

.. code-block:: javascript

   db.bios.findOne(
      { contribs: 'OOP' },
      { _id: 0, 'name.first': 0, birth: 0 }
   )

The ``findOne`` Result Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You cannot apply cursor methods to the result of
:method:`~db.collection.findOne()` because a single document is
returned. You have access to the document directly:

.. code-block:: javascript

   var myDocument = db.bios.findOne();

   if (myDocument) {
      var myName = myDocument.name;

      print (tojson(myName));
   }
==========================
db.collection.getIndexes()
==========================

.. default-domain:: mongodb

.. method:: db.collection.getIndexes()

   Returns an array that holds a list of documents that identify and
   describe the existing indexes on the collection. You must call the
   :method:`db.collection.getIndexes()` on a collection. For example:

   .. code-block:: javascript

      db.collection.getIndexes()

   Change ``collection`` to the name of the collection whose indexes
   you want to learn.

   The :method:`db.collection.getIndexes()` items consist of the following fields:

   .. data:: system.indexes.v

      Holds the version of the index.

      The index version depends on the version of :program:`mongod`
      that created the index. Before version 2.0 of MongoDB, the this
      value was 0; versions 2.0 and later use version 1.

   .. data:: system.indexes.key

      Contains a document holding the keys held in the index, and the
      order of the index. Indexes may be either descending or
      ascending order. A value of negative one (e.g. ``-1``)
      indicates an index sorted in descending order while a positive
      value (e.g. ``1``) indicates an index sorted in an ascending
      order.

   .. data:: system.indexes.ns

      The namespace context for the index.

   .. data:: system.indexes.name

      A unique name for the index comprised of the field names and
      orders of all keys.
=============================
db.collection.getIndexStats()
=============================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.getIndexStats(index)

   Displays a human-readable summary of aggregated statistics about an
   index's B-tree data structure. The information summarizes the
   output returned by the :dbcommand:`indexStats` command and
   :method:`~db.collection.indexStats()` method. The
   :method:`~db.collection.getIndexStats()` method
   displays the information on the screen and does not return an object.

   The :method:`~db.collection.getIndexStats()` method has the
   following form:

   .. code-block:: javascript

      db.<collection>.getIndexStats( { index : "<index name>" } )

   .. include:: /reference/method/db.collection.getIndexStats-param.rst

   The :method:`~db.collection.getIndexStats()` method is available only
   when connected to a :program:`mongod` instance that
   uses the :option:`--enableExperimentalIndexStatsCmd
   <mongod --enableExperimentalIndexStatsCmd>` option.

   To view :ref:`index names <index-names>` for a collection, use the
   :method:`~db.collection.getIndexes()` method.

   .. warning:: Do not use :method:`~db.collection.getIndexStats()`  or
     :dbcommand:`indexStats` with production deployments.

Example
-------

The following command returns information for an index named
``type_1_traits_1``:

   .. code-block:: javascript

      db.animals.getIndexStats({index:"type_1_traits_1"})

The command returns the following summary. For more information on the
B-tree statistics, see :dbcommand:`indexStats`.

.. code-block:: none

   -- index "undefined" --
     version 1 | key pattern {  "type" : 1,  "traits" : 1 } | storage namespace "test.animals.$type_1_traits_1"
     2 deep, bucket body is 8154 bytes

     bucket count	45513	on average 99.401 % (±0.463 %) full	49.581 % (±4.135 %) bson keys, 49.820 % (±4.275 %) key nodes

     -- depth 0 --
       bucket count	1	on average 71.511 % (±0.000 %) full	36.191 % (±0.000 %) bson keys, 35.320 % (±0.000 %) key nodes

     -- depth 1 --
       bucket count	180	on average 98.954 % (±5.874 %) full	49.732 % (±5.072 %) bson keys, 49.221 % (±5.161 %) key nodes

     -- depth 2 --
       bucket count	45332	on average 99.403 % (±0.245 %) full	49.580 % (±4.130 %) bson keys, 49.823 % (±4.270 %) key nodes
============================
db.collection.getPlanCache()
============================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.getPlanCache()

   Returns an interface to access the query plan cache for a
   collection. The interface provides methods to view and clear the
   query plan cache.

   :returns: Interface to access the query plan cache.

   .. include:: /includes/fact-query-optimizer-cache-behavior.rst

Methods
-------

The following methods are available through the interface:

.. include:: /includes/toc/table-spec-plan-cache-methods.rst
====================================
db.collection.getShardDistribution()
====================================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.getShardDistribution()

   :returns:

      Prints the data distribution statistics for a :term:`sharded
      <sharding>` collection. You must call the
      :method:`~db.collection.getShardDistribution()` method on a
      sharded collection, as in the following example:

      .. code-block:: javascript

         db.myShardedCollection.getShardDistribution()

   In the following example, the collection has two shards. The output
   displays both the individual shard distribution information as well
   the total shard distribution:

   .. code-block:: none

      Shard <shard-a> at <host-a>
       data : <size-a> docs : <count-a> chunks : <number of chunks-a>
       estimated data per chunk : <size-a>/<number of chunks-a>
       estimated docs per chunk : <count-a>/<number of chunks-a>

      Shard <shard-b> at <host-b>
       data : <size-b> docs : <count-b> chunks : <number of chunks-b>
       estimated data per chunk : <size-b>/<number of chunks-b>
       estimated docs per chunk : <count-b>/<number of chunks-b>

      Totals
       data : <stats.size> docs : <stats.count> chunks : <calc total chunks>
       Shard <shard-a> contains  <estDataPercent-a>% data, <estDocPercent-a>% docs in cluster, avg obj size on shard : stats.shards[ <shard-a> ].avgObjSize
       Shard <shard-b> contains  <estDataPercent-b>% data, <estDocPercent-b>% docs in cluster, avg obj size on shard : stats.shards[ <shard-b> ].avgObjSize

.. seealso:: :doc:`/sharding`

.. COMMENT -- not sure if I should mention the source of the data:
   ``shards`` collection and the ``chunks`` collection in the ``config`` database
   and from the :method:`db.collection.stats()` method.

Output
------

The output information displays:

- ``<shard-x>`` is a string that holds the shard name.

- ``<host-x>`` is a string that holds the host name(s).

- ``<size-x>`` is a number that includes the size of the data,
  including the unit of measure (e.g. ``b``, ``Mb``).

- ``<count-x>`` is a number that reports the number of
  documents in the shard.

- ``<number of chunks-x>`` is a number that reports the
  number of chunks in the shard.

- ``<size-x>/<number of chunks-x>`` is a calculated value
  that reflects the estimated data size per chunk for the shard,
  including the unit of measure (e.g. ``b``, ``Mb``).

- ``<count-x>/<number of chunks-x>`` is a calculated value
  that reflects the estimated number of documents per chunk for the
  shard.

- ``<stats.size>`` is a value that reports the total size of
  the data in the sharded collection, including the unit of measure.

- ``<stats.count>`` is a value that reports the total number
  of documents in the sharded collection.

- ``<calc total chunks>`` is a calculated number that reports the
  number of chunks from all shards, for example:

  .. code-block:: sh

     <calc total chunks> = <number of chunks-a> + <number of chunks-b>

- ``<estDataPercent-x>`` is a calculated value that
  reflects, for each shard, the data size as the percentage of the
  collection's total data size, for example:

  .. code-block:: sh

     <estDataPercent-x> = <size-x>/<stats.size>

- ``<estDocPercent-x>`` is a calculated value that
  reflects, for each shard, the number of documents as the
  percentage of the total number of documents for the collection,
  for example:

  .. code-block:: sh

     <estDocPercent-x> = <count-x>/<stats.count>

- ``stats.shards[ <shard-x> ].avgObjSize`` is a number that
  reflects the average object size, including the unit of measure,
  for the shard.

Example Output
--------------

For example, the following is a sample output for the distribution
of a sharded collection:

.. code-block:: sh

   Shard shard-a at shard-a/MyMachine.local:30000,MyMachine.local:30001,MyMachine.local:30002
   data : 38.14Mb docs : 1000003 chunks : 2
   estimated data per chunk : 19.07Mb
   estimated docs per chunk : 500001

   Shard shard-b at shard-b/MyMachine.local:30100,MyMachine.local:30101,MyMachine.local:30102
   data : 38.14Mb docs : 999999 chunks : 3
   estimated data per chunk : 12.71Mb
   estimated docs per chunk : 333333

   Totals
   data : 76.29Mb docs : 2000002 chunks : 5
   Shard shard-a contains 50% data, 50% docs in cluster, avg obj size on shard : 40b
   Shard shard-b contains 49.99% data, 49.99% docs in cluster, avg obj size on shard : 40b
===============================
db.collection.getShardVersion()
===============================

.. default-domain:: mongodb

.. method:: db.collection.getShardVersion()

   This method returns information regarding the state of data in a
   :term:`sharded cluster` that is useful when diagnosing underlying issues
   with a sharded cluster.

   For internal and diagnostic use only.
=====================
db.collection.group()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.group( { key, reduce, initial, [keyf,] [cond,] finalize } )

   Groups documents in a
   collection by the specified keys and performs simple aggregation
   functions such as computing counts and sums. The method is analogous
   to a ``SELECT <...> GROUP BY`` statement in SQL. The :method:`group()
   <db.collection.group()>` method returns an array.

   The :method:`db.collection.group()` accepts a single :term:`document` that
   contains the following:

   .. |obj-name| replace:: :method:`db.collection.group()`

   .. include:: /reference/method/db.collection.group-param.rst

   The :method:`db.collection.group()` method is a shell wrapper for
   the :dbcommand:`group` command. However, the
   :method:`db.collection.group()` method takes the ``keyf`` field and
   the ``reduce`` field whereas the :dbcommand:`group` command takes
   the ``$keyf`` field and the ``$reduce`` field.

Behavior
--------

Limits and Restrictions
~~~~~~~~~~~~~~~~~~~~~~~

The :method:`db.collection.group()` method does not work with
:term:`sharded clusters <sharded cluster>`. Use the :term:`aggregation
framework` or :term:`map-reduce` in :term:`sharded environments
<sharding>`.

The result set must fit within the :ref:`maximum BSON document size
<limit-bson-document-size>`.

In version 2.2, the returned array can contain at most 20,000 elements;
i.e. at most 20,000 unique groupings. For group by operations that
results in more than 20,000 unique groupings, use
:dbcommand:`mapReduce`. Previous versions had a limit of 10,000
elements.

Prior to 2.4, the :method:`db.collection.group()` method took the
:program:`mongod` instance's JavaScript lock, which blocked all other
JavaScript execution.

``mongo`` Shell JavaScript Functions/Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.4

   .. include:: /includes/fact-group-map-reduce-where-limitations-in-24.rst

Examples
--------

The following examples assume an ``orders`` collection with documents of
the following prototype:

.. code-block:: javascript

   {
     _id: ObjectId("5085a95c8fada716c89d0021"),
     ord_dt: ISODate("2012-07-01T04:00:00Z"),
     ship_dt: ISODate("2012-07-02T04:00:00Z"),
     item: { sku: "abc123",
             price: 1.99,
             uom: "pcs",
             qty: 25 }
   }

Group by Two Fields
~~~~~~~~~~~~~~~~~~~

The following example groups by the ``ord_dt`` and ``item.sku``
fields those documents that have ``ord_dt`` greater than
``01/01/2011``:

.. code-block:: javascript

   db.orders.group(
      {
        key: { ord_dt: 1, 'item.sku': 1 },
        cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
        reduce: function ( curr, result ) { },
        initial: { }
      }
   )

The result is an array of documents that contain the group by fields:

.. code-block:: javascript

   [
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc123"},
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc456"},
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "bcd123"},
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "efg456"},
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "abc123"},
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "efg456"},
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "ijk123"},
     { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc123"},
     { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc456"},
     { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc123"},
     { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc456"}
   ]

The method call is analogous to the SQL statement:

.. code-block:: sql

   SELECT ord_dt, item_sku
   FROM orders
   WHERE ord_dt > '01/01/2012'
   GROUP BY ord_dt, item_sku

Calculate the Sum
~~~~~~~~~~~~~~~~~

The following example groups by the ``ord_dt`` and ``item.sku``
fields, those documents that have ``ord_dt`` greater than
``01/01/2011`` and calculates the sum of the ``qty`` field for each
grouping:

.. code-block:: javascript

   db.orders.group(
      {
        key: { ord_dt: 1, 'item.sku': 1 },
        cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
        reduce: function( curr, result ) {
                    result.total += curr.item.qty;
                },
        initial: { total : 0 }
      }
   )

The result is an array of documents that contain the group by fields
and the calculated aggregation field:

.. code-block:: javascript

   [ { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "abc456", "total" : 25 },
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "bcd123", "total" : 10 },
     { "ord_dt" : ISODate("2012-07-01T04:00:00Z"), "item.sku" : "efg456", "total" : 10 },
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "efg456", "total" : 15 },
     { "ord_dt" : ISODate("2012-06-01T04:00:00Z"), "item.sku" : "ijk123", "total" : 20 },
     { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc123", "total" : 45 },
     { "ord_dt" : ISODate("2012-05-01T04:00:00Z"), "item.sku" : "abc456", "total" : 25 },
     { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc123", "total" : 25 },
     { "ord_dt" : ISODate("2012-06-08T04:00:00Z"), "item.sku" : "abc456", "total" : 25 } ]

The method call is analogous to the SQL statement:

.. code-block:: sql

   SELECT ord_dt, item_sku, SUM(item_qty) as total
   FROM orders
   WHERE ord_dt > '01/01/2012'
   GROUP BY ord_dt, item_sku


Calculate Sum, Count, and Average
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example groups by the calculated ``day_of_week`` field,
those documents that have ``ord_dt`` greater than ``01/01/2011`` and
calculates the sum, count, and average of the ``qty`` field for each
grouping:

.. code-block:: javascript

   db.orders.group(
      {
        keyf: function(doc) {
                  return { day_of_week: doc.ord_dt.getDay() };
              },
        cond: { ord_dt: { $gt: new Date( '01/01/2012' ) } },
       reduce: function( curr, result ) {
                   result.total += curr.item.qty;
                   result.count++;
               },
       initial: { total : 0, count: 0 },
       finalize: function(result) {
                     var weekdays = [
                          "Sunday", "Monday", "Tuesday",
                          "Wednesday", "Thursday",
                          "Friday", "Saturday"
                         ];
                     result.day_of_week = weekdays[result.day_of_week];
                     result.avg = Math.round(result.total / result.count);
                 }
      }
   )

The result is an array of documents that contain the group by fields
and the calculated aggregation field:

.. code-block:: javascript

   [
     { "day_of_week" : "Sunday", "total" : 70, "count" : 4, "avg" : 18 },
     { "day_of_week" : "Friday", "total" : 110, "count" : 6, "avg" : 18 },
     { "day_of_week" : "Tuesday", "total" : 70, "count" : 3, "avg" : 23 }
   ]

.. seealso:: :doc:`/core/aggregation`

.. STUB :doc:`/applications/simple-aggregation`
==========================
db.collection.indexStats()
==========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.indexStats(index)

   Aggregates statistics for the B-tree data structure that stores data
   for a MongoDB index.
   The :method:`~db.collection.indexStats()` method is a thin wrapper
   around the :dbcommand:`indexStats` command.
   The :method:`~db.collection.indexStats()` method is available only on
   :program:`mongod` instances running with the
   ``--enableExperimentalIndexStatsCmd`` option.

   .. important:: The :method:`~db.collection.indexStats()` method is
      not intended for production deployments.

   The :method:`~db.collection.indexStats()` method has the following
   form:

   .. code-block:: javascript

      db.<collection>.indexStats( { index: "<index name>" } )

   The :method:`~db.collection.indexStats()` method has the following
   parameter:

   .. include:: /reference/method/db.collection.indexStats-param.rst

   The method takes a read lock and pages into memory all the extents, or
   B-tree buckets, encountered. The method might be slow for large indexes
   if the underlying extents are not already in physical memory. Do not run
   :method:`~db.collection.indexStats()` on a :term:`replica set` :term:`primary`. When run
   on a :term:`secondary`, the command causes the secondary to fall behind
   on replication.

   The method aggregates statistics
   for the entire B-tree and for each individual level of the B-tree.
   For a description of the command's output, see
   :doc:`/reference/command/indexStats`.

   For more information about running :method:`~db.collection.indexStats()`, see
   `https://github.com/mongodb-labs/storage-viz#readme <https://github.com/mongodb-labs/storage-viz#readme>`_.
=======================================
db.collection.initializeOrderedBulkOp()
=======================================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.initializeOrderedBulkOp()

   Initializes and returns a new :method:`Bulk()` operations builder
   for a collection. The builder constructs an ordered list of write
   operations that MongoDB executes in bulk. With an *ordered*
   operations list, MongoDB executes the write operations in the list
   serially. If an error occurs during the processing of one of the
   write operations, MongoDB will return without processing any
   remaining write operations in the list.

   :returns: new :method:`Bulk()` operations builder object.

Examples
--------

The following initializes a :method:`Bulk()` operations builder on the
``users`` collection, adds a series of write operations, and executes
the operations:

.. code-block:: javascript

   var bulk = db.users.initializeOrderedBulkOp();
   bulk.insert( { user: "abc123", status: "A", points: 0 } );
   bulk.insert( { user: "ijk123", status: "A", points: 0 } );
   bulk.insert( { user: "mop123", status: "P", points: 0 } );
   bulk.find( { status: "D" } ).remove();
   bulk.find( { status: "P" } ).update( { $set: { comment: "Pending" } } );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeUnorderedBulkOp()`

   - :method:`Bulk.find()`

   - :method:`Bulk.find.removeOne()`

   - :method:`Bulk.execute()`
=========================================
db.collection.initializeUnorderedBulkOp()
=========================================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.initializeUnorderedBulkOp()

   Initializes and returns a new :method:`Bulk()` operations builder
   for a collection. The builder constructs an *unordered* list of
   write operations that MongoDB executes in bulk. With an *unordered*
   operations list, MongoDB can execute in parallel the write
   operations in the list. If an error occurs during the processing of
   one of the write operations, MongoDB will continue to process
   remaining write operations in the list.

   :returns: new :method:`Bulk()` operations builder object.

Examples
--------


The following initializes a :method:`Bulk()` operations builder and
adds a series of insert operations to add multiple documents:

.. code-block:: javascript

   var bulk = db.users.initializeUnorderedBulkOp();
   bulk.insert( { user: "abc123", status: "A", points: 0 } );
   bulk.insert( { user: "ijk123", status: "A", points: 0 } );
   bulk.insert( { user: "mop123", status: "P", points: 0 } );
   bulk.execute();

.. seealso::

   - :method:`db.collection.initializeOrderedBulkOp()`

   - :method:`Bulk()`

   - :method:`Bulk.insert()`

   - :method:`Bulk.execute()`
======================
db.collection.insert()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.insert()

   Inserts a document or documents into a collection.

   The :method:`~db.collection.insert()` method has the following
   syntax:

   .. versionchanged:: 2.6

   .. code-block:: none

      db.collection.insert(
         <document or array of documents>,
         {
           writeConcern: <document>,
           ordered: <boolean>
         }
      )

   .. include:: /reference/method/db.collection.insert-param.rst

   .. versionchanged:: 2.6
      The :method:`~db.collection.insert()` returns an object that
      contains the status of the operation.

   :returns:
      - A :ref:`writeresults-insert` object for single inserts.

      - A :ref:`bulkwriteresults-insert` object for bulk inserts.

Behaviors
---------

.. _insert-safe-writes:

Safe Writes
~~~~~~~~~~~

.. versionchanged:: 2.6

The :method:`~db.collection.insert()` method uses the
:dbcommand:`insert` command, which uses the default write concern. To
specify a different write concern, include the write concern in the
options parameter.

Create Collection
~~~~~~~~~~~~~~~~~

If the collection does not exist, then the
:method:`~db.collection.insert()` method will create the collection.

``_id`` Field
~~~~~~~~~~~~~

If the document does not specify an :term:`_id` field, then MongoDB
will add the ``_id`` field and assign a unique
:doc:`/reference/object-id` for the document before inserting. Most
drivers create an ObjectId and insert the ``_id`` field, but the
:program:`mongod` will create and populate the ``_id`` if the driver or
application does not.

If the document contains an ``_id`` field, the ``_id`` value must be
unique within the collection to avoid duplicate key error.

Examples
--------

The following examples insert documents into the ``products``
collection. If the collection does not exist, the
:method:`~db.collection.insert()` method creates the collection.

Insert a Document without Specifying an ``_id`` Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following example, the document passed to the
:method:`~db.collection.insert()` method does not contain the ``_id``
field:

.. code-block:: javascript

   db.products.insert( { item: "card", qty: 15 } )

During the insert, :program:`mongod` will create the ``_id`` field and
assign it a unique :doc:`/reference/object-id` value, as verified by
the inserted document:

.. code-block:: javascript

   { "_id" : ObjectId("5063114bd386d8fadbd6b004"), "item" : "card", "qty" : 15 }

.. include:: /includes/fact-object-id-may-differ.rst

Insert a Document Specifying an ``_id`` Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following example, the document passed to the
:method:`~db.collection.insert()` method includes the ``_id`` field.
The value of ``_id`` must be unique within the collection to avoid
duplicate key error.

.. code-block:: javascript

   db.products.insert( { _id: 10, item: "box", qty: 20 } )

The operation inserts the following document in the ``products``
collection:

.. code-block:: javascript

   { "_id" : 10, "item" : "box", "qty" : 20 }

Insert Multiple Documents
~~~~~~~~~~~~~~~~~~~~~~~~~

The following example performs a bulk insert of three documents by
passing an array of documents to the :method:`~db.collection.insert()`
method.

The documents in the array do not need to have the same fields. For
instance, the first document in the array has an ``_id`` field and a
``type`` field. Because the second and third documents do not contain
an ``_id`` field, :program:`mongod` will create the ``_id`` field for
the second and third documents during the insert:

.. code-block:: javascript

   db.products.insert(
      [
        { _id: 11, item: "pencil", qty: 50, type: "no.2" },
        { item: "pen", qty: 20 },
        { item: "eraser", qty: 25 }
      ]
   )

The operation inserted the following three documents:

.. code-block:: javascript

   { "_id" : 11, "item" : "pencil", "qty" : 50, "type" : "no.2" }
   { "_id" : ObjectId("51e0373c6f35bd826f47e9a0"), "item" : "pen", "qty" : 20 }
   { "_id" : ObjectId("51e0373c6f35bd826f47e9a1"), "item" : "eraser", "qty" : 25 }

Perform an Ordered Insert
~~~~~~~~~~~~~~~~~~~~~~~~~

The following example performs an *ordered* insert of four documents.
Unlike *unordered* inserts which continue on error, *ordered* inserts
return on error without processing the remaining documents in the array.

.. code-block:: javascript

   db.products.insert(
      [
        { _id: 20, item: "lamp", qty: 50, type: "desk" },
        { _id: 21, item: "lamp", qty: 20, type: "floor" },
        { _id: 22, item: "bulk", qty: 100 }
      ],
      { ordered: true }
   )

Override Default Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation to a replica set specifies a :doc:`write
concern </reference/write-concern>` of ``"w: majority"`` with a
``wtimeout`` of 5000 milliseconds such that the method returns after
the write propagates to a majority of the replica set members or the
method times out after 5 seconds.

.. code-block:: javascript

   db.products.insert(
       { item: "envelopes", qty : 100, type: "Clasp" },
       { writeConcern: { w: "majority", wtimeout: 5000 } }
   )

.. _writeresults-insert:

WriteResult
-----------

.. versionchanged:: 2.6

When passed a single document, :method:`~db.collection.insert()`
returns a ``WriteResult`` object.

Successful Results
~~~~~~~~~~~~~~~~~~

The :method:`~db.collection.insert()` returns a :method:`WriteResult`
object that contains the status of the operation. Upon success, the
:method:`WriteResult` object contains information on the number of
documents inserted:

.. code-block:: javascript

   WriteResult({ "nInserted" : 1 })

Write Concern Errors
~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.insert()` method encounters write
concern errors, the results include the
:data:`WriteResult.writeConcernError` field:

.. code-block:: javascript

   WriteResult({
      "nInserted" : 1,
      "writeConcernError" : {
         "code" : 64,
         "errmsg" : "waiting for replication timed out at shard-a"
      }
   })

Errors Unrelated to Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.insert()` method encounters a non-write
concern error, the results include the :data:`WriteResult.writeError`
field:

.. code-block:: javascript

   WriteResult({
      "nInserted" : 0,
      "writeError" : {
         "code" : 11000,
         "errmsg" : "insertDocument :: caused by :: 11000 E11000 duplicate key error index: test.foo.$_id_  dup key: { : 1.0 }"
      }
   })

.. _bulkwriteresults-insert:

BulkWriteResult
---------------

.. versionchanged:: 2.6

When passed an array of documents, :method:`~db.collection.insert()`
returns a :ref:`bulk-write-result`. See :ref:`bulk-write-result` for
details.
==========================
db.collection.isCapped()
==========================

.. default-domain:: mongodb

.. method:: db.collection.isCapped()

   :returns: Returns ``true`` if the collection is a :term:`capped
             collection`, otherwise returns ``false``.

   .. seealso:: :doc:`/core/capped-collections`
=========================
db.collection.mapReduce()
=========================

.. default-domain:: mongodb

.. method:: db.collection.mapReduce(map,reduce, {<out>,<query>,<sort>,<limit>,<finalize>,<scope>,<jsMode>,<verbose>})

   The :method:`db.collection.mapReduce()` method provides a wrapper
   around the :dbcommand:`mapReduce` command.

   .. code-block:: javascript

      db.collection.mapReduce(
                               <map>,
                               <reduce>,
                               {
                                 out: <collection>,
                                 query: <document>,
                                 sort: <document>,
                                 limit: <number>,
                                 finalize: <function>,
                                 scope: <document>,
                                 jsMode: <boolean>,
                                 verbose: <boolean>
                               }
                             )

   :method:`db.collection.mapReduce()` takes the following parameters:

   .. include:: /reference/method/db.collection.mapReduce-field.rst

   The following table describes additional arguments that
   :method:`db.collection.mapReduce()` can accept.

   .. include:: /reference/method/db.collection.mapReduce-args-field.rst

   .. note::

      .. versionchanged:: 2.4

      .. include:: /includes/fact-group-map-reduce-where-limitations-in-24.rst

.. _mapreduce-map-mtd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-map
   :end-before: end-map

.. _mapreduce-reduce-mtd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-reduce
   :end-before: end-reduce

.. _mapreduce-out-mtd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-out
   :end-before: end-out

.. _mapreduce-finalize-mtd:

.. include:: /includes/parameters-map-reduce.rst
   :start-after: start-finalize
   :end-before: end-finalize

.. include:: /includes/examples-map-reduce.rst

For more information and examples, see the
:doc:`Map-Reduce </core/map-reduce>` page and
:doc:`/tutorial/perform-incremental-map-reduce`.

.. seealso::

   - :doc:`/tutorial/troubleshoot-map-function`

   - :doc:`/tutorial/troubleshoot-reduce-function`

   - :dbcommand:`mapReduce` command

   - :doc:`/core/aggregation`
=======================
db.collection.reIndex()
=======================

.. default-domain:: mongodb

.. method:: db.collection.reIndex()

   The :method:`db.collection.reIndex()` drops all indexes on a
   collection and recreates them. This operation may be expensive for
   collections that have a large amount of data and/or a large number
   of indexes.

   Call this method, which takes no arguments, on a collection
   object. For example:

   .. code-block:: javascript

      db.collection.reIndex()

   Normally, MongoDB compacts indexes during routine updates. For most
   users, the :method:`db.collection.reIndex()` is unnecessary. However, it
   may be worth running if the collection size has changed significantly
   or if the indexes are consuming a disproportionate amount of disk space.

Behavior
--------

.. |cmd-name| replace:: :method:`db.collection.reIndex()`

.. include:: /includes/note-reindex-impact-on-replica-sets.rst

.. include:: /includes/important-reindex-locking.rst

.. |limit| replace:: :limit:`Maximum Index Key Length <Index Key>`

.. versionchanged:: 2.6

   .. include:: /includes/fact-index-key-length-operation-behaviors.rst
      :start-after: index-field-limit-reIndex
      :end-before: index-field-limit-insert

.. see:: :doc:`/core/index-creation` for more information on the
   behavior of indexing operations in MongoDB.
======================
db.collection.remove()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.remove()

   Removes documents from a collection.

   The :method:`db.collection.remove()` method can have one of two
   syntaxes. The :method:`~db.collection.remove()` method can take a
   query document and an optional ``justOne`` boolean:

   .. code-block:: javascript

      db.collection.remove(
         <query>,
         <justOne>
      )

   Or the method can take a query document and an optional remove
   options document:

   .. versionadded:: 2.6

   .. code-block:: javascript

      db.collection.remove(
         <query>,
         {
           justOne: <boolean>,
           writeConcern: <document>
         }
      )

   .. include:: /reference/method/db.collection.remove-param.rst

   .. versionchanged:: 2.6
      The :method:`~db.collection.remove()` returns an object that
      contains the status of the operation.

   :returns: A :ref:`writeresults-remove` object that contains the
      status of the operation.

Behavior
--------

.. _remove-safe-writes:

Safe Writes
~~~~~~~~~~~

.. versionchanged:: 2.6

The :method:`~db.collection.remove()` method uses the
:dbcommand:`delete` command, which uses the default write concern. To
specify a different write concern, include the write concern in the
options parameter.

Query Considerations
~~~~~~~~~~~~~~~~~~~~

By default, :method:`~db.collection.remove()` removes all documents
that match the ``query`` expression. Specify the ``justOne`` option to
limit the operation to removing a single document. To delete a single
document sorted by a specified order, use the :ref:`findAndModify()
<findAndModify-wrapper-sorted-remove>` method.

Capped Collections
~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-remove-capped-collection-restriction.rst

Sharded Collections
~~~~~~~~~~~~~~~~~~~

.. |single-modification-operation-names| replace:: :method:`~db.collection.remove()`

.. |single-modification-operation-option| replace:: ``justOne``

.. include:: /includes/fact-single-modification-in-sharded-collections.rst

Examples
--------

The following are examples of the :method:`~db.collection.remove()` method.

Remove All Documents from a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove all documents in a collection, call the :method:`remove
<db.collection.remove()>` method with an empty query document ``{}``.
The following operation deletes all documents from the :doc:`bios
collection </reference/bios-example-collection>`:

.. code-block:: javascript

   db.bios.remove( { } )

This operation is not equivalent to the
:method:`~db.collection.drop()` method.

To remove all documents from a collection, it may be more efficient
to use the :method:`~db.collection.drop()` method to drop the entire
collection, including the indexes, and then recreate the collection
and rebuild the indexes.

Remove All Documents that Match a Condition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove the documents that match a deletion criteria, call the
:method:`~db.collection.remove()` method with the ``<query>``
parameter:

The following operation removes all the documents from the collection
``products`` where ``qty`` is greater than ``20``:

.. code-block:: javascript

   db.products.remove( { qty: { $gt: 20 } } )

Override Default Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation to a replica set removes all the documents from
the collection ``products`` where ``qty`` is greater than ``20`` and
specifies a :doc:`write concern </reference/write-concern>` of ``"w:
majority"`` with a ``wtimeout`` of 5000 milliseconds such that the
method returns after the write propagates to a majority of the replica
set members or the method times out after 5 seconds.

.. code-block:: javascript

   db.products.remove(
       { qty: { $gt: 20 } },
       { writeConcern: { w: "majority", wtimeout: 5000 } }
   )

Remove a Single Document that Matches a Condition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To remove the first document that match a deletion criteria, call the
:method:`remove <db.collection.remove()>` method with the ``query``
criteria and the ``justOne`` parameter set to ``true`` or ``1``.

The following operation removes the first document from the collection
``products`` where ``qty`` is greater than ``20``:

.. code-block:: javascript

   db.products.remove( { qty: { $gt: 20 } }, true )

Isolate Removal Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~

If the ``<query>`` argument to the :method:`~db.collection.remove()`
method matches multiple documents in the collection, the delete
operation may interleave with other write operations to that collection.
For an unsharded collection, you have the option to override this
behavior with the :update:`$isolated` isolation operator, effectively
isolating the delete operation and blocking other write operations
during the delete. To isolate the query, include ``$isolated: 1`` in the
``<query>`` parameter as in the following examples:

.. code-block:: javascript

   db.products.remove( { qty: { $gt: 20 }, $isolated: 1 } )

.. _writeresults-remove:

WriteResult
-----------

.. versionchanged:: 2.6

Successful Results
~~~~~~~~~~~~~~~~~~

The :method:`~db.collection.remove()` returns a :method:`WriteResult`
object that contains the status of the operation. Upon success, the
:method:`WriteResult` object contains information on the number of
documents removed:

.. code-block:: javascript

   WriteResult({ "nRemoved" : 4 })

.. seealso:: :data:`WriteResult.nRemoved`

Write Concern Errors
~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.remove()` method encounters write
concern errors, the results include the
:data:`WriteResult.writeConcernError` field:

.. code-block:: javascript

   WriteResult({
      "nRemoved" : 21,
      "writeConcernError" : {
         "code" : 64,
         "errInfo" : {
            "wtimeout" : true
         },
         "errmsg" : "waiting for replication timed out"
      }
   })

.. seealso:: :method:`WriteResult.hasWriteConcernError()`

Errors Unrelated to Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.remove()` method encounters a non-write
concern error, the results include :data:`WriteResult.writeError` field:

.. code-block:: javascript

   WriteResult({
      "nRemoved" : 0,
      "writeError" : {
         "code" : 2,
         "errmsg" : "unknown top level operator: $invalidFieldName"
      }
   })

.. seealso:: :method:`WriteResult.hasWriteError()`
================================
db.collection.renameCollection()
================================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.renameCollection(target, dropTarget)

   Renames a collection. Provides a wrapper for the
   :dbcommand:`renameCollection` :term:`database command`.

   .. include:: /reference/method/db.collection.renameCollection-param.rst


Example
-------

Call the :method:`db.collection.renameCollection()` method on a
collection object. For example:

.. code-block:: javascript

   db.rrecord.renameCollection("record")

This operation will rename the ``rrecord`` collection to ``record``. If
the target name (i.e. ``record``) is the name of an existing collection,
then the operation will fail.

Limitations
-----------

The method has the following limitations:

- :method:`db.collection.renameCollection()` cannot move a collection
  between databases. Use :dbcommand:`renameCollection` for these rename
  operations.

- :method:`db.collection.renameCollection()` cannot operation on sharded
  collections.

The :method:`db.collection.renameCollection()` method operates within a
collection by changing the metadata associated with a given collection.

Refer to the documentation :dbcommand:`renameCollection` for additional
warnings and messages.

.. warning::

   The :method:`db.collection.renameCollection()` method and
   :dbcommand:`renameCollection` command will invalidate open cursors
   which interrupts queries that are currently returning data.
====================
db.collection.save()
====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.save()

   Updates an existing document or inserts a new document, depending on
   its ``document`` parameter.

   The :method:`~db.collection.save()` method has the following form:

   .. versionchanged:: 2.6

   .. code-block:: none

      db.collection.save(
         <document>,
         {
           writeConcern: <document>
         }
      )

   .. include:: /reference/method/db.collection.save-param.rst

   .. versionchanged:: 2.6
      The :method:`~db.collection.save()` returns an object that
      contains the status of the operation.

   :returns: A :ref:`writeresults-save` object that contains the
      status of the operation.

Behavior
--------

.. _save-safe-writes:

Safe Writes
~~~~~~~~~~~

.. versionchanged:: 2.6

The :method:`~db.collection.save()` method uses either the
:dbcommand:`insert` or the :dbcommand:`update` command, which use the
default write concern. To specify a different write concern, include
the write concern in the options parameter.

Insert
~~~~~~

If the document does **not** contain an :term:`_id` field, then the
:method:`~db.collection.save()` method performs an
:method:`~db.collection.insert()`. During the operation, the
:program:`mongo` shell will create an :doc:`/reference/object-id` and
assign it to the ``_id`` field.

.. include:: /includes/note-insert-id-field.rst

Upsert
~~~~~~

If the document contains an :term:`_id` field, then the
:method:`~db.collection.save()` method performs an :ref:`update with
upsert <upsert-parameter>`, querying by the ``_id`` field. If a
document does not exist with the specified ``_id`` value, the
:method:`~db.collection.save()` method performs an insert. If a
document exists with the specified ``_id`` value, the
:method:`~db.collection.save()` method performs an update that replaces
**all** fields in the existing document with the fields from the
``document``.

Examples
--------

.. _crud-create-insert-save:

Save a New Document without Specifying an ``_id`` Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following example, :method:`~db.collection.save()` method
performs an insert since the document passed to the method does not
contain the ``_id`` field:

.. code-block:: javascript

   db.products.save( { item: "book", qty: 40 } )

During the insert, :program:`mongod` will create the ``_id`` field with
a unique :doc:`/reference/object-id` value, as verified by the inserted
document:

.. code-block:: javascript

   { "_id" : ObjectId("50691737d386d8fadbd6b01d"), "item" : "book", "qty" : 40 }

.. include:: /includes/fact-object-id-may-differ.rst

.. _crud-create-save:

Save a New Document Specifying an ``_id`` Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the following example, :method:`~db.collection.save()` performs an
update with ``upsert`` since the document contains an ``_id`` field:

.. code-block:: javascript

   db.products.save( { _id: 100, item: "water", qty: 30 } )

Because the ``_id`` field holds a value that *does not* exist in the
collection, the operation inserts the document. The results of these
operations are identical to an :ref:`update operation with the upsert
flag <upsert-parameter>` set to ``true`` or ``1``.

The operation results in the following new document in the ``products``
collection:

.. code-block:: javascript

   { "_id" : 100, "item" : "water", "qty" : 30 }

Replace an Existing Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``products`` collection contains the following document:

.. code-block:: javascript

   { "_id" : 100, "item" : "water", "qty" : 30 }

The :method:`~db.collection.save()` method performs an update with
``upsert`` since the document contains an ``_id`` field:

.. code-block:: javascript

   db.products.save( { _id : 100, item : "juice" } )

Because the ``_id`` field holds a value that exists in the collection,
the operation performs an update to replace the document and results in
the following document:

.. code-block:: javascript

   { "_id" : 100, "item" : "juice" }

Override Default Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation to a replica set specifies a :doc:`write
concern </reference/write-concern>` of ``"w: majority"`` with a
``wtimeout`` of 5000 milliseconds such that the method returns after
the write propagates to a majority of the replsica set members or the
method times out after 5 seconds.

.. code-block:: javascript

   db.products.save(
       { item: "envelopes", qty : 100, type: "Clasp" },
       { writeConcern: { w: "majority", wtimeout: 5000 } }
   )

.. _writeresults-save:

WriteResult
-----------

.. versionchanged:: 2.6

The :method:`~db.collection.save()` returns a :method:`WriteResult`
object that contains the status of the insert or update operation. See
:ref:`WriteResult for insert <writeresults-insert>` and
:ref:`WriteResult for update <writeresults-update>` for details.
=====================
db.collection.stats()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.stats(scale)

   Returns statistics about the collection. The method includes
   the following parameter:

   .. include:: /reference/method/db.collection.stats-param.rst

   :returns: A :term:`document` containing statistics that
             reflecting the state of the specified collection.

   The :method:`~db.collection.stats()` method provides a wrapper
   around the database command
   :dbcommand:`collStats`.

Example
-------

The following operation returns stats on the ``people`` collection:

.. code-block:: javascript

   db.people.stats()

.. seealso:: :doc:`/reference/command/collStats` for an
   overview of the output of this command.
===========================
db.collection.storageSize()
===========================

.. default-domain:: mongodb

.. method:: db.collection.storageSize()

   :returns: The total amount of storage allocated to this collection
             for document storage. Provides a wrapper around the
             :data:`~collStats.storageSize` field of the
             :dbcommand:`collStats`
             (i.e. :method:`db.collection.stats()`) output.
==============================
db.collection.totalIndexSize()
==============================

.. default-domain:: mongodb

.. method:: db.collection.totalIndexSize()

   :returns: The total size of all indexes for the collection. This
             method provides a wrapper around the
             :data:`~collStats.totalIndexSize` output of the
             :dbcommand:`collStats`
             (i.e. :method:`db.collection.stats()`) operation.
=========================
db.collection.totalSize()
=========================

.. default-domain:: mongodb

.. method:: db.collection.totalSize()

   :returns: The total size of the data in the collection plus the
             size of every indexes on the collection.
======================
db.collection.update()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.collection.update(query, update, options)

   Modifies an existing document or documents in a collection. The
   method can modify specific fields of existing document or documents
   or replace an existing document entirely, depending on the
   :ref:`update parameter <update-parameter>`.

   By default, the :method:`~db.collection.update()` method updates a
   **single** document. Set the :ref:`multi-parameter` to update all
   documents that match the query criteria.

   The :method:`~db.collection.update()` method has the following form:

   .. versionchanged:: 2.6

   .. code-block:: javascript

      db.collection.update(
         <query>,
         <update>,
         {
           upsert: <boolean>,
           multi: <boolean>,
           writeConcern: <document>
         }
      )

   The :method:`~db.collection.update()` method takes the following
   parameters:

   .. include:: /reference/method/db.collection.update-param.rst

   .. versionchanged:: 2.6
      The :method:`~db.collection.update()` returns an object that
      contains the status of the operation.

   :returns: A :ref:`writeresults-update` object that contains the
      status of the operation.

Behavior
--------

.. _update-safe-writes:

Safe Writes
~~~~~~~~~~~

.. versionchanged:: 2.6

The :method:`~db.collection.update()` method uses the
:dbcommand:`update` command, which uses the default write concern. To
specify a different write concern, include the ``writeConcern`` option
in the options parameter. See :ref:`example-update-write-concern` for
an example.

.. _update-parameter:

Update Parameter
~~~~~~~~~~~~~~~~

The :method:`~db.collection.update()` method either modifies specific
fields in existing documents or replaces an existing document entirely.

Update Specific Fields
``````````````````````

If the ``<update>`` document contains :ref:`update operator
<update-operators>` expressions, such those using the
:update:`$set` operator, then:

- The ``<update>`` document must contain *only* :ref:`update operator
  <update-operators>` expressions.

- The :method:`~db.collection.update()` method updates only the
  corresponding fields in the document. For an example, see
  :ref:`example-update-specific-fields`.

Replace Document Entirely
`````````````````````````

If the ``<update>`` document contains *only* ``field:value``
expressions, then:

- The :method:`~db.collection.update()` method *replaces* the matching
  document with the ``<update>`` document. The
  :method:`~db.collection.update()` method *does not* replace the
  ``_id`` value. For an example, see
  :ref:`example-update-replace-fields`.

- :method:`~db.collection.update()` *cannot* :ref:`update multiple
  documents <multi-parameter>`.

.. _upsert-parameter:

Upsert Parameter
~~~~~~~~~~~~~~~~

Upsert Use
``````````

If ``upsert`` is ``true`` and if no document matches the query
criteria, :method:`~db.collection.update()` inserts a *single* document.
The *upsert* creates the new document with either:

- The fields and values of the ``<update>`` parameter, or

- The fields and values of the both the ``<query>`` and ``<update>``
  parameters. The *upsert* creates a document with data from both
  ``<query>`` and ``<update>`` if the ``<update>`` parameter contains
  *only* :ref:`update operator <update-operators>` expressions.

If ``upsert`` is ``true`` and there are documents that match the query
criteria, :method:`~db.collection.update()` performs an update.

Use Unique Indexes
``````````````````

.. warning:: To avoid inserting the same document more than once,
   only use ``upsert: true`` if the ``query`` field is uniquely
   indexed.

Given a collection named ``people`` where no documents have a
``name`` field that holds the value ``Andy``. Consider when multiple
clients issue the following *update* operation at the same time. The
following operation becomes an ``insert`` because the ``upsert``
flag is ``true``.:

.. code-block:: javascript

   db.people.update(
      { name: "Andy" },
      {
         name: "Andy",
         rating: 1,
         score: 1
      },
      { upsert: true }
   )

If all :method:`~db.collection.update()` operations complete the
``query`` portion before any client successfully inserts data,
**and** there is no unique index on the ``name`` field, then each update
operation may perform an insert.

To prevent MongoDB from inserting the same document more than once,
create a :ref:`unique index <index-type-unique>` on the ``name``
field. With a unique index, if an applications issues a group of upsert
operations, *Exactly one* :method:`~db.collection.update()` would
successfully insert a new document.

The remaining operations would either:

- update the newly inserted document, or

- fail when they attempted to insert a duplicate.

  If the operation fails because of a duplicate index key error,
  applications may retry the operation which will succeed as an update
  operation.

.. _multi-parameter:

Multi Parameter
~~~~~~~~~~~~~~~

If ``multi`` is set to ``true``, the :method:`~db.collection.update()`
method updates all documents that meet the ``<query>`` criteria. The
``multi`` update operation may interleave with other write operations.
For unsharded collections, you can override this behavior with the
:update:`$isolated` isolation operator, which isolates the update
operation and blocks other write operations during the update.

If the :ref:`\<update\> <update-parameter>` document contains *only*
``field:value`` expressions, then :method:`~db.collection.update()`
*cannot* update multiple documents.

For an example, see :ref:`example-update-multi`.

Sharded Collections
~~~~~~~~~~~~~~~~~~~

.. |single-modification-operation-names| replace:: :method:`~db.collection.update()`

.. |single-modification-operation-option| replace:: ``multi: false``

.. include:: /includes/fact-single-modification-in-sharded-collections.rst

Examples
--------

.. _example-update-specific-fields:

Update Specific Fields
~~~~~~~~~~~~~~~~~~~~~~

To update specific fields in a document, use :ref:`update operators
<update-operators>` in the ``<update>`` parameter. If the ``<update>``
parameter refers to fields non-existent in the current document, the
:method:`~db.collection.update()` method adds the fields to the
document.

For example, given a ``books`` collection with the following document:

.. code-block:: javascript

   { "_id" : 11, "item" : "Divine Comedy", "stock" : 2 }

The following operation adds a ``price`` field to the document and
increments the ``stock`` field by ``5``.

.. code-block:: javascript

   db.books.update(
      { item: "Divine Comedy" },
      {
         $set: { price: 18 },
         $inc: { stock: 5 }
      }
   )

The updated document is now the following:

.. code-block:: javascript

   { "_id" : 11, "item" : "Divine Comedy", "price" : 18, "stock" : 7 }

.. seealso:: :update:`$set`, :update:`$inc`, :doc:`/reference/operator/update`

Remove Fields
~~~~~~~~~~~~~

The following operation uses the :update:`$unset` operator to remove
the ``stock`` field:

.. code-block:: javascript

   db.books.update( { _id: 11 }, { $unset: { stock: 1 } } )

.. seealso:: :update:`$unset`, :update:`$rename`, :doc:`/reference/operator/update`

.. _example-update-replace-fields:

Replace All Fields
~~~~~~~~~~~~~~~~~~

Given the following document in the ``books`` collection:

.. code-block:: javascript

   {
       "_id" : 22,
       "item" : "The Banquet",
       "author" : "Dante",
       "price" : 20,
       "stock" : 4
   }

The following operation passes an ``<update>`` document that contains
only field and value pairs, which means the document replaces all the
fields in the original document. The operation *does not* replace the
``_id`` value. The operation contains the same value for the ``item``
field in both the ``<query>`` and ``<update>`` documents, which means
the field does not change:

.. code-block:: javascript

   db.books.update(
      { item: "The Banquet" },
      { item: "The Banquet", price: 19 , stock: 3 }
   )

The operation creates the following new document. The operation removed
the ``author`` field and changed the values of the ``price`` and
``stock`` fields:

.. code-block:: javascript

   {
       "_id" : 22,
       "item" : "The Banquet",
       "price" : 19,
       "stock" : 3
   }

.. _example-update-upsert:

Insert a New Document if No Match Exists (Upsert)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following command sets the *upsert* option to ``true`` so that
:method:`~db.collection.update()` creates a new document in the
``books`` collection if no document matches the ``<query>`` parameter:

.. code-block:: javascript

   db.books.update(
      { item: "The New Life" },
      { item: "The New Life", author: "Dante", price: 15 },
      { upsert: true }
   )

If no document matches the ``<query>`` parameter, the *upsert* inserts
a document with the fields and values of the ``<update>`` parameter and
a new unique ``ObjectId`` for the ``_id`` field:

.. code-block:: javascript

   {
       "_id" : ObjectId("51e5990c95098ed69d4a89f2"),
       "author" : "Dante",
       "item" : "The New Life",
       "price" : 15
   }

.. _example-update-multi:

Update Multiple Documents
~~~~~~~~~~~~~~~~~~~~~~~~~

To update multiple documents, set the ``multi`` option to ``true``. For
example, the following operation updates all documents where ``stock``
is less than ``5``:

.. code-block:: javascript

   db.books.update(
      { stock: { $lt: 5 } },
      { $set: { reorder: true } },
      { multi: true }
   )

.. _example-update-write-concern:

Override Default Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation to a replica set specifies a :doc:`write
concern </reference/write-concern>` of ``"w: majority"`` with a
``wtimeout`` of 5000 milliseconds such that the method returns after
the write propagates to a majority of the replica set members or the
method times out after 5 seconds.

.. code-block:: javascript

   db.books.update(
      { stock: { $lt: 5 } },
      { $set: { reorder: true } },
      {
        multi: true,
        writeConcern: { w: "majority", wtimeout: 5000 }
      }
   )

Combine the Upsert and Multi Parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given a ``books`` collection that includes the following documents:

.. code-block:: javascript

   { _id: 11, author: "Dante", item: "Divine Comedy", price: 18 }
   { _id: 22, author: "Dante", item: "The Banquet", price: 19 }
   { _id: 33, author: "Dante", item: "Monarchia", price: 14 }

The following command uses the ``multi`` parameter to update all
documents where ``author`` is ``"Dante"`` and uses the *upsert*
parameter to create a new document if no such documents are found:

.. code-block:: javascript

   db.books.update(
      { author: "Dante" },
      { $set: { born: "Florence", died: "Ravenna" } },
      { upsert: true, multi: true }
   )

The operation results in the following:

.. code-block:: javascript

   {
       "_id" : 11,
       "author" : "Dante",
       "born" : "Florence",
       "died" : "Ravenna",
       "item" : "Divine Comedy",
       "price" : 18
   }
   {
       "_id" : 22,
       "author" : "Dante",
       "born" : "Florence",
       "died" : "Ravenna",
       "item" : "The Banquet",
       "price" : 19
   }
   {
       "_id" : 33,
       "author" : "Dante",
       "born" : "Florence",
       "died" : "Ravenna",
       "item" : "Monarchia",
       "price" : 14
   }

.. TODO need to just replace the below section with a little behavior
   blurb or move it somewhere else instead of trying to give
   examples for various array update use cases.

Update Arrays
~~~~~~~~~~~~~

Update an Element by Position
`````````````````````````````

If the update operation requires an update of an element in an array
field, the :method:`~db.collection.update()` method can
perform the update using the position of the element and :term:`dot
notation`. Arrays in MongoDB are zero-based.

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` for the first document with
``_id`` field equal to ``1`` and updates the second element in the
``contribs`` array:

.. code-block:: javascript

   db.bios.update(
      { _id: 1 },
      { $set: { "contribs.1": "ALGOL 58" } }
   )

Update an Element if Position is Unknown
````````````````````````````````````````

If the position in the array is not known, the
:method:`~db.collection.update()` method can perform the update using
the :operator:`$` positional operator. The array field must appear in
the ``<query>`` parameter in order to determine which array element to
update.

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` for the first document where the
``_id`` field equals ``3`` and the ``contribs`` array contains an
element equal to ``compiler``. If found, the
:method:`~db.collection.update()` method updates the first matching
element in the array to ``A compiler`` in the document:

.. code-block:: javascript

   db.bios.update(
      { _id: 3, "contribs": "compiler" },
      { $set: { "contribs.$": "A compiler" } }
   )

Update a Document Element
`````````````````````````

The :method:`~db.collection.update()` method can perform the
update of an array that contains subdocuments by using the positional
operator (i.e. :operator:`$`) and the :term:`dot notation`.

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` for the first document where the
``_id`` field equals ``6`` and the ``awards`` array contains a
subdocument element with the ``by`` field equal to ``ACM``. If found,
the :method:`~db.collection.update()` method updates the ``by`` field in
the first matching subdocument:

.. code-block:: javascript

   db.bios.update(
      { _id: 6, "awards.by": "ACM"  } ,
      { $set: { "awards.$.by": "Association for Computing Machinery" } }
   )

Add an Element
``````````````

The following operation queries the :doc:`bios collection
</reference/bios-example-collection>` for the first document that has an
``_id`` field equal to ``1`` and adds a new element to the ``awards``
field:

.. code-block:: javascript

   db.bios.update(
      { _id: 1 },
      {
        $push: { awards: { award: "IBM Fellow", year: 1963, by: "IBM" } }
      }
   )

In the next example, the :update:`$set` operator uses :ref:`dot
notation <document-dot-notation>` to access the ``middle`` field in the
``name`` subdocument. The :update:`$push`
operator adds another document as element to the field ``awards``.

Consider the following operation:

.. code-block:: javascript

   db.bios.update(
      { _id: 1 },
      {
         $set: { "name.middle": "Warner" },
         $push: { awards: {
            award: "IBM Fellow",
               year: "1963",
               by: "IBM"
            }
         }
      }
   )

This :method:`~db.collection.update()` operation:

- Modifies the field ``name`` whose value is another document.
  Specifically, the :update:`$set` operator updates the ``middle``
  field in the ``name`` subdocument. The document uses :ref:`dot
  notation <document-dot-notation>` to access a field in a subdocument.

- Adds an element to the field ``awards``, whose value is an array.
  Specifically, the :update:`$push` operator adds another document as
  element to the field ``awards``.

.. _writeresults-update:

WriteResult
-----------

.. versionchanged:: 2.6

Successful Results
~~~~~~~~~~~~~~~~~~

The :method:`~db.collection.update()` returns a :method:`WriteResult`
object that contains the status of the operation. Upon success, the
:method:`WriteResult` object contains information on the number of
documents that matched the query condition, the number of documents
inserted via an ``upsert``, and the number of documents modified:

.. code-block:: javascript

   WriteResult({ "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 })

.. see:: :data:`WriteResult.nMatched`, :data:`WriteResult.nUpserted`,
   :data:`WriteResult.nModified`

Write Concern Errors
~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.update()` method encounters write
concern errors, the results include the
:data:`WriteResult.writeConcernError` field:

.. code-block:: javascript

   WriteResult({
      "nMatched" : 1,
      "nUpserted" : 0,
      "nModified" : 1,
      "writeConcernError" : {
         "code" : 64,
         "errmsg" : "waiting for replication timed out at shard-a"
      }
   })

.. seealso:: :method:`WriteResult.hasWriteConcernError()`

Errors Unrelated to Write Concern
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the :method:`~db.collection.update()` method encounters a non-write
concern error, the results include the :data:`WriteResult.writeError`
field:

.. code-block:: javascript

   WriteResult({
      "nMatched" : 0,
      "nUpserted" : 0,
      "nModified" : 0,
      "writeError" : {
         "code" : 7,
         "errmsg" : "could not contact primary for replica set shard-a"
      }
   })

.. seealso:: :method:`WriteResult.hasWriteError()`
========================
db.collection.validate()
========================

.. default-domain:: mongodb

Description
-----------

.. method:: db.collection.validate(full)

   Validates a collection. The method scans a collection's data
   structures for correctness and returns a single :term:`document` that
   describes the relationship between the logical collection and the
   physical representation of the data.

   The :method:`~db.collection.validate()` method has the following
   parameter:

   .. include:: /reference/method/db.collection.validate-param.rst

   The :method:`~db.collection.validate()` method output provides an
   in-depth view of how the collection
   uses storage. Be aware that this command is potentially resource
   intensive and may impact the performance of your MongoDB
   instance.

   The :method:`~db.collection.validate()` method is a wrapper
   around the :dbcommand:`validate` :term:`database
   command`.

   .. seealso:: :doc:`/reference/command/validate`
================
db.commandHelp()
================

.. default-domain:: mongodb

Description
-----------

.. method:: db.commandHelp(command)

   Displays help text for the specified :term:`database command`. See
   the :doc:`/reference/command`.

   The :method:`db.commandHelp()` method has the following parameter:

   .. include:: /reference/method/db.commandHelp-param.rst
=================
db.copyDatabase()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: db.copyDatabase(fromdb, todb, fromhost, username, password)

   Copies a database from a remote host to the current host or copies a
   database to another database within the current host.
   :method:`db.copyDatabase()` wraps the :dbcommand:`copydb` command
   and takes the following arguments:

   .. include:: /reference/method/db.copyDatabase-param.rst

Behavior
--------

.. |copydb| replace:: :method:`db.copyDatabase()`

.. include:: /includes/fact-copydb-behavior.rst

Required Access
---------------

.. versionchanged:: 2.6

The :dbcommand:`copydb` command requires the following authorization on
the target and source databases.

Source Database (``fromdb``)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Source is non-``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. source-not-admin
   :end-before: .. source-admin

Source is ``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. source-admin

Source Database is on a Remote Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If copying from a remote server and the remote server has
authentication enabled, you must authenticate to the remote host as a
user with the proper authorization. See
:ref:`copyDatabases-authentication`.

Target Database (``todb``)
~~~~~~~~~~~~~~~~~~~~~~~~~~

Copy from non-``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
.. include:: /includes/access-copydb.rst
   :start-after: .. target-non-admin-source
   :end-before: .. target-admin-source

Copy from ``admin`` Database
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. include:: /includes/access-copydb.rst
   :start-after: .. target-admin-source
   :end-before: .. source-not-admin

.. _copyDatabases-authentication:

Authentication
--------------

If copying from a remote server and the remote server has
authentication enabled, then you must include the ``<username>`` and
``<password>``. The method does not transmit the password in plaintext.

Example
-------

To copy a database named ``records`` into a database named
``archive_records``, use the following invocation of
:method:`db.copyDatabase()`:

.. code-block:: javascript

   db.copyDatabase('records', 'archive_records')

.. seealso:: :dbcommand:`clone`
=====================
db.createCollection()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.createCollection(name, options)

   Creates a new collection explicitly.

   Because MongoDB creates a collection implicitly when the collection
   is first referenced in a command, this method is used primarily for
   creating new :term:`capped collections <capped collection>`. This is
   also used to pre-allocate space for an ordinary collection.

   The :method:`db.createCollection()` method has the following prototype form:

   .. code-block:: javascript

      db.createCollection(name, {capped: <boolean>, autoIndexId: <boolean>, size: <number>, max: <number>} )

   The :method:`db.createCollection()` method has the following parameters:

   .. include:: /reference/method/db.createCollection-param.rst

   The ``options`` document creates a capped collection or preallocates
   space in a new ordinary collection. The ``options`` document contains
   the following fields:

   .. include:: /reference/method/db.createCollection-options-param.rst

Example
-------

The following example creates a capped collection. Capped collections
have maximum size or document counts that prevent them from growing
beyond maximum thresholds. All capped collections must specify a maximum
size and may also specify a maximum document count. MongoDB removes
older documents if a collection reaches the maximum size limit before it
reaches the maximum document count. Consider the following example:

.. code-block:: javascript

   db.createCollection("log", { capped : true, size : 5242880, max : 5000 } )

This command creates a collection named ``log`` with a maximum size of 5
megabytes and a maximum of 5000 documents.

The following command simply pre-allocates a 2-gigabyte, uncapped
collection named ``people``:

.. code-block:: javascript

   db.createCollection("people", { size: 2147483648 } )

This command provides a wrapper around the database command
:dbcommand:`create`. See :doc:`/core/capped-collections` for more
information about capped collections.
===============
db.createRole()
===============

.. default-domain:: mongodb

Definition
----------

.. method:: db.createRole ( role, writeConcern )

   Creates a role and specifies its :ref:`privileges <privileges>`.
   The role applies to the
   database on which you run the method. The :method:`db.createRole()`
   method returns a *duplicate role* error if the role already exists in the
   database.

   The :method:`db.createRole()` method takes the following arguments:

   .. include:: /reference/method/db.createRole-param.rst

   The ``role`` document has the following form:

   .. code-block:: javascript

      { role: "<name>",
        privileges: [
           { resource: { <resource> }, actions: [ "<action>", ... ] },
           ...
        ],
        roles: [
           { role: "<role>", db: "<database>" } | "<role>",
            ...
        ]
      }

   The ``role`` document has the following fields:

   .. include:: /reference/method/db.createRole-role-field.rst

   .. |local-cmd-name| replace:: :method:`db.createRole()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.createRole()` method wraps the :dbcommand:`createRole`
   command.

Behavior
--------

A role's privileges apply to the database where the role is created. The
role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>` and can inherit
privileges from roles in other databases.

Required Access
---------------

.. include:: /includes/access-create-role.rst

Example
-------

The following :method:`db.createRole()` method creates the
``myClusterwideAdmin`` role on the ``admin`` database:

.. code-block:: javascript

   use admin
   db.createRole({ role: "myClusterwideAdmin",
     privileges: [
       { resource: { cluster: true }, actions: [ "addShard" ] },
       { resource: { db: "config", collection: "" }, actions: [ "find", "update", "insert", "remove" ] },
       { resource: { db: "users", collection: "usersCollection" }, actions: [ "update", "insert", "remove" ] },
       { resource: { db: "", collection: "" }, actions: [ "find" ] }
     ],
     roles: [
       { role: "read", db: "admin" }
     ],
     writeConcern: { w: "majority" , wtimeout: 5000 }
   })
===============
db.createUser()
===============

.. default-domain:: mongodb

Definition
----------

.. method:: db.createUser ( user, writeConcern )

   Creates a new user for the database where the method runs. :method:`db.createUser()`
   returns a *duplicate user* error if the user already exists on the database.

   The :method:`db.createUser()` method has the following syntax:

   .. include:: /reference/method/db.createUser-param.rst

   The ``user`` document defines the user and has the following form:

   .. code-block:: javascript

      { user: "<name>",
        pwd: "<cleartext password>",
        customData: { <any information> },
        roles: [
          { role: "<role>", db: "<database>" } | "<role>",
          ...
        ]
      }

   The ``user`` document has the following fields:

   .. |local-cmd-name| replace:: :method:`db.createUser()`
   .. include:: /reference/method/db.createUser-user-doc-param.rst

   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.createUser()` method wraps the :dbcommand:`createUser`
   command.

Behavior
--------

Encryption
~~~~~~~~~~

:method:`db.createUser()` sends password to the MongoDB instance
*without* encryption. To encrypt the password during transmission,
use :doc:`SSL </tutorial/configure-ssl>`.

External Credentials
~~~~~~~~~~~~~~~~~~~~

Users created on the ``$external`` database should have credentials
stored externally to MongoDB, as, for example, with :doc:`MongoDB
Enterprise installations that use Kerberos
</tutorial/control-access-to-mongodb-with-kerberos-authentication>`.

Required Access
---------------

.. include:: /includes/access-create-user.rst

Examples
--------

The following :method:`db.createUser()` operation creates the
``accountAdmin01`` user on the ``products`` database.

.. code-block:: javascript

   use products
   db.createUser( { "user" : "accountAdmin01",
                    "pwd": "cleartext password",
                    "customData" : { employeeId: 12345 },
                    "roles" : [ { role: "clusterAdmin", db: "admin" },
                                { role: "readAnyDatabase", db: "admin" },
                                "readWrite"
                                ] },
                  { w: "majority" , wtimeout: 5000 } )

The operation gives ``accountAdmin01`` the following roles:

- the ``clusterAdmin`` and ``readAnyDatabase`` roles on the ``admin``
  database

- the ``readWrite`` role on the ``products`` database

Create User with Roles
~~~~~~~~~~~~~~~~~~~~~~

The following operation creates ``accountUser`` in the ``products`` database
and gives the user the ``readWrite`` and ``dbAdmin`` roles.

.. code-block:: javascript

   use products
   db.createUser(
      {
        user: "accountUser",
        pwd: "password",
        roles: [ "readWrite", "dbAdmin" ]
      }
   )

Create User Without Roles
~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation creates a user named ``reportsUser`` in the ``admin``
database but does not yet assign roles:

.. code-block:: javascript

   use admin
   db.createUser(
      {
        user: "reportsUser",
        pwd: "password",
        roles: [ ]
      }
   )

Create Administrative User with Roles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation creates a user named ``appAdmin`` in the
``admin`` database and gives the user ``readWrite`` access to the
``config`` database, which lets the user change certain settings for
sharded clusters, such as to the balancer setting.

.. code-block:: javascript

   use admin
   db.createUser(
      {
        user: "appAdmin",
        pwd: "password",
        roles:
          [
            "clusterAdmin",
            { db: "config", role: "readWrite" }
          ]
      }
   )
==============
db.currentOp()
==============

.. default-domain:: mongodb

Definition
----------

.. method:: db.currentOp()

   :returns: A :term:`document` that reports in-progress operations for
             the database instance.

   The :method:`db.currentOp()` method can take no arguments or take the
   ``true`` argument, which returns a more verbose output, including
   idle connections and system operations. The following example uses
   the ``true`` argument:

   .. code-block:: javascript

      db.currentOp(true)

   :method:`db.currentOp()` is available only for users with
   administrative privileges.

   You can use :method:`db.killOp()` in conjunction with the
   :data:`~currentOp.opid` field to terminate a currently running
   operation. The following JavaScript operations for the
   :program:`mongo` shell filter the output of specific types of
   operations:

   .. include:: /includes/example-filter-current-op.rst

   .. include:: /includes/warning-terminating-operations.rst

Example
-------

The following is an example of :method:`db.currentOp()` output. If you
specify the ``true`` argument, :method:`db.currentOp()` returns
more verbose output.

.. code-block:: javascript

   {
     "inprog": [
                {
                  "opid" : 3434473,
                  "active" : <boolean>,
                  "secs_running" : 0,
                  "op" : "<operation>",
                  "ns" : "<database>.<collection>",
                  "query" : {
                  },
                  "client" : "<host>:<outgoing>",
                  "desc" : "conn57683",
                  "threadId" : "0x7f04a637b700",
                  "connectionId" : 57683,
                  "locks" : {
                      "^" : "w",
                      "^local" : "W",
                      "^<database>" : "W"
                  },
                  "waitingForLock" : false,
                  "msg": "<string>"
                  "numYields" : 0,
                  "progress" : {
                      "done" : <number>,
                      "total" : <number>
                  }
                  "lockStats" : {
                      "timeLockedMicros" : {
                          "R" : NumberLong(),
                          "W" : NumberLong(),
                          "r" : NumberLong(),
                          "w" : NumberLong()
                          },
                          "timeAcquiringMicros" : {
                          "R" : NumberLong(),
                          "W" : NumberLong(),
                          "r" : NumberLong(),
                          "w" : NumberLong()
                      }
                  }
                },
               ]
   }


Output
------

.. versionchanged:: 2.2

The :method:`db.currentOp()` returns a document with an array named
``inprog``. The ``inprog`` array contains a document for each
in-progress operation. The fields that appear for a given operation
depend on the kind of operation and its state.

.. data:: currentOp.opid

   Holds an identifier for the operation. You can pass this value to
   :method:`db.killOp()` in the :program:`mongo` shell to terminate the
   operation.

.. data:: currentOp.active

   A boolean value, that is ``true`` if the operation has started
   or ``false`` if the operation is queued and waiting for a lock to run.
   :data:`~currentOp.active` may be ``true`` even if the operation has
   yielded to another operation.

.. data:: currentOp.secs_running

   The duration of the operation in seconds. MongoDB calculates this
   value by subtracting the current time from the start time of the
   operation.

   If the operation is not running, (i.e. if :data:`~currentOp.active` is
   ``false``), this field may not appear in the output of
   :method:`db.currentOp()`.

.. data:: currentOp.op

   A string that identifies the type of operation. The possible values
   are:

   - ``insert``
   - ``query``
   - ``update``
   - ``remove``
   - ``getmore``
   - ``command``

.. data:: currentOp.ns

   The :term:`namespace` the operation targets. MongoDB forms
   namespaces using the name of the :term:`database` and the name of
   the :term:`collection`.

.. data:: currentOp.query

   A document containing the current operation's query. The document
   is empty for operations that do not have queries: ``getmore``,
   ``insert``, and ``command``.

.. data:: currentOp.client

   The IP address (or hostname) and the ephemeral port of the client
   connection where the operation originates. If your ``inprog``
   array has operations from many different clients, use this string
   to relate operations to clients.

   For some commands, including :dbcommand:`findAndModify` and
   :method:`db.eval()`, the client will be ``0.0.0.0:0``, rather than
   an actual client.

.. data:: currentOp.desc

   A description of the client. This string includes the
   :data:`~currentOp.connectionId`.

.. data:: currentOp.threadId

   An identifier for the thread that services the operation and its
   connection.

.. data:: currentOp.connectionId

   An identifier for the connection where the operation originated.

.. data:: currentOp.locks

   .. versionadded:: 2.2

   The :data:`~currentOp.locks` document reports on the kinds of locks the
   operation currently holds. The following kinds of locks are possible:

   .. data:: currentOp.locks.^

      :data:`~currentOp.locks.^` reports on the use of the global lock
      for the :program:`mongod` instance. All operations must hold the
      global lock for some phases of operation.

   .. data:: currentOp.locks.^local

      :data:`~currentOp.locks.^local` reports on the lock for the ``local``
      database. MongoDB uses the ``local`` database for a number of
      operations, but the most frequent use of the ``local`` database
      is for the :term:`oplog` used in replication.

   .. data:: currentOp.locks.^<database>

      :data:`locks.^\<database\> <currentOp.locks.^<database>>` reports on the lock state for the
      database that this operation targets.

.. data:: currentOp.waitingForLock

   Returns a boolean value. :data:`~currentOp.waitingForLock` is ``true`` if the
   operation is waiting for a lock and ``false`` if the operation has
   the required lock.

.. data:: currentOp.msg

   The :data:`~currentOp.msg` provides a message that describes the status and
   progress of the operation. In the case of indexing or mapReduce
   operations, the field reports the completion percentage.

.. data:: currentOp.progress

   Reports on the progress of mapReduce or indexing operations. The
   :data:`~currentOp.progress` fields corresponds to the completion percentage in
   the :data:`~currentOp.msg` field. The :data:`~currentOp.progress` specifies the following
   information:

   .. data:: currentOp.progress.done

      Reports the number completed.

   .. data:: currentOp.progress.total

      Reports the total number.

.. data:: currentOp.killed

   Returns ``true`` if :program:`mongod` instance is in the process of
   killing the operation.

.. data:: currentOp.numYields

   :data:`~currentOp.numYields` is a counter that reports the number of times the
   operation has yielded to allow other operations to complete.

   Typically, operations yield when they need access to data that
   MongoDB has not yet fully read into memory. This allows
   other operations that have data in memory to complete quickly
   while MongoDB reads in data for the yielding operation.

.. data:: currentOp.lockStats

   .. versionadded:: 2.2

   The :data:`~currentOp.lockStats` document reflects the amount of time the
   operation has spent both acquiring and holding
   locks. :data:`~currentOp.lockStats` reports data on a per-lock type, with the
   following possible lock types:

   - ``R`` represents the global read lock,
   - ``W`` represents the global write lock,
   - ``r`` represents the database specific read lock, and
   - ``w`` represents the database specific write lock.

    .. data:: currentOp.timeLockedMicros

       The :data:`~currentOp.timeLockedMicros` document reports the amount of
       time the operation has spent holding a specific lock.

       For operations that require more than one lock, like those that
       lock the ``local`` database to update the :term:`oplog`, then
       the values in this document can be longer than this value may
       be longer than the total length of the operation
       (i.e. :data:`~currentOp.secs_running`.)

       .. data:: currentOp.timeLockedMicros.R

          Reports the amount of time in microseconds the operation has held the
          global read lock.

       .. data:: currentOp.timeLockedMicros.W

          Reports the amount of time in microseconds the operation has held the
          global write lock.

       .. data:: currentOp.timeLockedMicros.r

          Reports the amount of time in microseconds the operation has held the
          database specific read lock.

       .. data:: currentOp.timeLockedMicros.w

          Reports the amount of time in microseconds the operation has held the
          database specific write lock.

   .. data:: currentOp.timeAcquiringMicros

      The :data:`~currentOp.timeAcquiringMicros` document reports the amount of time
      the operation has spent *waiting* to acquire a specific lock.

      .. data:: currentOp.timeAcquiringMicros.R

         Reports the mount of time in microseconds the operation has waited for the
         global read lock.

      .. data:: currentOp.timeAcquiringMicros.W

         Reports the mount of time in microseconds the operation has waited for the
         global write lock.

      .. data:: currentOp.timeAcquiringMicros.r

         Reports the mount of time in microseconds the operation has waited for the
         database specific read lock.

      .. data:: currentOp.timeAcquiringMicros.w

         Reports the mount of time in microseconds the operation has waited for the
         database specific write lock.
=================
db.dropAllRoles()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: db.dropAllRoles( writeConcern )

   Deletes all :ref:`user-defined <user-defined-roles>` roles on the
   database where you run the method.

   .. warning::

      The :method:`dropAllRoles` method removes *all* :ref:`user-defined
      <user-defined-roles>` roles from the database.

   The :method:`dropAllRoles` method takes the following argument:

   .. include:: /reference/method/db.dropAllRoles-param.rst

   .. COMMENT I added the returns here because in the example for this
      method, you have what the method returns.  But we don't specify
      what is returned for the other user/role mgmt methods.

   :returns:
      The number of :ref:`user-defined <user-defined-roles>` roles dropped.

   The :method:`db.dropAllRoles()` method wraps the
   :dbcommand:`dropAllRolesFromDatabase` command.

   .. |local-cmd-name| replace:: :method:`db.dropAllRoles()`

Required Access
---------------

.. include:: /includes/access-drop-role.rst

Example
-------

The following operations drop all :ref:`user-defined
<user-defined-roles>` roles from the ``products`` database and uses a
:ref:`write concern <write-concern-operation>` of ``majority``.

.. code-block:: javascript

   use products
   db.dropAllRoles( { w: "majority" } )

The method returns the number of roles dropped:

.. code-block:: javascript

   4
=================
db.dropAllUsers()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: db.dropAllUsers ( writeConcern )

   Removes all users from the current database.

   .. warning::

      The :method:`dropAllUsers` method removes all users from the
      database.

   The :method:`dropAllUsers` method takes the following arguments:

   .. |local-cmd-name| replace:: :method:`db.dropAllUsers()`
   .. include:: /reference/method/db.dropAllUsers-param.rst

   The :method:`db.dropAllUsers()` method wraps the
   :dbcommand:`dropAllUsersFromDatabase` command.


Required Access
---------------

.. include:: /includes/access-drop-user.rst

Example
-------

The following :method:`db.dropAllUsers()` operation drops every user from
the ``products`` database.

.. code-block:: javascript

   use products
   db.dropAllUsers( {w: "majority", wtimeout: 5000} )

The ``n`` field in the results document shows the number of users
removed:

.. code-block:: javascript

   { "n" : 12, "ok" : 1 }
=================
db.dropDatabase()
=================

.. default-domain:: mongodb

.. method:: db.dropDatabase()

   Removes the current database. Does not change the current database,
   so the insertion of any documents in this database will allocate a
   fresh set of data files.
=============
db.dropRole()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: db.dropRole( rolename, writeConcern )

   Deletes a :ref:`user-defined <user-defined-roles>` role from the
   database on which you run the method.

   The :method:`db.dropRole()` method takes the following arguments:

   .. include:: /reference/method/db.dropRole-param.rst

   The :method:`db.dropRole()` method wraps the :dbcommand:`dropRole`
   command.

   .. |local-cmd-name| replace:: :method:`db.dropRole()`

Required Access
---------------

.. include:: /includes/access-drop-role.rst

Example
-------

The following operations remove the ``readPrices`` role from the
``products`` database:

.. code-block:: javascript

   use products
   db.dropRole( "readPrices", { w: "majority" } )
=============
db.dropUser()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: db.dropUser ( username, writeConcern )

   Removes the user from the current database.

   The :method:`db.dropUser()` method takes the following arguments:

   .. include:: /reference/method/db.dropUser-param.rst

   The :method:`db.dropUser()` method wraps the :dbcommand:`dropUser`
   command.

Required Access
---------------

.. |local-cmd-name| replace:: :method:`db.dropUser()`

.. include:: /includes/access-drop-user.rst

Example
-------

The following :method:`db.dropUser()` operation drops the ``accountAdmin01``
user on the ``products`` database.

.. code-block:: javascript

   use products
   db.dropUser("accountAdmin01", {w: "majority", wtimeout: 5000})
=========
db.eval()
=========

.. Edits to this page should be carried over to the command eval.txt
   file.  HOWEVER, the parameters are different between command and method.

.. default-domain:: mongodb

Definition
----------

.. method:: db.eval(function, arguments)

   Provides the ability to run JavaScript code on the MongoDB server.

   .. |eval-object| replace:: :method:`db.eval()`
   .. include:: /includes/access-eval.rst

   .. |list| replace:: A list

   .. include:: /includes/fact-eval-helper-method.rst

   The method accepts the following parameters:

   .. include:: /reference/method/db.eval-param.rst

   The JavaScript function need not take any arguments, as in the first example,
   or may optionally take arguments as in the second:

   .. code-block:: javascript

      function () {
        // ...
      }

   .. code-block:: javascript

      function (arg1, arg2) {
         // ...
      }

Examples
--------

The following is an example of the :method:`db.eval()` method:

.. include:: /includes/examples-eval.rst
   :start-after: .. eval-method-example

- The ``db`` in the function refers to the current database.

- ``"eliot"`` is the argument passed to the function, and corresponds to
  the ``name`` argument.

- ``5`` is an argument to the function and corresponds to the
  ``incAmount`` field.

If you want to use the server's interpreter, you must run
:method:`db.eval()`. Otherwise, the :program:`mongo` shell's JavaScript
interpreter evaluates functions entered directly into the shell.

If an error occurs, :method:`db.eval()` throws an exception. The
following is an example of an invalid function that uses the variable
``x`` without declaring it as an argument:

.. code-block:: javascript

   db.eval( function() { return x + x; }, 3 );

The statement results in the following exception:

.. code-block:: javascript

   {
      "errmsg" : "exception: JavaScript execution failed: ReferenceError: x is not defined near '{ return x + x; }' ",
      "code" : 16722,
      "ok" : 0
   }

.. |object| replace:: :method:`db.eval()`
.. |nolockobject| replace:: :dbcommand:`eval` *command*
.. include:: /includes/admonitions-eval.rst

.. seealso::

   :doc:`/core/server-side-javascript`
==============
db.fsyncLock()
==============

.. default-domain:: mongodb

.. method:: db.fsyncLock()

   Forces the :program:`mongod` to flush all pending write operations
   to the disk and locks the *entire* :program:`mongod` instance to
   prevent additional writes until the user releases the lock with the
   :method:`db.fsyncUnlock()` command. :method:`db.fsyncLock()` is an
   administrative command.

   This command provides a simple wrapper around a
   :dbcommand:`fsync` database command with the following
   syntax:

   .. code-block:: javascript

        { fsync: 1, lock: true }

   This function locks the database and create a window for
   :doc:`backup operations </core/backups>`.

   .. include:: /includes/note-disable-profiling-fsynclock.rst
================
db.fsyncUnlock()
================

.. default-domain:: mongodb

.. method:: db.fsyncUnlock()

   Unlocks a :program:`mongod` instance to allow writes and reverses the
   operation of a :method:`db.fsyncLock()` operation. Typically you will
   use :method:`db.fsyncUnlock()` following a database :doc:`backup
   operation </core/backups>`.

   :method:`db.fsyncUnlock()` is an administrative command.
==================
db.getCollection()
==================

.. default-domain:: mongodb

Description
-----------

.. method:: db.getCollection(name)

   Returns a collection name. This is useful for a collection whose name
   might interact with the shell itself, such names that begin with
   ``_`` or that mirror the :doc:`database commands
   </reference/command>`.

   The :method:`db.getCollection()` method has the following parameter:

   .. include:: /reference/method/db.getCollection-param.rst
=======================
db.getCollectionNames()
=======================

.. default-domain:: mongodb

.. method:: db.getCollectionNames()

   :returns: An array containing all collections in the existing
             database.
=================
db.getLastError()
=================

.. default-domain:: mongodb

.. method:: db.getLastError()

   :returns: The last error message string.

   Sets the level of :term:`write concern` for confirming the success of write operations.

   .. see:: :dbcommand:`getLastError` and
      :doc:`/reference/write-concern` for all options, :ref:`Write
      Concern <write-concern>` for a conceptual overview,
      :doc:`/core/write-operations` for information about all write
      operations in MongoDB, .
====================
db.getLastErrorObj()
====================

.. default-domain:: mongodb

.. method:: db.getLastErrorObj()

   :returns: A full :term:`document` with status information.

   .. seealso:: :doc:`Write Concern </core/write-concern>`,
      :doc:`/reference/write-concern`,
      and :ref:`replica-set-write-concern`.
=============
db.getMongo()
=============

.. default-domain:: mongodb

.. method:: db.getMongo()

   :returns: The current database connection.

   :method:`db.getMongo()` runs when the shell initiates. Use this
   command to test that the :program:`mongo` shell has a connection to
   the proper database instance.
============
db.getName()
============

.. default-domain:: mongodb

.. method:: db.getName()

   :returns: the current database name.
=================
db.getPrevError()
=================

.. default-domain:: mongodb

.. method:: db.getPrevError()

   :returns: A status document, containing the errors.

   .. deprecated:: 1.6

   This output reports all errors since the last time the database
   received a :dbcommand:`resetError` (also
   :method:`db.resetError()`) command.

   This method provides a wrapper around the
   :dbcommand:`getPrevError` command.
======================
db.getProfilingLevel()
======================

.. default-domain:: mongodb

.. method:: db.getProfilingLevel()

   This method provides a wrapper around the database command
   ":dbcommand:`profile`" and returns the current profiling level.

   .. deprecated:: 1.8.4
      Use :method:`db.getProfilingStatus()` for related functionality.
=======================
db.getProfilingStatus()
=======================

.. default-domain:: mongodb

.. method:: db.getProfilingStatus()

   :returns: The current :dbcommand:`profile` level and
             :setting:`~operationProfiling.slowOpThresholdMs` setting.
=======================
db.getReplicationInfo()
=======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.getReplicationInfo()

   :returns:

      A document with the status of the replica status, using data
      polled from the ":term:`oplog`". Use this output when diagnosing
      issues with replication.

Output
------

.. data:: db.getReplicationInfo.logSizeMB

   Returns the total size of the :term:`oplog` in megabytes. This refers
   to the total amount of space allocated to the oplog rather than the
   current size of operations stored in the oplog.

.. data:: db.getReplicationInfo.usedMB

   Returns the total amount of space used by the :term:`oplog` in
   megabytes. This refers to the total amount of space currently used by
   operations stored in the oplog rather than the total amount of space
   allocated.

.. data:: db.getReplicationInfo.errmsg

   Returns an error message if there are no entries in the oplog.

.. data:: db.getReplicationInfo.oplogMainRowCount

   Only present when there are no entries in the oplog. Reports a the
   number of items or rows in the :term:`oplog` (e.g. ``0``).

.. data:: db.getReplicationInfo.timeDiff

   Returns the difference between the first and
   last operation in the :term:`oplog`, represented in seconds.

   Only present if there are entries in the oplog.

.. data:: db.getReplicationInfo.timeDiffHours

   Returns the difference between the first and last
   operation in the :term:`oplog`, rounded and represented in hours.

   Only present if there are entries in the oplog.

.. data:: db.getReplicationInfo.tFirst

   Returns a time stamp for the first (i.e. earliest)
   operation in the :term:`oplog`. Compare this value to the last write
   operation issued against the server.

   Only present if there are entries in the oplog.

.. data:: db.getReplicationInfo.tLast

   Returns a time stamp for the last (i.e. latest)
   operation in the :term:`oplog`. Compare this value to the last write
   operation issued against the server.

   Only present if there are entries in the oplog.

.. data:: db.getReplicationInfo.now

   Returns a time stamp that reflects reflecting the current time.
   The shell process generates this value, and the datum may differ
   slightly from the server time if you're connecting from a remote host
   as a result. Equivalent to :method:`Date()`.

   Only present if there are entries in the oplog.
============
db.getRole()
============

.. default-domain:: mongodb

Definition
----------

.. method:: db.getRole( rolename , showPrivileges )

   Returns the roles from which this role inherits privileges. Optionally, the
   method can also return all the role's privileges.

   Run :method:`db.getRole()` from the database that contains the role. The
   command can retrieve information for both :ref:`user-defined roles
   <user-defined-roles>` and :ref:`built-in roles <built-in-roles>`.

   The :method:`db.getRole()` method takes the following arguments:

   .. include:: /reference/method/db.getRole-param.rst

   :method:`db.getRole()` wraps the :dbcommand:`rolesInfo` command.

Required Access
---------------

.. include:: /includes/access-roles-info.rst

Examples
--------

The following operation returns role inheritance information for the role
``associate`` defined on the ``products`` database:

.. code-block:: javascript

   use products
   db.getRole( "associate" )

The following operation returns role inheritance information *and privileges*
for the role ``associate`` defined on the ``products`` database:

.. code-block:: javascript

   use products
   db.getRole( "associate", { showPrivileges: true } )
=============
db.getRoles()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: db.getRoles()

   Returns information for all the roles in the database on which the command
   runs. The method can be run with or without an argument.

   If run without an argument, :method:`db.getRoles()` returns inheritance
   information for the database's :ref:`user-defined <user-defined-roles>`
   roles.

   To return more information, pass the :method:`db.getRoles()` a
   document with the following fields:

   .. include:: /reference/method/db.getRoles-fields.rst

   :method:`db.getRoles()` wraps the :dbcommand:`rolesInfo` command.

Required Access
---------------

.. include:: /includes/access-roles-info.rst

Example
-------

The following operations return documents for all the roles on the
``products`` database, including role privileges and built-in roles:

.. code-block:: javascript

   db.getRoles(
       {
         rolesInfo: 1,
         showPrivileges:true,
         showBuiltinRoles: true
       }
   )
=================
db.getSiblingDB()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: db.getSiblingDB(<database>)

   .. include:: /reference/method/db.getSiblingDB-param.rst

   :returns: A database object.

   Used to return another database without modifying the
   ``db`` variable in the shell environment.

Example
-------

You can use :method:`db.getSiblingDB()` as an alternative to the ``use
<database>`` helper. This is particularly useful when writing scripts
using the :program:`mongo` shell where the ``use`` helper is not
available. Consider the following sequence of operations:

.. code-block:: javascript

   db = db.getSiblingDB('users')
   db.active.count()

This operation sets the ``db`` object to point to the database named
``users``, and then returns a :doc:`count
</reference/method/db.collection.count>` of the collection named
``active``. You can create multiple ``db`` objects, that refer to
different databases, as in the following sequence of operations:

.. code-block:: javascript

   users = db.getSiblingDB('users')
   records = db.getSiblingDB('records')

   users.active.count()
   users.active.findOne()

   records.requests.count()
   records.requests.findOne()

This operation creates two ``db`` objects referring to different
databases (i.e. ``users`` and ``records``) and then returns a
:doc:`count </reference/method/db.collection.count>` and an
:doc:`example document </reference/method/db.collection.findOne>` from
one collection in that database (i.e. ``active`` and ``requests``
respectively.)
============
db.getUser()
============

.. default-domain:: mongodb

Definition
----------

.. method:: db.getUser(username)

   Returns user information for a specified user. Run this method on the
   user's database. The user must exist on the database on which the method
   runs.

   The :method:`db.getUser()` method has the following parameter:

   .. include:: /reference/method/db.getUser-param.rst

   :method:`db.getUser()` wraps the :dbcommand:`usersInfo` command.

Required Access
---------------

.. include:: /includes/access-user-info.rst

Example
-------

The following sequence of operations returns information about the
``appClient`` user on the ``accounts`` database:

.. code-block:: javascript

   use accounts
   db.getUser("appClient")
=============
db.getUsers()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: db.getUsers()

   Returns information for all the users in the database.

   :method:`db.getUsers()` wraps the :dbcommand:`usersInfo` command.

Required Access
---------------

.. include:: /includes/access-user-info.rst
==========================
db.grantPrivilegesToRole()
==========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.grantPrivilegesToRole ( rolename, privileges, writeConcern )

   Grants additional :ref:`privileges <privileges>` to a :ref:`user-defined
   <user-defined-roles>` role.

   The :method:`grantPrivilegesToRole()` method uses the following syntax:

   .. code-block:: javascript

      db.grantPrivilegesToRole(
          "< rolename >",
          [
              { resource: { <resource> }, actions: [ "<action>", ... ] },
              ...
          ],
          { < writeConcern > }
      )

   The :method:`grantPrivilegesToRole()` method takes the following arguments:

   .. include:: /reference/method/db.grantPrivilegesToRole-param.rst

   The :method:`grantPrivilegesToRole()` method can grant one or more
   privileges. Each ``<privilege>`` has the following syntax:

   .. code-block:: javascript

      { resource: { <resource> }, actions: [ "<action>", ... ] }

   .. |local-cmd-name| replace:: :method:`db.grantPrivilegesToRole()`

   The :method:`db.grantPrivilegesToRole()` method wraps the
   :dbcommand:`grantPrivilegesToRole` command.

Behavior
--------

A role's privileges apply to the database where the role is created. A
role created on the ``admin`` database can include privileges that apply
to all databases or to the :ref:`cluster <resource-cluster>`.

Required Access
---------------

.. include:: /includes/access-grant-privileges.rst

Example
-------

The following :method:`db.grantPrivilegesToRole()` operation grants two
additional privileges to the role ``inventoryCntrl01``, which exists on the
``products`` database. The operation is run on that database:

.. code-block:: javascript

   use products
   db.grantPrivilegesToRole(
     "inventoryCntrl01",
     [
       {
         resource: { db: "products", collection: "" },
         actions: [ "insert" ]
       },
       {
         resource: { db: "products", collection: "system.indexes" },
         actions: [ "find" ]
       }
     ],
     { w: "majority" }
   )

The first privilege permits users with this role to perform the
``insert`` :ref:`action <security-user-actions>` on all collections of
the ``products`` database, except the :doc:`system collections
</reference/system-collections>`. To access a system collection, a
privilege must explicitly specify the system collection in the resource
document, as in the second privilege.

The second privilege permits users with this role to perform the
:authaction:`find` :ref:`action <security-user-actions>` on the
``product`` database's system collection named :data:`system.indexes
<<database>.system.indexes>`.
=====================
db.grantRolesToRole()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.grantRolesToRole ( rolename, roles, writeConcern )

   Grants roles to a :ref:`user-defined role <user-defined-roles>`.

   The :method:`grantRolesToRole` method uses the following syntax:

   .. code-block:: javascript

      db.grantRolesToRole( "<rolename>", [ <roles> ], { <writeConcern> } )

   The :method:`grantRolesToRole` method takes the following arguments:

   .. include:: /reference/method/db.grantRolesToRole-param.rst

   .. |local-cmd-name| replace:: :method:`db.grantRolesToRole()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.grantRolesToRole()` method wraps the
   :dbcommand:`grantRolesToRole` command.

Behavior
--------

A role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can inherit privileges from roles in
any database.

Required Access
---------------

.. include:: /includes/access-grant-roles.rst

Example
-------

.. TODO Update this example

The following :method:`grantRolesToRole()` operation updates the
``productsReaderWriter`` role in the ``products`` database to :ref:`inherit
<inheritance>` the :ref:`privileges <privileges>` of ``productsReader`` role:

.. code-block:: javascript

   use products
   db.grantRolesToRole(
       "productsReaderWriter",
       [ "productsReader" ],
       { w: "majority" , wtimeout: 5000 }
   )
=====================
db.grantRolesToUser()
=====================

.. default-domain:: mongodb

Definition
----------

.. method:: db.grantRolesToUser ( username, roles, writeConcern )

   Grants additional roles to a user.

   The :method:`grantRolesToUser` method uses the following syntax:

   .. code-block:: javascript

      db.grantRolesToUser( "<username>", [ <roles> ], { <writeConcern> } )

   The :method:`grantRolesToUser` method takes the following arguments:

   .. include:: /reference/method/db.grantRolesToUser-param.rst

   .. |local-cmd-name| replace:: :method:`db.grantRolesToUser()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.grantRolesToUser()` method wraps the
   :dbcommand:`grantRolesToUser` command.

Required Access
---------------

.. include:: /includes/access-grant-roles.rst

Example
-------

Given a user ``accountUser01`` in the ``products`` database with the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       }
   ]

The following :method:`grantRolesToUser()` operation gives ``accountUser01``
the :authrole:`readWrite` role on the ``products`` database and the
:authrole:`read` role on the ``stock`` database.

.. code-block:: javascript

   use products
   db.grantRolesToUser( "accountUser01",
                        [ "readWrite" , { role: "read", db: "stock"} ],
                        { w: "majority" , wtimeout: 4000 }
                } )

The user ``accountUser01`` in the ``products`` database now has the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       },
       { "role" : "read",
         "db" : "stock"
       },
       { "role" : "readWrite",
         "db" : "products"
       }
   ]
=========
db.help()
=========

.. default-domain:: mongodb

.. method:: db.help()

   :returns: Text output listing common methods on the ``db`` object.
=============
db.hostInfo()
=============

.. default-domain:: mongodb

.. method:: db.hostInfo()

   .. versionadded:: 2.2

   :returns: A document with information about the underlying system
             that the :program:`mongod` or :program:`mongos` runs on.
             Some of the returned fields are only included on some
             platforms.

   :method:`db.hostInfo()` provides a helper in the :program:`mongo`
   shell around the :dbcommand:`hostInfo` The output of
   :method:`db.hostInfo()` on a Linux system will resemble the
   following:

   .. code-block:: javascript

      {
         "system" : {
                "currentTime" : ISODate("<timestamp>"),
                "hostname" : "<hostname>",
                "cpuAddrSize" : <number>,
                "memSizeMB" : <number>,
                "numCores" : <number>,
                "cpuArch" : "<identifier>",
                "numaEnabled" : <boolean>
         },
         "os" : {
                "type" : "<string>",
                "name" : "<string>",
                "version" : "<string>"
         },
         "extra" : {
                "versionString" : "<string>",
                "libcVersion" : "<string>",
                "kernelVersion" : "<string>",
                "cpuFrequencyMHz" : "<string>",
                "cpuFeatures" : "<string>",
                "pageSize" : <number>,
                "numPages" : <number>,
                "maxOpenFiles" : <number>
         },
         "ok" : <return>
      }

   See :data:`hostInfo` for full documentation of the output of
   :method:`db.hostInfo()`.
=============
db.isMaster()
=============

.. default-domain:: mongodb

.. method:: db.isMaster()

   :returns: A document that describes the role of the
      :program:`mongod` instance.

   If the :program:`mongod` is a member of a :term:`replica set`, then
   the :data:`~isMaster.ismaster` and :data:`~isMaster.secondary`
   fields report if the instance is the :term:`primary` or if it is a
   :term:`secondary` member of the replica set.

   .. see:: :dbcommand:`isMaster` for the complete documentation of
      the output of :method:`db.isMaster()`.
===========
db.killOp()
===========

.. default-domain:: mongodb

Description
-----------

.. method:: db.killOp(opid)

   Terminates an operation as specified by the operation ID. To find
   operations and their corresponding IDs, see :method:`db.currentOp()`.

   The :method:`db.killOp()` method has the following parameter:

   .. include:: /reference/method/db.killOp-param.rst

   .. include:: /includes/warning-terminating-operations.rst
=================
db.listCommands()
=================

.. default-domain:: mongodb

.. method:: db.listCommands()

   Provides a list of all database commands. See the
   :doc:`/reference/command` document for a more extensive index of
   these options.
======================
db.loadServerScripts()
======================

.. default-domain:: mongodb

.. method:: db.loadServerScripts()

   :method:`db.loadServerScripts()` loads all scripts in the
   ``system.js`` collection for the current database into the
   :program:`mongo` shell session.

   Documents in the ``system.js`` collection have the following
   prototype form:

   .. code-block:: javascript

      { _id : "<name>" , value : <function> } }

   The documents in the ``system.js`` collection provide functions
   that your applications can use in any JavaScript context with
   MongoDB in this database. These contexts include :query:`$where`
   clauses and :dbcommand:`mapReduce` operations.
===========
db.logout()
===========

.. default-domain:: mongodb

.. method:: db.logout()

   Ends the current authentication session. This function has no effect
   if the current session is not authenticated.

   .. |operation-name| replace:: :method:`db.logout()`
   .. include:: /includes/note-logout-namespace.rst

   .. example::

      .. include:: /includes/fact-change-database-context.rst

      When you have set the database context and ``db`` object, you
      can use the |operation-name| to log out of database as in the
      following operation:

      .. code-block:: javascript

         db.logout()

   :method:`db.logout()` function provides a wrapper around the
   database command :dbcommand:`logout`.
=========================
db.printCollectionStats()
=========================

.. default-domain:: mongodb

.. method:: db.printCollectionStats()

   Provides a wrapper around the :method:`db.collection.stats()`
   method. Returns statistics from every collection separated by three
   hyphen characters.

   .. |method| replace:: :method:`db.printCollectionStats()`
   .. |method-alternative| replace:: :method:`db.collection.stats()`

   .. include:: /includes/note-method-does-not-return-json.rst


   .. seealso:: :doc:`/reference/command/collStats`
=========================
db.printReplicationInfo()
=========================

.. default-domain:: mongodb

.. method:: db.printReplicationInfo()

   Provides a formatted report of the status of a :term:`replica set`
   from the perspective of the :term:`primary` set member. See the
   :doc:`/reference/command/replSetGetStatus` for more information regarding
   the contents of this output.

   .. |method| replace:: :method:`db.printReplicationInfo()`
   .. |method-alternative| replace:: :method:`rs.status()`

   .. include:: /includes/note-method-does-not-return-json.rst
========================
db.printShardingStatus()
========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.printShardingStatus()

   Prints a formatted report of the sharding configuration and the
   information regarding existing chunks in a :term:`sharded cluster`.

   Only use :method:`db.printShardingStatus()` when connected to a
   :program:`mongos` instance.

   The :method:`db.printShardingStatus()` method has the following
   parameter:

   .. include:: /reference/method/db.printShardingStatus-param.rst

   See :doc:`/reference/method/sh.status` for details of the output.

   .. |method| replace:: :method:`db.printShardingStatus()`
   .. |method-alternative| replace:: :doc:`/reference/config-database/`

   .. include:: /includes/note-method-does-not-return-json.rst

   .. seealso:: :method:`sh.status()`
==============================
db.printSlaveReplicationInfo()
==============================

.. default-domain:: mongodb

.. method:: db.printSlaveReplicationInfo()

   Provides a formatted report of the status of a :term:`replica set`
   from the perspective of the :term:`secondary` set member. See the
   :doc:`/reference/command/replSetGetStatus` for more information regarding
   the contents of this output.

   .. |method| replace:: :method:`db.printSlaveReplicationInfo()`
   .. |method-alternative| replace:: :method:`rs.status()`

   .. include:: /includes/note-method-does-not-return-json.rst
===============
db.removeUser()
===============

.. default-domain:: mongodb

.. deprecated:: 2.6
   Use :method:`db.dropUser()` instead
   of :method:`db.removeUser()`

Definition
----------

.. method:: db.removeUser(username)

   Removes the specified username from the database.

   The :method:`db.removeUser()` method has the following parameter:

   .. include:: /reference/method/db.removeUser-param.rst
===================
db.repairDatabase()
===================

.. default-domain:: mongodb

.. method:: db.repairDatabase()

   .. include:: /includes/warning-repair.rst

   .. include:: /includes/note-repair.rst

   :method:`db.repairDatabase()` provides a wrapper around the database
   command :dbcommand:`repairDatabase`, and has the same effect as
   the run-time option :option:`mongod --repair` option, limited to
   *only* the current database. See :dbcommand:`repairDatabase` for
   full documentation.
===============
db.resetError()
===============

.. default-domain:: mongodb

.. method:: db.resetError()

   .. deprecated:: 1.6

   Resets the error message returned by :method:`db.getPrevError` or
   :dbcommand:`getPrevError`. Provides a wrapper around the
   :dbcommand:`resetError` command.
=============================
db.revokePrivilegesFromRole()
=============================

.. default-domain:: mongodb

Definition
----------

.. method:: db.revokePrivilegesFromRole ( rolename, privileges, writeConcern )

   Removes the specified privileges from the :ref:`user-defined
   <user-defined-roles>` role on the database where the method runs. The
   :method:`revokePrivilegesFromRole` method has the following syntax:

   .. code-block:: javascript

      db.revokePrivilegesFromRole(
          "<rolename>",
          [
              { resource: { <resource> }, actions: [ "<action>", ... ] },
              ...
          ],
          { <writeConcern> }
      )

   The :method:`revokePrivilegesFromRole` method takes the following arguments:

   .. include:: /reference/method/db.revokePrivilegesFromRole-param.rst

   The :method:`db.revokePrivilegesFromRole()` method wraps the
   :dbcommand:`revokePrivilegesFromRole` command.

Behavior
--------

To revoke a privilege, the :doc:`resource document
</reference/resource-document>` pattern must match **exactly** the
``resource`` field of that privilege. The ``actions`` field can be a
subset or match exactly.

For example, given the role ``accountRole`` in the ``products``
database with the following privilege that specifies the ``products``
database as the resource:

.. code-block:: javascript

   {
     "resource" : {
         "db" : "products",
         "collection" : ""
     },
     "actions" : [
         "find",
         "update"
     ]
   }

You *cannot* revoke ``find`` and/or ``update`` from just *one*
collection in the ``products`` database. The following operations
result in no change to the role:

.. code-block:: javascript

   use products
   db.revokePrivilegesFromRole(
      "accountRole",
      [
        {
          resource : {
             db : "products",
             collection : "gadgets"
          },
          actions : [
             "find",
             "update"
          ]
        }
      ]
   )

   db.revokePrivilegesFromRole(
      "accountRole",
      [
        {
          resource : {
             db : "products",
             collection : "gadgets"
          },
          actions : [
             "find"
          ]
        }
      ]
   )

To revoke the ``"find"`` and/or the ``"update"`` action from the role
``accountRole``, you must match the resource document exactly. For
example, the following operation revokes just the ``"find"`` action
from the existing privilege.

.. code-block:: javascript

   use products
   db.revokePrivilegesFromRole(
      "accountRole",
      [
        {
          resource : {
             db : "products",
             collection : ""
          },
          actions : [
             "find"
          ]
        }
      ]
   )

Required Access
---------------

.. include:: /includes/access-revoke-privileges.rst

Example
-------

The following operation removes multiple privileges from the
``associates`` role:

.. code-block:: javascript

   db.revokePrivilegesFromRole(
      "associate",
      [
        {
          resource: { db: "products", collection: "" },
          actions: [ "createCollection", "createIndex", "find" ]
        },
        {
          resource: { db: "products", collection: "orders" },
          actions: [ "insert" ]
        }
      ],
      { w: "majority" }
   )
========================
db.revokeRolesFromRole()
========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.revokeRolesFromRole ( rolename, roles, writeConcern )

   Removes the specified inherited roles from a role.

   The :method:`revokeRolesFromRole` method uses the following syntax:

   .. code-block:: javascript

      db.revokeRolesFromRole( "<rolename>", [ <roles> ], { <writeConcern> } )

   The :method:`revokeRolesFromRole` method takes the following arguments:

   .. include:: /reference/method/db.revokeRolesFromRole-param.rst

   .. |local-cmd-name| replace:: :method:`db.revokeRolesFromRole()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.revokeRolesFromRole()` method wraps the
   :dbcommand:`revokeRolesFromRole` command.

Required Access
---------------

.. include:: /includes/access-revoke-roles.rst

Example
-------

The ``purchaseAgents`` role in the ``emea`` database inherits privileges
from several other roles, as listed in the ``roles`` array:

.. code-block:: javascript

   {
      "_id" : "emea.purchaseAgents",
      "role" : "purchaseAgents",
      "db" : "emea",
      "privileges" : [],
      "roles" : [
         {
            "role" : "readOrdersCollection",
            "db" : "emea"
         },
         {
            "role" : "readAccountsCollection",
            "db" : "emea"
         },
         {
            "role" : "writeOrdersCollection",
            "db" : "emea"
         }
      ]
   }

The following :method:`db.revokeRolesFromRole()` operation on the ``emea``
database removes two roles from the ``purchaseAgents`` role:

.. code-block:: javascript

   use emea
   db.revokeRolesFromRole( "purchaseAgents",
                           [
                             "writeOrdersCollection",
                             "readOrdersCollection"
                           ],
                           { w: "majority" , wtimeout: 5000 }
                         )

The ``purchaseAgents`` role now contains just one role:

.. code-block:: javascript

   {
      "_id" : "emea.purchaseAgents",
      "role" : "purchaseAgents",
      "db" : "emea",
      "privileges" : [],
      "roles" : [
         {
            "role" : "readAccountsCollection",
            "db" : "emea"
         }
      ]
   }
========================
db.revokeRolesFromUser()
========================

.. default-domain:: mongodb

Definition
----------

.. method:: db.revokeRolesFromUser ( )

   Removes a one or more roles from a user on the current
   database. The :method:`db.revokeRolesFromUser()` method uses the
   following syntax:

   .. code-block:: javascript

      db.revokeRolesFromUser( "<username>", [ <roles> ], { <writeConcern> } )

   The :method:`revokeRolesFromUser` method takes the following arguments:

   .. include:: /reference/method/db.revokeRolesFromUser-param.rst

   .. |local-cmd-name| replace:: :method:`db.revokeRolesFromUser()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.revokeRolesFromUser()` method wraps the
   :dbcommand:`revokeRolesFromUser` command.

Required Access
---------------

.. include:: /includes/access-revoke-roles.rst

Example
-------

The ``accountUser01`` user in the ``products`` database has the following
roles:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       },
       { "role" : "read",
         "db" : "stock"
       },
       { "role" : "readWrite",
         "db" : "products"
       }
   ]

The following :method:`db.revokeRolesFromUser()` method removes the two of
the user's roles: the :authrole:`read` role on the ``stock`` database and
the :authrole:`readWrite` role on the ``products`` database, which is also
the database on which the method runs:

.. code-block:: javascript

   use products
   db.revokeRolesFromUser( "accountUser01",
                           [ { role: "read", db: "stock" }, "readWrite" ],
                           { w: "majority" }
                         )

The user ``accountUser01`` user in the ``products`` database now has only
one remaining role:

.. code-block:: javascript

   "roles" : [
       { "role" : "assetsReader",
         "db" : "assets"
       }
   ]
===============
db.runCommand()
===============

.. default-domain:: mongodb

Definition
----------

.. method:: db.runCommand(command)

   Provides a helper to run specified :doc:`database commands
   </reference/command>`. This is the preferred method to issue
   database commands, as it provides a consistent interface between
   the shell and drivers.

   .. include:: /reference/method/db.runCommand-param.rst

   .. versionadded:: 2.6
      To specify a time limit in milliseconds, see
      :doc:`/tutorial/terminate-running-operations`.

Behavior
--------

:method:`db.runCommand()` runs the command in the context of the
current database. Some commands are only applicable in the context of
the ``admin`` database, and you must change your ``db`` object to
before running these commands.
====================
db.serverBuildInfo()
====================

.. default-domain:: mongodb

.. method:: db.serverBuildInfo()

   Provides a wrapper around the :dbcommand:`buildInfo` :term:`database
   command`. :dbcommand:`buildInfo` returns a document that contains
   an overview of parameters used to compile this :program:`mongod`
   instance.
=================
db.serverStatus()
=================

.. default-domain:: mongodb

.. method:: db.serverStatus()

   Returns a :term:`document` that provides an overview of the
   database process's state.

   This command provides a wrapper around the database command
   :dbcommand:`serverStatus`.

   .. versionchanged:: 2.4
      In 2.4 you can dynamically suppress portions of the
      :method:`db.serverStatus()` output, or include suppressed
      sections in a document passed to the :method:`db.serverStatus()`
      method, as in the following example:

   .. code-block:: javascript

      db.serverStatus( { repl: 0, indexCounters: 0, locks: 0 } )
      db.serverStatus( { workingSet: 1, metrics: 0, locks: 0 } )

   .. |operation-name| replace:: :method:`db.serverStatus()`
   .. include:: /includes/example-server-status-projection.rst

   .. seealso:: :doc:`/reference/command/serverStatus` for complete
      documentation of the output of this function.
======================
db.setProfilingLevel()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.setProfilingLevel(level, slowms)

   Modifies the current :term:`database profiler` level used by the
   database profiling system to capture data about performance. The
   method provides a wrapper around the :term:`database command`
   :dbcommand:`profile`.

   .. include:: /reference/method/db.setProfilingLevel-param.rst

   The level chosen can affect performance. It also can allow the
   server to write the contents of queries to the log, which might have
   information security implications for your deployment.

   Configure the :setting:`~operationProfiling.slowOpThresholdMs` option to set the threshold
   for the profiler to consider a query "slow." Specify this value in
   milliseconds to override the default, 100ms.

   :program:`mongod` writes the output of the database profiler to the
   ``system.profile`` collection.

   :program:`mongod` prints information about queries that take longer than
   the :setting:`~operationProfiling.slowOpThresholdMs` to the log even when the database profiler is
   not active.

   .. include:: /includes/note-disable-profiling-fsynclock.rst
===================
db.shutdownServer()
===================

.. default-domain:: mongodb

.. method:: db.shutdownServer()

   Shuts down the current :program:`mongod` or :program:`mongos`
   process cleanly and safely.

   This operation fails when the current database *is not* the
   :term:`admin database`.

   This command provides a wrapper around the :dbcommand:`shutdown`.
==========
db.stats()
==========

.. default-domain:: mongodb

Description
-----------

.. method:: db.stats(scale)

   Returns statistics that reflect the use state of a single :term:`database`.

   The :method:`db.stats()` method has the following parameter:

   .. include:: /reference/method/db.stats-param.rst

   :returns: A :term:`document` with statistics reflecting
             the database system's state. For an explanation of the
             output, see :doc:`/reference/command/dbStats`.

   The :method:`db.stats()` method is a wrapper around the
   :dbcommand:`dbStats` database command.

Example
-------

The following example converts the returned values to kilobytes:

.. code-block:: javascript

   db.stats(1024)

.. note::

   The scale factor rounds values to whole numbers. This can
   produce unpredictable and unexpected results in some situations.
===============
db.updateRole()
===============

.. default-domain:: mongodb

Definition
----------

.. method:: db.updateRole( rolename, update, writeConcern )

   Updates a :ref:`user-defined role <user-defined-roles>`. The
   :method:`db.updateRole()` method must run on the role's database.

   An update to a field **completely replaces** the previous field's values.
   To grant or remove roles or :ref:`privileges <privileges>` without
   replacing all values, use one or more of the following methods:

   - :method:`db.grantRolesToRole()`
   - :method:`db.grantPrivilegesToRole()`
   - :method:`db.revokeRolesFromRole()`
   - :method:`db.revokePrivilegesFromRole()`

   .. warning::

      An update to the ``privileges`` or ``roles`` array completely replaces
      the previous array's values.

   The :method:`updateRole()` method uses the following syntax:

   .. code-block:: javascript

      db.updateRole(
          "<rolename>",
          {
            privileges:
                [
                  { resource: { <resource> }, actions: [ "<action>", ... ] },
                  ...
                ],
            roles:
                [
                  { role: "<role>", db: "<database>" } | "<role>",
                  ...
                ]
          },
          { <writeConcern> }
      )

   The :method:`db.updateRole()` method takes the following arguments.

   .. include:: /reference/method/db.updateRole-param.rst

   The ``update`` document specifies the fields to update and the new
   values. Each field in the ``update`` document is optional, but the
   document must include at least one field. The ``update`` document has the
   following fields:

   .. include:: /reference/method/db.updateRole-update-fields.rst

   .. |local-cmd-name| replace:: :method:`db.updateRole()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.updateRole()` method wraps the :dbcommand:`updateRole`
   command.

Behavior
--------

A role's privileges apply to the database where the role is created. The
role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>` and can inherit
privileges from roles in other databases.

Required Access
---------------

.. include:: /includes/access-update-role.rst

Example
-------

The following :method:`db.updateRole()` method replaces the
:data:`~admin.system.roles.privileges` and the
:data:`~admin.system.roles.roles` for the ``inventoryControl`` role
that exists in the ``products`` database. The method runs on the
database that contains ``inventoryControl``:

.. code-block:: javascript

   use products
   db.updateRole(
       "inventoryControl",
       {
         privileges:
             [
               {
                 resource: { db:"products", collection:"clothing" },
                 actions: [ "update", "createCollection", "createIndex"]
               }
             ],
         roles:
             [
               {
                 role: "read",
                 db: "products"
               }
             ]
       },
       { w:"majority" }
   )

To view a role's privileges, use the :dbcommand:`rolesInfo` command.
===============
db.updateUser()
===============

.. default-domain:: mongodb

Definition
----------

.. method:: db.updateUser( username, update, writeConcern )

   Updates the user's profile on the database on which you run the method.
   An update to a field **completely replaces** the previous field's values.
   This includes updates to the user's ``roles`` array.

   .. warning::

      When you update the ``roles`` array, you completely replace the
      previous array's values. To add or remove roles without replacing all
      the user's existing roles, use the :method:`db.grantRolesToUser()` or
      :method:`db.revokeRolesFromUser()` methods.

   The :method:`db.updateUser()` method uses the following syntax:

   .. code-block:: javascript

      db.updateUser(
         "<username>",
         {
           customData : { <any information> },
           roles : [
                     { role: "<role>", db: "<database>" } | "<role>",
                     ...
                   ],
           pwd: "<cleartext password>"
          },
          writeConcern: { <write concern> }
      )

   The :method:`db.updateUser()` method has the following arguments.

   .. include:: /reference/method/db.updateUser-param.rst

   The ``update`` document specifies the fields to update and their
   new values. All  fields in the ``update`` document are optional,
   but *must* include at least one field.

   The ``update`` document has the following fields:

   .. include:: /reference/method/db.updateUser-update-fields.rst

   .. |local-cmd-name| replace:: :method:`db.updateUser()`
   .. include:: /includes/fact-roles-array-contents.rst

   The :method:`db.updateUser()` method wraps the :dbcommand:`updateUser`
   command.

Behavior
--------

:method:`db.updateUser()` sends password to the MongoDB instance
*without* encryption. To encrypt the password during transmission,
use :doc:`SSL </tutorial/configure-ssl>`.

Required Access
---------------

.. include:: /includes/access-update-user.rst

.. include:: /includes/access-change-own-password-and-custom-data.rst

Example
-------

Given a user ``appClient01`` in the ``products`` database with the following
user info:

.. code-block:: javascript

   {
      "_id" : "products.appClient01",
      "user" : "appClient01",
      "db" : "products",
      "customData" : { "empID" : "12345", "badge" : "9156" },
      "roles" : [
          { "role" : "readWrite",
            "db" : "products"
          },
          { "role" : "read",
            "db" : "inventory"
          }
      ]
   }

The following :method:`db.updateUser()` method **completely** replaces the
user's ``customData`` and ``roles`` data:

.. code-block:: javascript

   use products
   db.updateUser( "appClient01",
                  {
                    customData : { employeeId : "0x3039" },
                    roles : [
                              { role : "read", db : "assets"  }
                            ]
                   }
                )

The user ``appClient01`` in the ``products`` database now has the following
user information:

.. code-block:: javascript

   {
      "_id" : "products.appClient01",
      "user" : "appClient01",
      "db" : "products",
      "customData" : { "employeeId" : "0x3039" },
      "roles" : [
          { "role" : "read",
            "db" : "assets"
          }
      ]
   }
=================
db.upgradeCheck()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: db.upgradeCheck(<document>)

   .. versionadded:: 2.6

   Performs a preliminary check for upgrade preparedness to 2.6. The
   helper, available in the 2.6 :program:`mongo` shell, can run
   connected to either a 2.4 or a 2.6 server.

   The method checks for:

   - documents with index keys :ref:`longer than the index key limit
     <2.6-index-key-length-incompatibility>`,

   - documents with :limit:`illegal field names
     <Restrictions on Field Names>`,

   - collections without an ``_id`` index, and

   - indexes with invalid specifications, such as an index key with an
     empty or illegal field name.

   The method can accept a document parameter which determine the scope
   of the check:

   .. include:: /reference/method/db.upgradeCheck-param.rst

   The optional scope document has the following form:

   .. code-block:: javascript

      {
         collection: <string>
      }

   Additional 2.6 changes that affect compatibility with older versions
   require manual checks and intervention. See
   :doc:`/release-notes/2.6-compatibility` for details.

   .. seealso:: :method:`db.upgradeCheckAllDBs()`

Behavior
--------

.. |method| replace:: :method:`db.upgradeCheck()`

.. include:: /includes/fact-upgradeCheck-behavior.rst

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`find` action on all collections, including
the :doc:`system collections </reference/system-collections>`.

Example
-------

The following example connects to a secondary running on ``localhost``
and runs :method:`db.upgradeCheck()` against the ``employees``
collection in the ``records`` database. Because the output from the
method can be quite large, the example pipes the output to a file.

.. code-block:: sh

   ./mongo --eval "db.getMongo().setSlaveOk(); db.upgradeCheck( { collection: 'employees' } )"  localhost/records | tee /tmp/upgradecheck.txt

.. include:: /includes/output-upgrade-check.rst
=======================
db.upgradeCheckAllDBs()
=======================

.. default-domain:: mongodb

Definition
----------

.. method:: db.upgradeCheckAllDBs()

   .. versionadded:: 2.6

   Performs a preliminary check for upgrade preparedness to 2.6. The
   helper, available in the 2.6 :program:`mongo` shell, can run
   connected to either a 2.4 or a 2.6 server in the ``admin`` database.

   The method cycles through all the databases and checks for:

   - documents with index keys :ref:`longer than the index key limit
     <2.6-index-key-length-incompatibility>`,

   - documents with :limit:`illegal field names
     <Restrictions on Field Names>`,

   - collections without an ``_id`` index, and

   - indexes with invalid specifications, such as an index key with an
     empty or illegal field name.

   Additional 2.6 changes that affect compatibility with older versions
   require manual checks and intervention. See
   :doc:`/release-notes/2.6-compatibility` for details.

   .. seealso:: :method:`db.upgradeCheck()`

   .. admin-only

Behavior
--------

.. |method| replace:: :method:`db.upgradeCheckAllDBs()`

.. include:: /includes/fact-upgradeCheck-behavior.rst

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`listDatabases` action on all databases and
the :authaction:`find` action on all collections, including the
:doc:`system collections </reference/system-collections>`.

Example
-------

The following example connects to a secondary running on ``localhost``
and runs :method:`db.upgradeCheckAllDBs()` against the ``admin``
database. Because the output from the method can be quite large, the
example pipes the output to a file.

.. code-block:: sh

   ./mongo --eval "db.getMongo().setSlaveOk(); db.upgradeCheckAllDBs();" localhost/admin | tee /tmp/upgradecheckalldbs.txt

.. _upgrade-check-output:

.. include:: /includes/output-upgrade-check.rst
============
db.version()
============

.. default-domain:: mongodb

.. method:: db.version()

   :returns: The version of the :program:`mongod` or :program:`mongos` instance.
==========
fuzzFile()
==========

.. default-domain:: mongodb

Description
-----------

.. method:: fuzzFile(filename)

   For internal use.

   .. include:: /reference/method/fuzzFile-param.rst
=============
getHostName()
=============

.. default-domain:: mongodb

.. method:: getHostName()

   :returns: The hostname of the system running the :program:`mongo`
             shell process.
============
getMemInfo()
============

.. default-domain:: mongodb

.. method:: getMemInfo()

   Returns a document with two fields that report the amount of memory
   used by the JavaScript shell process. The fields returned are
   :term:`resident <resident memory>` and :term:`virtual <virtual
   memory>`.
==========
hostname()
==========

.. default-domain:: mongodb

.. method:: hostname()

   :returns: The hostname of the system running the :program:`mongo`
              shell process.
============
_isWindows()
============

.. default-domain:: mongodb

.. method:: _isWindows()

   :returns: boolean.

   Returns "true" if the :program:`mongo` shell is running on a
   system that is Windows, or "false" if the shell is running
   on a Unix or Linux systems.
======================
Bulk Operation Methods
======================

.. versionadded:: 2.6

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-bulk.rst

.. include:: /includes/toc/method-bulk.rst
==================
Collection Methods
==================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-collection.rst

.. include:: /includes/toc/method-collection.rst
==================
Connection Methods
==================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-connection.rst

.. include:: /includes/toc/method-connection.rst
===============================
Object Constructors and Methods
===============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-constructor.rst

.. include:: /includes/toc/method-constructor.rst
==============
Cursor Methods
==============

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-cursor.rst

.. include:: /includes/toc/method-cursor.rst
================
Database Methods
================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-database.rst

.. include:: /includes/toc/method-database.rst
==============
Native Methods
==============

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-native.rst

.. include:: /includes/toc/method-native.rst
========================
Query Plan Cache Methods
========================

.. default-domain:: mongodb

The PlanCache methods are only accessible from a collection's plan
cache object. To retrieve the plan cache object, use the
:method:`db.collection.getPlanCache()` method.

.. include:: /includes/toc/table-method-plan-cache.rst

.. include:: /includes/toc/method-plan-cache.rst
===================
Replication Methods
===================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-rs.rst

.. include:: /includes/toc/method-rs.rst
=======================
Role Management Methods
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-role-management.rst

.. include:: /includes/toc/method-role-management.rst
================
Sharding Methods
================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-sh.rst

.. include:: /includes/toc/method-sh.rst
==================
Subprocess Methods
==================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-subprocess.rst

.. include:: /includes/toc/method-subprocess.rst
=======================
User Management Methods
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-method-user-management.rst

.. include:: /includes/toc/method-user-management.rst
===========
listFiles()
===========

.. default-domain:: mongodb

.. method:: listFiles()

   Returns an array, containing one document per object in the
   directory. This function operates in the context of the
   :program:`mongo` process. The included fields are:

   .. describe:: name

      Returns a string which contains the name of the object.

   .. describe:: isDirectory

      Returns true or false if the object is a directory.

   .. describe:: size

      Returns the size of the object in bytes. This field is only
      present for files.
======
load()
======

.. default-domain:: mongodb

Definition
----------

.. method:: load(file)

   Loads and runs a JavaScript file into the current shell environment.

   The :method:`load()` method has the following parameter:

   .. include:: /reference/method/load-param.rst


   Specify filenames with relative or absolute paths. When using
   relative path names, confirm the current directory using the
   :method:`pwd()` method.

   After executing a file with :method:`load()`, you may reference any
   functions or variables defined the file from the :program:`mongo`
   shell environment.

Example
-------

Consider the following examples of the :method:`load()` method:

.. code-block:: javascript

   load("scripts/myjstest.js")
   load("/data/db/scripts/myjstest.js")
====
ls()
====

.. default-domain:: mongodb

.. method:: ls()

   Returns a list of the files in the current directory.

   This function returns with output relative to the current shell
   session, and does not impact the server.
============
md5sumFile()
============

.. default-domain:: mongodb

Description
-----------

.. method:: md5sumFile(filename)

   Returns a :term:`md5` hash of the specified file.

   The :method:`md5sumFile()` method has the following parameter:

   .. include:: /reference/method/md5sumFile-param.rst

   .. note:: The specified filename must refer to a file located on
             the system running the :program:`mongo` shell.
=======
mkdir()
=======

.. default-domain:: mongodb

Description
-----------

.. method:: mkdir(path)

   Creates a directory at the specified path. This method creates
   the entire path specified if the enclosing directory or
   directories do not already exit.

   This method is equivalent to :command:`mkdir -p` with BSD or GNU utilities.

   The :method:`mkdir()` method has the following parameter:

   .. include:: /reference/method/mkdir-param.rst
=============
Mongo.getDB()
=============

.. default-domain:: mongodb

Description
-----------

.. method:: Mongo.getDB(<database>)

   Provides access to database objects from
   the :program:`mongo` shell or from a JavaScript file.

   The :method:`Mongo.getDB()` method has the following parameter:

   .. include:: /reference/method/Mongo.getDB-param.rst

Example
-------

The following example instantiates a new connection to the MongoDB
instance running on the localhost interface and returns a reference
to ``"myDatabase"``:

.. code-block:: javascript

   db = new Mongo().getDB("myDatabase");

.. seealso:: :method:`Mongo()` and :doc:`/reference/method/connect`
=======================
Mongo.getReadPrefMode()
=======================

.. default-domain:: mongodb

.. method:: Mongo.getReadPrefMode()

   :returns: The current :term:`read preference` mode for the
             :method:`Mongo() <db.getMongo()>` connection object.

   See :doc:`/core/read-preference` for an introduction to read
   preferences in MongoDB. Use :method:`~Mongo.getReadPrefMode()` to
   return the current read preference mode, as in the following
   example:

   .. code-block:: javascript

      db.getMongo().getReadPrefMode()

   Use the following operation to return and print the current read
   preference mode:

   .. code-block:: javascript

      print(db.getMongo().getReadPrefMode());

   This operation will return one of the following read preference
   modes:

   - :readmode:`primary`
   - :readmode:`primaryPreferred`
   - :readmode:`secondary`
   - :readmode:`secondaryPreferred`
   - :readmode:`nearest`

   .. seealso:: :doc:`/core/read-preference`,
      :method:`~cursor.readPref()`, :method:`~Mongo.setReadPref()`, and
      :method:`~Mongo.getReadPrefTagSet()`.
=========================
Mongo.getReadPrefTagSet()
=========================

.. default-domain:: mongodb

.. method:: Mongo.getReadPrefTagSet()

   :returns: The current :term:`read preference` tag set for the
             :method:`Mongo() <db.getMongo()>` connection object.

   See :doc:`/core/read-preference` for an introduction to read
   preferences and tag sets in MongoDB. Use
   :method:`~Mongo.getReadPrefTagSet()` to return the current read
   preference tag set, as in the following example:

   .. code-block:: javascript

      db.getMongo().getReadPrefTagSet()

   Use the following operation to return and print the current read
   preference tag set:

   .. code-block:: javascript

      printjson(db.getMongo().getReadPrefTagSet());

   .. seealso:: :doc:`/core/read-preference`,
      :method:`~cursor.readPref()`, :method:`~Mongo.setReadPref()`, and
      :method:`~Mongo.getReadPrefTagSet()`.
===================
Mongo.setReadPref()
===================

.. default-domain:: mongodb

Definition
----------

.. method:: Mongo.setReadPref(mode, tagSet)

   Call the :method:`~Mongo.setReadPref()` method on a :method:`Mongo
   <db.getMongo()>` connection object to control how the client will
   route all queries to members of the replica set.

   .. include:: /reference/method/Mongo.setReadPref-param.rst

Examples
--------

To set a read preference mode in the :program:`mongo` shell, use the
following operation:

.. code-block:: javascript

   db.getMongo().setReadPref('primaryPreferred')

To set a read preference that uses a tag set, specify an array of tag
sets as the second argument to :method:`Mongo.setReadPref()`, as in the
following:

.. code-block:: javascript

   db.getMongo().setReadPref('primaryPreferred', [ { "dc": "east" } ] )

You can specify multiple tag sets, in order of preference, as in the
following:

.. code-block:: javascript

   db.getMongo().setReadPref('secondaryPreferred',
                             [ { "dc": "east", "use": "production" },
                               { "dc": "east", "use": "reporting" },
                               { "dc": "east" },
                               {}
                             ] )

If the replica set cannot satisfy the first tag set, the client will
attempt to use the second read preference. Each tag set can contain zero
or more field/value tag pairs, with an "empty" document acting as a
wildcard which matches a replica set member with any tag set or no tag
set.

.. note::

   You must call :method:`Mongo.setReadPref()` on the connection object
   before retrieving documents using that connection to use that read
   preference.
==================
mongo.setSlaveOk()
==================

.. default-domain:: mongodb

.. method:: Mongo.setSlaveOk()

   For the current session, this command permits read operations from
   non-master (i.e. :term:`slave` or :term:`secondary`)
   instances. Practically, use this method in the following form:

   .. code-block:: javascript

      db.getMongo().setSlaveOk()

   Indicates that ":term:`eventually consistent <eventual
   consistency>`" read operations are acceptable for the current
   application. This function provides the same functionality as
   :method:`rs.slaveOk()`.

   See the :method:`~cursor.readPref()` method for more
   fine-grained control over :doc:`read preference
   </core/read-preference>` in the :program:`mongo` shell.
=======
Mongo()
=======

.. default-domain:: mongodb

Description
-----------

.. method:: Mongo(host)

   JavaScript constructor to instantiate a database connection from the
   :program:`mongo` shell or from a JavaScript file.

   The :method:`Mongo()` method has the following parameter:

   .. include:: /reference/method/Mongo-param.rst

Instantiation Options
---------------------

Use the constructor without a parameter to instantiate a connection to
the localhost interface on the default port.

Pass the ``<host>`` parameter to the constructor to instantiate a
connection to the ``<host>`` and the default port.

Pass the ``<host><:port>`` parameter to the constructor to instantiate a
connection to the ``<host>`` and the ``<port>``.

.. seealso:: :method:`Mongo.getDB()` and :method:`db.getMongo()`.
=======================
ObjectId.getTimestamp()
=======================

.. default-domain:: mongodb

.. method:: ObjectId.getTimestamp()

   :returns: The timestamp portion of the :ref:`ObjectId()
             <core-object-id-class>` object as a Date.

   In the following example, call the :method:`getTimestamp()
   <ObjectId.getTimestamp()>` method on an ObjectId
   (e.g. ``ObjectId("507c7f79bcf86cd7994f6c0e")``):

   .. code-block:: javascript

      ObjectId("507c7f79bcf86cd7994f6c0e").getTimestamp()

   This will return the following output:

   .. code-block:: javascript

      ISODate("2012-10-15T21:26:17Z")
===================
ObjectId.toString()
===================

.. default-domain:: mongodb

.. method:: ObjectId.toString()

   :returns: The string representation of the :ref:`ObjectId()
             <core-object-id-class>` object. This value has the format
             of ``ObjectId(...)``.

   .. versionchanged:: 2.2
      In previous versions :method:`ObjectId.toString()` returns the
      value of the ObjectId as a hexadecimal string.

   In the following example, call the :method:`~ObjectId.toString()`
   method on an ObjectId (e.g. ``ObjectId("507c7f79bcf86cd7994f6c0e")``):

   .. code-block:: javascript

      ObjectId("507c7f79bcf86cd7994f6c0e").toString()

   This will return the following string:

   .. code-block:: javascript

      ObjectId("507c7f79bcf86cd7994f6c0e")

   You can confirm the type of this object using the following
   operation:

   .. code-block:: javascript

      typeof ObjectId("507c7f79bcf86cd7994f6c0e").toString()
==================
ObjectId.valueOf()
==================

.. default-domain:: mongodb

.. method:: ObjectId.valueOf()

   :returns: The value of the :ref:`ObjectId() <core-object-id-class>`
             object as a lowercase hexadecimal string. This value is
             the ``str`` attribute of the ``ObjectId()`` object.

   .. versionchanged:: 2.2
      In previous versions :method:`ObjectId.valueOf()` returns the
      ``ObjectId()`` object.

   In the following example, call the :method:`valueOf()
   <ObjectId.valueOf()>` method on an ObjectId
   (e.g. ``ObjectId("507c7f79bcf86cd7994f6c0e")``):

   .. code-block:: javascript

      ObjectId("507c7f79bcf86cd7994f6c0e").valueOf()

   This will return the following string:

   .. code-block:: javascript

      507c7f79bcf86cd7994f6c0e

   You can confirm the type of this object using the following
   operation:

   .. code-block:: javascript

      typeof ObjectId("507c7f79bcf86cd7994f6c0e").valueOf()
=================
PlanCache.clear()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: PlanCache.clear()

   Removes all cached query plans for a collection.

   The method is only available from the :method:`plan cache object
   <db.collection.getPlanCache()>` of a specific collection; i.e.

   .. code-block:: javascript

      db.collection.getPlanCache().clear()

   For example, to clear the cache for the ``orders`` collection:

   .. code-block:: javascript

      db.orders.getPlanCache().clear()

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheWrite` action.

.. seealso::

   - :method:`db.collection.getPlanCache()`

   - :method:`PlanCache.clearPlansByQuery()`
=============================
PlanCache.clearPlansByQuery()
=============================

.. default-domain:: mongodb

Definition
----------

.. method:: PlanCache.clearPlansByQuery( <query>, <projection>, <sort> )

   Clears the cached query plans for the specified :term:`query shape`.

   The method is only available from the :method:`plan cache object
   <db.collection.getPlanCache()>` of a specific collection; i.e.

   .. code-block:: javascript

      db.collection.getPlanCache().clearPlansByQuery( <query>, <projection>, <sort> )

   The :method:`PlanCache.clearPlansByQuery()` method accepts the
   following parameters:

   .. include:: PlanCache.clearPlansByQuery-param.rst

   To see the query shapes for which cached query plans exist, use the
   :method:`PlanCache.listQueryShapes()` method.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheWrite` action.

Example
-------

If a collection ``orders`` has the following query shape:

.. code-block:: javascript

     {
       "query" : { "qty" : { "$gt" : 10 } },
       "sort" : { "ord_date" : 1 },
       "projection" : { }
     }

The following operation removes the query plan cached for the shape:

.. code-block:: javascript

   db.orders.getPlanCache().clearPlansByQuery(
      { "qty" : { "$gt" : 10 } },
      { },
      { "ord_date" : 1 }
   )

.. seealso::

   - :method:`db.collection.getPlanCache()`
   - :method:`PlanCache.listQueryShapes()`
   - :method:`PlanCache.clear()`
===========================
PlanCache.getPlansByQuery()
===========================

.. default-domain:: mongodb

Definition
----------

.. method:: PlanCache.getPlansByQuery( <query>, <projection>, <sort> )

   Displays the cached query plans for the specified :term:`query
   shape`.

   .. include:: /includes/fact-query-optimizer-cache-behavior.rst

   The method is only available from the :method:`plan cache object
   <db.collection.getPlanCache()>` of a specific collection; i.e.

   .. code-block:: javascript

      db.collection.getPlanCache().getPlansByQuery( <query>, <projection>, <sort> )

   The :method:`PlanCache.getPlansByQuery()` method accepts the
   following parameters:

   .. include:: PlanCache.getPlansByQuery-param.rst

   :returns: Array of cached query plans for a query shape.

   To see the query shapes for which cached query plans exist, use the
   :method:`PlanCache.listQueryShapes()` method.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheRead` action.

Example
-------

If a collection ``orders`` has the following query shape:

.. code-block:: javascript

     {
       "query" : { "qty" : { "$gt" : 10 } },
       "sort" : { "ord_date" : 1 },
       "projection" : { }
     }

The following operation displays the query plan cached for the shape:

.. code-block:: javascript

   db.orders.getPlanCache().getPlansByQuery(
      { "qty" : { "$gt" : 10 } },
      { },
      { "ord_date" : 1 }
   )

.. seealso::

   - :method:`db.collection.getPlanCache()`
   - :method:`PlanCache.listQueryShapes()`
   - :method:`PlanCache.help()`
================
PlanCache.help()
================

.. default-domain:: mongodb

Definition
----------

.. method:: PlanCache.help()

   Displays the methods available to view and modify a collection's
   query plan cache.

   The method is only available from the :method:`plan cache object
   <db.collection.getPlanCache()>` of a specific collection; i.e.

   .. code-block:: javascript

      db.collection.getPlanCache().help()

.. seealso:: :method:`db.collection.getPlanCache()`
===========================
PlanCache.listQueryShapes()
===========================

.. default-domain:: mongodb

Definition
----------

.. method:: PlanCache.listQueryShapes()

   Displays the :term:`query shapes <query shape>` for which cached
   query plans exist.

   .. include:: /includes/fact-query-optimizer-cache-behavior.rst

   The method is only available from the :method:`plan cache object
   <db.collection.getPlanCache()>` of a specific collection; i.e.

   .. code-block:: javascript

      db.collection.getPlanCache().listQueryShapes()

   :returns: Array of :term:`query shape <query shape>` documents.

   The method wraps the :dbcommand:`planCacheListQueryShapes` command.

Required Access
---------------

On systems running with :setting:`~security.authentication`, a user must have access that
includes the :authaction:`planCacheRead` action.

Example
-------

The following returns the :term:`query shapes <query shape>` that have
cached plans for the ``orders`` collection:

.. code-block:: javascript

   db.orders.getPlanCache().listQueryShapes()

The method returns an array of the query shapes currently in the cache.
In the example, the ``orders`` collection had cached query plans
associated with the following shapes:

.. code-block:: javascript

   [
     {
       "query" : { "qty" : { "$gt" : 10 } },
       "sort" : { "ord_date" : 1 },
       "projection" : { }
     },
     {
       "query" : { "$or" :
          [
            { "qty" : { "$gt" : 15 }, "item" : "xyz123" },
            { "status" : "A" }
          ]
       },
       "sort" : { },
       "projection" : { }
     },
     {
       "query" : { "$or" : [ { "qty" : { "$gt" : 15 } }, { "status" : "A" } ] },
       "sort" : { },
       "projection" : { }
     }
   ]

.. seealso::
   - :method:`db.collection.getPlanCache()`
   - :method:`PlanCache.getPlansByQuery()`
   - :method:`PlanCache.help()`
   - :dbcommand:`planCacheListQueryShapes`
=====
pwd()
=====

.. default-domain:: mongodb

.. method:: pwd()

   Returns the current directory.

   This function returns with output relative to the current shell
   session, and does not impact the server.
======
quit()
======

.. default-domain:: mongodb

.. method:: quit()

   Exits the current shell session.
======
rand()
======

.. default-domain:: mongodb

.. method:: _rand()

   :returns: A random number between ``0`` and ``1``.

   This function provides functionality similar to the
   ``Math.rand()`` function from the standard library.
=======================
rawMongoProgramOutput()
=======================

.. default-domain:: mongodb

.. method:: rawMongoProgramOutput()

   For internal use.
============
removeFile()
============

.. default-domain:: mongodb

Description
-----------

.. method:: removeFile(filename)

   Removes the specified file from the local file system.

   The :method:`removeFile()` method has the following parameter:

   .. include:: /reference/method/removeFile-param.rst
=============
resetDbpath()
=============

.. default-domain:: mongodb

.. method:: resetDbpath()

   For internal use.
========
rs.add()
========

.. default-domain:: mongodb

Definition
----------

.. method:: rs.add(host, arbiterOnly)

   Adds a member to a :term:`replica set`.

   .. include:: /reference/method/rs.add-param.rst

   You may specify new hosts in one of two ways:

   #. as a "hostname" with an optional port number to use the default
      configuration as in the :ref:`replica-set-add-member` example.

   #. as a configuration :term:`document`, as in the
      :ref:`replica-set-add-member-alternate-procedure` example.

   This function will disconnect the shell briefly and forces a
   reconnection as the replica set renegotiates which member
   will be :term:`primary`. As a result, the shell will display an
   error even if this command succeeds.

   :method:`rs.add()` provides a wrapper around some of the
   functionality of the ":dbcommand:`replSetReconfig`" :term:`database
   command` and the corresponding shell helper
   :method:`rs.reconfig()`. See the :doc:`/reference/replica-configuration`
   document for full documentation of all replica set configuration
   options.

.. Example
.. -------

Example
-------

To add a :program:`mongod` accessible on the default port
``27017`` running on the host ``mongodb3.example.net``, use the
following :method:`rs.add()` invocation:

.. code-block:: javascript

   rs.add('mongodb3.example.net:27017')

If ``mongodb3.example.net`` is an arbiter, use the following form:

.. code-block:: javascript

   rs.add('mongodb3.example.net:27017', true)

To add ``mongodb3.example.net`` as a :ref:`secondary-only
<replica-set-secondary-only-members>` member of set, use the
following form of :method:`rs.add()`:

.. code-block:: javascript

   rs.add( { "_id": 3, "host": "mongodbd3.example.net:27017", "priority": 0 } )

Replace, ``3`` with the next unused ``_id`` value in the replica
set. See :method:`rs.conf()` to see the existing ``_id`` values
in the replica set configuration document.

See the :doc:`/reference/replica-configuration` and
:doc:`/administration/replica-sets` documents for more
information.
===========
rs.addArb()
===========

.. default-domain:: mongodb

Description
-----------

.. method:: rs.addArb(host)

   Adds a new :term:`arbiter` to an existing replica set.

   The :method:`rs.addArb()` method takes the following parameter:

   .. include:: /reference/method/rs.addArb-param.rst

   This function briefly disconnects the shell and forces a reconnection
   as the replica set renegotiates which member will be :term:`primary`.
   As a result, the shell displays an error even if this command
   succeeds.
=========
rs.conf()
=========

.. default-domain:: mongodb

.. method:: rs.conf()

   :returns: a :term:`document` that contains the current
             :term:`replica set` configuration document.

   See :doc:`/reference/replica-configuration` for more information on
   the replica set configuration document.

.. method:: rs.config()

   :method:`rs.config()` is an alias of :method:`rs.conf()`.
===========
rs.freeze()
===========

.. default-domain:: mongodb

Description
-----------

.. method:: rs.freeze(seconds)

   Makes the current :term:`replica set` member ineligible to become
   :term:`primary` for the period specified.

   The :method:`rs.freeze()` method has the following parameter:

   .. include:: /reference/method/rs.freeze-param.rst

   :method:`rs.freeze()` provides a wrapper around the :term:`database
   command` :dbcommand:`replSetFreeze`.
=========
rs.help()
=========

.. default-domain:: mongodb

.. method:: rs.help()

   Returns a basic help text for all of the :doc:`replication
   </replication>` related shell functions.
=============
rs.initiate()
=============

.. default-domain:: mongodb

Description
-----------

.. method:: rs.initiate(configuration)

   Initiates a :term:`replica set`. Optionally takes a configuration
   argument in the form of a :term:`document` that holds the
   configuration of a replica set.

   The :method:`rs.initiate()` method has the following parameter:

   .. include:: /reference/method/rs.initiate-param.rst

   The :method:`rs.initiate()` method provides a wrapper around the
   ":dbcommand:`replSetInitiate`" :term:`database command`.

Replica Set Configuration
-------------------------

See :doc:`/administration/replica-set-member-configuration` and
:doc:`/reference/replica-configuration` for examples of replica set
configuration and invitation objects.
=========================
rs.printReplicationInfo()
=========================

.. default-domain:: mongodb

.. method:: rs.printReplicationInfo()

   Returns a formatted report of the status of a :term:`replica set`
   from the perspective of the :term:`primary` member of the set. The
   output is identical to that of :method:`db.printReplicationInfo()`.
   See the :doc:`/reference/command/replSetGetStatus` for more
   information regarding the contents of this output.

   .. |method| replace:: :method:`rs.printReplicationInfo()`
   .. |method-alternative| replace:: :method:`rs.status()`

   .. include:: /includes/note-method-does-not-return-json.rst
==============================
rs.printSlaveReplicationInfo()
==============================

.. default-domain:: mongodb

.. method:: rs.printSlaveReplicationInfo()

   Returns a formatted report of the status of a :term:`replica set`
   from the perspective of the :term:`secondary` member of the set. The
   output is identical to that of
   :method:`db.printSlaveReplicationInfo()`.See the
   :doc:`/reference/command/replSetGetStatus` for more information
   regarding the contents of this output.

   .. |method| replace:: :method:`rs.printSlaveReplicationInfo()`
   .. |method-alternative| replace:: :method:`rs.status()`

   .. include:: /includes/note-method-does-not-return-json.rst
=============
rs.reconfig()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: rs.reconfig(configuration, force)

   Initializes a new :term:`replica set` configuration. Disconnects the
   shell briefly and forces a reconnection as the replica set
   renegotiates which member will be :term:`primary`. As a result, the
   shell will display an error even if this command succeeds.

   .. include:: /reference/method/rs.reconfig-param.rst

   :method:`rs.reconfig()` overwrites the existing replica set
   configuration. Retrieve the current configuration object with
   :method:`rs.conf()`, modify the configuration as needed and then
   use :method:`rs.reconfig()` to submit the modified configuration
   object.

   :method:`rs.reconfig()` provides a wrapper around the
   ":dbcommand:`replSetReconfig`" :term:`database command`.

Examples
--------

To reconfigure a replica set, use the following sequence of operations:

.. code-block:: javascript

   conf = rs.conf()

   // modify conf to change configuration

   rs.reconfig(conf)

If you want to force the reconfiguration if a majority of the set is not
connected to the current member, or you are issuing the command against a
secondary, use the following form:

.. code-block:: javascript

   conf = rs.conf()

   // modify conf to change configuration

   rs.reconfig(conf, { force: true } )

.. warning::

   Forcing a :method:`rs.reconfig()` can lead to :term:`rollback`
   situations and other difficult to recover from situations. Exercise
   caution when using this option.

.. seealso:: :doc:`/reference/replica-configuration` and :doc:`/administration/replica-sets`.
===========
rs.remove()
===========

.. default-domain:: mongodb

Definition
----------

.. method:: rs.remove(hostname)

   Removes the member described by the ``hostname`` parameter from the
   current :term:`replica set`. This function will disconnect the
   shell briefly and forces a reconnection as the :term:`replica set`
   renegotiates which member will be :term:`primary`. As a
   result, the shell will display an error even if this command
   succeeds.

   The :method:`rs.remove()` method has the following parameter:

   .. include:: /reference/method/rs.remove-param.rst

   .. note::

      Before running the :method:`rs.remove()` operation, you must *shut
      down* the replica set member that you're removing.

      .. versionchanged:: 2.2
         This procedure is no longer required when using
         :method:`rs.remove()`, but it remains good practice.
============
rs.slaveOk()
============

.. default-domain:: mongodb

.. method:: rs.slaveOk()

   Provides a shorthand for the following operation:

   .. code-block:: javascript

      db.getMongo().setSlaveOk()

   This allows the current connection to allow read operations to run
   on :term:`secondary` members. See the :method:`readPref()
   <cursor.readPref()>` method for more fine-grained control over
   :doc:`read preference </core/read-preference>` in the
   :program:`mongo` shell.
===========
rs.status()
===========

.. default-domain:: mongodb

.. method:: rs.status()

   :returns: A :term:`document` with status information.

   This output reflects the current status of the replica set, using
   data derived from the heartbeat packets sent by the other members
   of the replica set.

   This method provides a wrapper around the
   :dbcommand:`replSetGetStatus`\ :term:`database command`. See the
   documentation of the command for a complete description
   of the :ref:`output <rs-status-output>`.
=============
rs.stepDown()
=============

.. default-domain:: mongodb

Description
-----------

.. method:: rs.stepDown(seconds)

   Forces the current :term:`replica set` member to step down as
   :term:`primary` and then attempt to avoid election as primary for
   the designated number of seconds. Produces an error if the current
   member is not the primary.

   The :method:`rs.stepDown()` method has the following parameter:

   .. include:: /reference/method/rs.stepDown-param.rst

   This function disconnects the shell briefly and forces a
   reconnection as the replica set renegotiates which member
   will be primary. As a result, the shell will display an
   error even if this command succeeds.

   :method:`rs.stepDown()` provides a wrapper around the
   :term:`database command` :dbcommand:`replSetStepDown`.
=============
rs.syncFrom()
=============

.. default-domain:: mongodb

.. method:: rs.syncFrom()

   .. versionadded:: 2.2

   Provides a wrapper around the :dbcommand:`replSetSyncFrom`, which
   allows administrators to configure the member of a replica set that
   the current member will pull data from. Specify the name of the
   member you want to replicate from in the form of ``[hostname]:[port]``.

   See :dbcommand:`replSetSyncFrom` for more details.
=====
run()
=====

.. default-domain:: mongodb

.. method:: run()

   For internal use.
=================
runMongoProgram()
=================

.. default-domain:: mongodb

.. method:: runMongoProgram()

   For internal use.
============
runProgram()
============

.. default-domain:: mongodb

.. method:: runProgram()

   For internal use.
==================
sh._adminCommand()
==================

.. default-domain:: mongodb

Definition
----------

.. method:: sh._adminCommand(command, checkMongos)

   Runs a database command against the admin database of a
   :program:`mongos` instance.

   .. include:: /reference/method/sh._adminCommand-param.rst

   .. seealso:: :method:`db.runCommand()`
===================
sh._checkFullName()
===================

.. default-domain:: mongodb

Definition
----------

.. method:: sh._checkFullName(namespace)

   Verifies that a :term:`namespace` name is well formed. If the
   namespace is well formed, the :method:`sh._checkFullName()` method
   exits *with no message*.

   :throws:

      If the namespace is not well formed,
      :method:`sh._checkFullName()` throws: "name needs to be fully
      qualified <db>.<collection>"

   The :method:`sh._checkFullName()` method has the following parameter:

   .. include:: /reference/method/sh._checkFullName-param.rst
=================
sh._checkMongos()
=================

.. default-domain:: mongodb

.. method:: sh._checkMongos()

   :returns: nothing

   :throws: "not connected to a mongos"

   The :method:`sh._checkMongos()` method throws an error message if the
   :program:`mongo` shell is not connected to a :program:`mongos` instance.
   Otherwise it exits (no return document or return code).
===================
sh._lastMigration()
===================

.. default-domain:: mongodb

Definition
----------

.. method:: sh._lastMigration(namespace)

   Returns information on the last migration performed on the specified
   database or collection.

   The :method:`sh._lastMigration()` method has the following parameter:

   .. include:: /reference/method/sh._lastMigration-param.rst

Output
------

The :method:`sh._lastMigration()` method returns a document with details
about the last migration performed on the database or collection. The
document contains the following output:

.. data:: sh._lastMigration._id

   The id of the migration task.

.. data:: sh._lastMigration.server

   The name of the server.

.. data:: sh._lastMigration.clientAddr

   The IP address and port number of the server.

.. data:: sh._lastMigration.time

   The time of the last migration, formatted as :term:`ISODate`.

.. data:: sh._lastMigration.what

   The specific type of migration.

.. data:: sh._lastMigration.ns

   The complete :term:`namespace` of the collection affected by the
   migration.

.. data:: sh._lastMigration.details

   A document containing details about the migrated chunk. The document
   includes ``min`` and ``max`` sub-documents with the bounds of the
   migrated chunk.
=============
sh.addShard()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: sh.addShard(host)

   Adds a database instance or replica set to a :term:`sharded cluster`.
   The optimal configuration is to deploy shards across :term:`replica
   sets <replica set>`. This method must be run on a :program:`mongos`
   instance.

   The :method:`sh.addShard()` method has the following parameter:

   .. include:: /reference/method/sh.addShard-param.rst

   The :method:`sh.addShard()` method has the following prototype form:

   .. code-block:: javascript

      sh.addShard("<host>")

   The ``host`` parameter can be in any
   of the following forms:

   .. code-block:: none

      [hostname]
      [hostname]:[port]
      [replica-set-name]/[hostname]
      [replica-set-name]/[hostname]:port

   .. warning::

      Do not use ``localhost`` for the hostname unless your
      :term:`configuration server <config database>`
      is also running on ``localhost``.

      .. |mongodb-package| replace:: :program:`mongos`

      .. include:: /includes/note-deb-and-rpm-default-to-localhost.rst

The :method:`sh.addShard()` method is a helper for the
:dbcommand:`addShard` command.
The :dbcommand:`addShard` command has additional options which are
not available with this helper.

.. |cmd-name| replace:: :method:`sh.addShard()`
.. include:: /includes/important-add-shard-not-compatible-with-hidden-members.rst

Example
-------

To add a shard on a replica set, specify the name of the replica set and
the hostname of at least one member of the replica set, as a seed. If
you specify additional hostnames, all must be members of the same
replica set.

The following example adds a replica set named ``repl0`` and specifies
one member of the replica set:

.. code-block:: javascript

   sh.addShard("repl0/mongodb3.example.net:27327")
================
sh.addShardTag()
================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.addShardTag(shard, tag)

   .. versionadded:: 2.2

   Associates a shard with a tag or identifier. MongoDB uses these
   identifiers to direct :term:`chunks <chunk>` that fall within a
   tagged range to specific shards. :method:`sh.addTagRange()`
   associates chunk ranges with tag ranges.

   .. include:: /reference/method/sh.addShardTag-param.rst

   Only issue :method:`sh.addShardTag()` when connected to a
   :program:`mongos` instance.

Example
-------

The following example adds three tags, ``NYC``, ``LAX``, and ``NRT``, to
three shards:

.. code-block:: javascript

   sh.addShardTag("shard0000", "NYC")
   sh.addShardTag("shard0001", "LAX")
   sh.addShardTag("shard0002", "NRT")

.. seealso:: :method:`sh.addTagRange()` and :method:`sh.removeShardTag()`.
================
sh.addTagRange()
================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.addTagRange(namespace, minimum, maximum, tag)

   .. versionadded:: 2.2

   Attaches a range of shard key values to a shard tag created using the
   :method:`sh.addShardTag()` method. :method:`sh.addTagRange()` takes
   the following arguments:

   .. include:: /reference/method/sh.addTagRange-param.rst

   Use :method:`sh.addShardTag()` to ensure that the balancer migrates
   documents that exist within the specified range to a specific shard
   or set of shards.

   Only issue :method:`sh.addTagRange()` when connected to a
   :program:`mongos` instance.

   .. note::

      If you add a tag range to a collection using
      :method:`sh.addTagRange()` and then later drop the collection
      or its database, MongoDB does not remove the tag association. If you
      later create a new collection with the same name, the old tag
      association will apply to the new collection.

Example
-------

Given a shard key of ``{state: 1, zip: 1}``, the following operation
creates a tag range covering zip codes in New York State:

.. code-block:: javascript

   sh.addTagRange( "exampledb.collection",
                   { state: "NY", zip: MinKey },
                   { state: "NY", zip: MaxKey },
                   "NY"
                 )
=====================
sh.disableBalancing()
=====================

.. default-domain:: mongodb

Description
-----------

.. method:: sh.disableBalancing(namespace)

   Disables the balancer for the
   specified sharded collection. This does not affect the balancing of
   :term:`chunks <chunk>` for other sharded collections in the same cluster.

   The :method:`sh.disableBalancing()` method has the following
   parameter:

   .. include:: /reference/method/sh.disableBalancing-param.rst

   For more information on the balancing process, see
   :doc:`/tutorial/manage-sharded-cluster-balancer` and
   :ref:`sharding-balancing`.
=====================
sh.enableBalancing()
=====================

.. default-domain:: mongodb

Description
-----------

.. method:: sh.enableBalancing(collection)

   Enables the balancer for the specified sharded collection.

   The :method:`sh.enableBalancing()` method has the following parameter:

   .. include:: /reference/method/sh.enableBalancing-param.rst

   .. important:: :method:`sh.enableBalancing()` does not *start*
      balancing. Rather, it allows balancing of this collection the
      next time the balancer runs.

   For more information on the balancing process, see
   :doc:`/tutorial/manage-sharded-cluster-balancer` and
   :ref:`sharding-balancing`.
===================
sh.enableSharding()
===================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.enableSharding(database)

   Enables sharding on the specified database. This does not
   automatically shard any collections but makes it possible to begin
   sharding collections using :method:`sh.shardCollection()`.

   The :method:`sh.enableSharding()` method has the following parameter:

   .. include:: /reference/method/sh.enableSharding-param.rst

.. seealso:: :method:`sh.shardCollection()`
====================
sh.getBalancerHost()
====================

.. default-domain:: mongodb

.. method:: sh.getBalancerHost()

   :returns: String in form :samp:`{hostname}:{port}`

   :method:`sh.getBalancerHost()` returns the name of the server that is
   running the balancer.

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.startBalancer()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
=====================
sh.getBalancerState()
=====================

.. default-domain:: mongodb

.. method:: sh.getBalancerState()

   :returns: boolean

   :method:`sh.getBalancerState()` returns ``true`` when the
   :term:`balancer` is enabled and false if the balancer is
   disabled. This does not reflect the current state of balancing
   operations: use :method:`sh.isBalancerRunning()` to check the
   balancer's current state.

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.startBalancer()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
=========
sh.help()
=========

.. default-domain:: mongodb

.. method:: sh.help()

   :returns: a basic help text for all sharding related shell
             functions.
======================
sh.isBalancerRunning()
======================

.. default-domain:: mongodb

.. method:: sh.isBalancerRunning()

   :returns: boolean

   Returns true if the :term:`balancer` process is currently running
   and migrating chunks and false if the balancer process is not
   running. Use :method:`sh.getBalancerState()` to determine if the
   balancer is enabled or disabled.

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.startBalancer()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
==============
sh.moveChunk()
==============

.. default-domain:: mongodb

Definition
----------

.. method:: sh.moveChunk(namespace, query, destination)

   Moves the :term:`chunk` that contains the document specified by the
   ``query`` to the ``destination`` shard. :method:`sh.moveChunk()`
   provides a wrapper around the :dbcommand:`moveChunk` database
   command and takes the following arguments:

   .. include:: /reference/method/sh.moveChunk-param.rst

   .. important:: In most circumstances, allow the :term:`balancer` to
      automatically migrate :term:`chunks <chunk>`, and avoid calling
      :method:`sh.moveChunk()` directly.

.. seealso:: :dbcommand:`moveChunk`, :method:`sh.splitAt()`,
   :method:`sh.splitFind()`, :doc:`/sharding`, and :ref:`chunk
   migration <sharding-chunk-migration>`.

Example
-------

Given the ``people`` collection in the ``records`` database, the
following operation finds the chunk that contains the documents with the
``zipcode`` field set to ``53187`` and then moves that chunk to the
shard named ``shard0019``:

.. code-block:: javascript

   sh.moveChunk("records.people", { zipcode: "53187" }, "shard0019")
===================
sh.removeShardTag()
===================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.removeShardTag(shard, tag)

   .. versionadded:: 2.2

   Removes the association between a tag and a shard. Only issue
   :method:`sh.removeShardTag()` when connected to a :program:`mongos`
   instance.

   .. include:: /reference/method/sh.removeShardTag-param.rst

   .. seealso::

      :method:`sh.addShardTag()`,
      :method:`sh.addTagRange()`
=====================
sh.setBalancerState()
=====================

.. default-domain:: mongodb

Description
-----------

.. method:: sh.setBalancerState(state)

   Enables or disables the :term:`balancer`. Use
   :method:`sh.getBalancerState()` to determine if the balancer is
   currently enabled or disabled and :method:`sh.isBalancerRunning()`
   to check its current state.

   The :method:`sh.getBalancerState()` method has the following
   parameter:

   .. include:: /reference/method/sh.setBalancerState-param.rst

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.startBalancer()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
====================
sh.shardCollection()
====================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.shardCollection(namespace, key, unique)

   Shards a collection using the ``key`` as a the :term:`shard
   key`. :method:`sh.shardCollection()` takes the following arguments:

   .. include:: /reference/method/sh.shardCollection-param.rst

   .. todo:: add note about creating the shard key index if the
      collection is empty or failing if the collection has data and no
      shard key index.

   .. versionadded:: 2.4
      Use the form ``{field: "hashed"}`` to create a
      :term:`hashed shard key <hashed shard key>`.
      Hashed shard keys may not be compound indexes.

Considerations
--------------

.. include:: /includes/fact-cannot-unshard-collection.rst

Example
-------

Given the ``people`` collection in the ``records`` database, the
following command shards the collection by the ``zipcode`` field:

.. code-block:: javascript

   sh.shardCollection("records.people", { zipcode: 1} )

Additional Information
----------------------

:dbcommand:`shardCollection` for additional options,
:doc:`/sharding` and :doc:`/core/sharding-introduction` for an
overview of sharding, :doc:`/tutorial/deploy-shard-cluster` for a
tutorial, and :ref:`sharding-shard-key` for choosing a shard key.
============
sh.splitAt()
============

.. default-domain:: mongodb

Definition
----------

.. method:: sh.splitAt(namespace, query)

   Splits the chunk containing the document specified by the query as if
   that document were at the "middle" of the collection, even if the
   specified document is not the actual median of the collection.

   .. include:: /reference/method/sh.splitAt-param.rst

   Use this command to manually split chunks unevenly. Use the
   ":method:`sh.splitFind()`" function to split a chunk at the actual
   median.

   In most circumstances, you should leave chunk splitting to the
   automated processes within MongoDB. However, when initially deploying
   a :term:`sharded cluster` it is necessary to perform some measure of
   :term:`pre-splitting` using manual methods including
   :method:`sh.splitAt()`.
==============
sh.splitFind()
==============

.. default-domain:: mongodb

Definition
----------

.. method:: sh.splitFind(namespace, query)

   Splits the chunk containing the document specified by the ``query``
   at its median point, creating two roughly equal chunks. Use
   :method:`sh.splitAt()` to split a collection in a specific point.

   In most circumstances, you should leave chunk splitting to the
   automated processes. However, when initially deploying a
   :term:`sharded cluster` it is necessary to perform some measure of
   :term:`pre-splitting` using manual methods including
   :method:`sh.splitFind()`.

   .. include:: /reference/method/sh.splitFind-param.rst
==================
sh.startBalancer()
==================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.startBalancer(timeout, interval)

   Enables the balancer in a sharded cluster and waits for balancing to
   initiate.

   .. include:: /reference/method/sh.startBalancer-param.rst

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
===========
sh.status()
===========

.. default-domain:: mongodb

Definition
----------

.. start-include-here

.. method:: sh.status()

   Prints a formatted report of the sharding configuration and the
   information regarding existing chunks in a :term:`sharded cluster`.
   The default behavior suppresses the detailed chunk information if
   the total number of chunks is greater than or equal to 20.

   The :method:`sh.status()` method has the following parameter:

   .. include:: /reference/method/sh.status-param.rst

   .. seealso:: :method:`db.printShardingStatus()`

.. end-include-here

Output Examples
---------------

The :ref:`sharding-status-version-fields` section displays information on the
:term:`config database`:

.. code-block:: javascript

   --- Sharding Status ---
     sharding version: {
      "_id" : <num>,
      "version" : <num>,
      "minCompatibleVersion" : <num>,
      "currentVersion" : <num>,
      "clusterId" : <ObjectId>
   }

The :ref:`sharding-status-shards-fields` section lists information on
the shard(s). For each shard, the section displays the name, host, and
the associated tags, if any.

.. code-block:: javascript

     shards:
      {  "_id" : <shard name1>,
         "host" : <string>,
         "tags" : [ <string> ... ]
      }
      {  "_id" : <shard name2>,
         "host" : <string>,
         "tags" : [ <string> ... ]
      }
      ...

The :ref:`sharding-status-databases-fields` section lists information
on the database(s). For each database, the section displays the name,
whether the database has sharding enabled, and the :term:`primary
shard` for the database.

.. code-block:: javascript

     databases:
      {  "_id" : <dbname1>,
         "partitioned" : <boolean>,
         "primary" : <string>
      }
      {  "_id" : <dbname2>,
         "partitioned" : <boolean>,
         "primary" : <string>
      }
      ...

The :ref:`sharding-status-collection-fields` section provides
information on the sharding details for sharded collection(s). For each
sharded collection, the section displays the shard key, the number of
chunks per shard(s), the distribution of documents across chunks
[#chunk-details]_, and the tag information, if any, for shard key
range(s).

.. code-block:: javascript

   <dbname>.<collection>
      shard key: { <shard key> : <1 or hashed> }
      chunks:
         <shard name1> <number of chunks>
         <shard name2> <number of chunks>
         ...
      { <shard key>: <min range1> } -->> { <shard key> : <max range1> } on : <shard name> <last modified timestamp>
      { <shard key>: <min range2> } -->> { <shard key> : <max range2> } on : <shard name> <last modified timestamp>
      ...
      tag: <tag1>  { <shard key> : <min range1> } -->> { <shard key> : <max range1> }
      ...

Output Fields
-------------

.. _sharding-status-version-fields:

Sharding Version
~~~~~~~~~~~~~~~~

.. data:: sh.status.sharding-version._id

   The :data:`~sh.status.sharding-version._id` is an identifier
   for the version details.

.. data:: sh.status.sharding-version.version

   The :data:`~sh.status.sharding-version.version` is the version
   of the :term:`config server <config database>` for the sharded
   cluster.

.. data:: sh.status.sharding-version.minCompatibleVersion

   The :data:`~sh.status.sharding-version.minCompatibleVersion` is
   the minimum compatible version of the config server.

.. data:: sh.status.sharding-version.currentVersion

   The :data:`~sh.status.sharding-version.currentVersion` is
   the current version of the config server.

.. data:: sh.status.sharding-version.clusterId

   The :data:`~sh.status.sharding-version.clusterId` is the
   identification for the sharded cluster.

.. _sharding-status-shards-fields:

Shards
~~~~~~

.. data:: sh.status.shards._id

   The :data:`~sh.status.shards._id` displays the name of the shard.

.. data:: sh.status.shards.host

   The :data:`~sh.status.shards.host` displays the host location
   of the shard.

.. data:: sh.status.shards.tags

   The :data:`~sh.status.shards.tags` displays all the tags for
   the shard. The field only displays if the shard has tags.

.. _sharding-status-databases-fields:

Databases
~~~~~~~~~

.. data:: sh.status.databases._id

   The :data:`~sh.status.databases._id` displays the name of the database.

.. data:: sh.status.databases.partitioned

   The :data:`~sh.status.databases.partitioned` displays whether
   the database has sharding enabled. If ``true``, the database has
   sharding enabled.

.. data:: sh.status.databases.primary

   The :data:`~sh.status.databases.primary` displays the
   :term:`primary shard` for the database.

.. _sharding-status-collection-fields:

Sharded Collection
~~~~~~~~~~~~~~~~~~

.. data:: sh.status.databases.shard-key

   The :data:`~sh.status.databases.shard-key` displays the shard
   key specification document.

.. data:: sh.status.databases.chunks

   The :data:`~sh.status.databases.chunks` lists all the shards
   and the number of chunks that reside on each shard.

.. data:: sh.status.databases.chunk-details

   The :data:`~sh.status.databases.chunk-details` lists the details
   of the chunks [#chunk-details]_:

   - The range of shard key values that define the chunk,

   - The shard where the chunk resides, and

   - The last modified timestamp for the chunk.

.. data:: sh.status.databases.tag

   The :data:`~sh.status.databases.tag` lists the details of
   the tags associated with a range of shard key values.

.. [#chunk-details] The sharded collection section, by default,
   displays the chunk information if the total number of chunks is less
   than 20. To display the information when you have 20 or more chunks,
   call the :method:`sh.status()` methods with the ``verbose`` parameter
   set to ``true``, i.e. ``sh.status(true)``.
=================
sh.stopBalancer()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.stopBalancer(timeout, interval)

   Disables the balancer in a sharded cluster and waits for balancing to
   complete.

   .. include:: /reference/method/sh.stopBalancer-param.rst

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.startBalancer()`
   - :method:`sh.waitForBalancer()`
   - :method:`sh.waitForBalancerOff()`
====================
sh.waitForBalancer()
====================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.waitForBalancer(wait, timeout, interval)

   Waits for a change in the state of the balancer.
   :method:`sh.waitForBalancer()` is an internal method, which takes
   the following arguments:

   .. include:: /reference/method/sh.waitForBalancer-param.rst
=======================
sh.waitForBalancerOff()
=======================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.waitForBalancerOff(timeout, interval)

   Internal method that waits until the balancer is not running.

   .. include:: /reference/method/sh.waitForBalancerOff-param.rst

.. seealso::

   - :method:`sh.enableBalancing()`
   - :method:`sh.disableBalancing()`
   - :method:`sh.getBalancerHost()`
   - :method:`sh.getBalancerState()`
   - :method:`sh.isBalancerRunning()`
   - :method:`sh.setBalancerState()`
   - :method:`sh.startBalancer()`
   - :method:`sh.stopBalancer()`
   - :method:`sh.waitForBalancer()`
=================
sh.waitForDLock()
=================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.waitForDLock(lockname, wait, timeout, interval)

   Waits until the specified distributed lock changes state.
   :method:`sh.waitForDLock()` is an internal method that takes the
   following arguments:

   .. include:: /reference/method/sh.waitForDLock-param.rst
======================
sh.waitForPingChange()
======================

.. default-domain:: mongodb

Definition
----------

.. method:: sh.waitForPingChange(activePings, timeout, interval)

   :method:`sh.waitForPingChange()` waits for a change in ping state
   of one of the ``activepings``, and only returns when the specified
   ping changes state.

   .. include:: /reference/method/sh.waitForPingChange-param.rst
========
_srand()
========

.. default-domain:: mongodb

.. method:: _srand()

   For internal use.
===================
startMongoProgram()
===================

.. default-domain:: mongodb

.. method:: _startMongoProgram()

   For internal use.
============
stopMongod()
============

.. default-domain:: mongodb

.. method:: stopMongod()

   For internal use.
==================
stopMongoProgram()
==================

.. default-domain:: mongodb

.. method:: stopMongoProgram()

   For internal use.
=======================
stopMongoProgramByPid()
=======================

.. default-domain:: mongodb
.. method:: stopMongoProgramByPid()

   For internal use.
======
UUID()
======

.. default-domain:: mongodb

Definition
----------

.. method:: UUID(<string>)

   Generates a BSON UUID object.

   .. include:: /reference/method/UUID-param.rst

   :returns: A BSON UUID object.

Example
-------

Create a 32 byte hexadecimal string:

.. code-block:: javascript

   var myuuid = '0123456789abcdeffedcba9876543210'

Convert it to the BSON UUID subtype:

.. code-block:: javascript

   UUID(myuuid)
   BinData(3,"ASNFZ4mrze/+3LqYdlQyEA==")
=========
version()
=========

.. default-domain:: mongodb

.. method:: version()

   :returns: The version of the :program:`mongo` shell as a string.

   .. versionchanged:: 2.4
      In previous versions of the shell, :method:`version()` would
      print the version instead of returning a string.
========================
waitMongoProgramOnPort()
========================

.. default-domain:: mongodb

.. method:: waitMongoProgramOnPort()

   For internal use.
=============
waitProgram()
=============

.. default-domain:: mongodb

.. method:: waitProgram()

   For internal use.
==================================
WriteResult.hasWriteConcernError()
==================================

.. default-domain:: mongodb

Definition
----------

.. method:: WriteResult.hasWriteConcernError()

   Returns ``true`` if the result of a :program:`mongo` shell write
   method has :data:`WriteResult.writeConcernError`. Otherwise, the
   method returns ``false``.

.. seealso:: :method:`WriteResult()`
===========================
WriteResult.hasWriteError()
===========================

.. default-domain:: mongodb

Definition
----------

.. method:: WriteResult.hasWriteError()

   Returns ``true`` if the result of a :program:`mongo` shell write
   method has :data:`WriteResult.writeError`. Otherwise, the method
   returns ``false``.

.. seealso:: :method:`WriteResult()`
=============
WriteResult()
=============

.. default-domain:: mongodb

Definition
----------

.. method:: WriteResult()

   A wrapper that contains the result status of the :program:`mongo`
   shell write methods.

   .. see:: :method:`db.collection.insert()`,
            :method:`db.collection.update()`,
            :method:`db.collection.remove()`, and :method:`db.collection.save()`.

Properties
----------

The :method:`WriteResult` has the following properties:

.. data:: WriteResult.nInserted

   The number of documents inserted, excluding ``upserted`` documents.
   See :data:`WriteResult.nUpserted` for the number of documents
   inserted through an upsert.

.. data:: WriteResult.nMatched

   The number of documents selected for update. If the update operation
   results in no change to the document, e.g. :update:`$set` expression
   updates the value to the current value,
   :data:`~WriteResult.nMatched` can be greater than
   :data:`~WriteResult.nModified`.

.. data:: WriteResult.nModified

   The number of existing documents updated. If the update/replacement
   operation results in no change to the document, such as setting the
   value of the field to its current value,
   :data:`~WriteResult.nModified` can be less than
   :data:`~WriteResult.nMatched`.

.. data:: WriteResult.nUpserted

   The number of documents inserted by an :ref:`upsert
   <upsert-parameter>`.

.. data:: WriteResult._id

   The ``_id`` of the document inserted by an ``upsert``. Returned only
   if an ``upsert`` results in an insert.

.. data:: WriteResult.nRemoved

   The number of documents removed.

.. data:: WriteResult.writeError

   A document that contains information regarding any error, excluding
   write concern errors, encountered during the write operation.

   .. data:: WriteResult.writeError.code

      An integer value identifying the error.

   .. data:: WriteResult.writeError.errmsg

      A description of the error.


.. data:: WriteResult.writeConcernError

   A document that contains information regarding any write concern errors encountered
   during the write operation.

   .. data:: WriteResult.writeConcernError.code

      An integer value identifying the write concern error.

   .. data:: WriteResult.writeConcernError.errInfo

      A document identifying the write concern setting related to the
      error.

   .. data:: WriteResult.writeError.errmsg

      A description of the error.

.. seealso:: :method:`WriteResult.hasWriteError()`,
   :method:`WriteResult.hasWriteConcernError()`
.. _js-administrative-methods:

=======================
``mongo`` Shell Methods
=======================

.. default-domain:: mongodb

.. |javascript-using-operation| replace:: these methods use
.. include:: /includes/admonition-javascript-prevalence.rst

Collection
----------

.. only:: website

   .. include:: /includes/toc/table-method-collection.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-collection

.. _js-query-cursor-methods:

Cursor
------

.. only:: website

   .. include:: /includes/toc/table-method-cursor.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-cursor

Database
--------

.. only:: website

   .. include:: /includes/toc/table-method-database.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-database

Query Plan Cache
----------------

.. only:: website

   .. include:: /includes/toc/table-method-plan-cache.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-plan-cache

.. _bulk-operation-methods:

Bulk Write Operation
--------------------

.. only:: website

   .. include:: /includes/toc/table-method-bulk.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-bulk

.. _user-management-methods:

User Management
---------------

.. only:: website

   .. include:: /includes/toc/table-method-user-management.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-user-management

.. _role-management-methods:

Role Management
---------------

.. only:: website

   .. include:: /includes/toc/table-method-role-management.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-role-management

.. _replica-set-functions:

Replication
-----------

.. only:: website

   .. include:: /includes/toc/table-method-rs.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-replication

Sharding
--------

.. only:: website

   .. include:: /includes/toc/table-method-sh.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-sharding

Subprocess
----------

.. only:: website

   .. include:: /includes/toc/table-method-subprocess.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-subprocess

Constructors
------------

.. only:: website

   .. include:: /includes/toc/table-method-constructor.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-constructor

Connection
----------

.. only:: website

   .. include:: /includes/toc/table-method-connection.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-connection

Native
------

.. only:: website

   .. include:: /includes/toc/table-method-native.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/method/js-native
===============================
``mongo`` Shell Quick Reference
===============================

.. default-domain:: mongodb

``mongo`` Shell Command History
-------------------------------

You can retrieve previous commands issued in the :program:`mongo` shell
with the up and down arrow keys. Command history is stored in
``~/.dbshell`` file. See :ref:`.dbshell <mongo-dbshell-file>` for more
information.

Command Line Options
--------------------

The :program:`mongo` executable can be started with numerous options.
See :doc:`mongo executable </reference/program/mongo>` page for details on all
available options.

The following table displays some common options for :program:`mongo`:

.. list-table::
   :header-rows: 1

   * - **Option**
     - **Description**

   * - :option:`--help <mongo --help>`
     - Show command line options

   * - :option:`--nodb <mongo --nodb>`

     - Start :program:`mongo` shell without connecting to a database.

       To connect later, see :ref:`mongo-shell-new-connections`.

   * - :option:`--shell <mongo --shell>`

     - Used in conjunction with a JavaScript file (i.e.
       :ref:`\<file.js\> <mongo-shell-file>`) to continue in the
       :program:`mongo` shell after running the JavaScript file.

       See :ref:`JavaScript file <mongo-shell-javascript-file>` for an
       example.

.. _command-helpers:

Command Helpers
---------------

The :program:`mongo` shell provides various help. The following table
displays some common help methods and commands:

.. list-table::
   :header-rows: 1

   * - **Help Methods and Commands**
     - **Description**

   * - ``help``

     - Show help.

   * - ``db.help()``

     - Show help for database methods.

   * - ``db.<collection>.help()``

     - Show help on collection methods. The ``<collection>`` can be the
       name of an existing collection or a non-existing collection.

   * - ``show dbs``

     - Print a list of all databases on the server.

   * - ``use <db>``

     - Switch current database to ``<db>``. The :program:`mongo` shell
       variable ``db`` is set to the current database.

   * - ``show collections``

     - Print a list of all collections for current database

   * - ``show users``

     - Print a list of users for current database.

   * - ``show roles``

     - Print a list of all roles, both user-defined and built-in, for
       the current database.

   * - ``show profile``

     - Print the five most recent operations that took 1 millisecond or
       more. See documentation on the :doc:`database profiler
       </tutorial/manage-the-database-profiler>` for more information.

   * - ``show databases``

     - .. versionadded:: 2.4
          Print a list of all available databases.

   * - ``load()``

     - Execute a JavaScript file. See
       :doc:`/tutorial/getting-started-with-the-mongo-shell`
       for more information.

Basic Shell JavaScript Operations
----------------------------------

The :program:`mongo` shell provides numerous
:doc:`/reference/method` methods for database operations.

In the :program:`mongo` shell, ``db`` is the variable that references
the current database. The variable is automatically set to the default
database ``test`` or is set when you use the ``use <db>`` to switch
current database.

The following table displays some common JavaScript operations:

.. list-table::
   :header-rows: 1

   * - **JavaScript Database Operations**
     - **Description**

   * - :method:`db.auth()`

     - If running in secure mode, authenticate the user.

   * - ``coll = db.<collection>``

     - Set a specific collection in the current database to a variable
       ``coll``, as in the following example:

       .. code-block:: javascript

          coll = db.myCollection;

       You can perform operations on the ``myCollection`` using the
       variable, as in the following example:

       .. code-block:: javascript

          coll.find();

   * - :method:`~db.collection.find()`

     - Find all documents in the collection and returns a cursor.

       See the :method:`db.collection.find()` and
       :doc:`/tutorial/query-documents` for more information and
       examples.

       See :doc:`/core/cursors` for additional information on
       cursor handling in the :program:`mongo` shell.

   * - :method:`~db.collection.insert()`

     - Insert a new document into the collection.

   * - :method:`~db.collection.update()`

     - Update an existing document in the collection.

       See :doc:`/core/write-operations` for more information.

   * - :method:`~db.collection.save()`

     - Insert either a new document or update an existing document in
       the collection.

       See :doc:`/core/write-operations` for more information.

   * - :method:`~db.collection.remove()`

     - Delete documents from the collection.

       See :doc:`/core/write-operations` for more information.

   * - :method:`~db.collection.drop()`

     - Drops or removes completely the collection.

   * - :method:`~db.collection.ensureIndex()`

     - Create a new index on the collection if the index does not
       exist; otherwise, the operation has no effect.

   * - :method:`db.getSiblingDB()`

     - Return a reference to another database using this same
       connection without explicitly switching the current database.
       This allows for cross database queries. See
       :ref:`mongo-shell-getSiblingDB` for more information.

For more information on performing operations in the shell, see:

- :doc:`/core/crud`

- :doc:`/core/read-operations`

- :doc:`/core/write-operations`

- :doc:`/reference/method`

Keyboard Shortcuts
------------------

.. versionchanged:: 2.2

The :program:`mongo` shell provides most keyboard shortcuts similar to
those found in the ``bash`` shell or in Emacs. For some functions
:program:`mongo` provides multiple key bindings, to accommodate
several familiar paradigms.

The following table enumerates the keystrokes supported by the
:program:`mongo` shell:

.. list-table::
   :header-rows: 1

   * - **Keystroke**
     - **Function**
   * - Up-arrow
     - previous-history
   * - Down-arrow
     - next-history
   * - Home
     - beginning-of-line
   * - End
     - end-of-line
   * - Tab
     - autocomplete
   * - Left-arrow
     - backward-character
   * - Right-arrow
     - forward-character
   * - Ctrl-left-arrow
     - backward-word
   * - Ctrl-right-arrow
     - forward-word
   * - Meta-left-arrow
     - backward-word
   * - Meta-right-arrow
     - forward-word
   * - Ctrl-A
     - beginning-of-line
   * - Ctrl-B
     - backward-char
   * - Ctrl-C
     - exit-shell
   * - Ctrl-D
     - delete-char (or exit shell)
   * - Ctrl-E
     - end-of-line
   * - Ctrl-F
     - forward-char
   * - Ctrl-G
     - abort
   * - Ctrl-J
     - accept-line
   * - Ctrl-K
     - kill-line
   * - Ctrl-L
     - clear-screen
   * - Ctrl-M
     - accept-line
   * - Ctrl-N
     - next-history
   * - Ctrl-P
     - previous-history
   * - Ctrl-R
     - reverse-search-history
   * - Ctrl-S
     - forward-search-history
   * - Ctrl-T
     - transpose-chars
   * - Ctrl-U
     - unix-line-discard
   * - Ctrl-W
     - unix-word-rubout
   * - Ctrl-Y
     - yank
   * - Ctrl-Z
     - Suspend (job control works in linux)
   * - Ctrl-H (i.e. Backspace)
     - backward-delete-char
   * - Ctrl-I (i.e. Tab)
     - complete
   * - Meta-B
     - backward-word
   * - Meta-C
     - capitalize-word
   * - Meta-D
     - kill-word
   * - Meta-F
     - forward-word
   * - Meta-L
     - downcase-word
   * - Meta-U
     - upcase-word
   * - Meta-Y
     - yank-pop
   * - Meta-[Backspace]
     - backward-kill-word
   * - Meta-<
     - beginning-of-history
   * - Meta->
     - end-of-history



Queries
-------

In the :program:`mongo` shell, perform read operations using the
:method:`~db.collection.find()` and :method:`~db.collection.findOne()`
methods.

The :method:`~db.collection.find()` method returns a cursor object
which the :program:`mongo` shell iterates to print documents on
screen. By default, :program:`mongo` prints the first 20. The
:program:`mongo` shell will prompt the user to "``Type it``" to continue
iterating the next 20 results.

The following table provides some common read operations in the
:program:`mongo` shell:

.. list-table::
   :header-rows: 1

   * - Read Operations

     - Description

   * - :method:`db.collection.find(\<query\>) <db.collection.find()>`

     - Find the documents matching the ``<query>`` criteria in the
       collection. If the ``<query>`` criteria is not specified or is
       empty (i.e ``{}`` ), the read operation selects all documents in
       the collection.

       The following example selects the documents in the ``users``
       collection with the ``name`` field equal to ``"Joe"``:

       .. code-block:: javascript

          coll = db.users;
          coll.find( { name: "Joe" } );

       For more information on specifying the ``<query>`` criteria, see
       :ref:`read-operations-query-argument`.

   * - :method:`db.collection.find( \<query\>, \<projection\> )
       <db.collection.find()>`

     - Find documents matching the ``<query>`` criteria and return just
       specific fields in the ``<projection>``.

       The following example selects all documents from the collection
       but returns only the ``name`` field and the ``_id`` field. The
       ``_id`` is always returned unless explicitly specified to not
       return.

       .. code-block:: javascript

          coll = db.users;
          coll.find( { },
                     { name: true }
                   );

       For more information on specifying the ``<projection>``, see
       :ref:`read-operations-projection`.

   * - :method:`db.collection.find().sort( \<sort order\> ) <cursor.sort()>`

     - Return results in the specified ``<sort order>``.

       The following example selects all documents from the collection
       and returns the results sorted by the ``name`` field in
       ascending order (``1``).  Use ``-1`` for descending order:

       .. code-block:: javascript

          coll = db.users;
          coll.find().sort( { name: 1 } );

   * - :method:`db.collection.find( \<query\> ).sort( \<sort order\> )
       <cursor.sort()>`

     - Return the documents matching the ``<query>`` criteria in the
       specified ``<sort order>``.

   * - :method:`db.collection.find( ... ).limit( \<n\> ) <cursor.limit()>`

     - Limit result to ``<n>`` rows. Highly recommended if you need only
       a certain number of rows for best performance.

   * - :method:`db.collection.find( ... ).skip( \<n\> )
       <cursor.skip()>`

     - Skip ``<n>`` results.

   * - :method:`~db.collection.count()`

     - Returns total number of documents in the collection.

   * - :method:`db.collection.find( \<query\> ).count() <cursor.count()>`

     - Returns the total number of documents that match the query.

       The :method:`~cursor.count()` ignores :method:`~cursor.limit()` and :method:`~cursor.skip()`. For
       example, if 100 records match but the limit is 10,
       :method:`~cursor.count()` will return 100. This will be
       faster than iterating yourself, but still take time.

   * - :method:`db.collection.findOne( \<query\> ) <db.collection.findOne()>`

     - Find and return a single document. Returns null if not found.

        The following example selects a single document in the
        ``users`` collection with the ``name`` field matches to
        ``"Joe"``:

       .. code-block:: javascript

          coll = db.users;
          coll.findOne( { name: "Joe" } );

       Internally, the :method:`~db.collection.findOne()`
       method is the :method:`~db.collection.find()` method
       with a :method:`limit(1) <cursor.limit()>`.

See :doc:`/tutorial/query-documents` and
:doc:`/core/read-operations` documentation for more information and
examples. See :doc:`/reference/operator` to specify other query
operators.

Error Checking Methods
----------------------

The :program:`mongo` shell provides numerous :ref:`administrative
database methods <js-administrative-methods>`, including error checking
methods. These methods are:

.. list-table::
   :header-rows: 1

   * - **Error Checking Methods**
     - **Description**

   * - :method:`db.getLastError()`
     - Returns error message from the last operation.

   * - :method:`db.getLastErrorObj()`
     - Returns the error document from the last operation.

.. _mongo-dba-helpers:
.. _mongo-shell-admin-helpers:

Administrative Command Helpers
------------------------------

The following table lists some common methods to support database
administration:

.. list-table::
   :header-rows: 1

   * - **JavaScript Database Administration Methods**
     - **Description**

   * - :method:`db.cloneDatabase(\<host\>) <db.cloneDatabase()>`

     - Clone the current database from the ``<host>`` specified. The
       ``<host>`` database instance must be in noauth mode.

   * - :method:`db.copyDatabase(\<from\>, \<to\>, \<host\>) <db.copyDatabase()>`

     - Copy the ``<from>`` database from the ``<host>`` to the ``<to>``
       database on the current server.

       The ``<host>`` database instance must be in ``noauth`` mode.

   * - :method:`db.fromColl.renameCollection(\<toColl\>)
       <db.collection.renameCollection()>`

     - Rename collection from ``fromColl`` to ``<toColl>``.

   * - :method:`db.repairDatabase()`

     - Repair and compact the current database. This operation can be
       very slow on large databases.

   * - :method:`db.addUser( \<user\>, \<pwd\> ) <db.addUser()>`

     - Add user to current database.

   * - :method:`db.getCollectionNames()`

     - Get the list of all collections in the current database.

   * - :method:`db.dropDatabase()`

     - Drops the current database.

See also :ref:`administrative database methods
<js-administrative-methods>` for a full list of methods.

Opening Additional Connections
------------------------------

You can create new connections within the :program:`mongo` shell.

The following table displays the methods to create the connections:

.. list-table::
   :header-rows: 1

   * - JavaScript Connection Create Methods

     - Description

   * - .. code-block:: javascript

          db = connect("<host><:port>/<dbname>")

     - Open a new database connection.

   * - .. code-block:: javascript

          conn = new Mongo()
          db = conn.getDB("dbname")

     - Open a connection to a new server using ``new Mongo()``.

       Use ``getDB()`` method of the connection to select a database.

See also :ref:`mongo-shell-new-connections` for more information on the
opening new connections from the :program:`mongo` shell.

Miscellaneous
-------------

The following table displays some miscellaneous methods:

.. list-table::
   :header-rows: 1

   * - **Method**
     - **Description**

   * - ``Object.bsonsize(<document>)``
     - Prints the :term:`BSON` size of a <document>

See the `MongoDB JavaScript API Documentation
<http://api.mongodb.org/js/index.html>`_ for a full list of JavaScript
methods .

Additional Resources
--------------------

Consider the following reference material that addresses the
:program:`mongo` shell and its interface:

- :doc:`/reference/program/mongo`
- :doc:`/reference/method`
- :doc:`/reference/operator`
- :doc:`/reference/command`
- :doc:`/reference/aggregation`

Additionally, the MongoDB source code repository includes a `jstests
directory <https://github.com/mongodb/mongo/tree/master/jstests/>`_
which contains numerous :program:`mongo` shell scripts.
=====================
MongoDB Extended JSON
=====================

.. default-domain:: mongodb

:term:`JSON` can only represent a subset of the types supported by
:term:`BSON`. To preserve type information, MongoDB adds the following
extensions to the JSON format:

- *Strict mode*. Strict mode representations of BSON types conform to
  the `JSON RFC <http://www.json.org>`_. Any JSON parser can parse
  these strict mode representations as key/value pairs; however, only
  the MongoDB's internal JSON parser also recognizes the type
  information conveyed by the format.

- ``mongo`` *Shell mode*. The MongoDB's internal JSON parser and the
  :program:`mongo` shell can parse this mode.

The representation used for the various data types depends on the
context in which the JSON is parsed.

Parsers and Supported Format
----------------------------

Input in Strict Mode
~~~~~~~~~~~~~~~~~~~~

The following can parse representations in strict mode *with*
recognition of the type information.

- :ecosystem:`REST Interfaces </tools/http-interfaces>`

- :program:`mongoimport`

- ``--query`` option of various MongoDB tools

Other JSON parsers, including :program:`mongo` shell and
:method:`db.eval()`, can parse strict mode representations as key/value
pairs, but *without* recognition of the type information.

Input in ``mongo`` Shell Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following can parse representations in ``mongo`` shell mode *with*
recognition of the type information.

- :ecosystem:`REST Interfaces </tools/http-interfaces>`

- :program:`mongoimport`

- ``--query`` option of various MongoDB tools

- :program:`mongo` shell

Output in Strict mode
~~~~~~~~~~~~~~~~~~~~~

:program:`mongoexport` and :ecosystem:`REST and HTTP Interfaces
</tools/http-interfaces>` output data in *Strict mode*.

Output in ``mongo`` Shell Mode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:program:`bsondump` outputs in ``mongo`` *Shell mode*.

BSON Data Types and Associated Representations
----------------------------------------------

The following presents the BSON data types and the associated
representations in *Strict mode* and ``mongo`` *Shell mode*.

Binary
~~~~~~

.. bsontype:: data_binary

   .. list-table::
      :header-rows: 1
      :widths: 50 5 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$binary": "<bindata>", "$type": "<t>" }

        -

        - .. code-block:: none

             BinData ( <t>, <bindata> )

- ``<bindata>`` is the base64 representation of a binary string.

- ``<t>`` is the hexadecimal representation of a single byte that
  indicates the data type.

Date
~~~~

.. bsontype:: data_date

   .. list-table::
      :header-rows: 1
      :widths: 40 20 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$date": <date> }

        -

        - .. code-block:: none

             new Date ( <date> )

   ``<date>`` is the JSON representation of a 64-bit signed integer for
   milliseconds since epoch UTC (unsigned before version 1.9.1).

Timestamp
~~~~~~~~~

.. bsontype:: data_timestamp

   .. list-table::
      :header-rows: 1
      :widths: 40 10 30

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$timestamp": { "t": <t>, "i": <i> } }

        -

        - .. code-block:: none

             Timestamp( <t>, <i> )

- ``<t>`` is the JSON representation of a 32-bit unsigned integer for
  seconds since epoch.

- ``<i>`` is a 32-bit unsigned integer for the increment.

Regular Expression
~~~~~~~~~~~~~~~~~~

.. bsontype:: data_regex

   .. list-table::
      :header-rows: 1
      :widths: 50 5 30

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$regex": "<sRegex>", "$options": "<sOptions>" }

        -

        - .. code-block:: none

             /<jRegex>/<jOptions>

- ``<sRegex>`` is a string of valid JSON characters.

- ``<jRegex>`` is a string that may contain valid JSON characters and
  unescaped double quote (``"``) characters, but may not contain
  unescaped forward slash (``/``) characters.

- ``<sOptions>`` is a string containing the regex options represented
  by the letters of the alphabet.

- ``<jOptions>`` is a string that may contain only the characters 'g',
  'i', 'm' and 's' (added in v1.9). Because the ``JavaScript`` and
  ``mongo Shell`` representations support a limited range of options,
  any nonconforming options will be dropped when converting to this
  representation.

OID
~~~

.. bsontype:: data_oid

   .. list-table::
      :header-rows: 1
      :widths: 40 20 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$oid": "<id>" }

        -

        - .. code-block:: none

             ObjectId( "<id>" )

   ``<id>`` is a 24-character hexadecimal string.

DB Reference
~~~~~~~~~~~~

.. bsontype:: data_ref

   .. list-table::
      :header-rows: 1
      :widths: 50 5 30

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$ref": "<name>", "$id": "<id>" }

        -

        - .. code-block:: none

             DBRef("<name>", "<id>")

- ``<name>`` is a string of valid JSON characters.

- ``<id>`` is any valid extended JSON type.

Undefined Type
~~~~~~~~~~~~~~

.. bsontype:: data_undefined

   .. list-table::
      :header-rows: 1
      :widths: 40 20 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$undefined": true }

        -

        - .. code-block:: none

             undefined

   The representation for the JavaScript/BSON undefined type.

MinKey
~~~~~~

.. bsontype:: data_minkey

   .. list-table::
      :header-rows: 1
      :widths: 40 20 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$minKey": 1 }

        -

        - .. code-block:: none

             MinKey

   The representation of the MinKey BSON data type that compares lower
   than all other types. See
   :ref:`faq-dev-compare-order-for-BSON-types` for more information on
   comparison order for BSON types.

MaxKey
~~~~~~

.. bsontype:: data_maxkey

   .. list-table::
      :header-rows: 1
      :widths: 40 20 40

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$maxKey": 1 }

        -

        - .. code-block:: none

             MaxKey

   The representation of the MaxKey BSON data type that compares higher
   than all other types. See
   :ref:`faq-dev-compare-order-for-BSON-types` for more information on
   comparison order for BSON types.

NumberLong
~~~~~~~~~~

.. versionadded:: 2.6

.. bsontype:: data_numberlong

   .. list-table::
      :header-rows: 1
      :widths: 40 10 30

      * - Strict Mode

        -

        - mongo Shell Mode

      * - .. code-block:: none

             { "$numberLong": "<number>" }

        -

        - .. code-block:: none

             NumberLong( <number> )
========
ObjectId
========

.. default-domain:: mongodb

Overview
--------

:term:`ObjectId <objectid>` is a 12-byte :term:`BSON` type,
constructed using:

- a 4-byte value representing the seconds since the Unix epoch,
- a 3-byte machine identifier,
- a 2-byte process id, and
- a 3-byte counter, starting with a random value.

In MongoDB, documents stored in a collection require a unique
:term:`_id` field that acts as a :term:`primary key`. Because
ObjectIds are small, most likely unique, and fast to generate, MongoDB
uses ObjectIds as the default value for the ``_id`` field if the
``_id`` field is not specified. MongoDB clients should add an ``_id``
field with a unique ObjectId. However, if a client does not add an
``_id`` field, :program:`mongod` will add an ``_id`` field that holds
an ObjectId.

Using ObjectIds for the ``_id`` field provides the following
additional benefits:

- in the :program:`mongo` shell, you can access the creation time of
  the ``ObjectId``, using the :method:`~ObjectId.getTimestamp()` method.

- sorting on an ``_id`` field that stores ``ObjectId`` values is
  roughly equivalent to sorting by creation time.

  .. important:: The relationship between the order of ``ObjectId``
     values and generation time is not strict within a single
     second. If multiple systems, or multiple processes or threads on
     a single system generate values, within a single second;
     ``ObjectId`` values do not represent a strict insertion order.
     Clock skew between clients can also result in non-strict ordering
     even for values, because client drivers generate ``ObjectId``
     values, *not* the :program:`mongod` process.

Also consider the :doc:`/core/document/` section for related
information on MongoDB's document orientation.

.. _core-object-id-class:

ObjectId()
----------

The :program:`mongo` shell provides the ``ObjectId()`` wrapper class to
generate a new ObjectId, and to provide the following helper attribute
and methods:

- ``str``

  The hexadecimal string representation of the object.

- :method:`~ObjectId.getTimestamp()`

  Returns the timestamp portion of the object as a Date.

- :method:`~ObjectId.toString()`

  Returns the JavaScript representation in the form of a string literal
  "``ObjectId(...)``".

  .. versionchanged:: 2.2
     In previous versions :method:`~ObjectId.toString()` returns the
     hexadecimal string representation, which as of version 2.2 can be
     retrieved by the ``str`` property.

- :method:`~ObjectId.valueOf()`

  Returns the representation of the object as a hexadecimal
  string. The returned string is the ``str`` attribute.

  .. versionchanged:: 2.2
     In previous versions, :method:`~ObjectId.valueOf()` returns the
     object.

Examples
--------

Consider the following uses ``ObjectId()`` class in the
:program:`mongo` shell:

Generate a new ObjectId
~~~~~~~~~~~~~~~~~~~~~~~

To generate a new ObjectId, use the ``ObjectId()`` constructor with
no argument:

.. code-block:: javascript

   x = ObjectId()

In this example, the value of ``x`` would be:

.. code-block:: javascript

   ObjectId("507f1f77bcf86cd799439011")

To generate a new ObjectId using the ``ObjectId()`` constructor with
a unique hexadecimal string:

.. code-block:: javascript

   y = ObjectId("507f191e810c19729de860ea")

In this example, the value of ``y`` would be:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea")

- To return the timestamp of an ``ObjectId()`` object, use the
  :method:`~ObjectId.getTimestamp()` method as follows:

Convert an ObjectId into a Timestamp
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To return the timestamp of an ``ObjectId()`` object, use the
:method:`getTimestamp() <ObjectId.getTimestamp()>` method as follows:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea").getTimestamp()

This operation will return the following Date object:

.. code-block:: javascript

   ISODate("2012-10-17T20:46:22Z")

Convert ObjectIds into Strings
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Access the ``str`` attribute of an ``ObjectId()`` object, as follows:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea").str

This operation will return the following hexadecimal string:

.. code-block:: none

   507f191e810c19729de860ea

To return the hexadecimal string representation of an ``ObjectId()``,
use the :method:`~ObjectId.valueOf()` method as follows:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea").valueOf()

This operation returns the following output:

.. code-block:: none

   507f191e810c19729de860ea

To return the string representation of an ``ObjectId()`` object, use
the :method:`~ObjectId.toString()` method as follows:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea").toString()

This operation will return the following output:

.. code-block:: javascript

   ObjectId("507f191e810c19729de860ea")
==================
$add (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $add

   Takes an array of one or more numbers and adds them together,
   returning the sum.
=======================
$addToSet (aggregation)
=======================

.. default-domain:: mongodb

.. group:: $addToSet

   Returns an array of all the values found in the selected field
   among the documents in that group. *Every unique value only
   appears once* in the result set. There is no ordering guarantee
   for the output documents.

Example
-------

In the :program:`mongo` shell, insert documents into a collection
named ``products`` using the following operation:

.. code-block:: javascript

   db.products.insert( [
      { "type" : "phone", "price" : 389.99, "stocked" : 270000 },
      { "type" : "phone", "price" : 376.99 , "stocked" : 97000},
      { "type" : "phone", "price" : 389.99 , "stocked" : 97000},
      { "type" : "chair", "price" : 59.99, "stocked" : 108 }
   ] )

Use the following :method:`db.collection.aggregate()` operation in the
:program:`mongo` shell with the :group:`$addToSet` operator:

.. code-block:: javascript

   db.products.aggregate( {
        $group : {
                  _id : "$type",
                  price: { $addToSet: "$price" },
                  stocked: { $addToSet: "$stocked" },
      } }
   )

This aggregation pipeline returns documents grouped on the value of
the ``type`` field, with *sets* constructed from the unique values of
the ``price`` and ``stocked`` fields in the input documents. Consider
the following aggregation results:

.. code-block:: javascript

   {
           "_id" : "chair",
           "price" : [
                   59.99
           ],
           "stocked" : [
                   108
           ]
   },
   {
           "_id" : "phone",
           "price" : [
                   376.99,
                   389.99
           ],
           "stocked" : [
                   97000,
                   270000,
           ]
   }
==============================
$allElementsTrue (aggregation)
==============================

.. default-domain:: mongodb

.. expression:: $allElementsTrue

   .. versionadded:: 2.6

   Takes a single expression that returns an array and returns ``true``
   if all its values are ``true`` and ``false`` otherwise. An empty
   array returns ``true``.
==================
$and (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $and

   Takes an array one or more values and returns ``true`` if *all* of
   the values in the array are ``true``. Otherwise :expression:`$and`
   returns ``false``.

   .. note::

      :expression:`$and` uses short-circuit logic: the operation
      stops evaluation after encountering the first ``false``
      expression.
==============================
$anyElementTrue (aggregation)
==============================

.. default-domain:: mongodb

.. expression:: $anyElementTrue

   .. versionadded:: 2.6

   Takes a single expression that returns an array and returns ``true``
   if any of its values are ``true`` and ``false`` otherwise. An empty
   array returns ``false``.
==================
$avg (aggregation)
==================

.. default-domain:: mongodb

.. group:: $avg

   Returns the average of all the values of the field in all documents
   selected by this group.
==================
$cmp (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $cmp

   Takes two values in an array and returns an integer. The returned
   value is:

   - A negative number if the first value is less than the second.

   - A positive number if the first value is greater than the second.

   - ``0`` if the two values are equal.
=====================
$concat (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $concat

   .. versionadded:: 2.4

   Takes an array of strings, concatenates the strings, and returns the
   concatenated string. :expression:`$concat` can only accept an array
   of strings.

   Use :expression:`$concat` with the following syntax:

   .. code-block:: javascript

      { $concat: [ <string>, <string>, ... ] }

   If array element has a value of ``null`` or refers to a field that
   is missing, :expression:`$concat` will return ``null``.

   .. begin-examples

   .. example:: Project new concatenated values.

      A collection ``menu`` contains the documents that stores
      information on menu items separately in the ``section``, the
      ``category`` and the ``type`` fields, as in the following:

      .. code-block:: javascript

         { _id: 1, item: { sec: "dessert", category: "pie", type: "apple" } }
         { _id: 2, item: { sec: "dessert", category: "pie", type: "cherry" } }
         { _id: 3, item: { sec: "main", category: "pie", type: "shepherd's" } }
         { _id: 4, item: { sec: "main", category: "pie", type: "chicken pot" } }

      The following operation uses :expression:`$concat` to concatenate
      the ``type`` field from the sub-document ``item``, a space,
      and the ``category`` field from the sub-document ``item`` to
      project a new ``food`` field:

      .. code-block:: javascript

         db.menu.aggregate( { $project: { food:
                                                { $concat: [ "$item.type",
                                                             " ",
                                                             "$item.category"
                                                           ]
                                                }
                                         }
                            }
                          )

      The operation returns the following result set where the ``food``
      field contains the concatenated strings:

      .. code-block:: javascript

         {
           "result" : [
                        { "_id" : 1, "food" : "apple pie" },
                        { "_id" : 2, "food" : "cherry pie" },
                        { "_id" : 3, "food" : "shepherd's pie" },
                        { "_id" : 4, "food" : "chicken pot pie" }
                      ],
           "ok" : 1
         }

   .. example:: Group by a concatenated string.

      A collection ``menu`` contains the documents that stores
      information on menu items separately in the ``section``, the
      ``category`` and the ``type`` fields, as in the following:

      .. code-block:: javascript

         { _id: 1, item: { sec: "dessert", category: "pie", type: "apple" } }
         { _id: 2, item: { sec: "dessert", category: "pie", type: "cherry" } }
         { _id: 3, item: { sec: "main", category: "pie", type: "shepherd's" } }
         { _id: 4, item: { sec: "main", category: "pie", type: "chicken pot" } }

      The following aggregation uses :expression:`$concat` to
      concatenate the ``sec`` field from the sub-document ``item``, the
      string ``": "``, and the ``category`` field from the sub-document
      ``item`` to group by the new concatenated string and perform a
      count:

      .. code-block:: javascript

         db.menu.aggregate( { $group: { _id:
                                             { $concat: [ "$item.sec",
                                                          ": ",
                                                          "$item.category"
                                                        ]
                                             },
                                        count: { $sum: 1 }
                                      }
                            }
                          )

      The aggregation returns the following document:

      .. code-block:: javascript

         {
           "result" : [
                        { "_id" : "main: pie", "count" : 2 },
                        { "_id" : "dessert: pie", "count" : 2 }
                      ],
           "ok" : 1
         }

   .. example:: Concatenate ``null`` or missing values.

      A collection ``menu`` contains the documents that stores
      information on menu items separately in the ``section``, the
      ``category`` and the ``type`` fields. Not all documents have the
      all three fields. For example, the document with ``_id`` equal to
      ``5`` is missing the ``category`` field:

      .. code-block:: javascript

         { _id: 1, item: { sec: "dessert", category: "pie", type: "apple" } }
         { _id: 2, item: { sec: "dessert", category: "pie", type: "cherry" } }
         { _id: 3, item: { sec: "main", category: "pie", type: "shepherd's" } }
         { _id: 4, item: { sec: "main", category: "pie", type: "chicken pot" } }
         { _id: 5, item: { sec: "beverage", type: "coffee" } }

      The following aggregation uses the :expression:`$concat` to
      concatenate the ``type`` field from the sub-document ``item``, a
      space, and the ``category`` field from the sub-document ``item``:

      .. code-block:: javascript

         db.menu.aggregate( { $project: { food:
                                                { $concat: [ "$item.type",
                                                             " ",
                                                             "$item.category"
                                                           ]
                                                }
                                         }
                            }
                          )

      Because the document with ``_id`` equal to ``5`` is missing the
      ``type`` field in the ``item`` sub-document,
      :expression:`$concat` returns the value ``null`` as the
      concatenated value for the document:

      .. code-block:: javascript

         {
           "result" : [
                        { "_id" : 1, "food" : "apple pie" },
                        { "_id" : 2, "food" : "cherry pie" },
                        { "_id" : 3, "food" : "shepherd's pie" },
                        { "_id" : 4, "food" : "chicken pot pie" },
                        { "_id" : 5, "food" : null }
                      ],
           "ok" : 1
         }

      To handle possible missing fields, you can use
      :expression:`$ifNull` with :expression:`$concat`, as in the
      following example which substitutes ``<unknown type>`` if the
      field ``type`` is ``null`` or missing, and ``<unknown category>``
      if the field ``category`` is ``null`` or is missing:

      .. code-block:: javascript

         db.menu.aggregate( { $project: { food:
                                                { $concat: [ { $ifNull: ["$item.type", "<unknown type>"] },
                                                             " ",
                                                             { $ifNull: ["$item.category", "<unknown category>"] }
                                                           ]
                                                }
                                         }
                            }
                          )

      The aggregation returns the following result set:

      .. code-block:: javascript

         {
           "result" : [
                        { "_id" : 1, "food" : "apple pie" },
                        { "_id" : 2, "food" : "cherry pie" },
                        { "_id" : 3, "food" : "shepherd's pie" },
                        { "_id" : 4, "food" : "chicken pot pie" },
                        { "_id" : 5, "food" : "coffee <unknown category>" }
                      ],
           "ok" : 1
         }
===================
$cond (aggregation)
===================

.. default-domain:: mongodb

Definition
----------

.. expression:: $cond

   :expression:`$cond` is a ternary operator that takes three
   expressions and evaluates the first expression to determine which of
   the subsequent expressions to return. :expression:`$cond` accepts
   input either as an array with three items, or as an object.

   .. versionadded:: 2.6
      :expression:`$cond` now accepts expressions in the form of documents.

Syntax
------

Document
~~~~~~~~

.. versionadded:: 2.6
   :expression:`$cond` adds support for the document format.

When :expression:`$cond` takes a document, the document has three
fields: ``if``, ``then``, and ``else``. Consider the following
example:

.. code-block:: javascript

   { $cond: { if: <boolean-expression>,
              then: <true-case>,
              else: <false-case> } }

The ``if`` field takes an expression that evaluates to a Boolean
value.  If the expression evaluates to ``true``, then
:expression:`$cond` evaluates and returns the value of the ``then``
field. Otherwise, :expression:`$cond` evaluates and returns the value
of the ``else`` field.

The expressions in the ``if``, ``then``, and ``else`` fields may be
valid MongoDB :doc:`aggregation expressions
</reference/operator/aggregation>`. You cannot use
JavaScript in the expressions.

Array
~~~~~

When you specify :expression:`$cond` as an array of three expressions,
the first expression evaluates to a Boolean value. If the first
expression evaluates to``true``, then :expression:`$cond` evaluates
and returns the value of the second expression. If the first
expression evaluates to ``false``, then :expression:`$cond` evaluates
and returns the third expression.

Use the :expression:`$cond` operator with the following syntax:

.. code-block:: javascript

   { $cond: [ <boolean-expression>, <true-case>, <false-case> ] }

All three values in the array specified to :expression:`$cond`
must be valid MongoDB :doc:`aggregation expressions
</reference/operator/aggregation>` or document fields. Do not use
JavaScript in any aggregation statements, including
:expression:`$cond`.

Examples
--------

Specify ``$cond`` Expression as a Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation pipeline operation returns a
``weightedCount`` for each ``item_id``. The :group:`$sum` operator
uses the :expression:`$cond` expression to add ``2`` if the value
stored in the ``level`` field is ``E`` and ``1`` otherwise.

.. code-block:: javascript

   db.survey.aggregate(
      [
         {
            $group: {
               _id: "$item_id",
               weightedCount: { $sum: { $cond: { if: { $eq: [ "$level", "E" ] } ,
                                                 then: 2,
                                                 else: 1
                                               } } }
            }
         }
      ]
   )

Specify a ``$cond`` Expression using an Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation on the ``survey`` collection groups
by the ``item_id`` field and returns a ``weightedCount``
for each ``item_id``. The :group:`$sum` operator uses the
:expression:`$cond` expression to add either ``2`` if the value
stored in the ``level`` field is ``E`` and ``1`` otherwise.

.. code-block:: javascript

   db.survey.aggregate(
      [
         {
            $group: {
               _id: "$item_id",
               weightedCount: { $sum: { $cond: [ { $eq: [ "$level", "E" ] } , 2, 1 ] } }
            }
         }
      ]
   )
=========================
$dayOfMonth (aggregation)
=========================

.. default-domain:: mongodb

.. expression:: $dayOfMonth

   Takes a date and returns the day of the month as a number
   between 1 and 31.
========================
$dayOfWeek (aggregation)
========================

.. default-domain:: mongodb

.. expression:: $dayOfWeek

   Takes a date and returns the day of the week as a number
   between 1 (Sunday) and 7 (Saturday.)
========================
$dayOfYear (aggregation)
========================

.. default-domain:: mongodb

.. expression:: $dayOfYear

   Takes a date and returns the day of the year as a number
   between 1 and 366.
=====================
$divide (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $divide

   Takes an array that contains a pair of numbers and returns the
   value of the first number divided by the second number.
=================
$eq (aggregation)
=================

.. default-domain:: mongodb

.. expression:: $eq

   Takes two values in an array and returns a boolean. The returned
   value is:

   - ``true`` when the values are equivalent.

   - ``false`` when the values are **not** equivalent.
====================
$first (aggregation)
====================

.. default-domain:: mongodb

.. group:: $first

   Returns the first value it encounters for its group.

   .. note::

      Only use :group:`$first` when the :pipeline:`$group` follows
      a :pipeline:`$sort` operation. Otherwise, the result of this
      operation is unpredictable.
======================
$geoNear (aggregation)
======================

.. default-domain:: mongodb

Definition
----------

.. pipeline:: $geoNear

   .. versionadded:: 2.4

   :pipeline:`$geoNear` returns documents in order of nearest to
   farthest from a specified point and pass the documents
   through the aggregation :term:`pipeline`.

   The :pipeline:`$geoNear` operator accepts a :term:`document` that
   contains the following fields. Specify all distances in the same
   unites as the document coordinate system:

   .. include:: /reference/operator/aggregation/geoNear-field.rst

Behavior
--------

When using :pipeline:`$geoNear`, consider that:

- You can only use :pipeline:`$geoNear` as the first stage of a
  pipeline.

- You must include the ``distanceField`` option. The
  ``distanceField`` option specifies the field that will contain
  the calculated distance.

- The collection must have a :doc:`geospatial index
  </core/geospatial-indexes>`.

Generally, the options for :pipeline:`$geoNear` are similar to the
:dbcommand:`geoNear` command with the following exceptions:

- ``distanceField`` is a mandatory field for the
  :pipeline:`$geoNear` pipeline operator; the option does not exist
  in the :dbcommand:`geoNear` command.

- ``includeLocs`` accepts a ``string`` in the :pipeline:`$geoNear`
  pipeline operator and a ``boolean`` in the :dbcommand:`geoNear`
  command.

Example
-------

The following aggregation finds at most ``5`` unique documents
with a location at most .008 from the center ``[40.72, -73.99]``
and have ``type`` equal to ``public``:

.. code-block:: javascript

   db.places.aggregate([
                         {
                           $geoNear: {
                                       near: [40.724, -73.997],
                                       distanceField: "dist.calculated",
                                       maxDistance: 0.008,
                                       query: { type: "public" },
                                       includeLocs: "dist.location",
                                       num: 5
                                     }
                         }
                      ])

The aggregation returns the following:

.. code-block:: javascript

   {
     "result" : [
                  { "_id" : 7,
                    "name" : "Washington Square",
                    "type" : "public",
                    "location" : [
                                   [ 40.731, -73.999 ],
                                   [ 40.732, -73.998 ],
                                   [ 40.730, -73.995 ],
                                   [ 40.729, -73.996 ]
                                 ],
                    "dist" : {
                               "calculated" : 0.0050990195135962296,
                               "location" : [ 40.729, -73.996 ]
                             }
                  },
                  { "_id" : 8,
                    "name" : "Sara D. Roosevelt Park",
                    "type" : "public",
                    "location" : [
                                   [ 40.723, -73.991 ],
                                   [ 40.723, -73.990 ],
                                   [ 40.715, -73.994 ],
                                   [ 40.715, -73.994 ]
                                 ],
                    "dist" : {
                               "calculated" : 0.006082762530298062,
                               "location" : [ 40.723, -73.991 ]
                             }
                  }
                ],
     "ok" : 1
   }

The matching documents in the ``result`` field contain two new fields:

- ``dist.calculated`` field that contains the calculated distance, and

- ``dist.location`` field that contains the location used in the
  calculation.
====================
$group (aggregation)
====================

.. default-domain:: mongodb

.. pipeline:: $group

   Groups documents together for the purpose of calculating aggregate
   values based on a collection of documents. In practice,
   :pipeline:`$group` often supports tasks such as average page views
   for each page in a website on a daily basis.

   .. important:: The output of :pipeline:`$group` is not ordered.

   The output of :pipeline:`$group` depends on how you define groups.
   Begin by specifying an identifier (i.e. an ``_id`` field) for the
   group you're creating with this pipeline. For this ``_id`` field,
   you can specify various expressions, including a single field from
   the documents in the pipeline, a computed value from a previous
   stage, a document that consists of multiple fields, and other valid
   expressions, such as constant or subdocument fields. You can use
   :pipeline:`$project` operators in expressions for the ``_id`` field.

   The following example of an ``_id`` field specifies a document that
   consists of multiple fields:

   .. code-block:: javascript

      { $group: { _id : { author: '$author', pageViews: '$pageViews', posted: '$posted' } } }

   Every :pipeline:`$group` expression **must** specify an ``_id``
   field. In addition to the ``_id`` field, :pipeline:`$group`
   expression can include computed fields. These other fields must use
   one of the following :term:`accumulators <accumulator>`:

   - :group:`$addToSet`
   - :group:`$first`
   - :group:`$last`
   - :group:`$max`
   - :group:`$min`
   - :group:`$avg`
   - :group:`$push`
   - :group:`$sum`

   With the exception of the ``_id`` field, :pipeline:`$group` cannot
   output nested documents.

.. tip::

   Use :pipeline:`$project` as needed to rename the grouped field
   after a :pipeline:`$group` operation.

Variables
---------

.. versionchanged:: 2.6

You can use variables in expressions for the :pipeline:`$group` phase.
See :expression:`$let` and :expression:`$map`.

The system variables :variable:`$$CURRENT <CURRENT>` and
:variable:`$$ROOT <ROOT>` are also available directly. See
:ref:`group-stage-pivot-using-ROOT` for an example.

.. _group-memory-limit:

``$group`` Operator and Memory
------------------------------

The :pipeline:`$group` stage has a limit of 100 megabytes of RAM. By
default, if the stage exceeds this limit, :pipeline:`$group` will
produce an error. However, to allow for the handling of large datasets,
set the ``allowDiskUse`` option to ``true`` to enable
:pipeline:`$group` operations to write to temporary files. See the
``allowDiskUse`` option in :method:`db.collection.aggregate()` method
and the :dbcommand:`aggregate` command for details.

.. versionchanged:: 2.6
   MongoDB introduces a limit of 100 megabytes of RAM for the
   :pipeline:`$group` stage.

Examples
--------

Calculate Count and Sum
~~~~~~~~~~~~~~~~~~~~~~~

Consider the following example:

.. code-block:: javascript

   db.article.aggregate(
       { $group : {
           _id : "$author",
           docsPerAuthor : { $sum : 1 },
           viewsPerAuthor : { $sum : "$pageViews" }
       }}
   );

This aggregation pipeline groups by the ``author`` field and computes
two fields, ``docsPerAuthor`` and ``viewsPerAuthor``, per each group.
The ``docsPerAuthor`` field is a counter field that uses the
:group:`$sum` operator to add ``1`` for each document with a given
author. The ``viewsPerAuthor`` field is the sum of the values in the
``pageViews`` field for each group.

Pivot Data
~~~~~~~~~~

A collection ``books`` contains the following documents:

.. code-block:: javascript

   { "_id" : 8751, "title" : "The Banquet", "author" : "Dante", "copies" : 2 }
   { "_id" : 8752, "title" : "Divine Comedy", "author" : "Dante", "copies" : 1 }
   { "_id" : 8645, "title" : "Eclogues", "author" : "Dante", "copies" : 2 }
   { "_id" : 7000, "title" : "The Odyssey", "author" : "Homer", "copies" : 10 }
   { "_id" : 7020, "title" : "Iliad", "author" : "Homer", "copies" : 10 }

Group ``title`` by ``author``
`````````````````````````````

The following aggregation operation pivots the data in the ``books``
collection to have titles grouped by authors.

.. code-block:: javascript

   db.books.aggregate(
      [
        { $group : { _id : "$author", books: { $push: "$title" } } }
      ]
   )

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : "Homer", "books" : [ "The Odyssey", "Iliad" ] }
   { "_id" : "Dante", "books" : [ "The Banquet", "Divine Comedy", "Eclogues" ] }

.. _group-stage-pivot-using-ROOT:

Group Documents by ``author``
`````````````````````````````

The following aggregation operation uses the :variable:`$$ROOT <ROOT>`
system variable to group the documents by authors. The resulting
documents must not exceed the :limit:`BSON Document Size` limit.

.. code-block:: javascript

   db.books.aggregate(
      [
        { $group : { _id : "$author", books: { $push: "$$ROOT" } } }
      ]
   )

The operation returns the following documents:

.. code-block:: javascript

   {
     "_id" : "Homer",
     "books" :
        [
          { "_id" : 7000, "title" : "The Odyssey", "author" : "Homer", "copies" : 10 },
          { "_id" : 7020, "title" : "Iliad", "author" : "Homer", "copies" : 10 }
        ]
   }

   {
     "_id" : "Dante",
     "books" :
        [
          { "_id" : 8751, "title" : "The Banquet", "author" : "Dante", "copies" : 2 },
          { "_id" : 8752, "title" : "Divine Comedy", "author" : "Dante", "copies" : 1 },
          { "_id" : 8645, "title" : "Eclogues", "author" : "Dante", "copies" : 2 }
        ]
   }

.. seealso:: :ref:`push-with-root`
=================
$gt (aggregation)
=================

.. default-domain:: mongodb

.. expression:: $gt

   Takes two values in an array and returns a boolean. The returned
   value is:

   - ``true`` when the first value is *greater than* the second value.

   - ``false`` when the first value is *less than or equal to* the
     second value.
==================
$gte (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $gte

   Takes two values in an array and returns a boolean. The returned
   value is:

   - ``true`` when the first value is *greater than or equal* to the
     second value.

   - ``false`` when the first value is *less than* the second value.
===================
$hour (aggregation)
===================

.. default-domain:: mongodb

.. expression:: $hour

   Takes a date and returns the hour between 0 and 23.
=====================
$ifNull (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $ifNull

   Takes an array with two expressions. :expression:`$ifNull` returns
   the first expression if it evaluates to a non-``null`` value.
   Otherwise, :expression:`$ifNull` returns the second expression's
   value.

   Use the :expression:`$ifNull` operator with the following syntax:

   .. code-block:: javascript

      { $ifNull: [ <expression>, <replacement-if-null> ] }

   Both values in the array specified to :expression:`$ifNull`
   must be valid MongoDB :doc:`aggregation expressions
   </reference/operator/aggregation>` or document fields. Do
   not use JavaScript in any aggregation statements, including
   :expression:`$ifNull`.

   .. example::

      The following aggregation on the ``offSite`` collection
      groups by the ``location`` field and returns a count for each
      location. If the ``location`` field contains ``null`` or does
      not exist, the :expression:`$ifNull` returns ``"Unspecified"``
      as the value. MongoDB assigns the returned value to ``_id`` in
      the aggregated document.

      .. code-block:: javascript

         db.offSite.aggregate(
            [
               {
                  $group: {
                     _id: { $ifNull: [ "$location", "Unspecified" ] },
                     count: { $sum: 1 }
                  }
               }
            ]
         )
======================
Aggregation Interfaces
======================

.. default-domain:: mongodb

Aggregation Commands
--------------------

.. include:: /includes/toc/table-command-aggregation.rst

Aggregation Methods
-------------------

.. include:: /includes/toc/table-method-aggregation.rst
===================
$last (aggregation)
===================

.. default-domain:: mongodb

.. group:: $last

   Returns the last value it encounters for its group.

   .. note::

      Only use :group:`$last` when the :pipeline:`$group`
      follows an :pipeline:`$sort` operation. Otherwise, the
      result of this operation is unpredictable.
==================
$let (aggregation)
==================

.. default-domain:: mongodb

Definition
----------

.. expression:: $let

   Binds :doc:`variables </reference/aggregation-variables>` for use in
   subexpressions. To access the variable in the subexpressions, use a
   string with the variable name prefixed with double dollar signs
   (``$$``).

   The :expression:`$let` expression has the following syntax:

   .. code-block:: none

      {
        $let:
           {
             vars: { <var1>: <value1>, ... },
             in: { <expression using "$$var1", ...> }
           }
      }

   :returns: The value of the subexpression evaluated with the bound
      variables.

   See :doc:`/reference/aggregation-variables` for more information on
   using variables in the aggregation pipeline.

Behavior
--------

In the ``vars: { <var1>: <value1>, ... }`` assignment block, the order
of the assignment does not matter, and using ``$$var`` to access a
variable's value refers to the existing value, if any, of the variable.
Even if the variable is being reassigned, ``$$var`` would refer to the
current and not the reassigned value in the assignment block.

For example, the following :expression:`$let` expression is invalid
since in the ``vars: { low: 1, high: "$$low" }`` assignment block,
``"$$low"`` refers to the pre-assignment value of the variable ``low``,
which is undefined:

.. code-block:: none

   {
     $let:
       {
         vars: { low: 1, high: "$$low" },
         in: { $gt: [ "$$low", "$$high" ] }
       }
   }

:expression:`$let` can access variables defined outside its expression
block, including :ref:`system variables <agg-system-variables>`. If you
modify the values of externally defined variables in the ``vars``
block, the new values take effect only in the ``in`` subexpression, and
the variables retain to their previous values outside the ``in``
subexpression.

Examples
--------

.. _let-in-project:

Project Values Calculated Using Variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A ``sales`` collection has the following documents:

.. code-block:: javascript

   { _id: 1, price: 10, tax: 0.50, applyDiscount: true }
   { _id: 2, price: 10, tax: 0.25, applyDiscount: false }

The following aggregation uses :expression:`$let` in the the
:pipeline:`$project` pipeline stage to calculate and return the
``finalTotal`` for each document:

.. code-block:: none

   db.sales.aggregate( [
      {
         $project: {
            finalTotal: {
               $let: {
                  vars: {
                     total: { $add: [ '$price', '$tax' ] },
                     discounted: { $cond: { if: '$applyDiscount', then: 0.9, else: 1 } }
                  },
                  in: { $multiply: [ "$$total", "$$discounted" ] }
               }
            }
         }
      }
   ] )

The aggregation returns the following results:

.. code-block:: javascript

   { "_id" : 1, "finalTotal" : 9.450000000000001 }
   { "_id" : 2, "finalTotal" : 10.25 }

.. seealso:: :expression:`$map`
====================
$limit (aggregation)
====================

.. default-domain:: mongodb

.. pipeline:: $limit

   Restricts the number of :term:`documents <document>` that
   pass through the :pipeline:`$limit` in the :term:`pipeline`.

   :pipeline:`$limit` takes a single numeric (positive whole number)
   value as a parameter. Once the specified number of documents pass
   through the pipeline operator, no more will. Consider the following
   example:

   .. code-block:: javascript

      db.article.aggregate(
          { $limit : 5 }
      );

   This operation returns only the first 5 documents passed to it from
   by the pipeline. :pipeline:`$limit` has no effect on the content
   of the documents it passes.

   .. note::

      .. include:: /includes/fact-agg-sort-limit.rst
======================
$literal (aggregation)
======================

.. default-domain:: mongodb

Definition
----------

.. expression:: $literal

   Wraps an expression to prevent the aggregation pipeline from
   evaluating the expression.

Examples
--------

Treat ``$`` as a Literal
~~~~~~~~~~~~~~~~~~~~~~~~

In various aggregation expressions [#match-exception]_, the dollar sign
``$`` evaluates to a field path; i.e. provides access to the field. For
example, the :operator:`$eq` expression ``$eq: [ "$price", "$1" ]``
performs an equality check between the value in the field named
``price`` and the value in the field named ``1`` in the document.

The following example uses a :expression:`$literal` expression to treat
a string that contains a dollar sign ``"$1"`` as a constant value.

A collection ``records`` has the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "abc123", price: "$2.50" }
   { "_id" : 2, "item" : "xyz123", price: "1" }
   { "_id" : 3, "item" : "ijk123", price: "$1" }

.. code-block:: javascript

   db.records.aggregate( [
      { $project: { costsOneDollar: { $eq: [ "$price", { $literal: "$1" } ] } } }
   ] )

This operation projects a field named ``costsOneDollar`` that holds a
boolean value, indicating whether the value of the ``price`` field is
equal to the string ``"$1"``:

.. code-block:: javascript

   { "_id" : 1, "costsOneDollar" : false }
   { "_id" : 2, "costsOneDollar" : false }
   { "_id" : 3, "costsOneDollar" : true }

.. [#match-exception] The :pipeline:`$match` expressions do not evaluate
   ``$`` as the field path.

Project a New Field with Value ``1``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :pipeline:`$project` stage uses the expression ``<field>: 1`` to
include the ``<field>`` in the output. The following example uses the
:expression:`$literal` to return a new field set to the value of ``1``.

A collection ``bids`` has the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "abc123", condition: "new" }
   { "_id" : 2, "item" : "xyz123", condition: "new" }

The following aggregation evaluates the expression ``item: 1`` to mean
return the existing field ``item`` in the output, but uses the
:expression:`{ $literal: 1 } <$literal>` expression to return a new
field ``startAt`` set to the value ``1``:

.. code-block:: javascript

   db.bids.aggregate( [
      { $project: { item: 1, startAt: { $literal: 1 } } }
   ] )

The operation results in the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "abc123", "startAt" : 1 }
   { "_id" : 2, "item" : "xyz123", "startAt" : 1 }
=================
$lt (aggregation)
=================

.. default-domain:: mongodb

.. expression:: $lt

   Takes two values in an array and returns a boolean. The returned
   value is:

   - ``true`` when the first value is *less than* the second value.

   - ``false`` when the first value is *greater than or equal to* the
     second value.
==================
$lte (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $lte

   Takes two values in an array and returns a boolean. The returned
   value is:

   - ``true`` when the first value is *less than or equal to* the
     second value.

   - ``false`` when the first value is *greater than* the second
     value.
==================
$map (aggregation)
==================

.. default-domain:: mongodb

Definition
----------

.. expression:: $map

   :expression:`$map` applies a sub-expression to each item in an
   array and returns an array with the result of the sub-expression.

   :expression:`$map` is available in the :pipeline:`$project`,
   :pipeline:`$group`, and :pipeline:`$redact` pipeline stages.

Example
-------

Given an input document that resembles the following:

.. code-block:: javascript

   { skews: [ 1, 1, 2, 3, 5, 8 ] }

And the following :pipeline:`$project` statement:

.. code-block:: javascript

   { $project: { adjustments: { $map: { input: "$skews",
                                        as: "adj",
                                        in: { $add: [ "$$adj", 12 ] } } } } }

The :expression:`$map` would transform the input document into the
following output document:

.. code-block:: javascript

   { adjustments: [ 13, 13, 14, 15, 17, 20 ] }

.. seealso:: :expression:`$let`
====================
$match (aggregation)
====================

.. default-domain:: mongodb

.. pipeline:: $match

   :pipeline:`$match` pipes the documents that match its conditions to
   the next operator in the pipeline.

   The :pipeline:`$match` query syntax is identical to the
   :ref:`read operation query <read-operations-query-argument>` syntax.

Examples
--------

Equality Match
--------------

The following operation uses :pipeline:`$match` to perform a
simple equality match:

.. code-block:: javascript

   db.articles.aggregate(
       [ { $match : { author : "dave" } } ]
   );

The :pipeline:`$match` selects the documents where the ``author``
field equals ``dave``, and the aggregation returns the following:

.. code-block:: javascript

   {
     "result" : [
                  {
                    "_id" : ObjectId("512bc95fe835e68f199c8686"),
                    "author": "dave",
                    "score" : 80
                  },
                  { "_id" : ObjectId("512bc962e835e68f199c8687"),
                    "author" : "dave",
                    "score" : 85
                  }
                ],
     "ok" : 1
   }

Perform a Count
---------------

The following example selects documents to process using the
:pipeline:`$match` pipeline operator and then pipes the results
to the :pipeline:`$group` pipeline operator to compute a count of
the documents:

.. code-block:: javascript

  db.articles.aggregate( [
                          { $match : { score : { $gt : 70, $lte : 90 } } },
                          { $group: { _id: null, count: { $sum: 1 } } }
                         ] );

In the aggregation pipeline, :pipeline:`$match` selects the
documents where the ``score`` is greater than ``70`` and less
than or equal to ``90``. These documents are then piped to the
:pipeline:`$group` to perform a count. The aggregation returns
the following:

.. code-block:: javascript

   {
     "result" : [
                  {
                    "_id" : null,
                    "count" : 3
                  }
                ],
     "ok" : 1
   }

Behavior
--------

Pipeline Optimization
~~~~~~~~~~~~~~~~~~~~~

- Place the :pipeline:`$match` as early in the aggregation
  :term:`pipeline` as possible. Because :pipeline:`$match` limits
  the total number of documents in the aggregation pipeline,
  earlier :pipeline:`$match` operations minimize the amount of
  processing down the pipe.

- If you place a :pipeline:`$match` at the very beginning of a
  pipeline, the query can take advantage of :term:`indexes
  <index>` like any other :method:`db.collection.find()`
  or :method:`db.collection.findOne()`.

Restrictions
~~~~~~~~~~~~

- You cannot use :query:`$where` in :pipeline:`$match` queries as part
  of the aggregation pipeline.

- To use :query:`$text` in the :pipeline:`$match` stage, the
  :pipeline:`$match` stage has to be the first stage of the pipeline.
==================
$max (aggregation)
==================

.. default-domain:: mongodb

.. group:: $max

   Returns the highest value among all values of the field in all
   documents selected by this group.
===================
$meta (aggregation)
===================

.. default-domain:: mongodb

.. expression:: $meta

   .. versionadded:: 2.6

   The :expression:`$meta` operator returns the metadata associated
   with a document in a pipeline operations, e.g. ``"textScore"``
   when performing text search.

   .. |meta-object| replace:: :expression:`$meta`

   .. include:: /includes/fact-meta-syntax.rst

Behaviors
---------

The :expression:`$meta` expression can be a part of the
:pipeline:`$project` stage and the :pipeline:`$sort` stage.

Projected Field Name
~~~~~~~~~~~~~~~~~~~~

If the specified ``<projectedFieldName>`` already exists in the
matching documents, in the result set, the existing fields will return
with the :expression:`$meta` values instead of with the stored values.

Projection
~~~~~~~~~~

The :expression:`$meta` expression can be used in the
:pipeline:`$project` stage, as in:

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "cake" } } },
        { $project: { title: 1, score: { $meta: "textScore" } } }
      ]
   )

The inclusion of the :expression:`$meta` aggregation expression in the
:pipeline:`$project` pipeline specifies both the inclusion of the
metadata *as well as* the exclusion of the fields, other than ``_id``,
that are *not* explicitly included in the projection document. This
differs from the behavior of the :projection:`$meta` projection
operator in a :method:`db.collection.find()` operation which only
signifies the inclusion of the metadata and does *not* signify an
exclusion of other fields.

Sort
~~~~

To use the metadata to sort, specify the :expression:`$meta` expression
in :pipeline:`$sort` stage, as in:

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "cake tea" } } },
        { $sort: { score: { $meta: "textScore" } } },
        { $project: { title: 1, _id: 0 } }
      ]
   )

The specified metadata determines the sort order. For example, the
``"textScore"`` metadata sorts in descending order.

Examples
--------

.. TODO when $meta returns other data, subsection Examples section
   by keywords.

For examples of ``"textScore"`` projections and sorts, see
:doc:`/tutorial/text-search-in-aggregation`.
==========================
$millisecond (aggregation)
==========================

.. default-domain:: mongodb

.. expression:: $millisecond

   Takes a date and returns the millisecond portion of the date as an
   integer between 0 and 999.
==================
$min (aggregation)
==================

.. default-domain:: mongodb

.. group:: $min

   The :group:`$min` operator returns the lowest non-null value of a
   field in the documents for a :pipeline:`$group` operation.

   .. versionchanged:: 2.4
      If some, **but not all**, documents for the :group:`$min`
      operation have either a ``null`` value for the field or are
      missing the field, the :group:`$min` operator only considers the
      non-null and the non-missing values for the field. If **all**
      documents for the :group:`$min` operation have ``null`` value for
      the field or are missing the field, the :group:`$min` operator
      returns ``null`` for the minimum value.

      Before 2.4, if any of the documents for the :group:`$min`
      operation were missing the field, the :group:`$min` operator
      would not return any value. If any of the documents for the
      :group:`$min` had the value ``null``, the :group:`$min` operator
      would return a ``null``.

   .. example::

      The ``users`` collection contains the following documents:

      .. code-block:: javascript

         { "_id" : "abc001", "age" : 25 }
         { "_id" : "abe001", "age" : 35 }
         { "_id" : "efg001", "age" : 20 }
         { "_id" : "xyz001", "age" : 15 }

      - To find the minimum value of the ``age`` field from all the
        documents, use the :group:`$min` operator:

        .. code-block:: javascript

           db.users.aggregate( [ { $group: { _id:0, minAge: { $min: "$age"} } } ] )

        The operation returns the value of the ``age`` field in the
        ``minAge`` field:

        .. code-block:: javascript

           { "result" : [ { "_id" : 0, "minAge" : 15 } ], "ok" : 1 }

      - To find the minimum value of the ``age`` field for only those
        documents with ``_id`` starting with the letter ``a``, use the
        :group:`$min` operator after a :pipeline:`$match` operation:

        .. code-block:: javascript

           db.users.aggregate( [ { $match: { _id: /^a/ } },
                                 { $group: { _id: 0, minAge: { $min: "$age"} } }
                               ] )

        The operation returns the minimum value of the ``age`` field
        for the two documents with ``_id`` starting with the letter
        ``a``:

        .. code-block:: javascript

           { "result" : [ { "_id" : 0, "minAge" : 25 } ], "ok" : 1 }

   .. example::

      The ``users`` collection contains the following documents where
      some of the documents are either missing the ``age`` field or the
      ``age`` field contains ``null``:

      .. code-block:: javascript

         { "_id" : "abc001", "age" : 25 }
         { "_id" : "abe001", "age" : 35 }
         { "_id" : "efg001", "age" : 20 }
         { "_id" : "xyz001", "age" : 15 }
         { "_id" : "xxx001" }
         { "_id" : "zzz001", "age" : null }

      - The following operation finds the minimum value of the ``age``
        field in all the documents:

        .. code-block:: javascript

           db.users.aggregate( [ { $group: { _id:0, minAge: { $min: "$age"} } } ] )

        Because only some documents for the :group:`$min` operation are
        missing the ``age`` field or have ``age`` field equal to
        ``null``, :group:`$min` only considers the non-null and the
        non-missing values and the operation returns the following
        document:

        .. code-block:: javascript

           { "result" : [ { "_id" : 0, "minAge" : 15 } ], "ok" : 1 }

      - The following operation finds the minimum value of the ``age``
        field for only those documents where the ``_id`` equals
        ``"xxx001"`` or ``"zzz001"``:

        .. code-block:: javascript

           db.users.aggregate( [ { $match: { _id: {$in: [ "xxx001", "zzz001" ] } } },
                                 { $group: { _id: 0, minAge: { $min: "$age"} } }
                               ] )

        The :group:`$min` operation returns ``null`` for the minimum
        age since **all** documents for the :group:`$min` operation
        have ``null`` value for the field ``age`` or are missing the
        field:

        .. code-block:: javascript

           { "result" : [ { "_id" : 0, "minAge" : null } ], "ok" : 1 }
=====================
$minute (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $minute

   Takes a date and returns the minute between 0 and 59.
==================
$mod (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $mod

   Takes an array that contains a pair of numbers and returns the
   *remainder* of the first number divided by the second number.

   .. seealso:: :query:`$mod`
====================
$month (aggregation)
====================

.. default-domain:: mongodb

.. expression:: $month

   Takes a date and returns the month as a number between 1 and 12.
=======================
$multiply (aggregation)
=======================

.. default-domain:: mongodb

.. expression:: $multiply

   Takes an array of one or more numbers and multiples them, returning the
   resulting product.
=================
$ne (aggregation)
=================

.. default-domain:: mongodb

.. expression:: $ne

   Takes two values in an array returns a boolean. The returned value
   is:

   - ``true`` when the values are **not equivalent**.

   - ``false`` when the values are **equivalent**.
==================
$not (aggregation)
==================

.. default-domain:: mongodb

.. expression:: $not

   Returns the boolean opposite value passed to it. When passed a
   ``true`` value, :expression:`$not` returns ``false``; when passed
   a ``false`` value, :expression:`$not` returns ``true``.
=================
$or (aggregation)
=================

.. default-domain:: mongodb

.. expression:: $or

   Takes an array of one or more values and returns ``true`` if *any* of the values in the
   array are ``true``. Otherwise :expression:`$or` returns false.

   .. note::

      :expression:`$or` uses short-circuit logic: the operation
      stops evaluation after encountering the first ``true`` expression.
==================
$out (aggregation)
==================

.. default-domain:: mongodb

.. versionadded:: 2.6

Definition
----------

.. pipeline:: $out

   Takes the documents returned by the aggregation pipeline and writes
   them to a specified collection. The :pipeline:`$out` operator lets
   the aggregation framework return result sets of any size. The
   :pipeline:`$out` operator must be *the last stage* in the pipeline.

   The command has the following syntax, where ``<output-collection>`` is
   collection that will hold the output of the aggregation
   operation. :pipeline:`$out` is only permissible at the end of the
   pipeline:

   .. code-block:: javascript

      db.<collection>.aggregate( [
           { <operation> },
           { <operation> },
           ...,
           { $out : "<output-collection>" }
      ] )

   .. important::

      - You cannot specify a sharded collection as the output collection.
        The input collection for a pipeline can be sharded.

      - The :pipeline:`$out` operator cannot write results to a
        :doc:`capped collection </core/capped-collections>`.

Behaviors
---------

Create New Collection
~~~~~~~~~~~~~~~~~~~~~

The :pipeline:`$out` operation creates a new collection in the current
database if one does not already exist. The collection is not visible
until the aggregation completes. If the aggregation fails, MongoDB does
not create the collection.

Replace Existing Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the collection specified by the :pipeline:`$out` operation already
exists, then upon completion of the aggregation, the :pipeline:`$out`
stage atomically replaces the existing collection with the new
results collection. The :pipeline:`$out` operation does not change any
indexes that existed on the previous collection. If the aggregation
fails, the :pipeline:`$out` operation makes no changes to the pre-existing
collection.

Index Constraints
~~~~~~~~~~~~~~~~~

The pipeline will fail to complete if the documents produced by the
pipeline would violate any unique indexes, including the index on the
``_id`` field of the original output collection.

Example
-------

A collection ``books`` contains the following documents:

.. code-block:: javascript

   { "_id" : 8751, "title" : "The Banquet", "author" : "Dante", "copies" : 2 }
   { "_id" : 8752, "title" : "Divine Comedy", "author" : "Dante", "copies" : 1 }
   { "_id" : 8645, "title" : "Eclogues", "author" : "Dante", "copies" : 2 }
   { "_id" : 7000, "title" : "The Odyssey", "author" : "Homer", "copies" : 10 }
   { "_id" : 7020, "title" : "Iliad", "author" : "Homer", "copies" : 10 }

The following aggregation operation pivots the data in the ``books``
collection to have titles grouped by authors and then writes
the results to the ``authors`` collection.

.. code-block:: javascript

   db.books.aggregate( [
                         { $group : { _id : "$author", books: { $push: "$title" } } },
                         { $out : "authors" }
                     ] )

After the operation, the ``authors`` collection contains the following
documents:

.. code-block:: javascript

   { "_id" : "Homer", "books" : [ "The Odyssey", "Iliad" ] }
   { "_id" : "Dante", "books" : [ "The Banquet", "Divine Comedy", "Eclogues" ] }
======================
$project (aggregation)
======================

.. default-domain:: mongodb

.. pipeline:: $project

   Reshapes a document stream by renaming, adding, or removing
   fields. Also use :pipeline:`$project` to create computed values or
   sub-documents. Use :pipeline:`$project` to:

   - Include fields from the original document.
   - Insert computed fields.

     .. versionchanged:: 2.6
        You can use variables in the calculation of computed fields.
        See :expression:`$let` and :expression:`$map`. The system
        variables :variable:`$$CURRENT <CURRENT>` and :variable:`$$ROOT
        <ROOT>` are also available directly.

   - Rename fields.
   - Create and populate fields that hold sub-documents.

   Use :pipeline:`$project` to quickly select the fields that you
   want to include or exclude from the response. Consider the
   following aggregation framework operation.

   .. code-block:: javascript

      db.article.aggregate(
          { $project : {
              title : 1 ,
              author : 1 ,
          }}
       );

   This operation includes the ``title`` field and the ``author``
   field in the document that returns from the aggregation
   :term:`pipeline`.

   .. note::

      The ``_id`` field is always included by default. You may
      explicitly exclude ``_id`` as follows:

      .. code-block:: javascript

         db.article.aggregate(
             { $project : {
                 _id : 0 ,
                 title : 1 ,
                 author : 1
             }}
         );

      Here, the projection excludes the ``_id`` field but includes the
      ``title`` and ``author`` fields.

   Projections can also add computed fields to the document stream
   passing through the pipeline. A computed field can use any of the
   :ref:`expression operators <aggregation-expression-operators>` or
   for text search, use the :expression:`$meta` operator. Consider the
   following example:

   .. code-block:: javascript

      db.article.aggregate(
          { $project : {
              title : 1,
              doctoredPageViews : { $add:["$pageViews", 10] }
          }}
      );

   Here, the field ``doctoredPageViews`` represents the value of the
   ``pageViews`` field after adding 10 to the original field using the
   :expression:`$add`.

   .. note::

      You must enclose the expression that defines the computed field
      in braces, so that the expression is a valid object.

   You may also use :pipeline:`$project` to rename fields. Consider
   the following example:

   .. code-block:: javascript

      db.article.aggregate(
          { $project : {
              title : 1 ,
              page_views : "$pageViews" ,
              bar : "$other.foo"
          }}
      );

   This operation renames the ``pageViews`` field to ``page_views``,
   and renames the ``foo`` field in the ``other`` sub-document as
   the top-level field ``bar``. The field references used for
   renaming fields are direct expressions and do not use an operator
   or surrounding braces. All aggregation field references can use
   dotted paths to refer to fields in nested documents.

   Finally, you can use the :pipeline:`$project` to create and
   populate new sub-documents. Consider the following example that
   creates a new object-valued field named ``stats`` that holds a number
   of values:

   .. code-block:: javascript

      db.article.aggregate(
          { $project : {
              title : 1 ,
              stats : {
                  pv : "$pageViews",
                  foo : "$other.foo",
                  dpv : { $add:["$pageViews", 10] }
              }
          }}
      );

   This projection includes the ``title`` field and places
   :pipeline:`$project` into "inclusive" mode. Then, it creates the
   ``stats`` documents with the following fields:

   - ``pv`` which includes and renames the ``pageViews`` from the
     top level of the original documents.

   - ``foo`` which includes the value of ``other.foo`` from the
     original documents.

   - ``dpv`` which is a computed field that adds 10 to the value of
     the ``pageViews`` field in the original document using the
     :expression:`$add` aggregation expression.
===================
$push (aggregation)
===================

.. default-domain:: mongodb

.. group:: $push

   Returns an array of all the values found in the selected field
   among the documents in that group. A value may appear *more than
   once* in the result set if more than one field in the grouped
   documents has that value.

Example
-------

The following examples use the following collection named ``users`` as
the input for the aggregation pipeline:

.. code-block:: javascript

   { "_id": 1, "user" : "Jan", "age" : 25, "score": 80 }
   { "_id": 2, "user" : "Mel", "age" : 35, "score": 70 }
   { "_id": 3, "user" : "Ty", "age" : 20, "score": 102 }
   { "_id": 4, "user" : "Lee", "age" : 25, "score": 45 }

Push Values of a Single Field Into the Returned Array Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To group by ``age`` and return all the ``user`` values for each age, use the
:group:`$push` operator.

.. code-block:: javascript

   db.users.aggregate(
                        {
                          $group: {
                                    _id: "$age",
                                    users: { $push: "$user" }
                                  }
                        }
                     )

For each ``age``, the operation returns the field ``users`` that
contains an array of all the ``user`` values associated with that age:

.. code-block:: javascript

   {
      "result" : [
         {
            "_id" : 20,
            "users" : [
               "Ty"
            ]
         },
         {
            "_id" : 35,
            "users" : [
               "Mel"
            ]
         },
         {
            "_id" : 25,
            "users" : [
               "Jan",
               "Lee"
            ]
         }
      ],
      "ok" : 1
   }

Push Documents Into the Returned Array Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :group:`$push` operator can return an array of documents.

To group by ``age`` and return all the ``user`` and associated
``score`` values for each age, use the :group:`$push` operator.

.. code-block:: javascript

   db.users.aggregate(
                        {
                          $group: {
                                    _id: "$age",
                                    users: { $push: { userid: "$user", score: "$score" } }
                                  }
                        }
                     )

For each ``age``, the operation returns the field ``users`` that
contains an array of documents. These documents contain the fields
``userid`` and ``score`` that hold respectively the ``user`` value and
the ``score`` value associated with that age:

.. code-block:: javascript

   {
      "result" : [
         {
            "_id" : 20,
            "users" : [
               {
                  "userid" : "Ty",
                  "score" : 102
               }
            ]
         },
         {
            "_id" : 35,
            "users" : [
               {
                  "userid" : "Mel",
                  "score" : 70
               }
            ]
         },
         {
            "_id" : 25,
            "users" : [
               {
                  "userid" : "Jan",
                  "score" : 80
               },
               {
                  "userid" : "Lee",
                  "score" : 45
               }
            ]
         }
      ],
      "ok" : 1
   }

.. _push-with-root:

Push Current Document Into the Returned Array Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :group:`$push` operator can use the system variable
:variable:`$$ROOT <ROOT>` to push the current document being processed
into the array. The resulting documents must not exceed the
:limit:`BSON Document Size` limit.

To group by ``age`` and return the documents containing that age, use
the :group:`$push` operator with :variable:`$$ROOT <ROOT>`,

.. code-block:: javascript

   db.users.aggregate(
      {
        $group:
           {
             _id: "$age",
             users: { $push: "$$ROOT" }
           }
      }
   )

The operation returns the following documents:

.. code-block:: javascript

   {
     "_id" : 20,
     "users" : [ { "_id" : 3, "user" : "Ty", "age" : 20, "score" : 102 } ]
   }
   {
     "_id" : 35,
     "users" : [ { "_id" : 2, "user" : "Mel", "age" : 35, "score" : 70 } ]
   }
   {
     "_id" : 25,
     "users" :
        [
          { "_id" : 1, "user" : "Jan", "age" : 25, "score" : 80 },
          { "_id" : 4, "user" : "Lee", "age" : 25, "score" : 45 }
        ]
   }
=====================
$redact (aggregation)
=====================

.. default-domain:: mongodb

Definition
----------

.. pipeline:: $redact

   .. versionadded:: 2.6

   Restricts the contents of the documents based on information stored
   in the documents themselves.

   .. include:: /images/redact-security-architecture.rst

   The :pipeline:`$redact` pipeline operator takes an expression that
   evaluates to `$$DESCEND`_, `$$PRUNE`_, or `$$KEEP`_.

   For example, the following :pipeline:`$redact` pipeline uses the
   :expression:`$cond` expression [#cond-syntax]_:

   .. code-block:: none

      db.<collection>.aggregate(
         [
           { $redact:
              {
                 $cond:
                    {
                       if: <boolean-expression>,
                       then: <"$$DESCEND" | "$$PRUNE" | "$$KEEP">,
                       else: <"$$DESCEND" | "$$PRUNE" | "$$KEEP">
                    }
              }
           }
         ]
      )

   In the example :expression:`$cond` expression, the
   ``<boolean-expression>`` uses a field or fields in the document to
   specify the conditions for either returning or omitting content.

   .. tip:: To handle documents that are missing field(s) used in
      ``<boolean-expression>``, include :expression:`$ifNull` in the
      expression.

   .. list-table::
      :header-rows: 1
      :widths: 15 50

      * - System Variable

        - Description

      * - _`$$DESCEND`

        - :pipeline:`$redact` returns the *non-subdocument* fields at
          the current document/subdocument level. For subdocuments or
          subdocuments in arrays, apply the :expression:`$cond`
          expression to the subdocuments to determine access for these
          subdocuments.

      * - _`$$PRUNE`

        - :pipeline:`$redact` excludes all fields at this current
          document/subdocument level, **without** further inspection of
          any of the excluded fields. This applies even if the excluded
          field contains subdocuments that may have different access
          levels.

      * - _`$$KEEP`

        - :pipeline:`$redact` returns or keeps all fields at this
          current document/subdocument level, **without** further
          inspection of the fields at this level. This applies even if
          the included field contains subdocuments that may have
          different access levels.

.. seealso:: :expression:`$cond`.

.. [#cond-syntax] The :expression:`$cond` expression supports an
   alternate syntax that accepts an array instead of a document form.
   See :expression:`$cond` for details.

Examples
--------

The examples in this section use the
:method:`db.collection.aggregate()` helper provided in the 2.6 version
of the :program:`mongo` shell.

Evaluate Access at Every Document/Sub-document Level
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A ``forecasts`` collection contains documents of the following form
where the ``tags`` field lists the different access values for that
document/subdocument level; i.e. a value of ``[ "G", "STLW" ]``
specifies either ``"G"`` or ``"STLW"`` can access the data:

.. code-block:: javascript

   {
      _id: 1,
      title: "123 Department Report",
      tags: [ "G", "STLW" ],
      year: 2014,
      subsections: [
          {
              subtitle: "Section 1: Overview",
              tags: [ "SI", "G" ],
              content:  "Section 1: This is the content of section 1."
          },
          {
              subtitle: "Section 2: Analysis",
              tags: [ "STLW" ],
              content: "Section 2: This is the content of section 2."
          },
          {
              subtitle: "Section 3: Budgeting",
              tags: [ "TK" ],
              content: {
                  text: "Section 3: This is the content of section3.",
                  tags: [ "HCS" ]
              }
          }
      ]
   }

A user has access to view information with either the tag ``"STLW"`` or
``"G"``. To run a query on all documents with year ``2014`` for this
user, include a :pipeline:`$redact` stage as in the following:

.. code-block:: none

   var userAccess = [ "STLW", "G" ];
   db.forecasts.aggregate(
      [
        { $match: { year: 2014 } },
        { $redact:
            {
               $cond:
                  {
                    if: { $gt: [ { $size: { $setIntersection: [ "$tags", userAccess ] } }, 0 ] },
                    then: "$$DESCEND",
                    else: "$$PRUNE"
                  }
            }
        }
      ]
   )

The aggregation operation returns the following "redacted" document:

.. code-block:: none

   {
      "_id" : 1,
      "title" : "123 Department Report",
      "tags" : [ "G", "STLW" ],
      "year" : 2014,
      "subsections" : [
         {
            "subtitle" : "Section 1: Overview",
            "tags" : [ "SI", "G" ],
            "content" : "Section 1: This is the content of section 1."
         },
         {
            "subtitle" : "Section 2: Analysis",
            "tags" : [ "STLW" ],
            "content" : "Section 2: This is the content of section 2."
         }
      ]
   }

.. seealso:: :expression:`$size`, :expression:`$setIntersection`

Exclude All Fields at a Given Level
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``accounts`` contains the following document:

.. code-block:: javascript

   {
     _id: 1,
     level: 1,
     acct_id: "xyz123",
     cc: {
           level: 5,
           type: "yy",
           num: 000000000000,
           exp_date: ISODate("2015-11-01T00:00:00.000Z"),
           billing_addr: {
                           level: 5,
                           addr1: "123 ABC Street",
                           city: "Some City"
                         },
           shipping_addr: [
                             {
                               level: 3,
                               addr1: "987 XYZ Ave",
                               city: "Some City"
                             },
                             {
                               level: 3,
                               addr1: "PO Box 0123",
                               city: "Some City"
                             }
                          ]
         },
     status: "A"
   }

In this example document, the ``level`` field determines the access
level required to view the data.

To run a query on all documents with status ``A`` and exclude *all*
fields contained in a document/subdocument at level ``5``, include a
:pipeline:`$redact` stage that specifies the system variable
``"$$PRUNE"`` in the ``then`` field:

.. code-block:: none

   db.accounts.aggregate(
      [
        { $match: { status: "A" } },
        { $redact:
           {
               $cond: {
                        if: { $eq: [ "$level", 5 ] },
                        then: "$$PRUNE",
                        else: "$$DESCEND"
                      }
           }
        }
      ]
   )

The :pipeline:`$redact` stage evaluates the ``level`` field to
determine access. If the ``level`` field equals ``5``, then exclude all
fields at that level, even if the excluded field contains subdocuments
that may have different ``level`` values, such as the ``shipping_addr``
field.

The aggregation operation returns the following "redacted" document:

.. code-block:: none

   {
     "_id" : 1,
     "level" : 1,
     "acct_id" : "xyz123",
     "status" : "A"
   }

The result set shows that the :pipeline:`$redact` stage excluded
the field ``cc`` as a whole, including the ``shipping_addr`` field
which contained subdocuments that had ``level`` field values equal to
``3`` and not ``5``.

.. seealso:: :doc:`/tutorial/implement-field-level-redaction` for
   steps to set up multiple combinations of access for the same data.
=====================
$second (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $second

   Takes a date and returns the second between 0 and 59, but can be 60
   to account for leap seconds.
============================
$setDifference (aggregation)
============================

.. default-domain:: mongodb

.. expression:: $setDifference

   .. versionadded:: 2.6

   Takes two arrays and returns an array containing the elements that
   only exist in the first array.

.. |set-operator-name| replace:: :expression:`$setDifference`
.. include:: /includes/important-set-operator-semantics.rst
========================
$setEquals (aggregation)
========================

.. default-domain:: mongodb

.. expression:: $setEquals

   .. versionadded:: 2.6

   Takes two arrays and returns ``true`` when they contain the same
   elements, and ``false`` otherwise.

.. |set-operator-name| replace:: :expression:`$setEquals`
.. include:: /includes/important-set-operator-semantics.rst
==============================
$setIntersection (aggregation)
==============================

.. default-domain:: mongodb

.. expression:: $setIntersection

   .. versionadded:: 2.6

   Takes any number of arrays and returns an array that contains the
   elements that appear in every input array.

.. |set-operator-name| replace:: :expression:`$setIntersection`
.. include:: /includes/important-set-operator-semantics.rst
==========================
$setIsSubset (aggregation)
==========================

.. default-domain:: mongodb

.. expression:: $setIsSubset

   .. versionadded:: 2.6

   Takes two arrays and returns ``true`` when the first array is a
   subset of the second and ``false`` otherwise.

.. |set-operator-name| replace:: :expression:`$setIsSubset`
.. include:: /includes/important-set-operator-semantics.rst
=======================
$setUnion (aggregation)
=======================

.. default-domain:: mongodb

.. expression:: $setUnion

   .. versionadded:: 2.6

   Takes any number of arrays and returns an array containing the
   elements that appear in any input array.

.. |set-operator-name| replace:: :expression:`$setUnion`
.. include:: /includes/important-set-operator-semantics.rst
===================
$size (aggregation)
===================

.. default-domain:: mongodb

.. versionadded:: 2.6

Definition
----------

.. expression:: $size

   Counts and returns the total the number of items in an array.
   Consider the following syntax:

   .. code-block:: javascript

      { <field>: { $size: <array> } }

Example
-------

Given a ``survey`` collection that records town occupants by
household, and that includes documents similar to the following:

.. code-block:: javascript

   {
     "_id" : ObjectId("524d82e535edde4707c684c5"),
     "household" : "Carter",
     "st_num" : 300,
     "street" : "North Bond Street",
     "occupants" : [
         "Amy",
         "Donnel",
         "Jack",
         "James"
     ]
   }

The following aggregation pipeline operation counts the number of
residents on each street. The pipeline groups documents according to
the ``street`` field and uses the :query:`$size` operator to count
the entries in each household's ``occupants`` array. The pipeline
uses :group:`$sum` to add the number of residents on each street.

.. code-block:: javascript

   db.survey.aggregate(
      [
         {
            $group: {
               _id: "$street",
               numResidents: { $sum: {$size: "$occupants" } }
            }
         }
      ]
   )
===================
$skip (aggregation)
===================

.. default-domain:: mongodb

.. pipeline:: $skip

   Skips over the specified number of :term:`documents <document>`
   that pass through the :pipeline:`$skip` in the :term:`pipeline`
   before passing all of the remaining input.

   :pipeline:`$skip` takes a single numeric (positive whole number)
   value as a parameter. Once the operation has skipped the specified
   number of documents, it passes all the remaining documents along the
   :term:`pipeline` without alteration. Consider the following
   example:

   .. code-block:: javascript

      db.article.aggregate(
          { $skip : 5 }
      );

   This operation skips the first 5 documents passed to it by the
   pipeline. :pipeline:`$skip` has no effect on the content of the
   documents it passes along the pipeline.
===================
$sort (aggregation)
===================

.. default-domain:: mongodb

.. pipeline:: $sort

   The :pipeline:`$sort` :term:`pipeline` operator sorts all input
   documents and returns them to the pipeline in sorted order.

   Consider the following prototype form:

   .. code-block:: javascript

      db.<collection-name>.aggregate(
         [
           { $sort : { <sort-key> } }
         ]
      )

   This sorts the documents in the collection named
   ``<collection-name>``, according to the key and specification in the
   ``{ <sort-key> }`` document. The sort key has the following syntax:

   .. code-block:: javascript

      { field: value }

   The ``{ <sort-key> }`` document can specify :ref:`ascending or
   descending sort on existing fields <sort-pipeline-asc-desc>` or
   :ref:`sort on computed metadata <sort-pipeline-metadata>`.

Behaviors
---------

.. _sort-pipeline-asc-desc:

Ascending/Descending Sort
~~~~~~~~~~~~~~~~~~~~~~~~~

Specify in the ``{ <sort-key> }`` document the field or fields to sort
by and a value of ``1`` or ``-1`` to specify an ascending or descending
sort respectively, as in the following example:

.. code-block:: javascript

   db.users.aggregate(
      [
        { $sort : { age : -1, posts: 1 } }
      ]
   )

This operation sorts the documents in the ``users`` collection,
in descending order according by the ``age`` field and then in
ascending order according to the value in the ``posts`` field.

.. include:: /includes/fact-sort-order.rst

.. _sort-pipeline-metadata:

Metadata Sort
~~~~~~~~~~~~~

Specify in the ``{ <sort-key> }`` document, a new field name for the
computed metadata and specify the :expression:`$meta` expression as its
value, as in the following example:

.. code-block:: javascript

   db.users.aggregate(
      [
        { $match: { $text: { $search: "operating" } } },
        { $sort: { score: { $meta: "textScore" }, posts: -1 } }
      ]
   )

This operation uses the :query:`$text` operator to match the documents,
and then sorts first by the ``"textScore"`` metadata and then by
descending order of the ``posts`` field. The specified metadata
determines the sort order. For example, the ``"textScore"`` metadata
sorts in descending order. See :expression:`$meta` for more information
on metadata.

.. _sort-and-memory:

``$sort`` Operator and Memory
-----------------------------

.. _sort-limit-sequence:

``$sort`` + ``$limit`` Memory Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/fact-agg-sort-limit.rst

.. _sort-memory-limit:

``$sort`` and Memory Restrictions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :pipeline:`$sort` stage has a limit of 100 megabytes of RAM. By
default, if the stage exceeds this limit, :pipeline:`$sort` will
produce an error. To allow for the handling of large datasets, set the
``allowDiskUsage`` option to ``true`` to enable :pipeline:`$sort`
operations to write to temporary files. See the ``allowDiskUsage``
option in :method:`db.collection.aggregate()` method and the
:dbcommand:`aggregate` command for details.

.. versionchanged:: 2.6
   The memory limit for :pipeline:`$sort` changed from 10
   percent of RAM to 100 megabytes of RAM.

``$sort`` Operator and Performance
----------------------------------

:pipeline:`$sort` operator can take advantage of an index when
placed at the **beginning** of the pipeline or placed **before**
the following aggregation operators: :pipeline:`$project`,
:pipeline:`$unwind`, and :pipeline:`$group`.

.. todo:: if a sort precedes the first $group in a sharded system,
   all documents must go to the mongos for sorting.
=========================
$strcasecmp (aggregation)
=========================

.. default-domain:: mongodb

.. expression:: $strcasecmp

   Takes in two strings. Returns a number. :expression:`$strcasecmp`
   is positive if the first string is "greater than" the second and
   negative if the first string is "less than" the
   second. :expression:`$strcasecmp` returns 0 if the strings are
   identical.

   .. note::

      :expression:`$strcasecmp` may not make sense when applied to
      glyphs outside the Roman alphabet.

      :expression:`$strcasecmp` internally capitalizes strings before
      comparing them to provide a case-*insensitive* comparison.
      Use :expression:`$cmp` for a case sensitive comparison.
=====================
$substr (aggregation)
=====================

.. default-domain:: mongodb

.. expression:: $substr

   :expression:`$substr` takes a string and two numbers. The first
   number represents the number of bytes in the string to skip,
   and the second number specifies the number of bytes to return
   from the string.

   .. note::

      :expression:`$substr` is not encoding aware and if used
      improperly may produce a result string containing an invalid UTF-8
      character sequence.
=======================
$subtract (aggregation)
=======================

.. default-domain:: mongodb

.. expression:: $subtract

   Takes an array that contains a pair of numbers and subtracts the
   second from the first, returning their difference.
==================
$sum (aggregation)
==================

.. default-domain:: mongodb

.. group:: $sum

   Returns the sum of all the values for a specified
   field in the grouped documents.

   Alternately, if you specify a value as an argument,
   :group:`$sum` will increment this field by the specified value
   for every document in the grouping. Typically,
   specify a value of ``1`` in order to count members of the
   group.
======================
$toLower (aggregation)
======================

.. default-domain:: mongodb

.. expression:: $toLower

   Takes a single string and converts that string to lowercase,
   returning the result. All uppercase letters become lowercase.

   .. note::

      :expression:`$toLower` may not make sense when applied to glyphs outside
      the Roman alphabet.
======================
$toUpper (aggregation)
======================

.. default-domain:: mongodb

.. expression:: $toUpper

   Takes a single string and converts that string to uppercase,
   returning the result. All lowercase letters become uppercase.

   .. note::

      :expression:`$toUpper` may not make sense when applied to glyphs outside
      the Roman alphabet.
=====================
$unwind (aggregation)
=====================

.. default-domain:: mongodb

.. pipeline:: $unwind

   Peels off the elements of an array individually, and returns a
   stream of documents. :pipeline:`$unwind` returns one document for
   every member of the unwound array within every source
   document. Take the following aggregation command:

   .. code-block:: javascript

      db.article.aggregate(
          { $project : {
              author : 1 ,
              title : 1 ,
              tags : 1
          }},
          { $unwind : "$tags" }
      );

   .. note::

      The dollar sign (i.e. ``$``) must precede the field
      specification handed to the :pipeline:`$unwind` operator.

   In the above aggregation :pipeline:`$project` selects
   (inclusively) the ``author``, ``title``, and ``tags`` fields, as
   well as the ``_id`` field implicitly. Then the pipeline passes the
   results of the projection to the :pipeline:`$unwind` operator,
   which will unwind the ``tags`` field. This operation may return
   a sequence of documents that resemble the following for a
   collection that contains one document holding a ``tags`` field
   with an array of 3 items.

   .. code-block:: javascript

      {
           "result" : [
                   {
                           "_id" : ObjectId("4e6e4ef557b77501a49233f6"),
                           "title" : "this is my title",
                           "author" : "bob",
                           "tags" : "fun"
                   },
                   {
                           "_id" : ObjectId("4e6e4ef557b77501a49233f6"),
                           "title" : "this is my title",
                           "author" : "bob",
                           "tags" : "good"
                   },
                   {
                           "_id" : ObjectId("4e6e4ef557b77501a49233f6"),
                           "title" : "this is my title",
                           "author" : "bob",
                           "tags" : "fun"
                   }
           ],
           "OK" : 1
      }

   A single document becomes 3 documents: each document is identical
   except for the value of the ``tags`` field. Each value of ``tags``
   is one of the values in the original "tags" array.

   .. note::

      :pipeline:`$unwind` has the following behaviors:

      - :pipeline:`$unwind` is most useful in combination
        with :pipeline:`$group`.

      - You may undo the effects of unwind operation with the
        :pipeline:`$group` pipeline operator.

      - If you specify a target field for :pipeline:`$unwind` that
        does not exist in an input document, the pipeline ignores the
        input document, and will generate no result documents.

      - If you specify a target field for :pipeline:`$unwind` that is
        not an array, :method:`db.collection.aggregate()` generates an error.

      - If you specify a target field for :pipeline:`$unwind` that
        holds an empty array (``[]``) in an input document, the
        pipeline ignores the input document, and will generates no
        result documents.
===================
$week (aggregation)
===================

.. default-domain:: mongodb

.. expression:: $week

   Takes a date and returns the week of the year as a number
   between 0 and 53.

   Weeks begin on Sundays, and week 1 begins with the first Sunday
   of the year. Days preceding the first Sunday of the year are in
   week 0. This behavior is the same as the "``%U``" operator to the
   ``strftime`` standard library function.
===================
$year (aggregation)
===================

.. default-domain:: mongodb

.. expression:: $year

   Takes a date and returns the full year.
================================
Arithmetic Aggregation Operators
================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-arithmetic.rst

.. include:: /includes/toc/aggregation-arithmetic.rst
===========================
Array Aggregation Operators
===========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-array.rst

.. include:: /includes/toc/aggregation-array.rst
=============================
Boolean Aggregation Operators
=============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-boolean.rst

.. include:: /includes/toc/aggregation-boolean.rst
================================
Comparison Aggregation Operators
================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-comparison.rst

.. include:: /includes/toc/aggregation-comparison.rst
=================================
Conditional Aggregation Operators
=================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-conditional.rst

.. include:: /includes/toc/aggregation-conditional.rst
============================
Date Aggregation Operators
============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-date.rst

.. include:: /includes/toc/aggregation-date.rst
===========================
Group Aggregation Operators
===========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-group.rst

.. include:: /includes/toc/aggregation-group.rst
==============================
Pipeline Aggregation Operators
==============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-pipeline.rst

.. include:: /includes/toc/aggregation-pipeline.rst
==================================
Aggregation Projection Expressions
==================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-projection-expressions.rst

.. include:: /includes/toc/aggregation-projection-expressions.rst
===========================
Set Operators (Aggregation)
===========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-set.rst

.. include:: /includes/toc/aggregation-set.rst
============================
String Aggregation Operators
============================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-string.rst

.. include:: /includes/toc/aggregation-string.rst
=================================
Text Search Aggregation Operators
=================================

.. default-domain:: mongodb

.. include:: /includes/toc/table-aggregation-text-search.rst

.. include:: /includes/toc/aggregation-text-search.rst
.. ensure that the changes to this text are reflected in /reference/operator/aggregation/operator-nav.txt

===============================
Aggregation Framework Operators
===============================

.. default-domain:: mongodb

.. versionadded:: 2.2

.. _aggregation-pipeline-operator-reference:

Pipeline Operators
------------------

.. note::
   .. include:: /includes/fact-aggregation-types.rst

Pipeline operators appear in an array. Documents pass through the
operators in a sequence.

.. include:: /includes/toc/table-aggregation-pipeline.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-pipeline

.. _aggregation-expression-operators:

Expression Operators
--------------------

Expression operators calculate values within the
:ref:`aggregation-pipeline-operator-reference`.

``$group`` Operators
~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-aggregation-group.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-group

Boolean Operators
~~~~~~~~~~~~~~~~~

These operators accept Booleans as arguments and return Booleans as
results.

The operators convert non-Booleans to Boolean values according to the
BSON standards. Here, ``null``, ``undefined``, and ``0`` values become
``false``, while non-zero numeric values, and all other types, such as
strings, dates, objects become ``true``.

.. only:: website

   .. include:: /includes/toc/table-aggregation-boolean.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-boolean.txt

Set Operators
~~~~~~~~~~~~~

These operators provide operations on sets.

.. only:: website

   .. include:: /includes/toc/table-aggregation-set.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-set

Comparison Operators
~~~~~~~~~~~~~~~~~~~~

These operators perform comparisons between two values and return a
Boolean, in most cases reflecting the result of the comparison.

All comparison operators take an array with a pair of values. You may
compare numbers, strings, and dates. Except for :expression:`$cmp`,
all comparison operators return a Boolean value. :expression:`$cmp`
returns an integer.

.. only:: website

   .. include:: /includes/toc/table-aggregation-comparison.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-comparison

Arithmetic Operators
~~~~~~~~~~~~~~~~~~~~

Arithmetic operators support only numbers.

.. only:: website

   .. include:: /includes/toc/table-aggregation-arithmetic.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-arithmetic

String Operators
~~~~~~~~~~~~~~~~

String operators that manipulate strings.

.. only:: website

   .. include:: /includes/toc/table-aggregation-string.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-string

Text Search Operators
~~~~~~~~~~~~~~~~~~~~~

Operators to support text search.

.. only:: website

   .. include:: /includes/toc/table-aggregation-text-search.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-text-search

Array Operators
~~~~~~~~~~~~~~~

Operators that manipulate arrays.

.. only:: website

   .. include:: /includes/toc/table-aggregation-array.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-array

Projection Expressions
~~~~~~~~~~~~~~~~~~~~~~

Operators that increase the flexibility within aggregation projection
and projection-like expressions. These operators are available in the
:pipeline:`$project`, :pipeline:`$group`, and :pipeline:`$redact`
pipeline stages.

.. only:: website

   .. include:: /includes/toc/table-aggregation-projection-expressions.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-projection


Date Operators
~~~~~~~~~~~~~~

Date operators take a "Date" typed value as a single argument and return
a number.

.. only:: website

   .. include:: /includes/toc/table-aggregation-date.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-date

Conditional Expressions
~~~~~~~~~~~~~~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-aggregation-conditional.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/aggregation-conditional
========
$comment
========

.. default-domain:: mongodb

.. operator:: $comment

   The :operator:`$comment` makes it possible to attach a comment to a
   query. Because these comments propagate to the :dbcommand:`profile`
   log, adding :operator:`$comment` modifiers can make your profile
   data much easier to interpret and trace. Use one of the following
   forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial( "$comment", <comment> )
      db.collection.find( { $query: { <query> }, $comment: <comment> } )
========
$explain
========

.. default-domain:: mongodb

.. EDITS to explain.txt must be carried over to the method
   cursor.explain.txt and vice versa

.. operator:: $explain

   The :operator:`$explain` operator provides information on the query
   plan. It returns a document that describes
   the process and indexes used to return the query. This may provide
   useful insight when attempting to optimize a query.
   For details on the output, see :doc:`/reference/method/cursor.explain`.

   You can specify the :operator:`$explain` operator in either of the
   following forms:

   .. code-block:: javascript

       db.collection.find()._addSpecial( "$explain", 1 )
       db.collection.find( { $query: {}, $explain: 1 } )

   You also can specify :operator:`$explain` through the
   :method:`~cursor.explain()` method in the :program:`mongo`
   shell:

   .. code-block:: javascript

      db.collection.find().explain()

Behavior
--------

:operator:`$explain` runs the actual query to determine the result.
Although there are some differences between running the query with
:operator:`$explain` and running without, generally, the performance
will be similar between the two. So, if the query is slow, the
:operator:`$explain` operation is also slow.

Additionally, the :operator:`$explain` operation reevaluates a set
of candidate query plans, which may cause the :operator:`$explain`
operation to perform differently than a normal query. As a result,
these operations generally provide an accurate account of *how*
MongoDB would perform the query, but do not reflect the length of
these queries.

.. seealso::

   - :method:`~cursor.explain()`

   - :doc:`/administration/optimization` page for information
     regarding optimization strategies.

   - :doc:`/tutorial/manage-the-database-profiler` tutorial for
     information regarding the database profile.

   - :doc:`Current Operation Reporting </reference/method/db.currentOp>`
=====
$hint
=====

.. default-domain:: mongodb

.. operator:: $hint

   The :operator:`$hint` operator forces the :ref:`query optimizer
   <read-operations-query-optimization>` to use a specific index to
   fulfill the query. Specify the index either by the index name or by
   document.

   Use :operator:`$hint` for testing query performance and indexing
   strategies. The :program:`mongo` shell provides a helper method
   :method:`~cursor.hint()` for the :operator:`$hint` operator.

   Consider the following operation:

   .. code-block:: javascript

      db.users.find().hint( { age: 1 } )

   This operation returns all documents in the collection named
   ``users`` using the index on the ``age`` field.

   You can also specify a hint using either of the following forms:

   .. code-block:: javascript

      db.users.find()._addSpecial( "$hint", { age : 1 } )
      db.users.find( { $query: {}, $hint: { age : 1 } } )

   .. note::

      When the query specifies the :operator:`$hint` in the following
      form:

      .. code-block:: javascript

         db.users.find( { $query: {}, $hint: { age : 1 } } )

      Then, in order to include the :operator:`$explain` option, you
      must add the :operator:`$explain` option to the document, as in
      the following:

      .. code-block:: javascript

         db.users.find( { $query: {}, $hint: { age : 1 }, $explain: 1 } )

   When an :ref:`index filter <index-filters>` exists for the query
   shape, MongoDB ignores the :operator:`$hint`. The
   :data:`explain.filterSet` field of the :method:`~cursor.explain()`
   output indicates whether MongoDB applied an index filter for the
   query.
====
$max
====

.. default-domain:: mongodb

.. operator:: $max

   Specify a :operator:`$max` value to specify the *exclusive* upper
   bound for a specific index in order to constrain the results of
   :method:`~db.collection.find()`. The :program:`mongo` shell
   provides the :method:`~cursor.max()` wrapper method:

   .. code-block:: javascript

      db.collection.find( { <query> } ).max( { field1: <max value>, ... fieldN: <max valueN> } )

   You can also specify the option with either of the two forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial( "$max", { field1: <max value1>, ... fieldN: <max valueN> } )
      db.collection.find( { $query: { <query> }, $max: { field1: <max value1>, ... fieldN: <max valueN> } } )

   The :operator:`$max` specifies the upper bound for *all* keys of a
   specific index *in order*.

   Consider the following operations on a collection named
   ``collection`` that has an index ``{ age: 1 }``:

   .. code-block:: javascript

      db.collection.find( { <query> } ).max( { age: 100 } )

   This operation limits the query to those documents where the
   field ``age`` is less than ``100`` using the index ``{ age: 1 }``.

   You can explicitly specify the corresponding index with
   :method:`~cursor.hint()`. Otherwise, MongoDB selects the index using
   the fields in the ``indexBounds``; however, if multiple indexes
   exist on same fields with different sort orders, the selection of
   the index may be ambiguous.

   Consider a collection named ``collection`` that has the following
   two indexes:

   .. code-block:: javascript

      { age: 1, type: -1 }
      { age: 1, type: 1 }

   Without explicitly using :method:`~cursor.hint()`, MongoDB may
   select either index for the following operation:

   .. code-block:: javascript

      db.collection.find().max( { age: 50, type: 'B' } )

   Use :operator:`$max` alone or in conjunction with :operator:`$min` to limit
   results to a specific range for the *same* index, as in the
   following example:

   .. code-block:: javascript

      db.collection.find().min( { age: 20 } ).max( { age: 25 } )

   .. note::

      Because :method:`~cursor.max()` requires an index on a field, and
      forces the query to use this index, you may prefer the
      :query:`$lt` operator for the query if possible. Consider the
      following example:

      .. code-block:: javascript

         db.collection.find( { _id: 7 } ).max( { age: 25 } )

      The query uses the index on the ``age`` field, even if the
      index on ``_id`` may be better.
========
$maxScan
========

.. default-domain:: mongodb

.. operator:: $maxScan

   Constrains the query to only scan the specified number of documents
   when fulfilling the query. Use one of the following forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial( "$maxScan" , <number> )
      db.collection.find( { $query: { <query> }, $maxScan: <number> } )

   Use this modifier to prevent potentially long running queries from
   disrupting performance by scanning through too much data.
==========
$maxTimeMS
==========

.. default-domain:: mongodb

.. operator:: $maxTimeMS

   .. versionadded:: 2.6
      The :operator:`$maxTimeMS` operator specifies a cumulative
      time limit in milliseconds for processing operations on the
      cursor. MongoDB interrupts the operation at the earliest
      following :term:`interrupt point`.

   The :program:`mongo` shell provides the :method:`cursor.maxTimeMS()` method

   .. code-block:: javascript

      db.collection.find().maxTimeMS(100)

   You can also specify the option in either of the following forms:

   .. code-block:: javascript

      db.collection.find( $query: { } , $maxTimeMS: 100 } )
      db.collection.find()._addSpecial("$maxTimeMS", 100)

   Interrupted operations return an error message similar to the
   following:

   .. code-block:: javascript

      error: { "$err" : "operation exceeded time limit", "code" : 50 }
====
$min
====

.. default-domain:: mongodb

.. operator:: $min

   Specify a :operator:`$min` value to specify the *inclusive* lower
   bound for a specific index in order to constrain the results of
   :method:`~db.collection.find()`. The :program:`mongo` shell
   provides the :method:`~cursor.min()` wrapper method:

   .. code-block:: javascript

      db.collection.find( { <query> } ).min( { field1: <min value>, ... fieldN: <min valueN>} )

   You can also specify the option with either of the two forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial( "$min", { field1: <min value1>, ... fieldN: <min valueN> } )
      db.collection.find( { $query: { <query> }, $min: { field1: <min value1>, ... fieldN: <min valueN> } } )

   The :operator:`$min` specifies the lower bound for *all* keys of a
   specific index *in order*.

   Consider the following operations on a collection named
   ``collection`` that has an index ``{ age: 1 }``:

   .. code-block:: javascript

      db.collection.find().min( { age: 20 } )

   These operations limit the query to those documents where the field
   ``age`` is at least ``20`` using the index ``{ age: 1 }``.

   You can explicitly specify the corresponding index with
   :method:`~cursor.hint()`. Otherwise, MongoDB selects the index using
   the fields in the ``indexBounds``; however, if multiple indexes
   exist on same fields with different sort orders, the selection of
   the index may be ambiguous.

   Consider a collection named ``collection`` that has the following
   two indexes:

   .. code-block:: javascript

      { age: 1, type: -1 }
      { age: 1, type: 1 }

   Without explicitly using :method:`~cursor.hint()`, it is unclear
   which index the following operation will select:

   .. code-block:: javascript

      db.collection.find().min( { age: 20, type: 'C' } )

   You can use :operator:`$min` in conjunction with :operator:`$max` to
   limit results to a specific range for the *same* index, as in the
   following example:

   .. code-block:: javascript

      db.collection.find().min( { age: 20 } ).max( { age: 25 } )

   .. note::

      Because :method:`~cursor.min()` requires an index on a field, and
      forces the query to use this index, you may prefer the
      :query:`$gte` operator for the query if possible. Consider the
      following example:

      .. code-block:: javascript

         db.collection.find( { _id: 7 } ).min( { age: 25 } )

      The query will use the index on the ``age`` field, even if the
      index on ``_id`` may be better.
========
$natural
========

.. default-domain:: mongodb

.. operator:: $natural

  Use the :operator:`$natural` operator to use :term:`natural order` for
  the results of a sort operation. Natural order refers to the
  order of documents in the file on disk.

  The :operator:`$natural` operator uses the following syntax to return
  documents in the order they exist on disk:

  .. code-block:: javascript

     db.collection.find().sort( { $natural: 1 } )

  Use ``-1`` to return documents in the reverse order as they occur on
  disk:

  .. code-block:: javascript

     db.collection.find().sort( { $natural: -1 } )

  .. include:: /includes/fact-natural-sort-order-text-query-restriction.rst

  .. seealso:: :method:`cursor.sort()`
========
$orderby
========

.. default-domain:: mongodb

.. query:: $orderby

   The :operator:`$orderby` operator sorts the results of a query in
   ascending or descending order.

   The :program:`mongo` shell provides the :method:`cursor.sort()`
   method:

   .. code-block:: javascript

      db.collection.find().sort( { age: -1 } )

   You can also specify the option in either of the following forms:

   .. code-block:: javascript

      db.collection.find()._addSpecial( "$orderby", { age : -1 } )
      db.collection.find( { $query: {}, $orderby: { age : -1 } } )

   These examples return all documents in the collection named
   ``collection`` sorted by the ``age`` field in descending order.
   Specify a value to :operator:`$orderby` of negative one (e.g.
   ``-1``, as above) to sort in descending order or a positive value
   (e.g. ``1``) to sort in ascending order.

   Unless you have an index for the specified key pattern, use
   :operator:`$orderby` in conjunction with :operator:`$maxScan` and/or
   :method:`cursor.limit()` to avoid requiring MongoDB to perform a
   large in-memory sort. The :method:`cursor.limit()` increases the
   speed and reduces the amount of memory required to return this query
   by way of an optimized algorithm.
======
$query
======

.. default-domain:: mongodb

.. operator:: $query

   The :operator:`$query` operator provides an interface to describe
   queries. Consider the following operation:

   .. code-block:: javascript

      db.collection.find( { $query: { age : 25 } } )

   This is equivalent to the more familiar
   :method:`db.collection.find()` method:

   .. code-block:: javascript

      db.collection.find( { age : 25 } )

   These operations return only those documents in the collection named
   ``collection`` where the ``age`` field equals ``25``.

   .. note::

      Do not mix query forms. If you use the :operator:`$query`
      format, do not append :ref:`cursor methods
      <js-query-cursor-methods>` to the
      :method:`~db.collection.find()`. To modify the query use the
      :doc:`meta-query operators </reference/operator/query-modifier>`,
      such as :operator:`$explain`.

      Therefore, the following two operations are equivalent:

      .. code-block:: javascript

         db.collection.find( { $query: { age : 25 }, $explain: true } )
         db.collection.find( { age : 25 } ).explain()

   .. seealso:: For more information about queries in MongoDB see
      :doc:`/core/read-operations`, :doc:`/core/read-operations`,
      :method:`db.collection.find()`, and
      :doc:`/tutorial/getting-started`.
==========
$returnKey
==========

.. default-domain:: mongodb

.. operator:: $returnKey

   Only return the index field or fields for the results of the query. If
   :operator:`$returnKey` is set to ``true`` and the query does not use
   an index to perform the read operation, the returned documents will
   not contain any fields. Use one of the following forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial( "$returnKey", true )
      db.collection.find( { $query: { <query> }, $returnKey: true } )
============
$showDiskLoc
============

.. default-domain:: mongodb

.. operator:: $showDiskLoc

   :operator:`$showDiskLoc` option adds a field ``$diskLoc`` to the returned
   documents. The ``$diskLoc`` field contains the disk location
   information.

   The :program:`mongo` shell provides the :method:`cursor.showDiskLoc()`
   method:

   .. code-block:: javascript

      db.collection.find().showDiskLoc()

   You can also specify the option in either of the following forms:

   .. code-block:: javascript

      db.collection.find( { <query> } )._addSpecial("$showDiskLoc" , true)
      db.collection.find( { $query: { <query> }, $showDiskLoc: true } )
=========
$snapshot
=========

.. default-domain:: mongodb

.. operator:: $snapshot

   The :operator:`$snapshot` operator prevents the cursor from
   returning a document more than once because an intervening write
   operation results in a move of the document.

   Even in snapshot mode, objects inserted or deleted during the
   lifetime of the cursor may or may not be returned.

   The :program:`mongo` shell provides the :method:`cursor.snapshot()`
   method:

   .. code-block:: javascript

      db.collection.find().snapshot()

   You can also specify the option in either of the following forms:

   .. code-block:: javascript

      db.collection.find()._addSpecial( "$snapshot", true )
      db.collection.find( { $query: {}, $snapshot: true } )

   The :operator:`$snapshot` operator traverses the index on the
   ``_id`` field [#snapshot-alternative]_.

   .. warning::

      - You cannot use :operator:`$snapshot` with :term:`sharded
        collections <sharding>`.

      - Do **not** use :operator:`$snapshot` with :operator:`$hint` or
        :operator:`$orderby` (or the corresponding
        :method:`cursor.hint()` and :method:`cursor.sort()` methods.)

   .. [#snapshot-alternative] You can achieve the
      :operator:`$snapshot` isolation behavior using any *unique*
      index on invariable fields.
=======================
$elemMatch (projection)
=======================

.. seealso:: :doc:`/reference/operator/query/elemMatch`

.. default-domain:: mongodb

.. projection:: $elemMatch

   .. versionadded:: 2.2

   The :projection:`$elemMatch` projection operator limits the contents
   of an array field that is included in the query results to contain
   only the array element that matches the :projection:`$elemMatch`
   condition.

   .. note::

      - The elements of the array are documents.

      - If multiple elements match the :projection:`$elemMatch`
        condition, the operator returns the **first** matching element
        in the array.

      - The :projection:`$elemMatch` projection operator is similar to
        the positional :projection:`$` projection operator.

   The examples on the :projection:`$elemMatch` projection operator
   assumes a collection ``school`` with the following documents:

   .. code-block:: javascript

      {
       _id: 1,
       zipcode: "63109",
       students: [
                    { name: "john", school: 102, age: 10 },
                    { name: "jess", school: 102, age: 11 },
                    { name: "jeff", school: 108, age: 15 }
                 ]
      }
      {
       _id: 2,
       zipcode: "63110",
       students: [
                    { name: "ajax", school: 100, age: 7 },
                    { name: "achilles", school: 100, age: 8 },
                 ]
      }

      {
       _id: 3,
       zipcode: "63109",
       students: [
                    { name: "ajax", school: 100, age: 7 },
                    { name: "achilles", school: 100, age: 8 },
                 ]
      }

      {
       _id: 4,
       zipcode: "63109",
       students: [
                    { name: "barney", school: 102, age: 7 },
                 ]
      }

   .. example::

      The following :method:`~db.collection.find()` operation
      queries for all documents where the value of the ``zipcode``
      field is ``63109``. The :projection:`$elemMatch` projection
      returns only the **first** matching element of the ``students``
      array where the ``school`` field has a value of ``102``:

      .. code-block:: javascript

         db.schools.find( { zipcode: "63109" },
                          { students: { $elemMatch: { school: 102 } } } )

      The operation returns the following documents:

      .. code-block:: javascript

         { "_id" : 1, "students" : [ { "name" : "john", "school" : 102, "age" : 10 } ] }
         { "_id" : 3 }
         { "_id" : 4, "students" : [ { "name" : "barney", "school" : 102, "age" : 7 } ] }

      - For the document with ``_id`` equal to ``1``, the ``students``
        array contains multiple elements with the ``school`` field
        equal to ``102``. However, the :projection:`$elemMatch`
        projection returns only the first matching element from the
        array.

      - The document with ``_id`` equal to ``3`` does not contain the
        ``students`` field in the result since no element in its
        ``students`` array matched the :projection:`$elemMatch`
        condition.

   The :projection:`$elemMatch` projection can specify criteria on multiple
   fields:

   .. example::

      The following :method:`~db.collection.find()` operation
      queries for all documents where the value of the ``zipcode``
      field is ``63109``. The projection includes the **first**
      matching element of the ``students`` array where the ``school``
      field has a value of ``102`` **and** the ``age`` field is greater
      than ``10``:

      .. code-block:: javascript

         db.schools.find( { zipcode: "63109" },
                          { students: { $elemMatch: { school: 102, age: { $gt: 10} } } } )

      The operation returns the three documents that have ``zipcode`` equal to ``63109``:

      .. code-block:: javascript

         { "_id" : 1, "students" : [ { "name" : "jess", "school" : 102, "age" : 11 } ] }
         { "_id" : 3 }
         { "_id" : 4 }

      Documents with ``_id`` equal to ``3`` and ``_id`` equal to ``4``
      do not contain the ``students`` field since no element matched
      the :projection:`$elemMatch` criteria.

   When the :method:`~db.collection.find()` method includes a
   :method:`~cursor.sort()`, the :method:`~db.collection.find()` method
   applies the :method:`~cursor.sort()` to order the matching documents
   **before** it applies the projection.

   If an array field contains multiple documents with the same field
   name and the :method:`~db.collection.find()` method includes a
   :method:`~cursor.sort()` on that repeating field, the returned
   documents may not reflect the sort order because the
   :method:`~cursor.sort()` was applied to the elements of the array
   before the :projection:`$elemMatch` projection.

   .. example::

      The following query includes a :method:`~cursor.sort()` to order
      by descending ``students.age`` field:

      .. code-block:: javascript

         db.schools.find(
                          { zipcode: 63109 },
                          { students: { $elemMatch: { school: 102 } } }
                        ).sort( { "students.age": -1 } )

      The operation applies the :method:`~cursor.sort()` to order the
      documents that have the field ``zipcode`` equal to ``63109`` and
      then applies the projection. The operation returns the three
      documents in the following order:

      .. code-block:: javascript

         { "_id" : 1, "students" : [ { "name" : "john", "school" : 102, "age" : 10 } ] }
         { "_id" : 3 }
         { "_id" : 4, "students" : [ { "name" : "barney", "school" : 102, "age" : 7 } ] }

.. seealso::

   :projection:`$ (projection) <$>` operator
=====
$meta
=====

.. default-domain:: mongodb

.. Written in anticipation of future availability of other metadata

.. projection:: $meta

   .. versionadded:: 2.6

   The :projection:`$meta` projection operator returns for each
   matching document the metadata (e.g. ``"textScore"``) associated
   with the query. The :projection:`$meta` expression can be a part of
   the :term:`projection` document as well as a
   :method:`~cursor.sort()` expression.

   .. |meta-object| replace:: :projection:`$meta`

   .. include:: /includes/fact-meta-syntax.rst

Behaviors
---------

Projected Field Name
~~~~~~~~~~~~~~~~~~~~

The ``<projectedFieldName>`` cannot include a dot (``.``) in the name.

If the specified ``<projectedFieldName>`` already exists in the
matching documents, in the result set, the existing fields will return
with the :projection:`$meta` values instead of with the stored values.

Projection
~~~~~~~~~~

The :projection:`$meta` expression can be used in the
:term:`projection` document, as in:

.. code-block:: javascript

   db.collection.find(
      <query>,
      { score: { $meta: "textScore" } }
   )

The :projection:`$meta` expression specifies the inclusion of the field
to the result set and does not specify an exclusion of the other
fields.

The :projection:`$meta` expression can be a part of a projection
document that specifies exclusions of other fields or that specifies
inclusions of other fields.

The metadata returns information on the processing of the ``<query>``
operation. As such, the returned metadata, assigned to the
``<projectedFieldName>``, has no meaning inside a ``<query>``
expression; i.e. specifying a condition on the ``<projectedFieldName>``
as part of the ``<query>`` is similar to specifying a condition on a
non-existing field if no field exists in the documents with the
``<projectedFieldName>``.

Sort
~~~~

The :projection:`$meta` expression can be part of a
:method:`~cursor.sort()` expression, as in:

.. code-block:: javascript

   db.collection.find(
      <query>,
      { score: { $meta: "textScore" } }
   ).sort( { score: { $meta: "textScore" } } )

To include a :projection:`$meta` expression in a
:method:`~cursor.sort()` expression, the *same* :projection:`$meta`
expression, including the ``<projectedFieldName>``, must appear in the
projection document. The specified metadata determines the sort order.
For example, the ``"textScore"`` metadata sorts in descending order.

For additional examples, see :ref:`text-operator-example-compound-sort`.

``$meta`` Aggregation Operator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The behavior and requirements of the :projection:`$meta` operator
differs from that of the :expression:`$meta` aggregation operator. See
the :expression:`$meta` aggregation operator for details.

Examples
--------

.. TODO when $meta returns other data, subsection Examples section
   by keywords.

For examples of ``"textScore"`` projections and sorts, see
:query:`$text`.
===============
\$ (projection)
===============

.. default-domain:: mongodb

Definition
----------

.. projection:: $

   The positional :projection:`$` operator limits the contents of the
   ``<array>`` field that is included in the query results to contain
   the **first** matching element. To specify an array element to
   update, see the :doc:`positional $ operator for updates
   </reference/operator/update/positional>`.

   Used in the :term:`projection` document of the
   :method:`~db.collection.find()` method or the
   :method:`~db.collection.findOne()` method:

   - The :projection:`$` projection operator limits the content of the
     ``<array>`` field to the **first** element that matches the
     :ref:`query document <read-operations-query-argument>`.

   - The ``<array>`` field **must** appear in the :ref:`query document
     <read-operations-query-argument>`

     .. code-block:: javascript

        db.collection.find( { <array>: <value> ... },
                            { "<array>.$": 1 } )
        db.collection.find( { <array.field>: <value> ...},
                            { "<array>.$": 1 } )

     The ``<value>`` can be documents that contains :ref:`query operator
     expressions <query-selectors-comparison>`.

   - Only **one** positional :projection:`$` operator can appear in the
     projection document.

   - Only **one** array field can appear in the :ref:`query document
     <read-operations-query-argument>`; i.e. the following query is
     **incorrect**:

     .. code-block:: javascript

        db.collection.find( { <array>: <value>, <someOtherArray>: <value2> },
                            { "<array>.$": 1 } )

Behavior
--------

Array Field Limitation
~~~~~~~~~~~~~~~~~~~~~~

Since only **one** array field can appear in the query document,
if the array contains documents, to specify criteria on multiple
fields of these documents, use the
:query:`$elemMatch` operator. For example:

.. code-block:: javascript

   db.students.find( { grades: { $elemMatch: {
                                               mean: { $gt: 70 },
                                               grade: { $gt:90 }
                                             } } },
                     { "grades.$": 1 } )

Sorts and the Positional Operator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When the :method:`~db.collection.find()` method includes a
:method:`~cursor.sort()`, the :method:`~db.collection.find()`
method applies the :method:`~cursor.sort()` to order the matching
documents **before** it applies the positional :projection:`$`
projection operator.

If an array field contains multiple documents with the same field
name and the :method:`~db.collection.find()` method includes a
:method:`~cursor.sort()` on that repeating field, the returned
documents may not reflect the sort order because the sort was
applied to the elements of the array before the :projection:`$`
projection operator.


Examples
--------

Project Array Values
~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following documents:

.. code-block:: javascript

   { "_id" : 1, "semester" : 1, "grades" : [ 70, 87, 90 ] }
   { "_id" : 2, "semester" : 1, "grades" : [ 90, 88, 92 ] }
   { "_id" : 3, "semester" : 1, "grades" : [ 85, 100, 90 ] }
   { "_id" : 4, "semester" : 2, "grades" : [ 79, 85, 80 ] }
   { "_id" : 5, "semester" : 2, "grades" : [ 88, 88, 92 ] }
   { "_id" : 6, "semester" : 2, "grades" : [ 95, 90, 96 ] }

In the following query, the projection ``{ "grades.$": 1 }``
returns only the first element greater than or equal to ``85``
for the ``grades`` field.

.. code-block:: javascript

   db.students.find( { semester: 1, grades: { $gte: 85 } },
                     { "grades.$": 1 } )

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "grades" : [ 87 ] }
   { "_id" : 2, "grades" : [ 90 ] }
   { "_id" : 3, "grades" : [ 85 ] }

Although the array field ``grades`` may contain multiple elements
that are greater than or equal to ``85``, the :projection:`$`
projection operator returns only the first matching element from the
array.

Project Array Documents
~~~~~~~~~~~~~~~~~~~~~~~

A ``students`` collection contains the following documents
where the ``grades`` field is an array of documents; each document
contain the three field names ``grade``, ``mean``, and ``std``:

.. code-block:: javascript

   { "_id" : 7, semester: 3, "grades" : [ { grade: 80, mean: 75, std: 8 },
                                          { grade: 85, mean: 90, std: 5 },
                                          { grade: 90, mean: 85, std: 3 } ] }

   { "_id" : 8, semester: 3, "grades" : [ { grade: 92, mean: 88, std: 8 },
                                          { grade: 78, mean: 90, std: 5 },
                                          { grade: 88, mean: 85, std: 3 } ] }

In the following query, the projection ``{ "grades.$": 1 }``
returns only the first element with the ``mean`` greater
than ``70`` for the ``grades`` field:

.. code-block:: javascript

   db.students.find(
      { "grades.mean": { $gt: 70 } },
      { "grades.$": 1 }
   )

The operation returns the following documents:

.. code-block:: javascript

   { "_id" : 7, "grades" : [  {  "grade" : 80,  "mean" : 75,  "std" : 8 } ] }
   { "_id" : 8, "grades" : [  {  "grade" : 92,  "mean" : 88,  "std" : 8 } ] }

Further Reading
---------------

:projection:`$elemMatch (projection) <$elemMatch>`
===================
$slice (projection)
===================

.. default-domain:: mongodb

.. projection:: $slice

   The :projection:`$slice` operator controls the number of items of an
   array that a query returns. For information on limiting the size of
   an array during an update with :update:`$push`, see the
   :update:`$slice` modifier instead.

   Consider the following prototype query:

   .. code-block:: javascript

      db.collection.find( { field: value }, { array: {$slice: count } } );

   This operation selects the document ``collection`` identified by a
   field named ``field`` that holds ``value`` and returns the number
   of elements specified by the value of ``count`` from the array
   stored in the ``array`` field. If ``count`` has a value greater
   than the number of elements in ``array`` the query returns all
   elements of the array.

   :projection:`$slice` accepts arguments in a number of formats,
   including negative values and arrays. Consider the following
   examples:

   .. code-block:: javascript

      db.posts.find( {}, { comments: { $slice: 5 } } )

   Here, :projection:`$slice` selects the first five items in an array
   in the ``comments`` field.

   .. code-block:: javascript

      db.posts.find( {}, { comments: { $slice: -5 } } )

   This operation returns the last five items in array.

   The following examples specify an array as an argument to
   :projection:`$slice`. Arrays take the form of ``[ skip , limit ]``, where the
   first value indicates the number of items in the array to skip and
   the second value indicates the number of items to return.

   .. code-block:: javascript

      db.posts.find( {}, { comments: { $slice: [ 20, 10 ] } } )

   Here, the query will only return 10 items, after skipping the first
   20 items of that array.

   .. code-block:: javascript

      db.posts.find( {}, { comments: { $slice: [ -20, 10 ] } } )

   This operation returns 10 items as well, beginning with the item
   that is 20th from the last item of the array.
====================
Projection Operators
====================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-projection.rst

.. include:: /includes/toc/operator-projection.rst
====
$all
====

.. default-domain:: mongodb

.. query:: $all

   The :query:`$all` operator selects the documents where the value of
   a field is an array that contains all the specified elements. To
   specify an :query:`$all` expression, use the following prototype:

   .. code-block:: javascript

      { <field>: { $all: [ <value1> , <value2> ... ] }

.. _all-operator-behavior:

Behavior
--------

Equivalent to ``$and`` Operation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.6

The :query:`$all` is equivalent to an :query:`$and` operation of the
specified values; i.e. the following statement:

.. code-block:: javascript

   { tags: { $all: [ "ssl" , "security" ] } }

is equivalent to:

.. code-block:: javascript

   { $and: [ { tags: "ssl" }, { tags: "security" } ] }

Nested Array
~~~~~~~~~~~~

.. versionchanged:: 2.6

When passed an array of a nested array (e.g. ``[ [ "A" ] ]`` ),
:query:`$all` can now match documents where the field contains the
nested array as an element (e.g. ``field: [ [ "A" ], ... ]``), *or* the
field equals the nested array (e.g. ``field: [ "A" ]``).

For example, consider the following query [#illustrative]_:

.. code-block:: javascript

   db.articles.find( { tags: { $all: [ [ "ssl", "security" ] ] } } )

The query is equivalent to:

.. code-block:: javascript

   db.articles.find( { $and: [ { tags: [ "ssl", "security" ] } ] } )

which is equivalent to:

.. code-block:: javascript

   db.articles.find( { tags: [ "ssl", "security" ] } )

As such, the :query:`$all` expression can match documents where the
``tags`` field is an array that contains the nested array ``[ "ssl",
"security" ]`` or is an array that equals the nested array:

.. code-block:: javascript

   tags: [ [ "ssl", "security" ], ... ]
   tags: [ "ssl", "security" ]

This behavior for :query:`$all` allows for more matches than previous
versions of MongoDB. Earlier version could only match documents where
the field contains the nested array.

.. [#illustrative]
   The :query:`$all` expression with a *single* element is for
   illustrative purposes since the :query:`$all` expression is
   unnecessary if matching only a single element. Instead, when
   matching a single element, a "contains" expression (i.e.
   ``arrayField: element`` ) is more suitable.

Performance
~~~~~~~~~~~

Queries that use the :query:`$all` operator must scan all the documents
that match the first element in the :query:`$all` expression. As a
result, even with an index to support the query, the operation may be
long running, particularly when the first element in the :query:`$all`
expression is not very selective.

Examples
--------

The following examples use the ``inventory`` collection that contains
the documents:

.. code-block:: javascript

   {
      _id: ObjectId("5234cc89687ea597eabee675"),
      code: "xyz",
      tags: [ "school", "book", "bag", "headphone", "appliance" ],
      qty: [
             { size: "S", num: 10, color: "blue" },
             { size: "M", num: 45, color: "blue" },
             { size: "L", num: 100, color: "green" }
           ]
   }

   {
      _id: ObjectId("5234cc8a687ea597eabee676"),
      code: "abc",
      tags: [ "appliance", "school", "book" ],
      qty: [
             { size: "6", num: 100, color: "green" },
             { size: "6", num: 50, color: "blue" },
             { size: "8", num: 100, color: "brown" }
           ]
   }

   {
      _id: ObjectId("5234ccb7687ea597eabee677"),
      code: "efg",
      tags: [ "school", "book" ],
      qty: [
             { size: "S", num: 10, color: "blue" },
             { size: "M", num: 100, color: "blue" },
             { size: "L", num: 100, color: "green" }
           ]
   }

   {
      _id: ObjectId("52350353b2eff1353b349de9"),
      code: "ijk",
      tags: [ "electronics", "school" ],
      qty: [
             { size: "M", num: 100, color: "green" }
           ]
   }

Use ``$all`` to Match Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following operation uses the :query:`$all` operator to query the
``inventory`` collection for documents where the value of the ``tags``
field is an array whose elements include ``appliance``, ``school``, and
``book``:

.. code-block:: javascript

   db.inventory.find( { tags: { $all: [ "appliance", "school", "book" ] } } )

The above query returns the following documents:

.. code-block:: javascript

   {
      _id: ObjectId("5234cc89687ea597eabee675"),
      code: "xyz",
      tags: [ "school", "book", "bag", "headphone", "appliance" ],
      qty: [
             { size: "S", num: 10, color: "blue" },
             { size: "M", num: 45, color: "blue" },
             { size: "L", num: 100, color: "green" }
           ]
   }

   {
      _id: ObjectId("5234cc8a687ea597eabee676"),
      code: "abc",
      tags: [ "appliance", "school", "book" ],
      qty: [
             { size: "6", num: 100, color: "green" },
             { size: "6", num: 50, color: "blue" },
             { size: "8", num: 100, color: "brown" }
           ]
   }

Use ``$all`` with ``$elemMatch``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the field contains an array of documents, you can use the
:query:`$all` with the :query:`$elemMatch` operator.

The following operation queries the ``inventory`` collection for
documents where the value of the ``qty`` field is an array whose
elements match the :query:`$elemMatch` criteria:

.. code-block:: javascript

   db.inventory.find( {
                        qty: { $all: [
                                       { "$elemMatch" : { size: "M", num: { $gt: 50} } },
                                       { "$elemMatch" : { num : 100, color: "green" } }
                                     ] }
                      } )

The query returns the following documents:

.. code-block:: javascript

   {
      "_id" : ObjectId("5234ccb7687ea597eabee677"),
      "code" : "efg",
      "tags" : [ "school", "book"],
      "qty" : [
                { "size" : "S", "num" : 10, "color" : "blue" },
                { "size" : "M", "num" : 100, "color" : "blue" },
                { "size" : "L", "num" : 100, "color" : "green" }
              ]
   }

   {
      "_id" : ObjectId("52350353b2eff1353b349de9"),
      "code" : "ijk",
      "tags" : [ "electronics", "school" ],
      "qty" : [
                { "size" : "M", "num" : 100, "color" : "green" }
              ]
   }

The :query:`$all` operator exists to support queries on arrays. But
you may use the :query:`$all` operator to select against a non-array
``field``, as in the following example:

.. code-block:: javascript

   db.inventory.find( { qty: { $all: [ 50 ] } } )

**However**, use the following form to express the same query:

.. code-block:: javascript

   db.inventory.find( { qty: 50 } )

Both queries will select all documents in the ``inventory``
collection where the value of the ``qty`` field equals ``50``.

.. note::

   .. Comment -- do we need this note?

   In most cases, MongoDB does not treat arrays as sets. This operator
   provides a notable exception to this approach.

.. seealso::
   :method:`~db.collection.find()`, :method:`~db.collection.update()`, and :update:`$set`.
====
$and
====

.. default-domain:: mongodb

.. query:: $and

   .. versionadded:: 2.0

   *Syntax*: ``{ $and: [ { <expression1> }, { <expression2> } , ... , { <expressionN> } ] }``

   :query:`$and` performs a logical ``AND`` operation on an array
   of *two or more* expressions (e.g. ``<expression1>``,
   ``<expression2>``, etc.) and selects the documents that satisfy
   *all* the expressions in the array. The :query:`$and` operator
   uses *short-circuit evaluation*. If the first expression
   (e.g. ``<expression1>``) evaluates to ``false``, MongoDB will not
   evaluate the remaining expressions.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find({ $and: [ { price: 1.99 }, { qty: { $lt: 20 } }, { sale: true } ] } )

   This query will select all documents in the ``inventory``
   collection where:

   - ``price`` field value equals ``1.99`` **and**
   - ``qty`` field value is less than ``20`` **and**
   - ``sale`` field value is equal to ``true``.

   MongoDB provides an implicit ``AND`` operation when specifying a
   comma separated list of expressions. For example, you may write the
   above query as:

   .. code-block:: javascript

      db.inventory.find( { price: 1.99, qty: { $lt: 20 } , sale: true } )

   If, however, a query requires an ``AND`` operation on the same field
   such as ``{ price: { $ne: 1.99 } } AND { price: { $exists: true }
   }``, then either use the :query:`$and` operator for the two
   separate expressions or combine the operator expressions for the
   field ``{ price: { $ne: 1.99, $exists: true } }``.

   Consider the following examples:

   .. code-block:: javascript

      db.inventory.update( { $and: [ { price: { $ne: 1.99 } }, { price: { $exists: true } } ] }, { $set: { qty: 15 } } )

      db.inventory.update( { price: { $ne: 1.99, $exists: true } } , { $set: { qty: 15 } } )

   Both :method:`~db.collection.update()` operations will set
   the value of the ``qty`` field in documents where:

   -  the ``price`` field value does not equal ``1.99`` **and**
   -  the ``price`` field exists.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :query:`$ne`, :query:`$exists`,
      :update:`$set`.
====
$box
====

.. default-domain:: mongodb

.. query:: $box

   .. versionadded:: 1.4

   The :query:`$box` operator specifies a rectangle for a
   :term:`geospatial` :query:`$geoWithin` query. The query returns
   documents that are within the bounds of the rectangle, according to
   their point-based location data. The :query:`$box` operator
   returns documents based on :ref:`grid coordinates
   <geospatial-indexes-store-grid-coordinates>` and does *not* query for
   GeoJSON shapes.

   The query calculates distances using flat (planar) geometry. The
   ``2d`` geospatial index supports the :query:`$box` operator.

   To use the :query:`$box` operator, you must specify the bottom
   left and top right corners of the rectangle in an array object.
   Use the following syntax:

   .. code-block:: javascript

      { <location field> : { $geoWithin : { $box :
                                             [ [ <bottom left coordinates> ] ,
                                               [ <upper right coordinates> ] ] } } }

   .. important:: If you use longitude and latitude, specify **longitude first**.

   The following example query returns all documents that are within the
   box having points at: ``[ 0 , 0 ]``, ``[ 0 , 100 ]``, ``[ 100 , 0 ]``, and ``[ 100 , 100 ]``.

   .. code-block:: javascript

      db.places.find( { loc : { $geoWithin : { $box :
                                                [ [ 0 , 0 ] ,
                                                  [ 100 , 100 ] ] } } } )

   .. |operator| replace:: :query:`$box`

   .. include:: /includes/note-geospatial-index-must-exist.rst
=======
$center
=======

.. default-domain:: mongodb

.. query:: $center

   .. versionadded:: 1.4

   The :query:`$center` operator specifies a circle for a
   :term:`geospatial` :query:`$geoWithin` query. The query returns
   legacy coordinate pairs that are within the bounds of the circle. The
   operator does *not* return GeoJSON objects.

   The query calculates distances using flat (planar) geometry.

   The ``2d`` geospatial index supports the :query:`$center`
   operator.

   To use the :query:`$center` operator, specify an array that
   contains:

   - The grid coordinates of the circle's center point

   - The circle's radius, as measured in the units used by the
     coordinate system

   .. important:: If you use longitude and latitude, specify **longitude first**.

   Use the following syntax:

   .. code-block:: javascript

      { <location field> : { $geoWithin : { $center : [ [ <x>, <y> ] , <radius> ] } } }

   The following example query returns all documents that have
   coordinates that exist within the circle centered on ``[ -74 , 40.74 ]``
   and with a radius of ``10``:

   .. code-block:: javascript

      db.places.find( { loc: { $geoWithin :
                                { $center : [ [-74, 40.74], 10 ] }
                      } } )

   .. |operator| replace:: :query:`$center`

   .. include:: /includes/note-geospatial-index-must-exist.rst
=============
$centerSphere
=============

.. default-domain:: mongodb

.. query:: $centerSphere

   .. versionadded:: 1.8

   The :query:`$centerSphere` operator defines a circle for a
   :term:`geospatial` query that uses spherical geometry. The query
   returns documents that are within the bounds of the circle.

   You can use the :query:`$centerSphere` operator on both
   :term:`GeoJSON` objects and legacy coordinate pairs.

   The ``2d`` and ``2dsphere`` geospatial indexes both support
   :query:`$centerSphere`.

   To use :query:`$centerSphere`, specify an array that contains:

   - The grid coordinates of the circle's center point

   - The circle's radius measured in radians. To calculate radians, see
     :doc:`/tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes`.

   Use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $geoWithin :
                                  { $centerSphere : [ [ <x>, <y> ] , <radius> ] }
                            } } )

   .. important:: If you use longitude and latitude, specify **longitude first**.

   The following example queries grid coordinates and returns all
   documents within a 10 mile radius of longitude ``88 W`` and latitude
   ``30 N``. The query converts the distance to radians by dividing by
   the approximate radius of the earth, 3959 miles:

   .. code-block:: javascript

      db.places.find( { loc : { $geoWithin :
                                 { $centerSphere :
                                   [ [ 88 , 30 ] , 10 / 3959 ]
                      } } } )

   .. |operator| replace:: :query:`$centerSphere`

   .. include:: /includes/note-geospatial-index-must-exist.rst
==================
$elemMatch (query)
==================

.. seealso:: :doc:`/reference/operator/projection/elemMatch`

.. default-domain:: mongodb

.. query:: $elemMatch

   .. versionadded:: 1.4

   The :query:`$elemMatch` operator matches more than one component within
   an array element. For example,

   .. code-block:: javascript

      db.collection.find( { array: { $elemMatch: { value1: 1, value2: { $gt: 1 } } } } );

   returns all documents in ``collection`` where the array ``array``
   satisfies all of the conditions in the :query:`$elemMatch`
   expression.

   That is, where the value of ``value1`` is 1 and the value of
   ``value2`` is greater than 1. Matching arrays must have at least
   one element that matches all specified criteria. Therefore, the
   following document would not match the above query:

   .. code-block:: javascript

      { array: [ { value1:1, value2:0 }, { value1:2, value2:2 } ] }

   while the following document would match this query:

   .. code-block:: javascript

      { array: [ { value1:1, value2:0 }, { value1:1, value2:2 } ] }
=======
$exists
=======

.. default-domain:: mongodb

Definition
----------

.. query:: $exists

   *Syntax*: ``{ field: { $exists: <boolean> } }``

   When ``<boolean>`` is true, :query:`$exists` matches the documents that
   contain the field, including documents where the field value is
   ``null``. If ``<boolean>`` is false, the query returns only the
   documents that do not contain the field.

   MongoDB `$exists` does **not** correspond to SQL operator
   ``exists``. For SQL ``exists``, refer to the :query:`$in`
   operator.

.. seealso:: :query:`$nin`, :query:`$in`, and
   :ref:`faq-developers-query-for-nulls`.

Examples
--------

Exists and Not Equal To
~~~~~~~~~~~~~~~~~~~~~~~

Consider the following example:

.. code-block:: javascript

   db.inventory.find( { qty: { $exists: true, $nin: [ 5, 15 ] } } )

This query will select all documents in the ``inventory`` collection
where the ``qty`` field exists *and* its value does not equal ``5`` or
``15``.

Null Values
~~~~~~~~~~~

Given a collection named ``records`` with the following documents:

.. code-block:: javascript

   { a: 5, b: 5, c: null }
   { a: 3, b: null, c: 8 }
   { a: null, b: 3, c: 9 }
   { a: 1, b: 2, c: 3 }
   { a: 2, c: 5 }
   { a: 3, b: 2 }
   { a: 4 }
   { b: 2, c: 4 }
   { b: 2 }
   { c: 6 }

Consider the output of the following queries:

**Query**:

   .. code-block:: javascript

      db.records.find( { a: { $exists: true } } )

**Result**:

   .. code-block:: javascript

      { a: 5, b: 5, c: null }
      { a: 3, b: null, c: 8 }
      { a: null, b: 3, c: 9 }
      { a: 1, b: 2, c: 3 }
      { a: 2, c: 5 }
      { a: 3, b: 2 }
      { a: 4 }

**Query**:

   .. code-block:: javascript

      db.records.find( { b: { $exists: false } } )

**Result**:

   .. code-block:: javascript

      { a: 2, c: 5 }
      { a: 4 }
      { c: 6 }

**Query**:

   .. code-block:: javascript

      db.records.find( { c: { $exists: false } } )

**Result**:

   .. code-block:: javascript

      { a: 3, b: 2 }
      { a: 4 }
      { b: 2 }
==============
$geoIntersects
==============

.. default-domain:: mongodb

.. query:: $geoIntersects

   .. versionadded:: 2.4

   The :query:`$geoIntersects` operator is a geospatial query
   operator that selects all locations that intersect with a
   :term:`GeoJSON` object. A location intersects a GeoJSON object if the
   intersection is non-empty. This includes documents that have a shared
   edge. The :query:`$geoIntersects` operator uses spherical
   geometry.

   The ``2dsphere`` geospatial index supports
   :query:`$geoIntersects`.

   To query for intersection, pass the GeoJSON object to
   :query:`$geoIntersects` through the :query:`$geometry`
   operator. Use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $geoIntersects :
                                  { $geometry :
                                     { type : "<GeoJSON object type>" ,
                                       coordinates : [ <coordinates> ]
                            } } } } )

   .. important:: Specify coordinates in this order: **"longitude, latitude."**

   The following example uses :query:`$geoIntersects` to select all
   indexed points and shapes that intersect with the polygon defined by
   the ``coordinates`` array.

   .. code-block:: javascript

      db.places.find( { loc :
                        { $geoIntersects :
                          { $geometry :
                            { type : "Polygon" ,
                              coordinates: [ [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0 ] ] ]
                      } } } } )

   .. note::

      .. |geo-operator-method| replace:: :query:`$geoIntersects`

      .. include:: /includes/fact-geometry-hemisphere-limitation.rst
=========
$geometry
=========

.. default-domain:: mongodb

.. query:: $geometry

   .. versionadded:: 2.4

   The :query:`$geometry` operator specifies a :term:`GeoJSON` for a
   geospatial query operators. For details on using
   :query:`$geometry` with an operator, see the operator:

   - :query:`$geoWithin`

   - :query:`$geoIntersects`

   - :query:`$near`
==========
$geoWithin
==========

.. default-domain:: mongodb

.. query:: $geoWithin

   .. versionadded:: 2.4
      :query:`$geoWithin` replaces :query:`$within` which is
      deprecated.

   The :query:`$geoWithin` operator is a geospatial query operator
   that queries for a defined point, line or shape that exists entirely
   within another defined shape. When determining inclusion, MongoDB
   considers the border of a shape to be part of the shape, subject to
   the precision of floating point numbers.

   The :query:`$geoWithin` operator queries for inclusion in a
   :term:`GeoJSON` polygon or a shape defined by legacy coordinate
   pairs.

   The :query:`$geoWithin` operator does not return sorted results.
   As a result MongoDB can return :query:`$geoWithin` queries more
   quickly than geospatial :query:`$near` or :query:`$nearSphere`
   queries, which sort results.

   The ``2dsphere`` and ``2d`` indexes both support the
   :query:`$geoWithin` operator.

   .. versionchanged:: 2.2.3
      :query:`$geoWithin` does not require a geospatial
      index. However, a geospatial index will improve query
      performance.

   If querying for geometries that exist within a GeoJSON
   :term:`polygon <Polygon>` on a sphere, pass the polygon to
   :query:`$geoWithin` using the :query:`$geometry` operator.

   For a polygon with only an exterior ring use following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $geoWithin :
                                  { $geometry :
                                     { type : "Polygon" ,
                                       coordinates : [ [ [ <lng1>, <lat1> ] , [ <lng2>, <lat2> ] ... ] ]
                            } } } } )

   .. important:: Specify coordinates in ``longitude, latitude``
      order.

   For a polygon with an exterior and interior ring use following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $geoWithin :
                                  { $geometry :
                                     { type : "Polygon" ,
                                       coordinates : [ [ [ <lng1>, <lat1> ] , [ <lng2>, <lat2> ] ... ]
                                                       [ [ <lngA>, <latA> ] , [ <lngB>, <latB> ] ... ] ]
                            } } } } )

   The following example selects all indexed points and shapes that
   exist entirely within a GeoJSON polygon:

   .. code-block:: javascript

      db.places.find( { loc :
                        { $geoWithin :
                          { $geometry :
                            { type : "Polygon" ,
                              coordinates: [ [ [ 0 , 0 ] , [ 3 , 6 ] , [ 6 , 1 ] , [ 0 , 0 ] ] ]
                      } } } } )

   If querying for inclusion in a shape defined by legacy coordinate
   pairs on a plane, use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $geoWithin :
                                  { <shape operator> : <coordinates>
                            } } } )

   For the syntax of shape operators, see: :query:`$box`,
   :query:`$polygon`, :query:`$center` (defines a circle), and
   :query:`$centerSphere` (defines a circle on a sphere).

   .. note::

      .. |geo-operator-method| replace:: :query:`$geoWithin`
      .. include:: /includes/fact-geometry-hemisphere-limitation.rst

.. query:: $within

   .. deprecated:: 2.4
      :query:`$geoWithin` replaces :query:`$within` in MongoDB
      2.4.
===
$gt
===

.. default-domain:: mongodb

.. query:: $gt

   *Syntax*: ``{field: {$gt: value} }``

   :query:`$gt` selects those documents where the value of the
   ``field`` is greater than (i.e. ``>``) the specified ``value``.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { qty: { $gt: 20 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``qty`` field value is greater than ``20``.

   Consider the following example that uses the :query:`$gt`
   operator with a field from an embedded document:

   .. code-block:: javascript

      db.inventory.update( { "carrier.fee": { $gt: 2 } }, { $set: { price: 9.99 } } )

   This :method:`~db.collection.update()` operation will set
   the value of the ``price`` field in the first document found containing the
   embedded document ``carrier`` whose ``fee`` field value is
   greater than ``2``.

   To set the value of the ``price`` field in *all* documents containing the
   embedded document ``carrier`` whose ``fee`` field value is greater than ``2``,
   specify the ``multi:true`` option in the :method:`~db.collection.update()` method:

   .. code-block:: javascript

      db.inventory.update( { "carrier.fee": { $gt: 2 } },
                           { $set: { price: 9.99 },
                           { multi: true }
                        )


   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
====
$gte
====

.. default-domain:: mongodb

.. query:: $gte

   *Syntax*: ``{field: {$gte: value} }``

   :query:`$gte` selects the documents where the value of the
   ``field`` is greater than or equal to (i.e. ``>=``) a specified
   value (e.g. ``value``.)

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { qty: { $gte: 20 } } )

   This query would select all documents in ``inventory`` where
   the ``qty`` field value is greater than or equal to ``20``.

   Consider the following example which uses the :query:`$gte`
   operator with a field from an embedded document:

   .. code-block:: javascript

      db.inventory.update( { "carrier.fee": { $gte: 2 } }, { $set: { price: 9.99 } } )

   This :method:`~db.collection.update()` operation will set
   the value of the ``price`` field that contain the embedded document
   ``carrier`` whose ``fee`` field value is greater than or equal to
   ``2``.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
===
$in
===

.. default-domain:: mongodb

.. query:: $in

   The :query:`$in` operator selects the documents where the value
   of a field equals any value in the specified array. To specify an
   :query:`$in` expression, use the following prototype:

   .. code-block:: javascript

      { field: { $in: [<value1>, <value2>, ... <valueN> ] } }

   If the ``field`` holds an array, then the :query:`$in` operator
   selects the documents whose ``field`` holds an array that contains
   at least one element that matches a value in the specified array
   (e.g. ``<value1>``, ``<value2>``, etc.)

   .. versionchanged:: 2.6

      MongoDB 2.6 removes the combinatorial limit for the :query:`$in`
      operator that exists for :v2.4:`earlier versions
      </reference/operator/query/in>` of the operator.

Examples
--------

Use the ``$in`` Operator to Match Values
----------------------------------------

Consider the following example:

.. code-block:: javascript

   db.inventory.find( { qty: { $in: [ 5, 15 ] } } )

This query selects all documents in the ``inventory``
collection where the ``qty`` field value is either ``5`` or
``15``. Although you can express this query using the
:query:`$or` operator, choose the :query:`$in` operator rather
than the :query:`$or` operator when performing equality checks on
the same field.

Use the ``$in`` Operator to Match Values in an Array
----------------------------------------------------

The collection ``inventory`` contains documents that include the field
``tags``, as in the following:

.. code-block:: javascript

   { _id: 1, item: "abc", qty: 10, tags: [ "school", "clothing" ], sale: false }

Then, the following :method:`~db.collection.update()` operation will
set the ``sale`` field value to ``true`` where the ``tags`` field holds
an array with at least one element matching either ``"appliances"`` or
``"school"``.

.. code-block:: javascript

   db.inventory.update(
                        { tags: { $in: ["appliances", "school"] } },
                        { $set: { sale:true } }
                      )

Use the ``$in`` Operator with a Regular Expression
--------------------------------------------------

The :query:`$in` operator can specify matching values using
regular expressions or the :query:`$regex` operator expressions.

Consider the following example:

.. code-block:: javascript

   db.inventory.find( { tags: { $in: [ /^be/, /^st/ ] } } )

This query selects all documents in the ``inventory`` collection where
the ``tags`` field holds an array that contains at least one element
that starts with either ``be`` or ``st``.

.. seealso::

   :method:`~db.collection.find()`, :method:`~db.collection.update()`, :query:`$or`, :update:`$set`.
===
$lt
===

.. default-domain:: mongodb

.. query:: $lt

   *Syntax*: ``{field: {$lt: value} }``

   :query:`$lt` selects the documents where the value of the
   ``field`` is less than (i.e. ``<``) the specified ``value``.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { qty: { $lt: 20 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``qty`` field value is less than ``20``.

   Consider the following example which uses the :query:`$lt`
   operator with a field from an embedded document:

   .. code-block:: javascript

      db.inventory.update( { "carrier.fee": { $lt: 20 } }, { $set: { price: 9.99 } } )

   This :method:`~db.collection.update()` operation will set
   the ``price`` field value in the documents that contain the
   embedded document ``carrier`` whose ``fee`` field value is less
   than ``20``.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
====
$lte
====

.. default-domain:: mongodb

.. query:: $lte

   *Syntax*: ``{ field: { $lte: value} }``

   :query:`$lte` selects the documents where the value of the
   ``field`` is less than or equal to (i.e. ``<=``) the specified
   ``value``.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { qty: { $lte: 20 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``qty`` field value is less than or equal to ``20``.

   Consider the following example which uses the :query:`$lt`
   operator with a field from an embedded document:

   .. code-block:: javascript

      db.inventory.update( { "carrier.fee": { $lte: 5 } }, { $set: { price: 9.99 } } )

   This :method:`~db.collection.update()` operation will set
   the ``price`` field value in the documents that contain the embedded
   document ``carrier`` whose ``fee`` field value is less than or equal
   to ``5``.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
============
$maxDistance
============

.. default-domain:: mongodb

.. query:: $maxDistance

   The :query:`$maxDistance` operator constrains the results of a
   geospatial :query:`$near` or :query:`$nearSphere` query to the
   specified distance. The measuring units for the maximum distance are
   determined by the coordinate system in use. For :term:`GeoJSON` point
   object, specify the distance in meters, not radians.

   .. versionchanged:: 2.6

      Specify a non-negative number for :query:`$maxDistance`.

   The ``2d`` and ``2dsphere`` geospatial indexes both support
   :query:`$maxDistance`.

   The following example query returns documents with location values
   that are ``10`` or fewer units from the point ``[ 100 , 100 ]``.

   .. code-block:: javascript

      db.places.find( { loc : { $near : [ 100 , 100 ] ,
                                $maxDistance: 10 }
                      } )

   MongoDB orders the results by their distance from ``[ 100 , 100 ]``.
   The operation returns the first 100 results, unless you modify the
   query with the :method:`cursor.limit()` method.
====
$mod
====

.. default-domain:: mongodb

.. query:: $mod

   Select documents where the value of a field divided by a divisor has
   the specified remainder (i.e. perform a modulo operation to select
   documents). To specify a :query:`$mod` expression, use the following
   syntax:

   .. code-block:: javascript

      { field: { $mod: [ divisor, remainder ] } }

   .. versionchanged:: 2.6

      The :query:`$mod` operator errors when passed an array with fewer
      or more elements. In previous versions, if passed an array with
      one element, the :query:`$mod` operator uses ``0`` as the
      remainder value, and if passed an array with more than two
      elements, the :query:`$mod` ignores all but the first two
      elements. Previous versions do return an error when passed an
      empty array. See :ref:`mod-not-enough-elements` and
      :ref:`mod-too-many-elements` for details.

Examples
--------

Use ``$mod`` to Select Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``inventory`` with the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "abc123", "qty" : 0 }
   { "_id" : 2, "item" : "xyz123", "qty" : 5 }
   { "_id" : 3, "item" : "ijk123", "qty" : 12 }

Then, the following query selects those documents in the
``inventory`` collection where value of the ``qty`` field modulo
``4`` equals ``0``:

.. code-block:: javascript

   db.inventory.find( { qty: { $mod: [ 4, 0 ] } } )

The query returns the following documents:

.. code-block:: javascript

   { "_id" : 1, "item" : "abc123", "qty" : 0 }
   { "_id" : 3, "item" : "ijk123", "qty" : 12 }

.. _mod-not-enough-elements:

Not Enough Elements Error
~~~~~~~~~~~~~~~~~~~~~~~~~

The :query:`$mod` operator errors when passed an array with fewer than
two elements.

Array with Single Element
`````````````````````````

The following operation incorrectly passes the :query:`$mod` operator
an array that contains a single element:

.. code-block:: javascript

   db.inventory.find( { qty: { $mod: [ 4 ] } } )

The statement results in the following error:

.. code-block:: javascript

   error: {
    	"$err" : "bad query: BadValue malformed mod, not enough elements",
    	"code" : 16810
   }

.. versionchanged:: 2.6
   In previous versions, if passed an array with one element, the
   :query:`$mod` operator uses the specified element as the divisor
   and ``0`` as the remainder value.

Empty Array
```````````

The following operation incorrectly passes the :query:`$mod` operator
an empty array:

.. code-block:: javascript

   db.inventory.find( { qty: { $mod: [ ] } } )

The statement results in the following error:

.. code-block:: javascript

   error: {
    	"$err" : "bad query: BadValue malformed mod, not enough elements",
    	"code" : 16810
   }

.. versionchanged:: 2.6

   Previous versions returned the following error:

   .. code-block:: javascript

      error: { "$err" : "mod can't be 0", "code" : 10073 }

.. _mod-too-many-elements:

Too Many Elements Error
~~~~~~~~~~~~~~~~~~~~~~~

The :query:`$mod` operator errors when passed an array with more than
two elements.

For example, the following operation attempts to use the :query:`$mod`
operator with an array that contains four elements:

.. code-block:: javascript

   error: {
   	"$err" : "bad query: BadValue malformed mod, too many elements",
   	"code" : 16810
   }

.. versionchanged:: 2.6

   In previous versions, if passed an array with more than two
   elements, the :query:`$mod` ignores all but the first two
   elements.
===
$ne
===

.. default-domain:: mongodb

.. query:: $ne

   *Syntax*: ``{field: {$ne: value} }``

   :query:`$ne` selects the documents where the value of the
   ``field`` is not equal (i.e. ``!=``) to the specified ``value``.
   This includes documents that do not contain the ``field``.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { qty: { $ne: 20 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``qty`` field value does not equal ``20``,
   including those documents that do not contain the ``qty`` field.

   Consider the following example which uses the :query:`$ne`
   operator with a field in an embedded document:

   .. code-block:: javascript

      db.inventory.update( { "carrier.state": { $ne: "NY" } }, { $set: { qty: 20 } } )

   This :method:`~db.collection.update()` operation will set
   the ``qty`` field value in the documents that contain the embedded
   document ``carrier`` whose ``state`` field value does not equal "NY",
   or where the ``state`` field or the ``carrier`` embedded document
   do not exist.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
=====
$near
=====

.. default-domain:: mongodb

.. query:: $near

   .. versionchanged:: 2.4

   Specifies a point for which a :term:`geospatial` query returns the
   closest documents first. The query sorts the documents from nearest
   to farthest.

   The :query:`$near` operator can query for a :term:`GeoJSON`
   point or for a point defined by legacy coordinate pairs.

   The optional :query:`$maxDistance` operator limits a
   :query:`$near` query to return only those documents that fall
   within a maximum distance of a point. If you query for a GeoJSON
   point, specify :query:`$maxDistance` in meters. If you query for
   legacy coordinate pairs, specify :query:`$maxDistance` in radians.

   The :query:`$near` operator requires a geospatial index: a
   ``2dsphere`` index for GeoJSON points; a ``2d`` index for legacy
   coordinate pairs. By default, queries that use a ``2d`` index
   return a limit of 100 documents; however you may use
   :method:`~cursor.limit()` to change the number of results.

   .. TODO remove the following "include" statement in 2.6

   .. note::

      You cannot combine the :query:`$near` operator, which
      requires a special :ref:`geospatial index
      <index-feature-geospatial>`, with a query operator or command
      that uses a different type of special index. For example you
      cannot combine :query:`$near` with the :dbcommand:`text`
      command.

   For queries on GeoJSON data, use the following syntax:

   .. code-block:: javascript

      db.<collection>.find(
         { <location field> :
             { $near :
                {
                  $geometry : {
                     type : "Point" ,
                     coordinates : [ <longitude> , <latitude> ] },
                  $maxDistance : <distance in meters>
                }
             }
          }
      )

   .. important:: Specify coordinates in this order: **"longitude, latitude."**

   The following example selects the documents with coordinates
   nearest to ``[ 40 , 5 ]`` and limits the maximum distance to 500
   meters from the specified GeoJSON point:

   .. code-block:: javascript

      db.places.find(
         {
           loc:
             { $near :
                {
                  $geometry : { type : "Point" , coordinates: [ 40 , 5 ] },
                  $maxDistance : 500
                }
             }
         }
      )

   For queries on legacy coordinate pairs, use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $near : [ <x> , <y> ] ,
                                 $maxDistance: <distance>
                          } } )

   .. important:: If you use longitude and latitude, specify **longitude first**.

   The following example query returns documents with location values
   that are 10 or fewer units from the point ``[ 40 , 5 ]``.

   For GeoJSON point object, specify the $maxDistance in meters, not radians.

   .. code-block:: javascript

      db.places.find( { loc :
                         { $near : [ 40 , 5 ] ,
                           $maxDistance : 10
                      } } )

   .. note::

      You can further limit the number of results using
      :method:`cursor.limit()`.

      Specifying a batch size (i.e. :method:`batchSize()
      <cursor.batchSize()>`) in conjunction with queries that use the
      :query:`$near` is not defined. See :issue:`SERVER-5236` for
      more information.
===========
$nearSphere
===========

.. default-domain:: mongodb

.. query:: $nearSphere

   .. versionadded:: 1.8

   Specifies a point for which a :term:`geospatial` query returns the
   closest documents first. The query sorts the documents from nearest
   to farthest. MongoDB calculates distances for :query:`$nearSphere`
   using spherical geometry.

   The :query:`$nearSphere` operator queries for points defined by
   either :term:`GeoJSON` objects or legacy coordinate pairs.

   The optional :query:`$maxDistance` operator limits a
   :query:`$nearSphere` query to return only those documents that
   fall within a maximum distance of a point. If you use
   :query:`$maxDistance` on GeoJSON points, the distance is measured
   in meters. If you use :query:`$maxDistance` on legacy coordinate
   pairs, the distance is measured in radians.

   The :query:`$nearSphere` operator requires a geospatial
   index. The ``2dsphere`` and ``2d`` indexes both support
   :query:`$nearSphere` with both legacy coordinate pairs and
   GeoJSON points. Queries that use a ``2d`` index return a at most
   100 documents.

   .. important:: If you use longitude and latitude, specify **longitude first**.

   For queries on GeoJSON data, use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $nearSphere :
                                 { $geometry :
                                    { type : "Point" ,
                                      coordinates : [ <longitude> , <latitude> ] } ,
                                   $maxDistance : <distance in meters>
                         } } } )

   For queries on legacy coordinate pairs, use the following syntax:

   .. code-block:: javascript

      db.<collection>.find( { <location field> :
                               { $nearSphere: [ <x> , <y> ] ,
                                 $maxDistance: <distance in radians>
                            } } )

   The following example selects the 100 documents with legacy
   coordinates pairs nearest to ``[ 40 , 5 ]``, as calculated by
   spherical geometry:

   .. code-block:: javascript

      db.places.find( { loc :
                         { $nearSphere : [ 40 , 5 ] ,
                           $maxDistance : 10
                      } } )
====
$nin
====

.. default-domain:: mongodb

.. query:: $nin

   *Syntax*: ``{ field: { $nin: [ <value1>, <value2> ... <valueN> ]} }``

   :query:`$nin` selects the documents where:

   - the ``field`` value is not in the specified ``array`` **or**
   - the ``field`` does not exist.

   Consider the following query:

   .. code-block:: javascript

      db.inventory.find( { qty: { $nin: [ 5, 15 ] } } )

   This query will select all documents in the ``inventory`` collection
   where the ``qty`` field value does **not** equal ``5`` nor
   ``15``. The selected documents will include those documents that do
   *not* contain the ``qty`` field.

   If the ``field`` holds an array, then the :query:`$nin` operator
   selects the documents whose ``field`` holds an array with **no**
   element equal to a value in the specified array (e.g. ``<value1>``,
   ``<value2>``, etc.).

   Consider the following query:

   .. code-block:: javascript

      db.inventory.update( { tags: { $nin: [ "appliances", "school" ] } }, { $set: { sale: false } } )

   This :method:`~db.collection.update()` operation will set
   the ``sale`` field value in the ``inventory`` collection where the
   ``tags`` field holds an array with **no** elements matching an
   element in the array ``["appliances", "school"]`` or where a
   document does not contain the ``tags`` field.

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`.
====
$nor
====

.. default-domain:: mongodb

.. query:: $nor

   :query:`$nor` performs a logical ``NOR`` operation on an array
   of one or more query expression and selects the documents that **fail**
   all the query expressions in the array. The :query:`$nor` has
   the following syntax:

   .. code-block:: javascript

      { $nor: [ { <expression1> }, { <expression2> }, ...  { <expressionN> } ] }

.. seealso::

   :method:`~db.collection.find()`, :method:`~db.collection.update()`,
   :query:`$or`, :update:`$set`, and :query:`$exists`.

Examples
--------

``$nor`` Query with Two Expressions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following query which uses only the :query:`$nor` operator:

.. code-block:: javascript

   db.inventory.find( { $nor: [ { price: 1.99 }, { sale: true } ]  } )

This query will return all documents that:

- contain the ``price`` field whose value is *not* equal to ``1.99``
  and contain the ``sale`` field whose value *is not* equal to
  ``true`` **or**

- contain the ``price`` field whose value is *not* equal to ``1.99``
  *but* do *not* contain the ``sale`` field **or**

- do *not* contain the ``price`` field *but* contain the ``sale``
  field whose value *is not* equal to ``true`` **or**
- do *not* contain the ``price`` field *and* do *not* contain the
  ``sale`` field

``$nor`` and Additional Comparisons
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following query:

.. code-block:: javascript

   db.inventory.find( { $nor: [ { price: 1.99 }, { qty: { $lt: 20 } }, { sale: true } ] } )

This query will select all documents in the ``inventory`` collection
where:

- the ``price`` field value does *not* equal ``1.99`` **and**
- the ``qty`` field value is *not* less than ``20`` **and**
- the ``sale`` field value is *not* equal to ``true``

including those documents that do not contain these field(s).

The exception in returning documents that do not contain the field
in the :query:`$nor` expression is when the :query:`$nor` operator is
used with the :query:`$exists` operator.

``$nor`` and ``$exists``
~~~~~~~~~~~~~~~~~~~~~~~~

Compare that with the following query which uses the
:query:`$nor` operator with the :query:`$exists` operator:

.. code-block:: javascript

   db.inventory.find( { $nor: [ { price: 1.99 }, { price: { $exists: false } },
                                { sale: true }, { sale: { $exists: false } } ] } )

This query will return all documents that:

- contain the ``price`` field whose value is *not* equal to ``1.99``
  and contain the ``sale`` field whose value *is not* equal to
  ``true``
====
$not
====

.. default-domain:: mongodb

.. query:: $not

   *Syntax*: ``{ field: { $not: { <operator-expression> } } }``

   :query:`$not` performs a logical ``NOT`` operation on the
   specified ``<operator-expression>`` and selects the documents that
   do *not* match the ``<operator-expression>``. This includes
   documents that do not contain the ``field``.

   Consider the following query:

   .. code-block:: javascript

      db.inventory.find( { price: { $not: { $gt: 1.99 } } } )

   This query will select all documents in the ``inventory`` collection where:

   - the ``price`` field value is less than or equal to ``1.99`` **or**
   - the ``price`` field does not exist

   ``{ $not: { $gt: 1.99 } }`` is different from the :query:`$lte`
   operator. ``{ $lte: 1.99 }`` returns *only* the documents where
   ``price`` field exists and its value is less than or equal to
   ``1.99``.

   Remember that the :query:`$not` operator only affects *other
   operators* and cannot check fields and documents independently. So,
   use the :query:`$not` operator for logical disjunctions and the
   :query:`$ne` operator to test the contents of fields directly.

   Consider the following behaviors when using the :query:`$not`
   operator:

   - The operation of the :query:`$not` operator is consistent with
     the behavior of other operators but may yield unexpected results
     with some data types like arrays.

   - The :query:`$not` operator does **not** support operations with
     the :query:`$regex` operator. Instead use ``//`` or in your
     driver interfaces, use your language's regular expression
     capability to create regular expression objects.

     Consider the following example which uses the pattern match expression ``//``:

     .. code-block:: javascript

        db.inventory.find( { item: { $not: /^p.*/ } } )

     The query will select all documents in the ``inventory``
     collection where the ``item`` field value does *not* start with
     the letter ``p``.

     If you are using Python, you can write the above query with the
     PyMongo driver and Python's :py:meth:`python:re.compile()` method to
     compile a regular expression, as follows:

     .. code-block:: python

        import re
        for noMatch in db.inventory.find( { "item": { "$not": re.compile("^p.*") } } ):
            print noMatch

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.update()`, :update:`$set`, :query:`$gt`,
      :query:`$regex`, :api:`PyMongo <python\current>`,
      :term:`driver`.
===
$or
===

.. default-domain:: mongodb

.. query:: $or

   .. versionadded:: 1.6

   .. versionchanged:: 2.0
      You may nest :query:`$or` operations; however, these
      expressions are not as efficiently optimized as top-level.

   The :query:`$or` operator performs a logical ``OR`` operation on an
   array of *two or more* ``<expressions>`` and selects the documents
   that satisfy *at least* one of the ``<expressions>``. The
   :query:`$or` has the following syntax:

   .. code-block:: javascript

      { $or: [ { <expression1> }, { <expression2> }, ... , { <expressionN> } ] }

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { $or: [ { qty: { $lt: 20 } }, { sale: true } ] } )

   This query will select all documents in the ``inventory`` collection
   where either the ``qty`` field value is less than ``20`` **or** the
   ``sale`` field value is ``true``.

Behaviors
---------

``$or`` Clauses and Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~

When using indexes with :query:`$or` queries, each clause of an
:query:`$or` will execute in parallel. These clauses can each use their
own index. Consider the following query:

.. code-block:: javascript

   db.inventory.find ( { $or: [ { price: 1.99 }, { sale: true } ] } )

To support this query, rather than a compound index, you would create
one index on ``price`` and another index on ``sale``:

.. code-block:: javascript

   db.inventory.ensureIndex( { price: 1 } )
   db.inventory.ensureIndex( { sale: 1 } )

``$or`` and Sort Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~

When using the :query:`$or` operator with the :method:`~cursor.sort()`
method, the query will **not** use the indexes on the :query:`$or`
fields. Consider the following query which adds a
:method:`~cursor.sort()` method to the above query:

.. code-block:: javascript

   db.inventory.find ( { $or: [ { price: 1.99 }, { sale: true } ] } ).sort( { item:1 } )

This modified query will not use the index on ``price`` nor the index
on ``sale``.

``$or`` and ``text`` Queries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If :query:`$or` includes a :query:`$text` query, all clauses in the
:query:`$or` array must be indexed.

``$or`` and GeoSpatial Queries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.6

:operator:`$or` supports :doc:`geospatial clauses
</reference/operator/query-geospatial>` with the following exception
for the near clause (near clause includes :query:`$nearSphere` and
:query:`$near`). :operator:`$or` cannot contain a near clause with any
other clause.

``$or`` versus ``$in``
~~~~~~~~~~~~~~~~~~~~~~

When using :query:`$or` with ``<expressions>`` that are equality checks
for the value of the same field, use the :query:`$in` operator instead
of the :query:`$or` operator.

For example, to select all documents in the ``inventory`` collection
where the ``qty`` field value equals either ``20`` *or* ``50``, use the
:query:`$in` operator:

.. code-block:: javascript

   db.inventory.find ( { qty: { $in: [20, 50] } } )

.. seealso:: :query:`$and`, :method:`~db.collection.find()`,
   :method:`~cursor.sort()`, :query:`$in`
========
$polygon
========

.. default-domain:: mongodb

.. query:: $polygon

   .. versionadded:: 1.9

   The :query:`$polygon` operator specifies a polygon for a
   :term:`geospatial` :query:`$geoWithin` query on legacy coordinate
   pairs. The query returns pairs that are within the bounds of the
   polygon. The operator does *not* query for GeoJSON objects.

   The :query:`$polygon` operator calculates distances using flat (planar)
   geometry.

   The ``2d`` geospatial index supports the :query:`$polygon`
   operator.

   To define the polygon, specify an array of coordinate points. Use the
   following syntax:

   .. code-block:: javascript

      { <location field> : { $geoWithin : { $polygon : [ [ <x1> , <y1> ] ,
                                                         [ <x2> , <y2> ] ,
                                                         [ <x3> , <y3> ] ] } } }

   .. important:: If you use longitude and latitude, specify **longitude first**.

   The last point specified is always implicitly connected to the
   first. You can specify as many points, and therefore sides, as you
   like.

   The following query returns all documents that have coordinates that
   exist within the polygon defined by ``[ 0 , 0 ]``, ``[ 3 , 6 ]``, and
   ``[ 6 , 0 ]``:

   .. code-block:: javascript

      db.places.find( { loc : { $geoWithin : { $polygon : [ [ 0 , 0 ] ,
                                                            [ 3 , 6 ] ,
                                                            [ 6 , 0 ] ] } } } )

   .. |operator| replace:: :query:`$polygon`

   .. include:: /includes/note-geospatial-index-must-exist.rst
======
$regex
======

.. default-domain:: mongodb

.. query:: $regex

   The :query:`$regex` operator provides regular expression
   capabilities for pattern matching *strings* in queries. MongoDB uses Perl compatible regular
   expressions (i.e. "PCRE.")

   You can specify regular expressions using regular expression
   objects or using the :query:`$regex` operator. The following
   examples are equivalent:

   .. code-block:: javascript

      db.collection.find( { field: /acme.*corp/i } );
      db.collection.find( { field: { $regex: 'acme.*corp', $options: 'i' } } );

   These expressions match all documents in ``collection`` where the
   value of ``field`` matches the case-insensitive regular expression
   ``acme.*corp``.

   :query:`$regex` uses "Perl Compatible Regular Expressions" (PCRE) as the
   matching engine.

   .. operator:: $options

     :query:`$regex` provides four option flags:

     - ``i`` toggles case insensitivity, and allows all letters in the
       pattern to match upper and lower cases.

     - ``m`` toggles multiline regular expression. Without this option,
       all regular expression match within one line.

       If there are no newline characters (e.g. ``\n``) or no
       start/end of line construct, the ``m`` option has no effect.

     - ``x`` toggles an "extended" capability. When set,
       :query:`$regex` ignores all white space characters unless
       escaped or included in a character class.

       Additionally, it ignores characters between an un-escaped ``#``
       character and the next new line, so that you may include
       comments in complicated patterns. This only applies to data
       characters; white space characters may never appear within
       special character sequences in a pattern.

       The ``x`` option does not affect the handling of the VT character
       (i.e. code 11.)

     .. versionadded:: 1.9.0

     - ``s`` allows the dot (e.g. ``.``) character to match all
       characters *including* newline characters.

     :query:`$regex` only provides the ``i`` and ``m`` options for
     the native JavaScript regular expression objects (e.g. ``/acme.*corp/i``). To use ``x``
     and ``s`` you must use the ":query:`$regex`" operator with the
     ":operator:`$options`" syntax.

   To combine a regular expression match with other operators, you
   need to use the ":query:`$regex`" operator. For example:

   .. code-block:: javascript

      db.collection.find( { field: { $regex: /acme.*corp/i, $nin: [ 'acmeblahcorp' ] } } );

   This expression returns all instances of ``field`` in
   ``collection`` that match the case insensitive regular expression
   ``acme.*corp`` that *don't* match ``acmeblahcorp``.

   If an index exists for the field, then MongoDB matches the regular
   expression against the values in the index, which can be faster than
   a collection scan. Further optimization can occur if the regular
   expression is a "prefix expression", which means that all potential
   matches start with the same string. This allows MongoDB to construct
   a "range" from that prefix and only match against those values from
   the index that fall within that range.

   A regular expression is a "prefix expression" if it starts with a caret
   (``^``) or a left anchor (``\A``), followed by a string of simple
   symbols. For example, the regex ``/^abc.*/`` will be optimized by
   matching only against the values from the index that start with ``abc``.

   Additionally, while ``/^a/``, ``/^a.*/``, and ``/^a.*$/`` match
   equivalent strings, they have different performance characteristics.
   All of these expressions use an index if an appropriate index
   exists; however, ``/^a.*/``, and ``/^a.*$/`` are slower. ``/^a/``
   can stop scanning after matching the prefix.
=====
$size
=====

.. default-domain:: mongodb

.. query:: $size

   The :query:`$size` operator matches any array with the number of
   elements specified by the argument. For example:

   .. code-block:: javascript

      db.collection.find( { field: { $size: 2 } } );

   returns all documents in ``collection`` where ``field`` is an array
   with 2 elements. For instance, the above expression will
   return ``{ field: [ red, green ] }`` and ``{ field: [ apple,
   lime ] }`` but *not* ``{ field: fruit }`` or ``{ field: [
   orange, lemon, grapefruit ] }``. To match fields with only one
   element within an array use :query:`$size` with a value of 1, as
   follows:

   .. code-block:: javascript

      db.collection.find( { field: { $size: 1 } } );

   :query:`$size` does not accept ranges of values. To select
   documents based on fields with different numbers of elements,
   create a counter field that you increment when you add elements to
   a field.

   Queries cannot use indexes for the :query:`$size` portion of a
   query, although the other portions of a query can use indexes if
   applicable.
=====
$text
=====

.. default-domain:: mongodb

.. query:: $text

   .. versionadded:: 2.6

   :query:`$text` performs a text search on the content of the fields
   indexed with a :doc:`text index </core/index-text>`. A
   :query:`$text` expression has the following syntax:

   .. code-block:: javascript

      { $text: { $search: <string>, $language: <string> } }

   The :query:`$text` operator accepts a text query document with the
   following fields:

   .. |object-behavior| replace:: :ref:`text-query-operator-behavior`

   .. include:: /reference/operator/query/text-fields.rst

   The :query:`$text` operator, by default, does *not* return results
   sorted in terms of the results' score. For more information, see
   the :ref:`text-operator-text-score` documentation.

.. _text-query-operator-behavior:

Behavior
--------

Restrictions
~~~~~~~~~~~~

- A query can specify, at most, one :query:`$text` expression.

- The :query:`$text` query can not appear in :query:`$nor` expressions.

- To use a :query:`$text` query in an :query:`$or` expression, all
  clauses in the :query:`$or` array must be indexed.

- .. include:: /includes/fact-hint-text-query-restriction.rst

- .. include:: /includes/fact-natural-sort-order-text-query-restriction.rst

  .. |operation| replace:: :query:`$text` expression

- .. include:: /includes/fact-special-indexes-and-text.rst

.. |text-object| replace:: :query:`$text`
.. |meta-object| replace:: :projection:`$meta` projection operator
.. |sort-object| replace:: :method:`~cursor.sort()` method

``$search`` Field
~~~~~~~~~~~~~~~~~

In the ``$search`` field, specify a string of words that the
:query:`text` operator parses and uses to query the :doc:`text index
</core/index-text>`. The :query:`text` operator treats most punctuation
in the string as delimiters, except a hyphen ``-`` that negates term or
an escaped double quotes ``\"`` that specifies a phrase.

.. _text-operator-phrases:

Phrases
^^^^^^^

To match on a phrase, as opposed to individual terms, enclose the
phrase in escaped double quotes (``\"``), as in:

.. code-block:: javascript

   "\"ssl certificate\""

If the ``$search`` string includes a phrase and individual terms, text
search will only match the documents that include the phrase. More
specifically, the search performs a logical ``AND`` of the phrase with
the individual terms in the search string.

For example, passed a ``$search`` string:

.. code-block:: javascript

   "\"ssl certificate\" authority key"

The :query:`$text` operator searches for the phrase ``"ssl
certificate"`` **and** (``"authority"`` **or** ``"key"`` **or**
``"ssl"`` **or** ``"certificate"`` ).

.. _text-operator-term-negation:

Negations
^^^^^^^^^

Prefixing a word with a hyphen sign (``-``) negates a word:

- The negated word excludes documents that contain the
  negated word from the result set.

- When passed a search string that only contains negated words, text
  search will not match any documents.

- A hyphenated word, such as ``pre-market``, is not a negation. The
  :query:`$text` operator treats the hyphen as a delimiter.

The :query:`$text` operator adds all negations to the query with the
logical ``AND`` operator.

Match Operation
~~~~~~~~~~~~~~~

The :query:`$text` operator ignores language-specific stop words, such
as ``the`` and ``and`` in English.

The :query:`$text` operator matches on the complete *stemmed* word. So
if a document field contains the word ``blueberry``, a search on the
term ``blue`` will not match. However, ``blueberry`` or ``blueberries``
will match.

For non-diacritics, text search is case insensitive; i.e. case
insensitive for ``[A-z]``.

.. _text-operator-text-score:

Text Score
~~~~~~~~~~

.. include:: /includes/fact-text-search-score.rst

.. _text-query-examples:

Examples
--------

The following examples assume a collection ``articles`` that has a text
index on the field ``subject``:

.. code-block:: javascript

   db.articles.ensureIndex( { subject: "text" } )

Search for a Single Word
~~~~~~~~~~~~~~~~~~~~~~~~

The following query searches for the term ``coffee``:

.. code-block:: javascript

   db.articles.find( { $text: { $search: "coffee" } } )

This query returns documents that contain the term ``coffee`` in the
indexed ``subject`` field.

Match Any of the Search Terms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the search string is a space-delimited string, :query:`$text`
operator performs a logical ``OR`` search on each term and returns
documents that contains any of the terms.

The following query searches specifies a ``$search`` string of three
terms delimited by space, ``"bake coffee cake"``:

.. code-block:: javascript

   db.articles.find( { $text: { $search: "bake coffee cake" } } )

This query returns documents that contain either ``bake`` **or**
``coffee`` **or** ``cake`` in the indexed ``subject`` field.

Search for a Phrase
~~~~~~~~~~~~~~~~~~~

To match the exact phrase as a single term, escape the quotes.

The following query searches for the phrase ``coffee cake``:

.. code-block:: javascript

   db.articles.find( { $text: { $search: "\"coffee cake\"" } } )

This query returns documents that contain the phrase ``coffee cake``.

.. seealso:: :ref:`text-operator-phrases`

Exclude Documents That Contain a Term
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A *negated* term is a term that is prefixed by a minus sign ``-``. If
you negate a term, the :query:`$text` operator will exclude the
documents that contain those terms from the results.

The following example searches for documents that contain the words
``bake`` or ``coffee`` but do **not** contain the term ``cake``:

.. code-block:: javascript

   db.articles.find( { $text: { $search: "bake coffee -cake" } } )

.. seealso:: :ref:`text-operator-term-negation`

.. _ex-return-text-search-score:

Return the Text Search Score
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following query searches for the term ``cake`` and returns the
score assigned to each matching document:

.. code-block:: javascript

   db.articles.find(
      { $text: { $search: "cake" } },
      { score: { $meta: "textScore" } }
   )

In the result set, the returned documents includes an *additional*
field ``score`` that contains the document's score associated with the
text search. [#meta-aggregation]_

.. seealso:: :ref:`text-operator-text-score`

.. _ex-sort-text-search-score:

Sort by Text Search Score
~~~~~~~~~~~~~~~~~~~~~~~~~

To sort by the text score, include the **same** :projection:`$meta`
expression in **both** the projection document and the sort expression.
[#meta-aggregation]_ The following query searches for the term ``cake``
and sorts the results by the descending score:

.. code-block:: javascript

   db.articles.find(
      { $text: { $search: "cake" } },
      { score: { $meta: "textScore" } }
   ).sort( { score: { $meta: "textScore" } } )

In the result set, the returned documents includes an additional field
``score`` that contains the document's score associated with the text
search.

.. seealso:: :ref:`text-operator-text-score`

.. _ex-sort-limit-three:

Return Top 3 Matching Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the :method:`~cursor.limit()` method in conjunction with a
:method:`~cursor.sort()` to return the top three matching documents.
The following query searches for the term ``cake`` and sorts the
results by the descending score:

.. code-block:: javascript

   db.articles.find(
      { $text: { $search: "cake" } },
      { score: { $meta: "textScore" } }
   ).sort( { score: { $meta: "textScore" } } ).limit(3)

.. seealso:: :ref:`text-operator-text-score`

.. _text-operator-example-compound-sort:

Text Search with Additional Query and Sort Expressions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following query searches for documents with status equal to ``"A"``
that contain the terms ``coffee`` or ``cake`` in the indexed field
``subject`` and specifies a sort order of ascending date, descending
text score:

.. code-block:: javascript

   db.articles.find(
      { status: "A", $text: { $search: "coffee cake" } },
      { score: { $meta: "textScore" } }
   ).sort( { date: 1, score: { $meta: "textScore" } } )

Search a Different Language
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the optional ``$language`` field in the :query:`$text` expression
to specify a language that determines the list of stop words and the
rules for the stemmer and tokenizer for the search string.

The following query specifies ``es`` for Spanish as the language that
determines the tokenization, stemming, and stop words:

.. code-block:: javascript

   db.articles.find(
      { $text: { $search: "leche", $language: "es" } }
   )

The :query:`$text` expression can also accept the language by name,
``spanish``. See :ref:`text-search-languages` for the supported
languages.

.. seealso:: :doc:`/tutorial/text-search-in-aggregation`

.. [#meta-aggregation]
   The behavior and requirements of the :projection:`$meta` operator
   differs from that of the :expression:`$meta` aggregation operator.
   See the :expression:`$meta` aggregation operator for details.
=====
$type
=====

.. default-domain:: mongodb

.. query:: $type

   *Syntax*: ``{ field: { $type: <BSON type> } }``

   :query:`$type` selects the documents where the *value* of the
   ``field`` is the specified :term:`BSON` type.

   Consider the following example:

   .. code-block:: javascript

      db.inventory.find( { price: { $type : 1 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``price`` field value is a Double.

   If the ``field`` holds an array, the :query:`$type` operator
   performs the type check against the array elements and **not** the
   ``field``.

   Consider the following example where the ``tags`` field holds an array:

   .. code-block:: javascript

      db.inventory.find( { tags: { $type : 4 } } )

   This query will select all documents in the ``inventory`` collection
   where the ``tags`` array contains an element that is itself an array.

   If instead you want to determine whether the ``tags`` field is an
   array type, use the :query:`$where` operator:

   .. code-block:: javascript

      db.inventory.find( { $where : "Array.isArray(this.tags)" } )

   See the :issue:`SERVER-1475` for more information about the
   array type.

   Refer to the following table for the available :term:`BSON` types
   and their corresponding numbers.

   =======================  ==========
   **Type**                 **Number**
   -----------------------  ----------
   Double                       1
   String                       2
   Object                       3
   Array                        4
   Binary data                  5
   Undefined (deprecated)       6
   Object id                    7
   Boolean                      8
   Date                         9
   Null                        10
   Regular Expression          11
   JavaScript                  13
   Symbol                      14
   JavaScript (with scope)     15
   32-bit integer              16
   Timestamp                   17
   64-bit integer              18
   Min key                    255
   Max key                    127
   =======================  ==========

   ``MinKey`` and ``MaxKey`` compare less than and greater than all
   other possible :term:`BSON` element values, respectively, and exist
   primarily for internal use.

   .. note::

      To query if a field value is a ``MinKey``, you must use the :query:`$type` with
      ``-1`` as in the following example:

      .. code-block:: javascript

         db.collection.find( { field: { $type: -1 } } )

   .. example::

      Consider the following example operation sequence that
      demonstrates both type comparison *and* the special ``MinKey``
      and ``MaxKey`` values:

      .. code-block:: javascript

         db.test.insert( {x : 3});
         db.test.insert( {x : 2.9} );
         db.test.insert( {x : new Date()} );
         db.test.insert( {x : true } );
         db.test.insert( {x : MaxKey } )
         db.test.insert( {x : MinKey } )

         db.test.find().sort({x:1})
         { "_id" : ObjectId("4b04094b7c65b846e2090112"), "x" : { $minKey : 1 } }
         { "_id" : ObjectId("4b03155dce8de6586fb002c7"), "x" : 2.9 }
         { "_id" : ObjectId("4b03154cce8de6586fb002c6"), "x" : 3 }
         { "_id" : ObjectId("4b031566ce8de6586fb002c9"), "x" : true }
         { "_id" : ObjectId("4b031563ce8de6586fb002c8"), "x" : "Tue Jul 25 2012 18:42:03 GMT-0500 (EST)" }
         { "_id" : ObjectId("4b0409487c65b846e2090111"), "x" : { $maxKey : 1 } }

      To query for the minimum value of a :term:`shard key` of a
      :term:`sharded cluster`, use the following operation when
      connected to the :program:`mongos`:

      .. code-block:: javascript

         use config
         db.chunks.find( { "min.shardKey": { $type: -1 } } )

   .. include:: /includes/warning-mixing-types.rst

   .. seealso::

      :method:`~db.collection.find()`, :method:`~db.collection.insert()`, :query:`$where`, :term:`BSON`,
      :term:`shard key`, :term:`sharded cluster` .
===========
$uniqueDocs
===========

.. default-domain:: mongodb

Definition
----------

.. query:: $uniqueDocs

   .. include:: /includes/deprecation-uniqueDocs.rst

   Returns a document only once for a geospatial query even if the
   document matches the query multiple times.
======
$where
======

.. default-domain:: mongodb

.. query:: $where

   Use the :query:`$where` operator to pass either a string
   containing a JavaScript expression or a full JavaScript function to
   the query system. The :query:`$where` provides greater
   flexibility, but requires that the database processes the
   JavaScript expression or function for *each* document in the
   collection. Reference the document in the JavaScript expression or
   function using either ``this`` or ``obj`` .

   .. warning::

      - Do not write to the database within the :query:`$where`
        JavaScript function.

      - :query:`$where` evaluates JavaScript and cannot take
        advantage of indexes. Therefore, query performance improves
        when you express your query using the standard MongoDB
        operators (e.g., :query:`$gt`, :query:`$in`).

      - In general, you should use :query:`$where` only when you
        can't express your query using another operator. If you must
        use :query:`$where`, try to include at least one other
        standard query operator to filter the result set. Using
        :query:`$where` alone requires a table scan.

   Consider the following examples:

   .. code-block:: javascript

      db.myCollection.find( { $where: "this.credits == this.debits" } );
      db.myCollection.find( { $where: "obj.credits == obj.debits" } );

      db.myCollection.find( { $where: function() { return (this.credits == this.debits) } } );
      db.myCollection.find( { $where: function() { return obj.credits == obj.debits; } } );

   Additionally, if the query consists only of the :query:`$where`
   operator, you can pass in just the JavaScript expression or
   JavaScript functions, as in the following examples:

   .. code-block:: javascript

      db.myCollection.find( "this.credits == this.debits || this.credits > this.debits" );

      db.myCollection.find( function() { return (this.credits == this.debits || this.credits > this.debits ) } );

   You can include both the standard MongoDB operators and the
   :query:`$where` operator in your query, as in the following
   examples:

   .. code-block:: javascript

      db.myCollection.find( { active: true, $where: "this.credits - this.debits < 0" } );
      db.myCollection.find( { active: true, $where: function() { return obj.credits - obj.debits < 0; } } );

   Using normal non\-:query:`$where` query statements provides the
   following performance advantages:

   - MongoDB will evaluate non\-:query:`$where` components of query
     before :query:`$where` statements. If the
     non\-:query:`$where` statements match no documents, MongoDB
     will not perform any query evaluation using :query:`$where`.

   - The non\-:query:`$where` query statements may use an
     :term:`index`.

   .. note::

      .. versionchanged:: 2.4

      .. include:: /includes/fact-group-map-reduce-where-limitations-in-24.rst
====================
Query Operator Array
====================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-query-array.rst

.. include:: /includes/toc/operator-query-array.rst
.. _query-selectors-comparison:

==========================
Comparison Query Operators
==========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-query-comparison.rst

.. include:: /includes/toc/operator-query-comparison.rst
=======================
Element Query Operators
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-query-element.rst

.. include:: /includes/toc/operator-query-element.rst
==========================
Evaluation Query Operators
==========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-query-evaluation.rst

.. include:: /includes/toc/operator-query-evaluation.rst
==========================
Geospatial Query Operators
==========================

.. default-domain:: mongodb

Operators
---------

Query Selectors
~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-operator-query-geospatial.rst

.. include:: /includes/toc/operator-query-geospatial.rst

Geometry Specifiers
~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-operator-query-geospatial-modifiers.rst

.. include:: /includes/toc/operator-query-geospatial-modifiers.rst

.. _geospatial-query-compatibility-chart:

Geospatial Query Compatibility
------------------------------

While numerous combinations of query operators are possible, the
following table shows the recommended operators for different types of
queries. The table uses the :query:`$geoWithin`,
:query:`$geoIntersects` and :query:`$near` operators.

.. include:: /includes/table/geospatial-queries.rst
=======================
Logical Query Operators
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-query-logical.rst

.. include:: /includes/toc/operator-query-logical.rst
===============
Query Modifiers
===============

.. default-domain:: mongodb

Introduction
------------

In addition to the :doc:`MongoDB Query Operators
</reference/operator>`, there are a number of "meta" operators that
let you modify the output or behavior of a query. On the server,
MongoDB treats the query and the options as a single object. The
:program:`mongo` shell and driver interfaces may provide :ref:`cursor methods
<js-query-cursor-methods>` that wrap these options. When possible, use these
methods; otherwise, you can add these options using either of the
following syntax:

.. code-block:: javascript

   db.collection.find( { <query> } )._addSpecial( <option> )
   db.collection.find( { $query: { <query> }, <option> } )

Operators
---------

Modifiers
~~~~~~~~~

Many of these operators have corresponding :ref:`methods in the shell
<js-query-cursor-methods>`. These methods provide a straightforward and
user-friendly interface and are the preferred way to add these options.

.. include:: /includes/toc/table-operator-meta.rst

.. include:: /includes/toc/operator-meta.rst

Sort Order
~~~~~~~~~~

.. include:: /includes/toc/table-operator-sort-order.rst

.. include:: /includes/toc/operator-sort-order.rst
==============================
Query and Projection Operators
==============================

.. default-domain:: mongodb

.. _query-selectors:

Query Selectors
---------------

Comparison
~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-comparison.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-comparison

Logical
~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-logical.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-logical

Element
~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-element.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-element

Evaluation
~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-evaluation.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-evaluation

Geospatial
~~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-geospatial.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-geospatial

Array
~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-query-array.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/query-array

Projection Operators
--------------------

.. only:: website

   .. include:: /includes/toc/table-operator-projection.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/projection
=========
$addToSet
=========

.. default-domain:: mongodb

Definition
----------

.. update:: $addToSet

   The :update:`$addToSet` operator adds a value to an array only *if* the
   value is *not* already in the array. If the value *is* in the
   array, :update:`$addToSet` does not modify the
   array.

   .. code-block:: javascript

      db.collection.update( <query>, { $addToSet: { <field>: <value> } } );

   For example, if a collection ``inventory`` has the following document:

   .. code-block:: javascript

      { _id: 1, item: "filter", tags: [ "electronics", "camera" ] }

   The following operation adds the element ``"accessories"`` to the ``tags``
   array since ``"accessories"`` does not exist in the array:

   .. code-block:: javascript

      db.inventory.update(
                           { _id: 1 },
                           { $addToSet: { tags: "accessories"  } }
                         )


   However, the following operation has no effect as ``"camera"`` is already
   an element of the ``tags`` array:

   .. code-block:: javascript

      db.inventory.update(
                           { _id: 1 },
                           { $addToSet: { tags: "camera"  } }
                         )

Behavior
--------

- :update:`$addToSet` only ensures that there are no duplicate
  items *added* to the set and does not affect existing duplicate
  elements. :update:`$addToSet` does not guarantee a particular
  ordering of elements in the modified set.

- If the field is absent in the document to update,
  :update:`$addToSet` adds the array field with the value as
  its element.

- If the field is **not** an array, the operation will fail.

- If the value is an array, :update:`$addToSet` appends the whole
  array as a *single* element. To add each element of the value
  separately, use :update:`$addToSet` with the :update:`$each`
  modifier. See :ref:`addToSet-modifiers` for details.

.. _addToSet-modifiers:

Modifiers
---------

You can use the :update:`$addToSet` operator with the
:update:`$each` modifier. The :update:`$each` modifier allows to
:update:`$addToSet` operator to add multiple values to the array
field.

.. include:: /includes/example-addToSet-each.rst

.. seealso:: :update:`$push`
====
$bit
====

.. default-domain:: mongodb

.. update:: $bit

   .. versionchanged:: 2.6
      Added support for bitwise ``xor`` operation.

   The :update:`$bit` operator performs a bitwise update of a field.
   The :update:`$bit` operator supports bitwise ``and``, bitwise
   ``or``, and bitwise ``xor`` (i.e. exclusive or) operations. To
   specify a :update:`$bit` operator expression, use the following
   prototype:

   .. code-block:: javascript

      { $bit: { field: { <and|or|xor>: <int> } } }

   Only use this operator with integer fields (either 32-bit integer or
   64-bit integer).

   .. note::

      All numbers in the :program:`mongo` shell are doubles, not
      integers. Use the ``NumberInt()`` or the ``NumberLong()``
      constructor to specify integers. See :ref:`shell-type-int` or
      :ref:`shell-type-long` for more information.

Examples
--------

Bitwise AND
~~~~~~~~~~~

Consider the following document inserted into the collection
``switches``:

.. code-block:: javascript

   { _id: 1, expdata: NumberInt(13) }

The following :method:`~db.collection.update()` operation updates the
``expdata`` field to the result of a bitwise ``and`` operation between
the current value ``NumberInt(13)`` (i.e. ``1101``) and
``NumberInt(10)`` (i.e. ``1010``):

.. code-block:: javascript

   db.switches.update(
                       { _id: 1 },
                       { $bit: { expdata: { and: NumberInt(10) } } }
                     )

The bitwise ``and`` operation results in the integer 8 (i.e. ``1000``):

.. code-block:: none

   1101
   1010
   ----
   1000

And the updated document has the following value for ``expdata``:

.. code-block:: javascript

   { "_id" : 1, "expdata" : 8 }

The :program:`mongo` shell displays ``NumberInt(8)`` as ``8``.

Bitwise OR
~~~~~~~~~~

Consider the following document inserted into the collection
``switches``:

.. code-block:: javascript

   { _id: 2, expdata: NumberLong(3) }

The following :method:`~db.collection.update()` operation updates the
``expdata`` field to the result of a bitwise ``or`` operation between
the current value ``NumberLong(3)`` (i.e. ``0011``) and
``NumberInt(5)`` (i.e. ``0101``):

.. code-block:: javascript

   db.switches.update(
                       { _id: 2 },
                       { $bit: { expdata: { or: NumberInt(5) } } }
                     )

The bitwise ``or`` operation results in the integer 7 (i.e. ``0111``):

.. code-block:: none

   0011
   0101
   ----
   0111

And the updated document has the following value for ``expdata``:

.. code-block:: javascript

   { "_id" : 2, "expdata" : NumberLong(7) }

Bitwise XOR
~~~~~~~~~~~

Consider the following document in the collection ``switches``:

.. code-block:: javascript

   { _id: 3, expdata: NumberLong(1) }

The following :method:`~db.collection.update()` operation updates the
``expdata`` field to the result of a bitwise ``xor`` operation between
the current value ``NumberLong(1)`` (i.e. ``0001``) and
``NumberInt(5)`` (i.e. ``0101``):

.. code-block:: javascript

   db.switches.update(
                       { _id: 3 },
                       { $bit: { expdata: { xor: NumberInt(5) } } }
                     )

The bitwise ``or`` operation results in the integer 4:

.. code-block:: none

   0001
   0101
   ----
   0100

And the updated document has the following value for ``expdata``:

.. code-block:: javascript

   { "_id" : 3, "expdata" : NumberLong(4) }
============
$currentDate
============

.. default-domain:: mongodb

.. update:: $currentDate

   The :update:`$currentDate` operator sets the value of a field to
   the current date, either as a :ref:`Date <document-bson-type-date>`
   or a :ref:`timestamp <document-bson-type-timestamp>`. The default
   type is :ref:`date <document-bson-type-date>`.

   The :update:`$currentDate` operator can take as its operand either

   - a boolean ``true`` which creates a Date, or

   - a document which explicitly specifies the type, i.e. ``{ $type:
     "timestamp" }`` or ``{ $type: "date" }``. The operator is
     *case-sensitive* and accepts only the lowercase ``"timestamp"`` or
     the lowercase ``"date"``.

Example
-------

.. TODO may want to break this into two separate examples

Consider the following document in the ``users`` collection:

.. code-block:: javascript

   { _id: 1, status: "a", lastModified: ISODate("2013-10-02T01:11:18.965Z") }

The following updates the ``lastModified`` field to the current
date and the ``lastModifiedTS`` field to the current timestamp as
well as setting the ``status`` field to ``"D"``.

.. code-block:: javascript

   db.users.update( { _id: 1 },
                    {
                      $currentDate: {
                                      lastModified: true,
                                      lastModifiedTS: { $type: "timestamp" }
                                    },
                      $set: { status: "D" }
                    }
                  )

Following this operation, the updated document would resemble:

.. code-block:: javascript

   {
     _id: 1,
     status: "D",
     lastModified: ISODate("2013-10-02T01:11:53.976Z"),
     lastModifiedTS: Timestamp(1380676313, 1)
   }
=====
$each
=====

.. default-domain:: mongodb

.. update:: $each

   The :update:`$each` modifier is available for use with the
   :update:`$addToSet` operator and the :update:`$push`
   operator.

   .. start-each-example-for-add-to-set

   Use the :update:`$each` modifier with the :update:`$addToSet`
   operator to add multiple values to an array ``<field>`` if the
   values do not exist in the ``<field>``.

   .. code-block:: javascript

      db.collection.update( <query>,
                            {
                              $addToSet: { <field>: { $each: [ <value1>, <value2> ... ] } }
                            }
                          )

   .. end-each-example-for-add-to-set

   Use the :update:`$each` modifier with the :update:`$push`
   operator to append multiple values to an array ``<field>``.

   .. code-block:: javascript

      db.collection.update( <query>,
                            {
                              $push: { <field>: { $each: [ <value1>, <value2> ... ] } }
                            }
                          )

   .. versionchanged:: 2.4
      MongoDB adds support for the :update:`$each` modifier to the
      :update:`$push` operator. The :update:`$push` operator can
      use :update:`$each` modifier with other modifiers. See
      :update:`$push` for details.

Examples
--------

Use ``$each``  with ``$push`` Operator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/example-push-each.rst

Use ``$each``  with ``$addToSet`` Operator
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/example-addToSet-each.rst
====
$inc
====

.. default-domain:: mongodb

.. update:: $inc

   The :update:`$inc` operator increments a value of a field by a
   specified amount. If the field does not exist, :update:`$inc` adds
   the field and sets the field to the specified amount.
   :update:`$inc` accepts positive and negative incremental
   amounts. Consider the following syntax:

   .. code-block:: javascript

      { $inc: { <field1>: <amount1>, ... } }

   The following example increments the value of ``quantity`` by ``5``
   for the *first* matching document in the ``products`` collection
   where ``sku`` equals ``abc123``:

   .. code-block:: javascript

      db.products.update( { sku: "abc123" },
                          { $inc: { quantity: 5 } } )

   To update all matching documents in the collection, specify
   ``multi:true`` option in the :method:`~db.collection.update()`
   method. For example:

   .. code-block:: javascript

      db.records.update( { age: 20 }, { $inc: { age: 1 } }, { multi: true } );

   The :method:`~db.collection.update()` operation increments the value
   of the ``age`` field by ``1`` for all documents in the ``records``
   collection that have an ``age`` field equal to ``20``.

   The :update:`$inc` operator can operate on multiple fields in a
   document. The following :method:`~db.collection.update()` operation
   uses the :update:`$inc` operator to modify both the ``quantity``
   field and the ``sales`` field for the *first* matching document in
   the ``products`` collection where ``sku`` equals ``abc123``:

   .. code-block:: javascript

      db.products.update( { sku: "abc123" },
                          { $inc: { quantity: -2, sales: 2 } } )

   In the above example, the :update:`$inc` operator expression
   specifies ``-2`` for the ``quantity`` field to *decrease* the value
   of the ``quantity`` field (i.e. increment by ``-2``) and specifies
   ``2`` for the ``sales`` field to increase the value of the ``sales``
   field by ``2``.
=========
$isolated
=========

.. default-domain:: mongodb

.. update:: $isolated

   The :update:`$isolated` isolation operator **isolates** a write
   operation that affects multiple documents from other write operations.

   .. note::

      The :update:`$isolated` isolation operator does **not** provide
      "all-or-nothing" atomicity for write operations.

   Consider the following example:

   .. code-block:: javascript

      db.foo.update( { field1 : 1 , $isolated : 1 }, { $inc : { field2 : 1 } } , { multi: true } )

   Without the :update:`$isolated` operator, multi-updates will allow
   other operations to interleave with these updates. If these
   interleaved operations contain writes, the update operation may
   produce unexpected results. By specifying :update:`$isolated` you
   can guarantee isolation for the entire multi-update.

   .. warning::

      :update:`$isolated` does not work with :term:`sharded clusters
      <sharded cluster>`.

   .. seealso:: See :method:`db.collection.update()` for more information about the
      :method:`db.collection.update()` method.

.. update:: $atomic

   .. deprecated:: 2.2
      The :update:`$isolated` operator replaces :operator:`$atomic`.
====
$max
====

.. default-domain:: mongodb

.. update:: $max

   The :update:`$max` operator updates the value of the field to a
   specified value *if* the specified value is **greater than** the
   current value of the field. If the field does not exists, the
   :update:`$max` operator sets the field to the specified value. The
   :update:`$max` operator can compare values of different types,
   using the :ref:`BSON comparison order
   <faq-dev-compare-order-for-BSON-types>`.

Examples
--------

Use ``$max`` to Compare Numbers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following document in the collection ``scores``:

.. code-block:: javascript

   { _id: 1, highScore: 800, lowScore: 200 }

The ``highScore`` for the document currently has the value
``800``. The following operation uses :operator:`$max` to compare
the ``800`` and the specified value ``950`` and updates the value
of ``highScore`` to ``950`` since ``950`` is greater than ``800``:

.. code-block:: javascript

   db.scores.update( { _id: 1 }, { $max: { highScore: 950 } } )

The ``scores`` collection now contains the following modified document:

.. code-block:: javascript

   { _id: 1, highScore: 950, lowScore: 200 }

The next operation has no effect since the current value of the
field ``highScore``, i.e. ``950``, is greater than ``870``:

.. code-block:: javascript

   db.scores.update( { _id: 1 }, { $max: { highScore: 870 } } )

The document remains unchanged in the ``scores`` collection:

.. code-block:: javascript

   { _id: 1, highScore: 950, lowScore: 200 }

Use ``$max`` to Compare Dates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following document in the collection ``tags``:

.. code-block:: javascript

   {
     _id: 1,
     desc: "crafts",
     dateEntered: ISODate("2013-10-01T05:00:00Z"),
     dateExpired: ISODate("2013-10-01T16:38:16.163Z")
   }

The following operation compares the current value of the
``dateExpired`` field, i.e.
``ISODate("2013-10-01T16:38:16.163Z")``, with the specified date
``new Date("2013-09-30")`` to determine whether to update the
field:

.. code-block:: javascript

   db.tags.update( { _id: 1 },
                   {
                     $max: {
                             dateExpired: new Date("2013-09-30"),
                           }
                    }
                 )

The operation does *not* update the ``dateExpired`` field:

.. code-block:: javascript
   :emphasize-lines: 5

   {
      _id: 1,
      desc: "decorative arts",
      dateEntered: ISODate("2013-10-01T05:00:00Z"),
      dateExpired: ISODate("2013-10-01T16:38:16.163Z")
   }
====
$min
====

.. default-domain:: mongodb

.. update:: $min

   The :update:`$min` updates the value of the field to a specified
   value *if* the specified value is **less than** the current value of
   the field. If the field does not exists, the :update:`$min`
   operator sets the field to the specified value. The :update:`$min`
   operator can compare values of different types, using the :ref:`BSON
   comparison order <faq-dev-compare-order-for-BSON-types>`.

Examples
--------

Use ``$min`` to Compare Numbers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following document in the collection ``scores``:

.. code-block:: javascript

   { _id: 1, highScore: 800, lowScore: 200 }

The ``lowScore`` for the document currently has the value
``200``. The following operation uses :update:`$min` to compare
``200`` to the specified value ``150`` and updates the value of
``lowScore`` to ``150`` since ``150`` is less than ``200``:

.. code-block:: javascript

   db.scores.update( { _id: 1 }, { $min: { lowScore: 150 } } )

The ``scores`` collection now contains the following modified document:

.. code-block:: javascript

   { _id: 1, highScore: 800, lowScore: 150 }

The next operation has no effect since the current value of the
field ``lowScore``, i.e ``150``, is less than ``200``:

.. code-block:: javascript

   db.scores.update( { _id: 1 }, { $min: { lowScore: 250 } } )

The document remains unchanged in the ``scores`` collection:

.. code-block:: javascript

   { _id: 1, highScore: 800, lowScore: 150 }

Use ``$min`` to Compare Dates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider the following document in the collection ``tags``:

.. code-block:: javascript

   {
     _id: 1,
     desc: "crafts",
     dateEntered: ISODate("2013-10-01T05:00:00Z"),
     dateExpired: ISODate("2013-10-01T16:38:16Z")
   }

The following operation compares the current value of the
``dateEntered`` field, i.e. ``ISODate("2013-10-01T05:00:00Z")``,
with the specified date ``new Date("2013-09-25")`` to determine
whether to update the field:

.. code-block:: javascript

   db.tags.update( { _id: 1 },
                   {
                     $min: {
                             dateEntered: new Date("2013-09-25")
                           }
                    }
                 )

The operation updates the ``dateEntered`` field:

.. code-block:: javascript
   :emphasize-lines: 4

   {
     _id: 1,
     desc: "crafts",
     dateEntered: ISODate("2013-09-25T00:00:00Z"),
     dateExpired: ISODate("2013-10-01T16:38:16Z")
   }
====
$mul
====

.. default-domain:: mongodb

.. update:: $mul

   .. versionadded:: 2.6

   Multiply the value of a field by a number. To specify a
   :update:`$mul` expression, use the following prototype:

   .. code-block:: javascript

      { $mul: { field: <number> } }

   The field to update must contain a numeric value. If the field does
   not exist in a document, :update:`$mul` creates the field and sets
   the value to zero of the same numeric type as the multiplier.

   Multiplication with values of mixed numeric types (32-bit integer,
   64-bit integer, float) may result in conversion of numeric type. See
   :ref:`Multiplication Type Conversion Rules
   <faq-developers-multiplication-type-conversion>` for details.

Examples
--------

Multiply the Value of a Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``products`` with the following document:

.. code-block:: javascript

   { _id: 1, item: "ABC", price: 10.99 }

The following :method:`db.collection.update()` operation updates the
document, using the :update:`$mul` operator to multiply the value in
the ``price`` field by ``1.25``:

.. code-block:: javascript

   db.products.update(
                       { _id: 1 },
                       { $mul: { price: 1.25 } }
                     )

The operation results in the following document, where the new value of
the ``price`` field ``13.7375`` reflects the original value ``10.99``
multiplied by ``1.25``:

.. code-block:: javascript

   { _id: 1, item: "ABC", price: 13.7375 }

Apply ``$mul`` Operator to a Non-existing Field
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``products`` with the following document:

.. code-block:: javascript

   { _id: 2,  item: "Unknown" }

The following :method:`db.collection.update()` operation updates the
document, applying the :update:`$mul` operator to the field ``price``
that does not exist in the document:

.. code-block:: javascript

    db.products.update(
                        { _id: 2 },
                        { $mul: { price: NumberLong(100) } }
                      )

The operation results in the following document with a ``price`` field
set to value 0 of numeric type :ref:`shell-type-long`, the same type as
the multiplier:

.. code-block:: javascript

   { "_id" : 2, "item" : "Unknown", "price" : NumberLong(0) }

Multiply Mixed Numeric Types
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``products`` with the following document:

.. code-block:: javascript

   { _id: 3,  item: "XYZ", price: NumberLong(10) }

The following :method:`db.collection.update()` operation uses the
:update:`$mul` operator to multiply the value in the ``price`` field
:ref:`NumberLong(10) <shell-type-long>` by :ref:`NumberInt(5)
<shell-type-int>`:

.. code-block:: javascript

    db.products.update(
                        { _id: 3 },
                        { $mul: { price: NumberInt(5) } }
                      )

The operation results in the following document:

.. code-block:: javascript

   { "_id" : 3, "item" : "XYZ", "price" : NumberLong(50) }

The value in the ``price`` field is of type :ref:`shell-type-long`. See
:ref:`Multiplication Type Conversion Rules
<faq-developers-multiplication-type-conversion>` for details.
====
$pop
====

.. default-domain:: mongodb

.. update:: $pop

   .. versionadded:: 1.1

   The :update:`$pop` operator removes the first or last element of an
   array. Pass :update:`$pop` a value of ``-1`` to remove the first
   element of an array and ``1`` to remove the last element in an
   array.


   .. code-block:: javascript

      db.collection.update( <query>,
                            { $pop: { <field>: <-1 | 1> } }
                          )

Behavior
--------

The :update:`$pop` operation fails if the ``<field>`` is not an array.

If the :update:`$pop` operator removes the last item in the
``<field>``, the ``<field>`` will then hold an empty array.

Examples
--------

Remove the First Item of an Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given the following document in a collection ``students``:

.. code-block:: javascript

   { _id: 1, scores: [ 8, 9, 10 ] }

The following example removes the *first* element (``8``) in the
``scores`` array:

.. code-block:: javascript

   db.students.update( { _id: 1 }, { $pop: { scores: -1 } } )

After the operation, the updated document has the first item ``8``
removed from its ``scores`` array:

.. code-block:: javascript

   { _id: 1, scores: [ 9, 10 ] }

Remove the Last Item of an Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given the following document in a collection ``students``:

.. code-block:: javascript

    { _id: 1, scores: [ 9, 10 ] }

The following example removes the *last* element (``10``) in the
``scores`` array by specifying ``1`` in the :update:`$pop` expression:

.. code-block:: javascript

   db.students.update( { _id: 1 }, { $pop: { scores: 1 } } )

After the operation, the updated document has the last item ``10``
removed from its ``scores`` array:

.. code-block:: javascript

   { _id: 1, scores: [ 9 ] }
=========
$position
=========

.. default-domain:: mongodb

.. update:: $position

   .. versionadded:: 2.6

   The :update:`$position` modifier specifies the location in the
   array at which the :update:`$push` operator insert elements.
   Without the :update:`$position` modifier, the :update:`$push`
   operator inserts elements to the end of the array. See :ref:`$push
   operator <push-modifiers>` for more information.

   To use the :update:`$position` modifier, it must appear with the
   :update:`$each` modifier.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $push: {
                                        <field>: {
                                                   $each: [ <value1>, <value2>, ... ],
                                                   $position: <num>
                                        }
                                     }
                            }
                          )

   The ``<num>`` is either a zero or a positive number that correspond
   to position in the array, based on a zero-based index. If the number
   is greater or equal to the length of the array, the
   :update:`$position` modifier has no effect and the operator adds
   elements to the end of the array.

Examples
--------

Add Elements at the Start of the Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``students`` that contains the following document:

.. code-block:: javascript

   { "_id" : 1, "scores" : [ 100 ] }

The following operation updates the ``scores`` field to add the
elements ``50`` and ``60`` to the beginning of the array.

.. code-block:: javascript

   db.students.update( { _id: 1 },
                       { $push: { scores: {
                                            $each: [ 50, 60, 70 ],
                                            $position: 0
                                          }
                                }
                       }
                     )

The operation results in the following updated document:

.. code-block:: javascript

   { "_id" : 1, "scores" : [  50,  60,  70,  100 ] }

Add Elements to the Middle of the Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``students`` that contains the following document:

.. code-block:: javascript

   { "_id" : 1, "scores" : [  50,  60,  70,  100 ] }

The following operation updates the ``scores`` field to add the
elements ``20`` and ``30`` at the array index of ``2``:

.. code-block:: javascript

   db.students.update( { _id: 1 },
                       { $push: { scores: {
                                            $each: [ 20, 30 ],
                                            $position: 2
                                          }
                                }
                       }
                     )

The operation results in the following updated document:

.. code-block:: javascript

   { "_id" : 1, "scores" : [  50,  60,  20,  30,  70,  100 ] }
===========
\$ (update)
===========

.. default-domain:: mongodb

Definition
----------

.. update:: $

   *Syntax*: ``{ "<array>.$" : value }``

   The positional :operator:`$` operator identifies an element in an
   ``array`` field to update without explicitly specifying the
   position of the element in the array. To project, or return, an
   array element from a read operation, see the :projection:`$`
   projection operator.

   When used with the :method:`~db.collection.update()`
   method,

   - the positional :operator:`$` operator acts as a placeholder for
     the **first** element that matches the :ref:`query document
     <read-operations-query-document>`, and

   - the ``array`` field **must** appear as part of the ``query
     document``.

   .. code-block:: javascript

      db.collection.update( { <array>: value ... }, { <update operator>: { "<array>.$" : value } } )

Behavior
--------

Upserts
~~~~~~~

Do not use the positional operator :operator:`$` with
:term:`upsert` operations because inserts will use the ``$`` as
a field name in the inserted document.

Nested Arrays
~~~~~~~~~~~~~

The positional :operator:`$` operator cannot be used for queries which
traverse more than one array, such as queries that traverse arrays
nested within other arrays, because the replacement for the
:operator:`$` placeholder is a single value

Unsets
~~~~~~

When used with the :update:`$unset` operator, the positional
:operator:`$` operator does not remove the matching element
from the array but rather sets it to ``null``.

Negations
~~~~~~~~~

.. see SERVER-6982

If the query matches the array using a negation operator, such as
:query:`$ne`, :query:`$not`, or :query:`$nin`, then you cannot use the
positional operator to update values from this array.

However, if the negated portion of the query is inside of an
:query:`$elemMatch` expression, then you *can* use the positional
operator to update this field.

Examples
--------

Update Values in an Array
~~~~~~~~~~~~~~~~~~~~~~~~~

Consider a collection ``students`` with the following documents:

.. code-block:: javascript

   { "_id" : 1, "grades" : [ 80, 85, 90 ] }
   { "_id" : 2, "grades" : [ 88, 90, 92 ] }
   { "_id" : 3, "grades" : [ 85, 100, 90 ] }

To update ``80`` to ``82`` in the ``grades`` array in the
first document, use the positional :operator:`$` operator if
you do not know the position of the element in the array:

.. code-block:: javascript

   db.students.update( { _id: 1, grades: 80 }, { $set: { "grades.$" : 82 } } )

Remember that the positional :operator:`$` operator acts as a
placeholder for the **first match** of the update :ref:`query
document <read-operations-query-document>`.

Update Documents in an Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The positional :operator:`$` operator facilitates updates to arrays
that contain embedded documents. Use the positional :operator:`$`
operator to access the fields in the embedded documents with the
:ref:`dot notation <document-dot-notation>` on the
:operator:`$` operator.

.. code-block:: javascript

   db.collection.update( { <query selector> }, { <update operator>: { "array.$.field" : value } } )

Consider the following document in the ``students`` collection whose
``grades`` field value is an array of embedded documents:

.. code-block:: javascript

   { "_id" : 4, "grades" : [ { grade: 80, mean: 75, std: 8 },
                             { grade: 85, mean: 90, std: 5 },
                             { grade: 90, mean: 85, std: 3 } ] }

Use the positional :operator:`$` operator to update the value of the
``std`` field in the embedded document with the ``grade`` of ``85``:

.. code-block:: javascript

   db.students.update( { _id: 4, "grades.grade": 85 }, { $set: { "grades.$.std" : 6 } } )

Further Reading
---------------

:method:`~db.collection.update()`, :update:`$set` and
:update:`$unset`
=====
$pull
=====

.. default-domain:: mongodb

.. update:: $pull

   The :update:`$pull` operator removes from an existing array all
   instances of a value or values that match a specified query.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $pull: { <arrayField>: <query2> } }
                          )

   To specify the query to remove values from the array, use
   :doc:`query operators </reference/operator>`.

Examples
--------

Remove All Items That Equals a Specified Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given the following documents in the ``cpuinfo`` collection:

.. code-block:: javascript

   { _id: 1, flags: [ "vme", "de", "msr", "tsc", "pse", "msr" ] }
   { _id: 2, flags: [ "msr", "pse", "tsc" ] }

The following operation removes the value ``"msr"`` from the ``flags``
array:

.. code-block:: javascript

   db.cpuinfo.update(
                      { flags: "msr" },
                      { $pull: { flags: "msr" } },
                      { multi: true }
                    )

After the operation, the documents no long have any ``"msr"`` values:

.. code-block:: javascript

   { _id: 1, flags: [  "vme",  "de",  "tsc",  "pse" ] }
   { _id: 2, flags: [  "pse",  "tsc" ] }

Remove All Items Greater Than a Specified Value
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Given the following document in the ``profiles`` collection:

.. code-block:: javascript

   { _id: 1, votes: [ 3, 5, 6, 7, 7, 8 ] }

The following operation will remove all items from the ``votes`` array
that are greater than or equal (:query:`$gte`) ``6``:

.. code-block:: javascript

   db.profiles.update( { _id: 1 }, { $pull: { votes: { $gte: 6 } } } )

After the update operation, the document only has values less than 6:

.. code-block:: javascript

   { _id: 1, votes: [  3,  5 ] }
========
$pullAll
========

.. default-domain:: mongodb

.. update:: $pullAll

   The :update:`$pullAll` operator removes all instances of the
   specified values from an existing array.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $pullAll: { <arrayField>: [ <value1>, <value2> ... ] } }
                          )

   Unlike the :update:`$pull` operator that removes elements by
   specifying a query, :update:`$pullAll` removes elements that
   match the listed values.

   For example, given the following document in the ``survey``
   collection:

   .. code-block:: javascript

      { _id: 1, scores: [ 0, 2, 5, 5, 1, 0 ] }

   The following operation removes all instances of the value ``0`` and
   ``5`` from the ``scores`` array:

   .. code-block:: javascript

      db.survey.update( { _id: 1 }, { $pullAll: { scores: [ 0, 5 ] } } )

   After the operation, the updated document has all instances of ``0``
   and ``5`` removed from the ``scores`` field:

   .. code-block:: javascript

      { "_id" : 1, "scores" : [  2,  1 ] }
=====
$push
=====

.. default-domain:: mongodb

.. update:: $push

   The :update:`$push` operator appends a specified value to an array.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $push: { <field>: <value> } }
                         )

   The following example appends ``89`` to the ``scores`` array for the
   first document where the ``_id`` field equals ``1``:

   .. code-block:: javascript

      db.students.update(
                          { _id: 1 },
                          { $push: { scores: 89 } }
                        )

   .. note::

      - If the field is absent in the document to update,
        :update:`$push` adds the array field with the value as its
        element.

      - If the field is **not** an array, the operation will fail.

      - If the value is an array, :update:`$push` appends the whole
        array as a *single* element. To add each element of the value
        separately, use :update:`$push` with the :update:`$each`
        modifier.

        .. versionchanged:: 2.4
           MongoDB adds support for the :update:`$each` modifier to
           the :update:`$push` operator. Before 2.4, use
           :update:`$pushAll` for similar functionality.

        .. include:: /includes/example-push-each.rst

.. _push-modifiers:

Modifiers
---------

.. versionadded:: 2.4

You can use the :update:`$push` operator with the following modifiers:

- :update:`$each` appends multiple values to the array field,

  .. versionchanged:: 2.6

     When used in conjunction with the other modifiers, the
     :update:`$each` modifier no longer needs to be first.

- :update:`$slice`, which is only available when used with
  :update:`$each`, limits the number of array elements,

- :update:`$sort`, which is only available when used with
  :update:`$each`, orders elements of the array, and

  .. versionchanged:: 2.6

     In previous versions, :update:`$sort` is only available
     when used with both :update:`$each` and :update:`$slice`.

- :update:`$position`, which is only available when used with
  :update:`$each`, specifies the location in the array at which to
  insert the new elements. Without the :update:`$position` modifier,
  the :update:`$push` appends the elements to the end of the array.

  .. versionadded:: 2.6

The processing of the :update:`push` operation with modifiers occur
in the following order, regardless of the order in which the modifiers
appear:

#. Update array to add elements in the correct position.

#. Apply sort, if specified.

#. Slice the array, if specified.

#. Store the array.

Examples
--------

Use ``$push`` Operator with Multiple Modifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/example-push-with-multiple-modifiers.rst
========
$pushAll
========

.. default-domain:: mongodb

.. update:: $pushAll

   .. deprecated:: 2.4
      Use the :update:`$push` operator with :update:`$each` instead.

   The :update:`$pushAll` operator is similar to the :update:`$push` but
   adds the ability to append several values to an array at once.

   .. code-block:: javascript

      db.collection.update( { field: value }, { $pushAll: { field1: [ value1, value2, value3 ] } } );

   Here, :update:`$pushAll` appends the values in ``[ value1, value2,
   value3 ]`` to the array in ``field1`` in the document
   matched by the statement ``{ field: value }`` in ``collection``.

   If you specify a single value, :update:`$pushAll` will behave as
   :update:`$push`.
=======
$rename
=======

.. default-domain:: mongodb

.. update:: $rename

   .. versionadded:: 1.7.2

   *Syntax*: ``{$rename: { <old name1>: <new name1>, <old name2>: <new name2>, ... } }``

   The :update:`$rename` operator updates the name of a field. The new
   field name must differ from the existing field name.

   Consider the following example:

   .. code-block:: javascript

      db.students.update( { _id: 1 }, { $rename: { 'nickname': 'alias', 'cell': 'mobile' } } )

   This operation renames the field ``nickname`` to ``alias``, and the
   field ``cell`` to ``mobile``.

Behavior
--------

The :update:`$rename` operator logically performs an :update:`$unset`
of both the old name and the new name, and then performs a
:update:`$set` operation with the new name. As such, the operation may
not preserve the order of the fields in the document; i.e. the renamed
field may move within the document.

If the document already has a field with the *new* field name, the
:update:`$rename` operator removes that field and renames the
field with the *old* field name to the *new* field name.

For fields in embedded documents, the :update:`$rename` operator can
rename these fields as well as move the fields in and out of embedded
documents. :update:`$rename` does not work if these fields are in array
elements.

Examples
--------

A collection ``students`` the following document where a field
``nmae`` appears misspelled, i.e. should be ``name``:

.. code-block:: javascript

   { "_id": 1,
     "alias": [ "The American Cincinnatus", "The American Fabius" ],
     "mobile": "555-555-5555",
     "nmae": { "first" : "george", "last" : "washington" }
   }

The examples in this section successively updates this document.

Rename a Field
~~~~~~~~~~~~~~

To rename a field, call the :update:`$rename` operator with the current
name of the field and the new name:

.. code-block:: javascript

   db.students.update( { _id: 1 }, { $rename: { "nmae": "name" } } )

This operation renames the field ``nmae`` to ``name``:

.. code-block:: javascript

   {
     "_id": 1,
     "alias": [ "The American Cincinnatus", "The American Fabius" ],
     "mobile": "555-555-5555",
     "name": { "first" : "george", "last" : "washington" }
   }

.. _rename-field-in-embedded-document:

Rename a Field in an Embedded Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To rename a field in an embedded document, call the :update:`$rename`
operator using the :ref:`dot notation <document-dot-notation>` to refer
to the field. If the field is to remain in the same embedded document,
also use the dot notation in the new name, as in the following:

.. code-block:: javascript

   db.students.update( { _id: 1 }, { $rename: { "name.first": "name.fname" } } )

This operation renames the embedded field ``first`` to ``fname``:

.. code-block:: javascript

   {
     "_id" : 1,
     "alias" : [ "The American Cincinnatus", "The American Fabius" ],
     "mobile" : "555-555-5555",
     "name" : { "fname" : "george", "last" : "washington" }
   }

Rename a Field That Does Not Exist
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When renaming a field and the existing field name refers to a field
that does not exist, the :update:`$rename` operator does nothing, as in
the following:

.. code-block:: javascript

   db.students.update( { _id: 1 }, { $rename: { 'wife': 'spouse' } } )

This operation does nothing because there is no field named
``wife``.
====
$set
====

.. default-domain:: mongodb

.. update:: $set

   *Syntax*: ``{ $set: { <field1>: <value1>, ... } }``

   Use the :update:`$set` operator to replace the value of a field to
   the specified value. If the field does not exist, the
   :update:`$set` operator will add the field with the specified
   value.

   The following example uses the :update:`$set` operator to update
   the value of the ``quantity`` field to ``500`` and the ``instock``
   field to ``true`` for the *first* document where the field ``sku``
   has the value ``abc123``:

   .. code-block:: javascript

      db.products.update( { sku: "abc123" },
                          { $set: {
                                    quantity: 500,
                                    instock: true
                                  }
                          }
                        )

   To update all matching documents in the collection, specify ``multi:
   true`` option in the :method:`~db.collection.update()` method, as in
   the following example which sets the value of the field ``instock``
   to ``true`` for all documents in the ``products`` collection where
   the ``quantity`` field is greater than (i.e. :query:`$gt`) ``0`` :

   .. code-block:: javascript

      db.products.update( { quantity: { $gt: 0 } },
                          { $set: { instock: true } },
                          { multi: true }
                        )
============
$setOnInsert
============

.. default-domain:: mongodb

.. update:: $setOnInsert

   .. versionadded:: 2.4

   If an :term:`upsert` results in an insert of a document, then
   :update:`$setOnInsert` assigns the specified values to the fields in
   the document. You can specify an upsert by specifying the
   :term:`upsert` option for either the
   :method:`db.collection.update()` or
   :method:`db.collection.findAndModify()` methods. If the upsert
   results in an :doc:`update </core/write-operations>`,
   :update:`$setOnInsert` has no effect.

   .. code-block:: javascript

      db.collection.update(
         <query>,
         { $setOnInsert: { <field1>: <value1>, ... } },
         { upsert: true }
      )

Examples
--------

Upsert Results in an Insert
~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection named ``products`` contains no documents.

Then, the following :method:`upsert <db.collection.update()>`
operation performs an insert and applies the
:update:`$setOnInsert` to set the field ``defaultQty`` to
``100``:

.. code-block:: javascript

   db.products.update(
     { _id: 1 },
     { $setOnInsert: { defaultQty: 100 } },
     { upsert: true }
   )

The ``products`` collection contains the newly-inserted document:

.. code-block:: javascript

   { "_id" : 1, "defaultQty" : 100 }


Upsert Results in an Update
~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the :method:`db.collection.update()` or the
:method:`db.collection.findAndModify()` method has the ``upsert`` flag
and performs an update and not an insert, :update:`$setOnInsert` has no
effect.

A collection named ``products`` has the following document:

.. code-block:: javascript

   { "_id" : 1, "defaultQty" : 100 }

The following :method:`~db.collection.update()` with the
*upsert* flag operation performs an update:

.. code-block:: javascript

   db.products.update(
     { _id: 1 },
     {
       $setOnInsert: { defaultQty: 500, inStock: true },
       $set: { item: "apple" }
     },
     { upsert: true }
   )

Because the :method:`~db.collection.update()` with *upsert* only
performs an update, MongoDB ignores the :update:`$setOnInsert`
operation and only applies the :update:`$set` operation.

The ``products`` collection now contains the following modified
document:

.. code-block:: javascript

   { "_id" : 1, "defaultQty" : 100, "item" : "apple" }
======
$slice
======

.. default-domain:: mongodb

.. update:: $slice

   .. versionadded:: 2.4

   The :update:`$slice` modifier limits the number of array
   elements during a :update:`$push` operation. To project, or return,
   a specified number of array elements from a read operation, see the
   :projection:`$slice` projection operator instead.

   To use the :update:`$slice` modifier, it must appear with the
   :update:`$each` modifier.

   .. tip::
      You can pass an empty array ``[]`` to the :update:`$each`
      modifier such that only the :update:`$slice` modifier has an
      effect.

   .. versionchanged:: 2.6

      - The :update:`$slice` can slice from the beginning of the
        array.

      - Trying to use the :update:`$slice` modifier without the
        :update:`$each` modifier results in an error.

      - The order in which the modifiers appear is immaterial. Previous
        versions required the :update:`$each` modifier to appear as
        the first modifier if used in conjunction with
        :update:`$slice`.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $push: {
                                       <field>: {
                                                  $each: [ <value1>, <value2>, ... ],
                                                  $slice: <num>
                                                }
                                     }
                            }
                          )

   The ``<num>`` can be:

   - **zero** to update the array ``<field>`` to an empty array,

   - **negative** to update the array ``<field>`` to contain only the
     last ``<num>`` elements, or

   - **positive** to update the array ``<field>`` contain only the
     first ``<num>`` elements.

     .. versionadded:: 2.6

Examples
--------

Slice from the End of the Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id" : 1, "scores" : [ 40, 50, 60 ] }

The following operation adds new elements to the ``scores`` array, and
then uses the :update:`$slice` modifier to trim the array to the
last five elements:

.. code-block:: javascript

   db.students.update( { _id: 1 },
                       { $push: { scores: {
                                            $each: [ 80, 78, 86 ],
                                            $slice: -5
                                          }
                                }
                       }
                     )

The result of the operation is slice the elements of the updated
``scores`` array to the last five elements:

.. code-block:: javascript

   { "_id" : 1, "scores" : [  50,  60,  80,  78,  86 ] }

Slice from the Front of the Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id" : 2, "scores" : [ 89, 90 ] }

The following operation adds new elements to the ``scores`` array, and
then uses the :update:`$slice` modifier to trim the array to the
first three elements.

.. code-block:: javascript

   db.students.update( { _id: 2 },
                       { $push: { scores: { $each: [ 100, 20 ], $slice: 3 } } }
                     )

The result of the operation is to slice the elements of the updated
``scores`` array to the first three elements:

.. code-block:: javascript

   { "_id" : 2, "scores" : [  89,  90,  100 ] }

Update Array Using Slice Only
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id" : 3, "scores" : [  89,  70,  100,  20 ] }

To update the ``scores`` field with just the effects of the
:update:`$slice` modifier, specify the number of elements to slice
(e.g. ``-3``) for the :update:`$slice` modifier and an empty
array ``[]`` for the :update:`$each` modifier, as in the following:

.. code-block:: javascript

   db.students.update(
                       { _id: 3 },
                       { $push: { scores: { $each: [ ], $slice: -3 } } }
                     )

The result of the operation is to slice the elements of the ``scores``
array to the last three elements:

.. code-block:: javascript

   { "_id" : 3, "scores" : [  70,  100,  20 ] }

Use ``$slice`` with Other ``$push`` Modifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/example-push-with-multiple-modifiers.rst

The order of the modifiers is immaterial to the order in which the
modifiers are processed. See :ref:`push-modifiers` for details.
=====
$sort
=====

.. default-domain:: mongodb

.. update:: $sort

   .. versionadded:: 2.4

   The :update:`$sort` modifier orders the elements of an array
   during a :update:`$push` operation.

   To use the :update:`$sort` modifier, it must appear with the
   :update:`$each` modifier.

   .. tip::
      You can pass an empty array ``[]`` to the :update:`$each`
      modifier such that only the :update:`$sort` modifier has an
      effect.

   .. versionchanged:: 2.6

      - The :update:`$sort` modifier can sort array elements that are
        not documents. In previous versions, the :update:`$sort`
        modifier required the array elements be documents.

      - If the array elements are documents, the modifier can sort by
        either the whole document or by a specific field in the
        documents. In previous versions, the :update:`$sort` modifier
        can only sort by a specific field in the documents.

      - The :update:`$sort` no longer requires the :update:`$slice`
        modifier.

      - Trying to use the :update:`$sort` modifier without the
        :update:`$each` modifier results in an error.

   .. code-block:: javascript

      db.collection.update( <query>,
                            { $push: {
                                       <arrayField>: {
                                                        $each: [ <value1>,
                                                                 <value2>,
                                                                 ...
                                                               ],
                                                        $sort: <sort specification>,
                                                     }
                                     }
                            }
                          )

   For ``<sort specification>``:

   - To sort array elements that are not documents, or if the array
     elements are documents, to sort by the whole documents, specify
     ``1`` for ascending or ``-1`` for descending.

   - If the array elements are documents, to sort by a field in the
     documents, specify a sort document with the field and the
     direction, i.e. ``{ field: 1 }`` or ``{ field: -1 }``. Do **not**
     reference the containing array field in the sort specification
     (e.g. ``{ "arrayField.field": 1 }`` is incorrect).

Examples
--------

.. _example-sort-by-field-in-documents:

Sort Array of Documents by a Field in the Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id": 1,
     "quizzes": [
                  { "id" : 1, "score" : 6 },
                  { "id" : 2, "score" : 9 }
                ]
   }

The following update appends additional documents to the ``quizzes``
array and then sorts all the elements of the array by the ascending
``score`` field:

.. code-block:: javascript

   db.students.update( { _id: 1 },
                       { $push: { quizzes: { $each: [ { id: 3, score: 8 },
                                                      { id: 4, score: 7 },
                                                      { id: 5, score: 6 } ],
                                             $sort: { score: 1 },
                                           }
                                }
                       }
                     )

.. important:: The sort document refers directly to the field in the
   documents and does not reference the containing array field
   ``quizzes``; i.e. ``{ score: 1 }`` and **not** ``{ "quizzes.score": 1}``

After the update, the array elements are in order of ascending
``score`` field.:

.. code-block:: javascript

   {
     "_id" : 1,
     "quizzes" : [
                   { "id" : 1, "score" : 6 },
                   { "id" : 5, "score" : 6 },
                   { "id" : 4, "score" : 7 },
                   { "id" : 3, "score" : 8 },
                   { "id" : 2, "score" : 9 }
                 ]
   }

Sort Array Elements That Are Not Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id" : 2, "tests" : [  89,  70,  89,  50 ] }

The following operation adds two more elements to the ``scores`` array
and sorts the elements:

.. code-block:: javascript

   db.students.update(
                       { _id: 2 },
                       { $push: { tests: { $each: [ 40, 60 ], $sort: 1 } } }
                     )

The updated document has the elements of the ``scores`` array in
ascending order:

.. code-block:: javascript

   { "_id" : 2, "tests" : [  40,  50,  60,  70,  89,  89 ] }

Update Array Using Sort Only
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A collection ``students`` contains the following document:

.. code-block:: javascript

   { "_id" : 3, "tests" : [  89,  70,  100,  20 ] }

To update the ``tests`` field to sort its elements in descending
order, specify the ``{ $sort: -1 }`` and specify an empty array ``[]``
for the :update:`$each` modifier, as in the following:

.. code-block:: javascript

   db.students.update(
                       { _id: 3 },
                       { $push: { tests: { $each: [ ], $sort: -1 } } }
                     )

The result of the operation is to update the ``scores`` field to sort
its elements in descending order:

.. code-block:: javascript

   { "_id" : 3, "tests" : [ 100,  89,  70,  20 ] }

Use ``$sort`` with Other ``$push`` Modifiers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/example-push-with-multiple-modifiers.rst

The order of the modifiers is immaterial to the order in which the
modifiers are processed. See :ref:`push-modifiers` for details.
======
$unset
======

.. default-domain:: mongodb

.. update:: $unset

   The :update:`$unset` operator deletes a particular field. The
   specified value in the :update:`$unset` expression (i.e. ``""``
   below) does not impact the operation. If the field does not exist,
   then :update:`$unset` has no effect. Consider the following
   syntax:

   .. code-block:: javascript

     { $unset: { <field1>: "", ... } }

   For example, the following :method:`~db.collection.update()`
   operation uses the :update:`$unset` operator to remove the fields
   ``quantity`` and ``instock`` from the *first* document found in the
   ``products`` collection where the field ``sku`` has a value of
   ``unknown``.

   .. code-block:: javascript

      db.products.update( { sku: "unknown" },
                          { $unset: {
                                      quantity: "",
                                      instock: ""
                                    }
                          }
                        )

   To remove the fields from *all* documents in the collection where
   the field ``sku`` has a value of ``unknown``, specify the ``multi:
   true`` option in the :method:`~db.collection.update()` method, as in
   the following example:

   .. code-block:: javascript

      db.products.update( { sku: "unknown" },
                          { $unset: {
                                      quantity: "",
                                      instock: ""
                                    }
                          },
                          { multi: true }
                        )
======================
Array Update Operators
======================

.. default-domain:: mongodb

Update Operators
----------------

.. include:: /includes/toc/table-operator-update-array.rst

.. include:: /includes/toc/operator-update-array.rst

Update Operator Modifiers
-------------------------

.. include:: /includes/toc/table-operator-update-array-modifiers.rst

.. include:: /includes/toc/operator-update-array-modifiers.rst
=======================
Bitwise Update Operator
=======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-update-bitwise.rst

.. include:: /includes/toc/operator-update-bitwise.rst
======================
Field Update Operators
======================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-update-field.rst

.. include:: /includes/toc/operator-update-field.rst
=========================
Isolation Update Operator
=========================

.. default-domain:: mongodb

.. include:: /includes/toc/table-operator-update-isolation.rst

.. include:: /includes/toc/operator-update-isolation.rst
================
Update Operators
================

.. default-domain:: mongodb

.. _update-operators:

Update Operators
----------------

Fields
~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-update-field.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/update-field

Array
~~~~~

.. only:: website

   Operators
   `````````

   .. include:: /includes/toc/table-operator-update-array.rst

   Modifiers
   `````````

   .. include:: /includes/toc/table-operator-update-array-modifiers.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/update-array

Bitwise
~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-update-bitwise.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/update-bitwise

Isolation
~~~~~~~~~

.. only:: website

   .. include:: /includes/toc/table-operator-update-isolation.rst

.. class:: hidden

   .. toctree::
      :titlesonly:

      /reference/operator/update-isolation
=========
Operators
=========

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-operator-landing.rst

.. include:: /includes/toc/operator-landing.rst
=========================
MongoDB Server Parameters
=========================

.. default-domain:: mongodb

.. versionchanged:: 2.4

Synopsis
--------

MongoDB provides a number of configuration options that are accessible
via the :option:`--setParameter <mongod --setParameter>` option to
:program:`mongod`. This document documents all of these options.

For additional run time configuration options, see
:doc:`/reference/configuration-options` and :doc:`Manual Page for
mongod </reference/program/mongod>`.

Parameters
----------

.. parameter:: enableLocalhostAuthBypass

   .. versionadded:: 2.4

   Specify ``0`` to disable localhost authentication bypass.  Enabled
   by default.

   :parameter:`enableLocalhostAuthBypass` is not available using
   :dbcommand:`setParameter` database command. Use the
   :setting:`setParameter` option in the configuration file or the
   :option:`--setParameter <mongod --setParameter>` option on the
   command line.

.. parameter:: enableTestCommands

   .. versionadded:: 2.4

   :parameter:`enableTestCommands` enables a set of internal commands
   useful for internal testing
   operations. :parameter:`enableTestCommands` is only available when
   starting :program:`mongod` and you cannot use
   :dbcommand:`setParameter` to modify this parameter. Consider the
   following :program:`mongod` invocation, which sets
   :parameter:`enableTestCommands`:

   .. code-block:: sh

      mongod --setParameter enableTestCommands=1

   :parameter:`enableTestCommands` provides access to the following
   internal commands:

   - :dbcommand:`captrunc`
   - :dbcommand:`configureFailPoint`
   - :dbcommand:`emptycapped`
   - :dbcommand:`godinsert`
   - :dbcommand:`_hashBSONElement`
   - :dbcommand:`journalLatencyTest`
   - :dbcommand:`replSetTest`
   - :dbcommand:`_skewClockCommand`
   - :dbcommand:`sleep`
   - :dbcommand:`_testDistLockWithSkew`
   - :dbcommand:`_testDistLockWithSyncCluster`

.. parameter:: journalCommitInterval

   Specify an integer between ``1`` and ``500`` signifying the number
   of milliseconds (ms) between journal commits.

   Consider the following example which sets the
   :parameter:`journalCommitInterval` to ``200`` ms:

   .. code-block:: javascript

      db.getSiblingDB("admin").runCommand( { setParameter: 1, journalCommitInterval: 200 } )

   .. seealso:: :setting:`~storage.journal.commitIntervalMs`.

.. parameter:: logUserIds

   .. versionadded:: 2.4

   Specify ``1`` to enable logging of userids.

   Disabled by default.

.. parameter:: logLevel

   Specify an integer between ``0`` and ``5`` signifying the verbosity
   of the logging, where ``5`` is the most verbose.

   Consider the following example which sets the
   :parameter:`logLevel` to ``2``:

   .. code-block:: javascript

      use admin
      db.runCommand( { setParameter: 1, logLevel: 2 } )

   .. seealso::

      :setting:`~systemLog.verbosity`.

.. parameter:: notablescan

   Specify whether queries must use indexes. If ``1``, queries that
   perform a table scan instead of using an index will fail.

   Consider the following example which sets :parameter:`notablescan` to
   true:

   .. code-block:: javascript

      db.getSiblingDB("admin").runCommand( { setParameter: 1, notablescan: 1 } )

   .. seealso:: :setting:`notablescan`

.. parameter:: replIndexPrefetch

   .. versionadded:: 2.2

   Use :parameter:`replIndexPrefetch` in conjunction with
   :setting:`~replication.replSetName` when configuring a replica
   set. The default value is ``all`` and available
   options are:

   - ``none``
   - ``all``
   - ``_id_only``

   By default :term:`secondary` members of a :term:`replica set` will
   load all indexes related to an operation into memory before
   applying operations from the oplog. You can modify this behavior so
   that the secondaries will only load the ``_id`` index. Specify
   ``_id_only`` or ``none`` to prevent the :program:`mongod` from
   loading *any* index into memory.

.. parameter:: replApplyBatchSize

   .. versionadded:: 2.4

   Specify the number of oplog entries to apply as a single batch.
   :parameter:`replApplyBatchSize` must be an integer between 1 and 1024.
   This option only applies to replica set members when they are in the :term:`secondary` state.

   Batch sizes must be ``1`` for members with :setting:`slaveDelay`
   configured.

.. parameter:: saslHostName

   .. versionadded:: 2.4

   :parameter:`saslHostName` overrides MongoDB's default hostname
   detection for the purpose of configuring SASL and Kerberos
   authentication.

   :parameter:`saslHostName` does not affect the hostname of the
   :program:`mongod` or :program:`mongos` instance for any purpose
   beyond the configuration of SASL and Kerberos.

   You can only set :parameter:`saslHostName` during start-up, and
   cannot change this setting using the :dbcommand:`setParameter`
   database command.

   .. note::

      :parameter:`saslHostName` supports Kerberos authentication and is
      only included in MongoDB Enterprise. For Linux systems, see
      :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
      for more information.

      .. COMMENT
         .. versionadded:: 2.6
            MongoDB Enterprise now supports Windows systems,
            which includes support for Kerberos. See also
            :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication-under-windows`.

.. parameter:: supportCompatibilityFormPrivilegeDocuments

   .. versionadded:: 2.4

   .. deprecated:: 2.6
      :parameter:`supportCompatibilityFormPrivilegeDocuments` has no
      effect in 2.6 and will be removed in 2.8.

   :parameter:`supportCompatibilityFormPrivilegeDocuments` is not
   available using :dbcommand:`setParameter` database command. Use the
   :setting:`setParameter` option in the configuration file or the
   :option:`--setParameter <mongod --setParameter>` option on the
   command line.

.. parameter:: syncdelay

   Specify the interval in seconds between :term:`fsync` operations
   where :program:`mongod` flushes its working memory to disk. By
   default, :program:`mongod` flushes memory to disk every 60
   seconds. In almost every situation you should not set this value
   and use the default setting.

   Consider the following example which sets the ``syncdelay`` to
   ``60`` seconds:

   .. code-block:: javascript

      db.getSiblingDB("admin").runCommand( { setParameter: 1, syncdelay: 60 } )

   .. seealso:: :setting:`~storage.syncPeriodSecs` and
      :parameter:`journalCommitInterval`.

.. parameter:: traceExceptions

   .. versionadded:: 2.2

   Configures :program:`mongod` log full stack traces on assertions or
   errors. If ``1``, :program:`mongod` will log full stack
   traces on assertions or errors.

   Consider the following example which sets the
   ``traceExceptions`` to ``true``:

   .. code-block:: javascript

      db.getSiblingDB("admin").runCommand( { setParameter: 1, traceExceptions: true } )

   .. seealso:: :setting:`traceExceptions`

.. parameter:: quiet

   Sets quiet logging mode. If
   ``1``, :program:`mongod` will go into a quiet logging
   mode which will not log the following events/activities:

   - connection events;

   - the :dbcommand:`drop` command, the
     :dbcommand:`dropIndexes` command, the
     :dbcommand:`diagLogging` command, the
     :dbcommand:`validate` command, and the
     :dbcommand:`clean` command; and

   - replication synchronization activities.

   Consider the following example which sets the
   ``quiet`` to ``1``:

   .. code-block:: javascript

      db = db.getSiblingDB("admin")
      db.runCommand( { setParameter: 1, quiet: 1 } )

   .. seealso:: :setting:`~systemLog.quiet`

.. parameter:: textSearchEnabled

   .. deprecated:: 2.6
      MongoDB enables the text search feature by default.
      Manual enabling of this feature is unnecessary.

   Enables the :doc:`text search </core/index-text>` feature. When
   manually enabling, you must enable on **each and every**
   :program:`mongod` for replica sets.

.. parameter:: releaseConnectionsAfterResponse

   .. versionadded:: 2.2.4 and 2.4.2

   Changes the behavior of the connection pool that :program:`mongos` uses
   to connect to the shards. As a result, each :program:`mongos`
   should need to maintain fewer connections to each shard. When
   enabled, the :program:`mongos` will release a connection into the
   thread pool *after* each read operation or command.

   .. warning:: For applications that do not use the :ref:`default
      <write-concern-acknowledged>`, :ref:`journaled
      <write-concern-replica-journaled>`, or :ref:`replica
      acknowledged <write-concern-replica-acknowledged>` write concern
      modes of the driver, :parameter:`releaseConnectionsAfterResponse`
      will affect the meaning of :dbcommand:`getLastError`.

      If an application allows read operations in between
      write operations and :dbcommand:`getLastError` calls,
      the resulting :dbcommand:`getLastError` will **not** report on
      the success of the proceeding write operation.

      Use with caution.

   To enable, use the following command while connected to a
   :program:`mongos`:

   .. code-block:: javascript

      db.getSiblingDB('admin').runCommand( { setParameter: 1, releaseConnectionsAfterResponse: true } )

   Alternately, you may start the :program:`mongos` instance with the
   following run-time option:

   .. code-block:: sh

      mongos --setParameter releaseConnectionsAfterResponse=true

   To change this policy for the entire cluster, you must set
   :parameter:`releaseConnectionsAfterResponse` on each
   :program:`mongos` instance in the cluster.

.. parameter:: connPoolMaxShardedConnectionsPerHost

   .. versionadded:: 2.6

   *Default*: 200

   :program:`mongos` instances reuse connections to the shards to
   service multiple requests. However, in some situations very active
   :program:`mongos` instances may need to maintain a pool larger than
   the default maximum of 200 connections.

   The :parameter:`connPoolMaxShardedConnectionsPerHost` value
   defines the size of the largest connection pool that
   :program:`mongos` instances will maintain for communication to the
   shards. The size of a pool does not prevent :program:`mongos`
   instances from creating additional connections, but *does* prevent
   the connection pools from retaining connections above this limit.

   Increase the :parameter:`connPoolMaxShardedConnectionsPerHost`
   value **only** if the number of connections in a connection pool
   has a high level of churn, or if the total number of created
   connections increase.

   :parameter:`connPoolMaxShardedConnectionsPerHost` only applies for
   :program:`mongos` instances. You can only set
   :parameter:`connPoolMaxShardedConnectionsPerHost` during start up
   in the config file or on the command line, as follows to increase
   the size of the connection pool:

   .. code-block:: sh

      mongos --setParameter connPoolMaxShardedConnectionsPerHost=250

.. parameter:: connPoolMaxConnectionsPerHost

   .. versionadded:: 2.6

   *Default*: 200

   :program:`mongod` maintains connection pools for outgoing
   connections to other :program:`mongod` instances to maximize
   connection reuse.

   The :parameter:`connPoolMaxConnectionsPerHost` value defines the
   size of the largest connection pool that :program:`mongod`
   instances will maintain for inter-process communication. The size
   of a pool does not prevent a :program:`mongod` from creating
   additional connections, but *does* prevent a connection pool
   from retaining connections in excess of the value of
   :parameter:`connPoolMaxConnectionsPerHost`.

   **Only** adjust this setting if your driver does *not* pool
   connections and you're using authentication in the
   context of a sharded cluster.

   :parameter:`connPoolMaxShardedConnectionsPerHost` applies to
   :program:`mongod` instances only. You can only set
   :parameter:`connPoolMaxShardedConnectionsPerHost` during start up
   in the config file or on the command line, as follows to increase
   the size of the connection pool:

   .. code-block:: sh

      mongos --setParameter connPoolMaxConnectionsPerHost=250

.. parameter:: authenticationMechanisms

   .. versionchanged:: 2.6
      Added support for the ``PLAIN`` and ``MONGODB-X509`` authentication
      mechanisms.

   Specifies the list of authentication mechanisms the server accepts. Set
   this to one or more of the following values. If you specify multiple
   values, use a comma-separated list and no spaces. For descriptions
   of the authentication mechanisms, see :doc:`/core/authentication`.

   .. list-table::
      :header-rows: 1
      :widths: 20 40

      * - Value

        - Description

      * - MONGODB-CR

        - MongoDB challenge/response authentication.

      * - MONGODB-X509

        - MongoDB SSL certificate authentication.

      * - PLAIN

        - External authentication using LDAP. You can also use ``PLAIN``
          for authenticating in-database users. ``PLAIN`` transmits
          passwords in plain text. This mechanism is available only in
          `MongoDB Enterprise
          <http://www.mongodb.com/products/mongodb-enterprise>`_.

      * - GSSAPI

        - External authentication using Kerberos. This mechanism is
          available only in `MongoDB Enterprise
          <http://www.mongodb.com/products/mongodb-enterprise>`_.

   .. todo:: Per DOCS-2940, combine this with similar info in
             /includes/options-conf
             /includes/options-shared
             /reference/connection-string
             /tutorial/control-access-to-mongodb-windows-with-kerberos-authentication
             /tutorial/control-access-to-mongodb-with-kerberos-authentication.txt

   For example, to specify ``PLAIN`` as the authentication mechanism, use
   the following command:

   .. code-block:: sh

      mongod --setParameter authenticationMechanisms=PLAIN --auth

.. parameter:: saslauthdPath

   .. note:: Available only in MongoDB Enterprise (except MongoDB Enterprise for Windows).

   Specify the path to the Unix Domain Socket of the ``saslauthd``
   instance to use for proxy authentication.

.. parameter:: sslMode

   .. versionadded:: 2.6

   Set the :setting:`net.ssl.mode` to either ``preferSSL`` or
   ``requireSSL``. Useful during :doc:`rolling upgrade to SSL
   </tutorial/upgrade-cluster-to-ssl>` to minimize downtime.

   .. include:: /includes/fact-ssl-supported.rst

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, sslMode: "preferSSL" } )

.. parameter:: clusterAuthMode

   .. versionadded:: 2.6

   Set the :setting:`clusterAuthMode` to either ``sendX509`` or
   ``x509``. Useful during :ref:`rolling upgrade to use x509 for
   membership authentication <upgrade-to-x509-internal-authentication>`
   to minimize downtime.

   .. include:: /includes/fact-ssl-supported.rst

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, clusterAuthMode: "sendX509" } )

.. parameter:: ttlMonitorEnabled

   .. versionadded:: 2.4.6

   To support :doc:`TTL Indexes </core/index-ttl>`, :program:`mongod`
   instances have a background thread that is responsible for deleting
   documents from collections with TTL indexes.

   To disable this worker thread for a :program:`mongod`, set
   :parameter:`ttlMonitorEnabled` to ``false``, as in the following
   operations:

   .. code-block:: javascript

      db.getSiblingDB('admin').runCommand( { setParameter: 1, ttlMonitorEnabled: false } )

   Alternately, you may disable the thread at run-time by starting the
   :program:`mongod` instance with the following option:

   .. code-block:: sh

      mongod --setParameter ttlMonitorEnabled=false

.. parameter:: saslServiceName

   .. versionadded:: 2.4.6

   Allows users to override the default :doc:`Kerberos
   </tutorial/control-access-to-mongodb-with-kerberos-authentication>`
   service name component of the :doc:`Kerberos
   </tutorial/control-access-to-mongodb-with-kerberos-authentication>`
   principal name, on a per-instance basis. If unspecified, the
   default value is ``mongodb``.

   MongoDB only permits setting :parameter:`saslServiceName` at
   startup. The :dbcommand:`setParameter` command can not change
   this setting.

   :parameter:`saslServiceName` is only available in MongoDB
   Enterprise.

   .. important::

      Ensure that your driver supports alternate service names.

.. parameter:: newCollectionsUsePowerOf2Sizes

   .. versionadded:: 2.6

   *Default*: ``true``.

   :program:`mongod` uses an allocation strategy called
   :collflag:`usePowerOf2Sizes` where each record has a size, in bytes
   that is a power of 2 (e.g. 32, 64, 128, 256, 512...16777216.) The
   minimum allocation for a document is 32 bytes.

   MongoDB stores documents in contiguous spaces on disk, and each
   record includes both the document itself and some additional space
   to allow the document to grow slightly through updates.

   By default, all *new* collections, created after 2.6 use
   the :collflag:`usePowerOf2Sizes` strategy. To revert to the prior
   *exact fit allocation* strategy, set
   :parameter:`newCollectionsUsePowerOf2Sizes` to
   ``false``.

   New collections include those: created during
   :ref:`initial sync <replica-set-initial-sync>`, as well as those
   created by the :program:`mongorestore` and :program:`mongoimport`
   tools, by running :program:`mongod` with the :option:`--repair
   <mongod --repair>` option, as well as the
   :dbcommand:`restoreDatabase` command.

   Issue the following command to change the allocation strategy for a
   running :program:`mongod` instance:

   .. code-block:: javascript

      db.getSiblingDB('admin').runCommand( { setParameter: 1, newCollectionsUsePowerOf2Sizes: false } )

   You can also set :parameter:`newCollectionsUsePowerOf2Sizes`  at
   run-time with the following operation.

   .. code-block:: sh

      mongod --setParameter newCollectionsUsePowerOf2Sizes=false

.. parameter:: userCacheInvalidationIntervalSecs

   .. versionadded:: 2.6

   *Default*: 600.

   On a :program:`mongos` instance, this specifies the amount of time in
   seconds to allow before the :program:`mongos` instance purges the
   in-memory cache of user objects. The cache includes the users'
   credentials and roles.

   This parameter has a minimum value of ``30`` seconds and a maximum
   value of ``86400`` seconds (24 hours).

.. parameter:: failIndexKeyTooLong

   .. versionadded:: 2.6

   In MongoDB 2.6, if you attempt to insert or update a document so
   that the value of an indexed field is longer than the
   :limit:`Index Key Length Limit <Index Key>`, the operation
   will fail and return an error to the client. In previous versions
   of MongoDB, these operations would successfully insert or modify a
   document but the index or indexes would not include references to
   the document.

   To avoid this issue, consider using :doc:`hashed indexes
   </core/index-hashed>` or indexing a computed value. If you have an
   existing data set and want to disable this behavior so you can
   upgrade and then gradually resolve these indexing issues, you can
   use:parameter:`failIndexKeyTooLong` to disable this behavior.

   :parameter:`failIndexKeyTooLong` defaults to ``true``. When
   ``false``, a 2.6 :program:`mongod` instance will provide the 2.4
   behavior.

   Issue the following command to disable the index key length
   validation: for a running:program:`mongod` instance:

   .. code-block:: javascript

      db.getSiblingDB('admin').runCommand( { setParameter: 1, failIndexKeyTooLong: false } )

   You can also set :parameter:`failIndexKeyTooLong`  at
   run-time with the following operation.

   .. code-block:: sh

      mongod --setParameter failIndexKeyTooLong=false
.. _security-user-actions:

=================
Privilege Actions
=================

.. versionadded:: 2.6

.. default-domain:: mongodb

Privilege actions define the operations a user can perform on a :ref:`resource
<resource-document>`. A MongoDB :ref:`privilege <privileges>` comprises a
:ref:`resource <resource-document>` and the permitted actions. This page lists
available actions grouped by common purpose.

MongoDB provides built-in roles with pre-defined pairings of resources and
permitted actions. For lists of the actions granted, see
:doc:`/reference/built-in-roles`. To define custom roles, see
:doc:`/tutorial/define-roles`.

Query and Write Actions
-----------------------

.. authaction:: find

   User can perform the :method:`db.collection.find()` method. Apply this
   action to database or collection resources.

.. authaction:: insert

   User can perform the :dbcommand:`insert` command. Apply this action to
   database or collection resources.

.. authaction:: remove

   User can perform the :method:`db.collection.remove()` method. Apply this
   action to database or collection resources.

.. authaction:: update

   User can perform the :dbcommand:`update` command. Apply this action to
   database or collection resources.

Database Management Actions
---------------------------

.. authaction:: changeCustomData

   User can change the custom information of any user in the given
   database. Apply this action to database or collection resources.

.. authaction:: changeOwnCustomData

   Users can change their own custom information. Apply this action to
   database or collection resources.

.. authaction:: changeOwnPassword

   Users can change their own passwords. Apply this action to database or
   collection resources.

.. authaction:: changePassword

   User can change the password of any user in the given database. Apply
   this action to database or collection resources.

.. authaction:: createCollection

   User can perform the :method:`db.createCollection()` method. Apply this
   action to database or collection resources.

.. authaction:: createIndex

   Provides access to the :method:`db.collection.createIndex()` method
   and the :dbcommand:`createIndexes` command.
   Apply this action to database or collection resources.

.. authaction:: createRole

   User can create new roles in the given database. Apply this action to
   database or collection resources.

.. authaction:: createUser

   User can create new users in the given database. Apply this action to
   database or collection resources.

.. authaction:: dropCollection

   User can perform the :method:`db.collection.drop()` method. Apply this
   action to database or collection resources.

.. authaction:: dropRole

   User can delete any role from the given database. Apply this action to
   database or collection resources.

.. authaction:: dropUser

   User can remove any user from the given database. Apply this action to
   database or collection resources.

.. authaction:: emptycapped

   User can perform the :dbcommand:`emptycapped` command. Apply this
   action to database or collection resources.

.. authaction:: enableProfiler

   User can perform the :method:`db.setProfilingLevel()` method. Apply
   this action to database or collection resources.

.. authaction:: grantRole

   User can grant any role in the database to any user from any database
   in the system. Apply this action to database or collection resources.

.. authaction:: killCursors

   User can kill cursors on the target collection.

.. authaction:: revokeRole

   User can remove any role from any user from any database in the system.
   Apply this action to database or collection resources.

.. authaction:: unlock

   User can perform the :method:`db.fsyncUnlock()` method. Apply this
   action to the ``cluster`` resource.

.. authaction:: viewRole

   User can view information about any role in the given database. Apply
   this action to database or collection resources.

.. authaction:: viewUser

   User can view the information of any user in the given database. Apply
   this action to database or collection resources.

Deployment Management Actions
-----------------------------

.. authaction:: authSchemaUpgrade

   User can perform the :dbcommand:`authSchemaUpgrade` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: cleanupOrphaned

   User can perform the :dbcommand:`cleanupOrphaned` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: cpuProfiler

   User can enable and use the CPU profiler. Apply this action to the
   ``cluster`` resource.

.. authaction:: inprog

   User can use the :method:`db.currentOp()` method to return pending and
   active operations. Apply this action to the ``cluster`` resource.

.. authaction:: invalidateUserCache

   Provides access to the :dbcommand:`invalidateUserCache` command. Apply
   this action to the ``cluster`` resource.

.. authaction:: killop

   User can perform the :method:`db.killOp()` method. Apply this action to
   the ``cluster`` resource.

.. authaction:: planCacheRead

   User can perform the :dbcommand:`planCacheListPlans` and
   :dbcommand:`planCacheListQueryShapes` commands and the
   :method:`PlanCache.getPlansByQuery()` and
   :method:`PlanCache.listQueryShapes()` methods. Apply this action to
   database or collection resources.

.. authaction:: planCacheWrite

   User can perform the :dbcommand:`planCacheClear` command and the
   :method:`PlanCache.clear()` and :method:`PlanCache.clearPlansByQuery()`
   methods. Apply this action to database or collection resources.

.. authaction:: storageDetails

   User can perform the :dbcommand:`storageDetails` command. Apply this
   action to database or collection resources.

Replication Actions
-------------------

.. authaction:: appendOplogNote

   User can append notes to the oplog. Apply this action to the
   ``cluster`` resource.

.. authaction:: replSetConfigure

   User can configure a replica set. Apply this action to the ``cluster``
   resource.

.. authaction:: replSetGetStatus

   User can perform the :dbcommand:`replSetGetStatus` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: replSetHeartbeat

   User can perform the :dbcommand:`replSetHeartbeat` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: replSetStateChange

   User can change the state of a replica set through the
   :dbcommand:`replSetFreeze`, :dbcommand:`replSetMaintenance`,
   :dbcommand:`replSetStepDown`, and :dbcommand:`replSetSyncFrom`
   commands. Apply this action to the ``cluster`` resource.

.. authaction:: resync

   User can perform the :dbcommand:`resync` command. Apply this action to
   the ``cluster`` resource.

Sharding Actions
----------------

.. authaction:: addShard

   User can perform the :dbcommand:`addShard` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: enableSharding

   User can perform the :dbcommand:`enableSharding` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: flushRouterConfig

   User can perform the :dbcommand:`flushRouterConfig` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: getShardMap

   User can perform the :dbcommand:`getShardMap` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: getShardVersion

   User can perform the :dbcommand:`getShardVersion` command. Apply this
   action to database or collection resources.

.. authaction:: listShards

   User can perform the :dbcommand:`listShards` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: moveChunk

   User can perform the :dbcommand:`moveChunk` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: removeShard

   User can perform the :dbcommand:`removeShard` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: shardingState

   User can perform the :dbcommand:`shardingState` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: splitChunk

   User can perform the :dbcommand:`splitChunk` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: splitVector

   User can perform the :dbcommand:`splitVector` command. Apply this
   action to the ``cluster`` resource.

Server Administration Actions
-----------------------------

.. authaction:: applicationMessage

   User can perform the :dbcommand:`logApplicationMessage` command. Apply
   this action to the ``cluster`` resource.

.. authaction:: closeAllDatabases

   User can perform the :dbcommand:`closeAllDatabases` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: collMod

   User can perform the :dbcommand:`collMod` command. Apply this action to
   database or collection resources.

.. authaction:: compact

   User can perform the :dbcommand:`compact` command. Apply this action to
   database or collection resources.

.. authaction:: connPoolSync

   User can perform the :dbcommand:`connPoolSync` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: convertToCapped

   User can perform the :dbcommand:`convertToCapped` command. Apply this
   action to database or collection resources.

.. authaction:: dropDatabase

   User can perform the :dbcommand:`dropDatabase` command. Apply this action
   to database or collection resources.

.. authaction:: dropIndex

   User can perform the :dbcommand:`dropIndexes` command. Apply this action
   to database or collection resources.

.. authaction:: fsync

   User can perform the :dbcommand:`fsync` command. Apply this action to
   the ``cluster`` resource.

.. authaction:: getParameter

   User can perform the :dbcommand:`getParameter` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: hostInfo

   Provides information about the server the MongoDB instance runs on. Apply
   this action to the ``cluster`` resource.

.. authaction:: logRotate

   User can perform the :dbcommand:`logRotate` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: reIndex

   User can perform the :dbcommand:`reIndex` command. Apply this action to
   database or collection resources.

.. authaction:: renameCollectionSameDB

   Allows the user to rename collections on the current database using the
   :dbcommand:`renameCollection` command. Apply this action to database
   resources.

   Additionally, the user must either *have* :authaction:`find` on the
   source collection or *not have* :authaction:`find` on the destination
   collection.

   If a collection with the new name already exists, the user must also
   have the :authaction:`dropCollection` action on the destination
   collection.

   .. CITE: https://github.com/mongodb/mongo/blob/master/src/mongo/db/commands/rename_collection_common.cpp#L53-56

.. authaction:: repairDatabase

   User can perform the :dbcommand:`repairDatabase` command. Apply this
   action to database or collection resources.

.. authaction:: setParameter

   User can perform the :dbcommand:`setParameter` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: shutdown

   User can perform the :dbcommand:`shutdown` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: touch

   User can perform the :dbcommand:`touch` command. Apply this action to
   the ``cluster`` resource.

Diagnostic Actions
------------------

.. authaction:: collStats

   User can perform the :dbcommand:`collStats` command. Apply this action
   to database or collection resources.

.. authaction:: connPoolStats

   User can perform the :dbcommand:`connPoolStats` and :dbcommand:`shardConnPoolStats`
   commands. Apply this action to the ``cluster`` resource.

.. authaction:: cursorInfo

   User can perform the :dbcommand:`cursorInfo` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: dbHash

   User can perform the :dbcommand:`dbHash` command. Apply this action to
   database or collection resources.

.. authaction:: dbStats

   User can perform the :dbcommand:`dbStats` command. Apply this action to
   database or collection resources.

.. authaction:: diagLogging

   User can perform the :dbcommand:`diagLogging` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: getCmdLineOpts

   User can perform the :dbcommand:`getCmdLineOpts` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: getLog

   User can perform the :dbcommand:`getLog` command. Apply this action to
   the ``cluster`` resource.

.. authaction:: indexStats

   User can perform the :dbcommand:`indexStats` command. Apply this action
   to database or collection resources.

.. authaction:: listDatabases

   User can perform the :dbcommand:`listDatabases` command. Apply this
   action to the ``cluster`` resource.

.. authaction:: netstat

   User can perform the :dbcommand:`netstat` command. Apply this action to
   the ``cluster`` resource.

.. authaction:: serverStatus

   User can perform the :dbcommand:`serverStatus` command. Apply this action
   to the ``cluster`` resource.

.. authaction:: validate

   User can perform the :dbcommand:`validate` command. Apply this action
   to database or collection resources.

.. authaction:: top

   User can perform the :dbcommand:`top` command. Apply this action to the
   ``cluster`` resource.

Internal Actions
----------------

.. authaction:: anyAction

   Allows any action on a resource. **Do not** assign this action except
   for exceptional circumstances.

.. authaction:: internal

   Allows internal actions. **Do not** assign this action except for
   exceptional circumstances.
:orphan:

====================================
``system.users`` Privilege Documents
====================================

.. default-domain:: mongodb

.. deprecated:: 2.6
   MongoDB 2.6 introduced a new model for user
   credentials and privileges and no longer uses privilege documents.
   See :doc:`/reference/system-users-collection`.

For information on privilege documents, see :v2.4:`Privilege Documents
in the v2.4 Manual </reference/privilege-documents>`.
.. _bsondump:

============
``bsondump``
============

.. default-domain:: mongodb

Synopsis
--------

The :program:`bsondump` converts :term:`BSON` files into human-readable
formats, including :term:`JSON`. For example, :program:`bsondump` is useful
for reading the output files generated by :program:`mongodump`.

.. important:: :program:`bsondump` is a diagnostic tool for inspecting
   BSON files, not a tool for data ingestion or other application use.

Options
-------

.. binary:: bsondump

.. program:: bsondump

.. include:: /includes/option/option-bsondump-help.rst

.. include:: /includes/option/option-bsondump-verbose.rst

.. include:: /includes/option/option-bsondump-quiet.rst

.. include:: /includes/option/option-bsondump-version.rst

.. include:: /includes/option/option-bsondump-objcheck.rst

.. include:: /includes/option/option-bsondump-noobjcheck.rst

.. include:: /includes/option/option-bsondump-filter.rst

.. include:: /includes/option/option-bsondump-type.rst

.. include:: /includes/option/option-bsondump-<bsonFilename>.rst

Use
---

By default, :program:`bsondump` outputs data to standard output. To
create corresponding :term:`JSON` files, you will need to use the
shell redirect. See the following command:

.. code-block:: sh

   bsondump collection.bson > collection.json

Use the following command (at the system shell) to produce debugging
output for a :term:`BSON` file:

.. code-block:: sh

   bsondump --type=debug collection.bson
.. _mongo:

=========
``mongo``
=========

.. default-domain:: mongodb

.. only:: html

   .. meta::
      :description: The mongo command man page.
      :keywords: mongo, mongodb, man page, mongo process, mongo shell

Description
-----------

.. only:: (not man)

   .. binary:: mongo

:program:`mongo` is an interactive JavaScript shell interface to
MongoDB, which provides a powerful interface for systems
administrators as well as a way for developers to test queries and
operations directly with the database. :program:`mongo` also provides
a fully functional JavaScript environment for use with a MongoDB. This
document addresses the basic invocation of the :program:`mongo` shell
and an overview of its usage.

.. _mongo-shell-options:

Options
-------

Core Options
~~~~~~~~~~~~

.. program:: mongo

.. include:: /includes/option/option-mongo-shell.rst

.. include:: /includes/option/option-mongo-nodb.rst

.. include:: /includes/option/option-mongo-norc.rst

.. include:: /includes/option/option-mongo-quiet.rst

.. include:: /includes/option/option-mongo-port.rst

.. include:: /includes/option/option-mongo-host.rst

.. include:: /includes/option/option-mongo-eval.rst

.. include:: /includes/option/option-mongo-username.rst

.. include:: /includes/option/option-mongo-password.rst

.. include:: /includes/option/option-mongo-help.rst

.. include:: /includes/option/option-mongo-version.rst

.. include:: /includes/option/option-mongo-verbose.rst

.. include:: /includes/option/option-mongo-ipv6.rst

.. include:: /includes/option/option-mongo-<db address>.rst

.. _mongo-shell-file:

.. include:: /includes/option/option-mongo-<file.js>.rst

Authentication Options
~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/option/option-mongo-authenticationDatabase.rst

.. include:: /includes/option/option-mongo-authenticationMechanism.rst

SSL Options
~~~~~~~~~~~

.. include:: /includes/option/option-mongo-ssl.rst

.. include:: /includes/option/option-mongo-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongo-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongo-sslCAFile.rst

.. include:: /includes/option/option-mongo-sslCRLFile.rst

.. include:: /includes/option/option-mongo-sslFIPSMode.rst

.. include:: /includes/option/option-mongo-sslAllowInvalidCertificates.rst

Files
-----

.. _mongo-dbshell-file:

:file:`~/.dbshell`
   :program:`mongo` maintains a history of commands in the :file:`.dbshell`
   file.

   .. note::

      :program:`mongo` does not recorded interaction related to
      authentication in the history file, including
      :dbcommand:`authenticate` and :method:`db.addUser()`.

   .. warning::

      Versions of Windows :program:`mongo.exe` earlier than 2.2.0 will
      save the `.dbshell` file in the :program:`mongo.exe` working
      directory.

.. _mongo-mongorc-file:

:file:`~/.mongorc.js`
   :program:`mongo` will read the ``.mongorc.js`` file from the home
   directory of the user invoking :program:`mongo`. In the file, users
   can define variables, customize the :program:`mongo` shell prompt,
   or update information that they would like updated every time they
   launch a shell. If you use the shell to evaluate a JavaScript file
   or expression either on the command line with :option:`--eval` or
   by specifying :ref:`a .js file to mongo <mongo-shell-file>`,
   :program:`mongo` will read the ``.mongorc.js`` file *after* the
   JavaScript has finished processing.

   Specify the :option:`--norc` option to disable
   reading ``.mongorc.js``.

.. _mongo-global-mongorc-file:

:file:`/etc/mongorc.js`
   Global ``mongorc.js`` file which the :program:`mongo` shell
   evaluates upon start-up. If a user also has a :file:`.mongorc.js`
   file located in the :envvar:`HOME` directory, the :program:`mongo`
   shell evaluates the global :file:`/etc/mongorc.js` file *before*
   evaluating the user's :file:`.mongorc.js` file.

   :file:`/etc/mongorc.js` must have read permission for the user
   running the shell. The :option:`--norc` option for :program:`mongo`
   suppresses only the user's :file:`.mongorc.js` file.

   On Windows, the global :file:`mongorc.js </etc/mongorc.js>` exists
   in the :file:`%ProgramData%\\MongoDB` directory.

:file:`/tmp/mongo_edit{<time_t>}.js`
   Created by :program:`mongo` when editing a file. If the file exists,
   :program:`mongo` will append an integer from ``1`` to ``10`` to the
   time value to attempt to create a unique file.

:file:`%TEMP%\mongo_edit{<time_t>}.js`
   Created by :program:`mongo.exe` on Windows when editing a file. If
   the file exists, :program:`mongo` will append an integer from ``1``
   to ``10`` to the time value to attempt to create a unique file.


Environment
-----------

.. envvar:: EDITOR

   Specifies the path to an editor to use with the ``edit`` shell
   command.  A JavaScript variable ``EDITOR`` will override the value of
   :envvar:`EDITOR`.

.. envvar:: HOME

   Specifies the path to the home directory where :program:`mongo` will
   read the :file:`.mongorc.js` file and write the :file:`.dbshell`
   file.

.. envvar:: HOMEDRIVE

   On Windows systems, :envvar:`HOMEDRIVE` specifies the path the
   directory where :program:`mongo` will read the :file:`.mongorc.js`
   file and write the :file:`.dbshell` file.

.. envvar:: HOMEPATH

   Specifies the Windows path to the home directory where
   :program:`mongo` will read the :file:`.mongorc.js` file and write
   the :file:`.dbshell` file.

.. _mongo-keyboard-shortcuts:

Keyboard Shortcuts
------------------

The :program:`mongo` shell supports the following keyboard shortcuts:
[#multiple-bindings]_

.. list-table::
   :header-rows: 1

   * - **Keybinding**
     - **Function**

   * - Up arrow
     - Retrieve previous command from history

   * - Down-arrow
     - Retrieve next command from history

   * - Home
     - Go to beginning of the line

   * - End
     - Go to end of the line

   * - Tab
     - Autocomplete method/command

   * - Left-arrow
     - Go backward one character

   * - Right-arrow
     - Go forward one character

   * - Ctrl-left-arrow
     - Go backward one word

   * - Ctrl-right-arrow
     - Go forward one word

   * - Meta-left-arrow
     - Go backward one word

   * - Meta-right-arrow
     - Go forward one word

   * - Ctrl-A
     - Go to the beginning of the line

   * - Ctrl-B
     - Go backward one character

   * - Ctrl-C
     - Exit the :program:`mongo` shell

   * - Ctrl-D
     - Delete a char (or exit the :program:`mongo` shell)

   * - Ctrl-E
     - Go to the end of the line

   * - Ctrl-F
     - Go forward one character

   * - Ctrl-G
     - Abort

   * - Ctrl-J
     - Accept/evaluate the line

   * - Ctrl-K
     - Kill/erase the line

   * - Ctrl-L or type ``cls``
     - Clear the screen

   * - Ctrl-M
     - Accept/evaluate the line

   * - Ctrl-N
     - Retrieve next command from history

   * - Ctrl-P
     - Retrieve previous command from history

   * - Ctrl-R
     - Reverse-search command history

   * - Ctrl-S
     - Forward-search command history

   * - Ctrl-T
     - Transpose characters

   * - Ctrl-U
     - Perform Unix line-discard

   * - Ctrl-W
     - Perform Unix word-rubout

   * - Ctrl-Y
     - Yank

   * - Ctrl-Z
     - Suspend (job control works in linux)

   * - Ctrl-H
     - Backward-delete a character

   * - Ctrl-I
     - Complete, same as Tab

   * - Meta-B
     - Go backward one word

   * - Meta-C
     - Capitalize word

   * - Meta-D
     - Kill word

   * - Meta-F
     - Go forward one word

   * - Meta-L
     - Change word to lowercase

   * - Meta-U
     - Change word to uppercase

   * - Meta-Y
     - Yank-pop

   * - Meta-Backspace
     - Backward-kill word

   * - Meta-<
     - Retrieve the first command in command history

   * - Meta->
     - Retrieve the last command in command history

.. [#multiple-bindings] MongoDB accommodates multiple keybinding.
   Since 2.0, :program:`mongo` includes support for basic emacs
   keybindings.

.. _mongo-usage-examples:

Use
---

Typically users invoke the shell with the :program:`mongo` command at
the system prompt. Consider the following examples for other
scenarios.

To connect to a database on a remote host using authentication and a
non-standard port, use the following form:

.. code-block:: sh

   mongo --username <user> --password <pass> --host <host> --port 28015

Alternatively, consider the following short form:

.. code-block:: sh

   mongo -u <user> -p <pass> --host <host> --port 28015

Replace ``<user>``, ``<pass>``, and ``<host>`` with the appropriate
values for your situation and substitute or omit the :option:`--port`
as needed.

To execute a JavaScript file without evaluating the :file:`~/.mongorc.js`
file before starting a shell session, use the following form:

.. code-block:: sh

   mongo --shell --norc alternate-environment.js

To execute a JavaScript file with authentication, with password prompted
rather than provided on the command-line, use the following form:

.. code-block:: sh

   mongo script-file.js -u <user> -p

To print return a query as :term:`JSON`, from the system prompt using
the :option:`--eval <mongo --eval>` option, use the following form:

.. code-block:: sh

   mongo --eval 'db.collection.find().forEach(printjson)'

Use single quotes (e.g. ``'``) to enclose the JavaScript, as well as
the additional JavaScript required to generate this output.
==============
``mongod.exe``
==============

.. default-domain:: mongodb

Synopsis
--------

:program:`mongod.exe` is the build of the MongoDB daemon
(i.e. :program:`mongod`) for the Windows
platform. :program:`mongod.exe` has all of the features of
:program:`mongod` on Unix-like platforms and is completely compatible
with the other builds of :program:`mongod`. In addition,
:program:`mongod.exe` provides several options for interacting with
the Windows platform itself.

This document *only* references options that are unique to
:program:`mongod.exe`. All :program:`mongod` options are
available. See the :doc:`/reference/program/mongod` and the
:doc:`/reference/configuration-options` documents for more
information regarding :program:`mongod.exe`.

To install and use :program:`mongod.exe`, read the
:doc:`/tutorial/install-mongodb-on-windows` document.

Options
-------

.. binary:: mongod.exe

.. program:: mongod.exe

.. include:: /includes/option/option-mongod.exe-install.rst

.. include:: /includes/option/option-mongod.exe-remove.rst

.. include:: /includes/option/option-mongod.exe-reinstall.rst

.. include:: /includes/option/option-mongod.exe-serviceName.rst

.. include:: /includes/option/option-mongod.exe-serviceDisplayName.rst

.. include:: /includes/option/option-mongod.exe-serviceDescription.rst

.. include:: /includes/option/option-mongod.exe-serviceUser.rst

.. include:: /includes/option/option-mongod.exe-servicePassword.rst
.. _mongod:

==========
``mongod``
==========

.. default-domain:: mongodb

Synopsis
--------

:program:`mongod` is the primary daemon process for the MongoDB
system. It handles data requests, manages data format, and performs
background management operations.

This document provides a complete overview of all command line options
for :program:`mongod`. These options are primarily useful for testing
purposes. In common operation, use the :doc:`configuration file
options </reference/configuration-options>` to control the behavior of
your database, which is fully capable of all operations described
below.

Options
-------

.. only:: (not man)

   .. class:: hidden

      .. binary:: mongod

Core Options
~~~~~~~~~~~~

.. program:: mongod

.. include:: /includes/option/option-mongod-help.rst

.. include:: /includes/option/option-mongod-version.rst

.. include:: /includes/option/option-mongod-config.rst

.. include:: /includes/option/option-mongod-verbose.rst

.. include:: /includes/option/option-mongod-quiet.rst

.. include:: /includes/option/option-mongod-port.rst

.. include:: /includes/option/option-mongod-bind_ip.rst

.. include:: /includes/option/option-mongod-maxConns.rst

.. include:: /includes/option/option-mongod-syslog.rst

.. include:: /includes/option/option-mongod-syslogFacility.rst

.. include:: /includes/option/option-mongod-logpath.rst

.. include:: /includes/option/option-mongod-logappend.rst

.. include:: /includes/option/option-mongod-timeStampFormat.rst

.. include:: /includes/option/option-mongod-diaglog.rst

.. include:: /includes/option/option-mongod-traceExceptions.rst

.. include:: /includes/option/option-mongod-pidfilepath.rst

.. include:: /includes/option/option-mongod-keyFile.rst

.. include:: /includes/option/option-mongod-setParameter.rst

.. include:: /includes/option/option-mongod-httpinterface.rst

.. include:: /includes/option/option-mongod-nohttpinterface.rst

.. include:: /includes/option/option-mongod-clusterAuthMode.rst

.. include:: /includes/option/option-mongod-nounixsocket.rst

.. include:: /includes/option/option-mongod-unixSocketPrefix.rst

.. include:: /includes/option/option-mongod-fork.rst

.. include:: /includes/option/option-mongod-auth.rst

.. include:: /includes/option/option-mongod-noauth.rst

.. include:: /includes/option/option-mongod-ipv6.rst

.. include:: /includes/option/option-mongod-jsonp.rst

.. include:: /includes/option/option-mongod-rest.rst

.. include:: /includes/option/option-mongod-slowms.rst

.. include:: /includes/option/option-mongod-profile.rst

.. include:: /includes/option/option-mongod-cpu.rst

.. include:: /includes/option/option-mongod-sysinfo.rst

.. include:: /includes/option/option-mongod-dbpath.rst

.. include:: /includes/option/option-mongod-directoryperdb.rst

.. include:: /includes/option/option-mongod-noIndexBuildRetry.rst

.. include:: /includes/option/option-mongod-noprealloc.rst

.. include:: /includes/option/option-mongod-nssize.rst

.. include:: /includes/option/option-mongod-quota.rst

.. include:: /includes/option/option-mongod-quotaFiles.rst

.. include:: /includes/option/option-mongod-smallfiles.rst

.. include:: /includes/option/option-mongod-syncdelay.rst

.. include:: /includes/option/option-mongod-upgrade.rst

.. include:: /includes/option/option-mongod-repair.rst

.. include:: /includes/option/option-mongod-repairpath.rst

.. include:: /includes/option/option-mongod-objcheck.rst

.. include:: /includes/option/option-mongod-noobjcheck.rst

.. include:: /includes/option/option-mongod-noscripting.rst

.. include:: /includes/option/option-mongod-notablescan.rst

.. include:: /includes/option/option-mongod-journal.rst

.. include:: /includes/option/option-mongod-nojournal.rst

.. include:: /includes/option/option-mongod-journalOptions.rst

.. include:: /includes/option/option-mongod-journalCommitInterval.rst

.. include:: /includes/option/option-mongod-shutdown.rst

.. _cli-mongod-replica-set:

Replication Options
~~~~~~~~~~~~~~~~~~~

.. include:: /includes/option/option-mongod-replSet.rst

.. include:: /includes/option/option-mongod-oplogSize.rst

.. include:: /includes/option/option-mongod-replIndexPrefetch.rst

.. _cli-mongod-master-slave:

Master-Slave Replication
~~~~~~~~~~~~~~~~~~~~~~~~

These options provide access to conventional master-slave database
replication. While this functionality remains accessible in MongoDB,
replica sets are the preferred configuration for database replication.

.. include:: /includes/option/option-mongod-master.rst

.. include:: /includes/option/option-mongod-slave.rst

.. include:: /includes/option/option-mongod-source.rst

.. include:: /includes/option/option-mongod-only.rst

.. include:: /includes/option/option-mongod-slavedelay.rst

.. include:: /includes/option/option-mongod-autoresync.rst

.. include:: /includes/option/option-mongod-fastsync.rst

Sharded Cluster Options
~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/option/option-mongod-configsvr.rst

.. include:: /includes/option/option-mongod-shardsvr.rst

.. include:: /includes/option/option-mongod-moveParanoia.rst

SSL Options
~~~~~~~~~~~

.. see:: :doc:`/tutorial/configure-ssl` for full
   documentation of MongoDB's support.

.. include:: /includes/option/option-mongod-ssl.rst

.. include:: /includes/option/option-mongod-sslMode.rst

.. include:: /includes/option/option-mongod-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongod-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongod-sslClusterFile.rst

.. include:: /includes/option/option-mongod-sslClusterPassword.rst

.. include:: /includes/option/option-mongod-sslCAFile.rst

.. include:: /includes/option/option-mongod-sslCRLFile.rst

.. include:: /includes/option/option-mongod-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongod-sslWeakCertificateValidation.rst

.. include:: /includes/option/option-mongod-sslFIPSMode.rst

Audit Options
~~~~~~~~~~~~~

.. include:: /includes/option/option-mongod-auditDestination.rst

.. include:: /includes/option/option-mongod-auditFormat.rst

.. include:: /includes/option/option-mongod-auditPath.rst

.. include:: /includes/option/option-mongod-auditFilter.rst

SNMP Options
~~~~~~~~~~~~

.. include:: /includes/option/option-mongod-snmp-subagent.rst

.. include:: /includes/option/option-mongod-snmp-master.rst
.. _mongodump:

=============
``mongodump``
=============

.. default-domain:: mongodb

.. |tool-binary| replace:: mongodump

Synopsis
--------

:program:`mongodump` is a utility for creating a binary export of the
contents of a database. Consider using this utility as part an
effective :doc:`backup strategy </core/backups>`. Use
:program:`mongodump` in conjunction with :program:`mongorestore` to
restore databases.

:program:`mongodump` can read data from either :program:`mongod` or :program:`mongos`
instances, in addition to reading directly from MongoDB data files
without an active :program:`mongod`.

.. seealso:: :program:`mongorestore`,
   :doc:`/tutorial/backup-sharded-cluster-with-database-dumps`
   and :doc:`/core/backups`.

Behavior
--------

.. include:: /includes/fact-mongodump-local-database.rst

.. include:: /includes/warning-mongodump-compatibility-2.2.rst

When running :program:`mongodump` against a :program:`mongos` instance
where the :term:`sharded cluster` consists of :term:`replica sets <replica
set>`, the :term:`read preference` of the operation will prefer reads
from :term:`secondary` members of the set.

.. include:: /includes/warning-fsync-lock-mongodump.rst

Required Access
---------------

Backup Collections
~~~~~~~~~~~~~~~~~~

.. include:: /includes/access-mongodump-collections.rst

Backup Users
~~~~~~~~~~~~

.. include:: /includes/access-mongodump-users.rst

Options
-------

.. binary:: mongodump

.. program:: mongodump

.. include:: /includes/option/option-mongodump-help.rst

.. include:: /includes/option/option-mongodump-verbose.rst

.. include:: /includes/option/option-mongodump-quiet.rst

.. include:: /includes/option/option-mongodump-version.rst

.. include:: /includes/option/option-mongodump-host.rst

.. include:: /includes/option/option-mongodump-port.rst

.. include:: /includes/option/option-mongodump-ipv6.rst

.. include:: /includes/option/option-mongodump-ssl.rst

.. include:: /includes/option/option-mongodump-sslCAFile.rst

.. include:: /includes/option/option-mongodump-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongodump-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongodump-sslCRLFile.rst

.. include:: /includes/option/option-mongodump-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongodump-sslFIPSMode.rst

.. include:: /includes/option/option-mongodump-username.rst

.. include:: /includes/option/option-mongodump-password.rst

.. include:: /includes/option/option-mongodump-authenticationDatabase.rst

.. include:: /includes/option/option-mongodump-authenticationMechanism.rst

.. include:: /includes/option/option-mongodump-dbpath.rst

.. include:: /includes/option/option-mongodump-directoryperdb.rst

.. include:: /includes/option/option-mongodump-journal.rst

.. include:: /includes/option/option-mongodump-db.rst

.. include:: /includes/option/option-mongodump-collection.rst

.. include:: /includes/option/option-mongodump-out.rst

.. include:: /includes/option/option-mongodump-query.rst

.. include:: /includes/option/option-mongodump-oplog.rst

.. include:: /includes/option/option-mongodump-repair.rst

.. include:: /includes/option/option-mongodump-forceTableScan.rst

.. include:: /includes/option/option-mongodump-dumpDbUsersAndRoles.rst

Use
---

See the :doc:`/tutorial/backup-with-mongodump`
for a larger overview of :program:`mongodump` usage. Also see the
:doc:`mongorestore` document for an overview of the
:program:`mongorestore`, which provides the related inverse
functionality.

The following command creates a dump file that contains only the
collection named ``collection`` in the database named ``test``. In
this case the database is running on the local interface on port
``27017``:

.. code-block:: sh

   mongodump --collection collection --db test

In the next example, :program:`mongodump` creates a backup of the
database instance stored in the ``/srv/mongodb`` directory on the
local machine. This requires that no :program:`mongod` instance is
using the ``/srv/mongodb`` directory.

.. code-block:: sh

   mongodump --dbpath /srv/mongodb

In the final example, :program:`mongodump` creates a database dump
located at ``/opt/backup/mongodump-2011-10-24``, from a database
running on port ``37017`` on the host ``mongodb1.example.net`` and
authenticating using the username ``user`` and the password
``pass``, as follows:

.. code-block:: sh

   mongodump --host mongodb1.example.net --port 37017 --username user --password pass --out /opt/backup/mongodump-2011-10-24

.. _mongodump-behavior:
.. _mongoexport:

===============
``mongoexport``
===============

.. default-domain:: mongodb
.. |tool-binary| replace:: mongoexport

Synopsis
--------

:program:`mongoexport` is a utility that produces a JSON or CSV export
of data stored in a MongoDB instance. See the
:doc:`/core/import-export` document for a more in depth
usage overview, and the :doc:`mongoimport` document for more
information regarding the :program:`mongoimport` utility, which
provides the inverse "importing" capability.

Considerations
--------------

Do not use :program:`mongoimport` and :program:`mongoexport` for
full-scale production backups because they may not reliably capture
data type information. Use :program:`mongodump` and
:program:`mongorestore` as described in :doc:`/core/backups` for this
kind of functionality.

Options
-------

.. binary:: mongoexport

.. program:: mongoexport

.. include:: /includes/option/option-mongoexport-help.rst

.. include:: /includes/option/option-mongoexport-verbose.rst

.. include:: /includes/option/option-mongoexport-quiet.rst

.. include:: /includes/option/option-mongoexport-version.rst

.. include:: /includes/option/option-mongoexport-host.rst

.. include:: /includes/option/option-mongoexport-port.rst

.. include:: /includes/option/option-mongoexport-ipv6.rst

.. include:: /includes/option/option-mongoexport-ssl.rst

.. include:: /includes/option/option-mongoexport-sslCAFile.rst

.. include:: /includes/option/option-mongoexport-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongoexport-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongoexport-sslCRLFile.rst

.. include:: /includes/option/option-mongoexport-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongoexport-sslFIPSMode.rst

.. include:: /includes/option/option-mongoexport-username.rst

.. include:: /includes/option/option-mongoexport-password.rst

.. include:: /includes/option/option-mongoexport-authenticationDatabase.rst

.. include:: /includes/option/option-mongoexport-authenticationMechanism.rst

.. include:: /includes/option/option-mongoexport-dbpath.rst

.. include:: /includes/option/option-mongoexport-directoryperdb.rst

.. include:: /includes/option/option-mongoexport-journal.rst

.. include:: /includes/option/option-mongoexport-db.rst

.. include:: /includes/option/option-mongoexport-collection.rst

.. include:: /includes/option/option-mongoexport-fields.rst

.. include:: /includes/option/option-mongoexport-fieldFile.rst

.. include:: /includes/option/option-mongoexport-query.rst

.. include:: /includes/option/option-mongoexport-csv.rst

.. include:: /includes/option/option-mongoexport-out.rst

.. include:: /includes/option/option-mongoexport-jsonArray.rst

.. include:: /includes/option/option-mongoexport-slaveOk.rst

.. include:: /includes/option/option-mongoexport-forceTableScan.rst

.. include:: /includes/option/option-mongoexport-skip.rst

.. include:: /includes/option/option-mongoexport-limit.rst

.. include:: /includes/option/option-mongoexport-sort.rst

Use
---

Export in CSV Format
~~~~~~~~~~~~~~~~~~~~

In the following example, :program:`mongoexport` exports the
collection ``contacts`` from the ``users`` database from the
:program:`mongod` instance running on the localhost port number
``27017``. This command writes the export data in :term:`CSV` format
into a file located at ``/opt/backups/contacts.csv``.  The
``fields.txt`` file contains a line-separated list of fields to
export.

.. code-block:: sh

   mongoexport --db users --collection contacts --csv --fieldFile fields.txt --out /opt/backups/contacts.csv

Export in JSON Format
~~~~~~~~~~~~~~~~~~~~~

The next example creates an export of the collection ``contacts``
from the MongoDB instance running on the localhost port number ``27017``,
with journaling explicitly enabled. This writes the export to the
``contacts.json`` file in :term:`JSON` format.

.. code-block:: sh

   mongoexport --db sales --collection contacts --out contacts.json --journal

Export Collection Directly From Data Files
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example exports the collection ``contacts`` from the
``sales`` database located in the MongoDB data files located at
``/srv/mongodb/``. This operation writes the export to standard output
in :term:`JSON` format.

.. code-block:: sh

   mongoexport --db sales --collection contacts --dbpath /srv/mongodb/

.. warning::

   The above example will only succeed if there is no :program:`mongod`
   connected to the data files located in the ``/srv/mongodb/``
   directory.

Export from Remote Host Running with Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following example exports the collection ``contacts`` from the
database ``marketing`` . This data resides on the MongoDB instance
located on the host ``mongodb1.example.net`` running on port ``37017``,
which requires the username ``user`` and the password ``pass``.

.. code-block:: sh

   mongoexport --host mongodb1.example.net --port 37017 --username user --password pass --collection contacts --db marketing --out mdb1-examplenet.json

.. _mongoexport-type-fidelity:

Type Fidelity
-------------

.. include:: /includes/warning-type-fidelity-loss.rst

JSON can only represent a subset of the types supported by BSON. To
preserve type information, :program:`mongoexport` uses the :doc:`strict
mode representation </reference/mongodb-extended-json>` for certain
types.

For example, the following insert operation in the :program:`mongo`
shell uses the :doc:`mongoShell mode representation
</reference/mongodb-extended-json>` for the BSON types
:bsontype:`data_date` and :bsontype:`data_numberlong`:

.. code-block:: javascript

   use test
   db.traffic.insert( { _id: 1, volume: NumberLong(2980000), date: new Date() } )

Use :program:`mongoexport` to export the data:

.. code-block:: none

   mongoexport --db test --collection traffic --out traffic.json

The exported data is in :doc:`strict mode representation
</reference/mongodb-extended-json>` to preserve type information:

.. code-block:: javascript

   { "_id" : 1, "volume" : { "$numberLong" : "2980000" }, "date" : { "$date" : "2014-03-13T13:47:42.483-0400" } }

See :doc:`/reference/mongodb-extended-json` for a complete list of
these types and the representations used.
==============
``mongofiles``
==============

.. default-domain:: mongodb

.. |tool-binary| replace:: mongofiles

.. only:: (not man)

   .. binary:: mongofiles

Synopsis
--------

The :program:`mongofiles` utility makes it possible to manipulate files
stored in your MongoDB instance in :term:`GridFS` objects from the
command line. It is particularly useful as it provides an interface
between objects stored in your file system and GridFS.

All :program:`mongofiles` commands have the following form:

.. code-block:: sh

   mongofiles <options> <commands> <filename>

The components of the :program:`mongofiles` command are:

1. :ref:`Options <mongofiles-options>`. You may use one or more of
   these options to control the behavior of :program:`mongofiles`.

2. :ref:`Commands <mongofiles-commands>`. Use one of these commands to
   determine the action of :program:`mongofiles`.

3. A filename which is either: the name of a file on your local's file
   system, or a GridFS object.

:program:`mongofiles`, like :program:`mongodump`, :program:`mongoexport`,
:program:`mongoimport`, and :program:`mongorestore`, can access data
stored in a MongoDB data directory without requiring a running
:program:`mongod` instance, if no other :program:`mongod` is running.

.. important:: For :term:`replica sets <replica set>`,
   :program:`mongofiles` can only read from the set's
   ':term:`primary`.

.. _mongofiles-options:

Options
-------

.. program:: mongofiles

.. include:: /includes/option/option-mongofiles-help.rst

.. include:: /includes/option/option-mongofiles-verbose.rst

.. include:: /includes/option/option-mongofiles-quiet.rst

.. include:: /includes/option/option-mongofiles-version.rst

.. include:: /includes/option/option-mongofiles-host.rst

.. include:: /includes/option/option-mongofiles-port.rst

.. include:: /includes/option/option-mongofiles-ipv6.rst

.. include:: /includes/option/option-mongofiles-ssl.rst

.. include:: /includes/option/option-mongofiles-sslCAFile.rst

.. include:: /includes/option/option-mongofiles-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongofiles-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongofiles-sslCRLFile.rst

.. include:: /includes/option/option-mongofiles-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongofiles-sslFIPSMode.rst

.. include:: /includes/option/option-mongofiles-username.rst

.. include:: /includes/option/option-mongofiles-password.rst

.. include:: /includes/option/option-mongofiles-authenticationDatabase.rst

.. include:: /includes/option/option-mongofiles-authenticationMechanism.rst

.. include:: /includes/option/option-mongofiles-dbpath.rst

.. include:: /includes/option/option-mongofiles-directoryperdb.rst

.. include:: /includes/option/option-mongofiles-journal.rst

.. include:: /includes/option/option-mongofiles-db.rst

.. include:: /includes/option/option-mongofiles-collection.rst

.. include:: /includes/option/option-mongofiles-local.rst

.. include:: /includes/option/option-mongofiles-type.rst

.. include:: /includes/option/option-mongofiles-replace.rst

.. _mongofiles-commands:

Commands
--------

.. describe:: list <prefix>

   Lists the files in the GridFS store. The characters specified after
   ``list`` (e.g. ``<prefix>``) optionally limit the list of
   returned items to files that begin with that string of characters.

.. describe:: search <string>

   Lists the files in the GridFS store with names that match any
   portion of ``<string>``.

.. describe:: put <filename>

   Copy the specified file from the local file system into GridFS
   storage.

   Here, ``<filename>`` refers to the name the object will have in
   GridFS, and :program:`mongofiles` assumes that this reflects the name the
   file has on the local file system. If the local filename is
   different use the :option:`mongofiles --local` option.

.. describe:: get <filename>

   Copy the specified file from GridFS storage to the local file
   system.

   Here, ``<filename>`` refers to the name the object will have in
   GridFS, and :program:`mongofiles` assumes that this reflects the name the
   file has on the local file system. If the local filename is
   different use the :option:`mongofiles --local` option.

.. describe:: delete <filename>

   Delete the specified file from GridFS storage.

Examples
--------

To return a list of all files in a :term:`GridFS` collection in the
``records`` database, use the following invocation at the system shell:

.. code-block:: sh

   mongofiles -d records list

This :program:`mongofiles` instance will connect to the
:program:`mongod` instance running on the ``27017`` localhost
interface to specify the same operation on a different port or
hostname, and issue a command that resembles one of the following:

.. code-block:: sh

   mongofiles --port 37017 -d records list
   mongofiles --hostname db1.example.net -d records list
   mongofiles --hostname db1.example.net --port 37017 -d records list

Modify any of the following commands as needed if you're connecting
the :program:`mongod` instances on different ports or hosts.

To upload a file named ``32-corinth.lp`` to the GridFS collection in
the ``records`` database, you can use the following command:

.. code-block:: sh

   mongofiles -d records put 32-corinth.lp

To delete the ``32-corinth.lp`` file from this GridFS collection in
the ``records`` database, you can use the following command:

.. code-block:: sh

   mongofiles -d records delete 32-corinth.lp

To search for files in the GridFS collection in the ``records``
database that have the string ``corinth`` in their names, you can use
following command:

.. code-block:: sh

   mongofiles -d records search corinth

To list all files in the GridFS collection in the ``records`` database
that begin with the string ``32``, you can use the following command:

.. code-block:: sh

   mongofiles -d records list 32

To fetch the file from the GridFS collection in the ``records``
database named ``32-corinth.lp``, you can use the following command:

.. code-block:: sh

   mongofiles -d records get 32-corinth.lp
.. _mongoimport:

===============
``mongoimport``
===============

.. default-domain:: mongodb
.. |tool-binary| replace:: mongoimport

Synopsis
--------

The :program:`mongoimport` tool provides a route to import content from a
JSON, CSV, or TSV export created by :program:`mongoexport`, or
potentially, another third-party export tool. See the
:doc:`/core/import-export` document for a more in depth
usage overview, and the :doc:`mongoexport` document for more
information regarding :program:`mongoexport`, which
provides the inverse "exporting" capability.

Considerations
--------------

Do not use :program:`mongoimport` and :program:`mongoexport` for
full instance, production backups because they will not reliably capture data type
information. Use :program:`mongodump` and :program:`mongorestore` as
described in :doc:`/core/backups` for this kind of
functionality.

Options
-------

.. binary:: mongoimport

.. program:: mongoimport

.. include:: /includes/option/option-mongoimport-help.rst

.. include:: /includes/option/option-mongoimport-verbose.rst

.. include:: /includes/option/option-mongoimport-quiet.rst

.. include:: /includes/option/option-mongoimport-version.rst

.. include:: /includes/option/option-mongoimport-host.rst

.. include:: /includes/option/option-mongoimport-port.rst

.. include:: /includes/option/option-mongoimport-ipv6.rst

.. include:: /includes/option/option-mongoimport-ssl.rst

.. include:: /includes/option/option-mongoimport-sslCAFile.rst

.. include:: /includes/option/option-mongoimport-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongoimport-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongoimport-sslCRLFile.rst

.. include:: /includes/option/option-mongoimport-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongoimport-sslFIPSMode.rst

.. include:: /includes/option/option-mongoimport-username.rst

.. include:: /includes/option/option-mongoimport-password.rst

.. include:: /includes/option/option-mongoimport-authenticationDatabase.rst

.. include:: /includes/option/option-mongoimport-authenticationMechanism.rst

.. include:: /includes/option/option-mongoimport-dbpath.rst

.. include:: /includes/option/option-mongoimport-directoryperdb.rst

.. include:: /includes/option/option-mongoimport-journal.rst

.. include:: /includes/option/option-mongoimport-db.rst

.. include:: /includes/option/option-mongoimport-collection.rst

.. include:: /includes/option/option-mongoimport-fields.rst

.. include:: /includes/option/option-mongoimport-fieldFile.rst

.. include:: /includes/option/option-mongoimport-ignoreBlanks.rst

.. include:: /includes/option/option-mongoimport-type.rst

.. include:: /includes/option/option-mongoimport-file.rst

.. include:: /includes/option/option-mongoimport-drop.rst

.. include:: /includes/option/option-mongoimport-headerline.rst

.. include:: /includes/option/option-mongoimport-upsert.rst

.. include:: /includes/option/option-mongoimport-upsertFields.rst

.. include:: /includes/option/option-mongoimport-stopOnError.rst

.. include:: /includes/option/option-mongoimport-jsonArray.rst

Use
---

In this example, :program:`mongoimport` imports the :term:`csv`
formatted data in the ``/opt/backups/contacts.csv`` into the
collection ``contacts`` in the ``users`` database on the MongoDB
instance running on the localhost port numbered ``27017``.

.. code-block:: sh

   mongoimport --db users --collection contacts --type csv --file /opt/backups/contacts.csv

Since :program:`mongoimport` uses the input file name (minus the
extension) as the collection name if ``-c`` or ``--collection`` is
unspecified, the following example is equivalent to the previous
example:

.. code-block:: none

   mongoimport --db users --type csv --file /opt/backups/contacts.csv

In the following example, :program:`mongoimport` imports the data in
the :term:`JSON` formatted file ``contacts.json`` into the collection
``contacts`` on the MongoDB instance running on the localhost port
number 27017.

.. code-block:: sh

   mongoimport --collection contacts --file contacts.json

In the next example, :program:`mongoimport` takes data passed to it on
standard input (i.e. with a ``|`` pipe.)  and imports it into the
MongoDB datafiles located at ``/srv/mongodb/``. if the import process
encounters an error, the :program:`mongoimport` will halt because of
the :option:`--stopOnError <mongoimport --stopOnError>` option.

.. code-block:: sh

   mongoimport --db sales --collection contacts --stopOnError --dbpath /srv/mongodb/

In the final example, :program:`mongoimport` imports data from the
file ``/opt/backups/mdb1-examplenet.json`` into the collection
``contacts`` within the database ``marketing`` on a remote MongoDB
database. This :program:`mongoimport` accesses the :program:`mongod`
instance running on the host ``mongodb1.example.net`` over port
``37017``, which requires the username ``user`` and the password
``pass``.

.. code-block:: sh

   mongoimport --host mongodb1.example.net --port 37017 --username user --password pass --collection contacts --db marketing --file /opt/backups/mdb1-examplenet.json

.. _mongoimport-type-fidelity:

Type Fidelity
-------------

.. include:: /includes/warning-type-fidelity-loss.rst

JSON can only represent a subset of the types supported by BSON. To
preserve type information, :program:`mongoimport` accepts :doc:`strict
mode representation </reference/mongodb-extended-json>` for certain
types.

For example, to preserve type information for BSON types
:bsontype:`data_date` and :bsontype:`data_numberlong` during
:program:`mongoimport`, the data should be in strict mode
representation, as in the following:

.. code-block:: javascript

   { "_id" : 1, "volume" : { "$numberLong" : "2980000" }, "date" : { "$date" : "2014-03-13T13:47:42.483-0400" } }

For the :bsontype:`data_numberlong` type, :program:`mongoimport`
converts into a float during the import.

See :doc:`/reference/mongodb-extended-json` for a complete list of
these types and the representations used.
.. _mongooplog:

==============
``mongooplog``
==============

.. default-domain:: mongodb
.. |tool-binary| replace:: mongooplog

.. versionadded:: 2.2

Synopsis
--------

:program:`mongooplog` is a simple tool that polls operations from
the :term:`replication` :term:`oplog` of a remote server, and applies
them to the local server. This capability supports certain classes of
real-time migrations that require that the source server remain online
and in operation throughout the migration process.

Typically this command will take the following form:

.. code-block:: sh

   mongooplog  --from mongodb0.example.net --host mongodb1.example.net

This command copies oplog entries from the :program:`mongod` instance
running on the host ``mongodb0.example.net`` and duplicates
operations to the host ``mongodb1.example.net``. If you do not need
to keep the :option:`--from <mongooplog --from>` host running during
the migration, consider using :program:`mongodump` and
:program:`mongorestore` or another :doc:`backup
</core/backups>` operation, which may be better suited to
your operation.

.. note::

   If the :program:`mongod` instance specified by the :option:`--from <mongooplog --from>`
   argument is running with :setting:`authentication <auth>`, then
   :program:`mongooplog` will not be able to copy oplog entries.

.. seealso:: :program:`mongodump`, :program:`mongorestore`,
   :doc:`/core/backups`, :doc:`/core/replica-set-oplog`.

Options
-------

.. binary:: mongooplog

.. program:: mongooplog

.. include:: /includes/option/option-mongooplog-help.rst

.. include:: /includes/option/option-mongooplog-verbose.rst

.. include:: /includes/option/option-mongooplog-quiet.rst

.. include:: /includes/option/option-mongooplog-version.rst

.. include:: /includes/option/option-mongooplog-host.rst

.. include:: /includes/option/option-mongooplog-port.rst

.. include:: /includes/option/option-mongooplog-ipv6.rst

.. include:: /includes/option/option-mongooplog-ssl.rst

.. include:: /includes/option/option-mongooplog-sslCAFile.rst

.. include:: /includes/option/option-mongooplog-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongooplog-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongooplog-sslCRLFile.rst

.. include:: /includes/option/option-mongooplog-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongooplog-sslFIPSMode.rst

.. include:: /includes/option/option-mongooplog-username.rst

.. include:: /includes/option/option-mongooplog-password.rst

.. include:: /includes/option/option-mongooplog-authenticationDatabase.rst

.. include:: /includes/option/option-mongooplog-authenticationMechanism.rst

.. include:: /includes/option/option-mongooplog-dbpath.rst

.. include:: /includes/option/option-mongooplog-directoryperdb.rst

.. include:: /includes/option/option-mongooplog-journal.rst

.. include:: /includes/option/option-mongooplog-db.rst

.. include:: /includes/option/option-mongooplog-collection.rst

.. include:: /includes/option/option-mongooplog-seconds.rst

.. include:: /includes/option/option-mongooplog-from.rst

.. include:: /includes/option/option-mongooplog-oplogns.rst

Use
---

Consider the following prototype :program:`mongooplog` command:

.. code-block:: sh

   mongooplog  --from mongodb0.example.net --host mongodb1.example.net

Here, entries from the :term:`oplog` of the :program:`mongod` running
on port ``27017``. This only pull entries from the last 24 hours.

Use the :option:`--seconds <mongooplog --seconds>` argument to capture
a greater or smaller amount of time. Consider the following example:

.. code-block:: sh

   mongooplog  --from mongodb0.example.net --seconds 172800

In this operation, :program:`mongooplog` captures 2 full days of
operations. To migrate 12 hours of :term:`oplog` entries, use the
following form:

.. code-block:: sh

   mongooplog  --from mongodb0.example.net --seconds 43200

For the previous two examples, :program:`mongooplog` migrates entries
to the :program:`mongod` process running on the localhost interface
connected to the ``27017`` port. :program:`mongooplog` can also
operate directly on MongoDB's data files if no :program:`mongod` is
running on the *target* host. Consider the following example:

.. code-block:: sh

   mongooplog  --from mongodb0.example.net --dbpath /srv/mongodb --journal

Here, :program:`mongooplog` imports :term:`oplog` operations from the
:program:`mongod` host connected to port ``27017``. This migrates
operations to the MongoDB data files stored in the ``/srv/mongodb``
directory. Additionally :program:`mongooplog` will use the durability
:term:`journal` to ensure that the data files remain valid.
.. _mongoperf:

=============
``mongoperf``
=============

.. http://www.mongodb.org/display/DOCS/mongoperf

.. default-domain:: mongodb

Synopsis
--------

:program:`mongoperf` is a utility to check disk I/O performance
independently of MongoDB.

It times tests of random disk I/O and presents the results. You can
use :program:`mongoperf` for any case apart from MongoDB. The
:setting:`~mongoperf.mmf` ``true`` mode is completely generic. In
that mode it is somewhat analogous to tools such as `bonnie++
<http://sourceforge.net/projects/bonnie/>`_ (albeit mongoperf is
simpler).

Specify options to :program:`mongoperf` using a JavaScript document.

.. seealso::

   * `bonnie <http://www.textuality.com/bonnie/>`_
   * `bonnie++ <http://sourceforge.net/projects/bonnie/>`_
   * `Output from an example run <https://gist.github.com/1694664>`_
   * `Checking Disk Performance with the mongoperf Utility <http://blog.mongodb.org/post/40769806981/checking-disk-performance-with-the-mongoperf-utility>`_

.. _mongoperf-options:

Options
-------

.. binary:: mongoperf

.. program:: mongoperf

.. include:: /includes/option/option-mongoperf-help.rst

.. include:: /includes/option/option-mongoperf-<jsonconfig>.rst

.. _mongoperf-fields:

Configuration Fields
--------------------

.. setting:: mongoperf.nThreads

   *Type:* Integer.

   *Default:* 1

   Defines the number of threads :program:`mongoperf` will use in the
   test. To saturate your system's storage system you will need
   multiple threads. Consider setting :setting:`~mongoperf.nThreads` to ``16``.

.. setting:: mongoperf.fileSizeMB

   *Type:* Integer.

   *Default:* 1 megabyte (i.e. 1024\ :sup:`2` bytes)

   Test file size.

.. setting:: mongoperf.sleepMicros

   *Type:* Integer.

   *Default:* 0

   :program:`mongoperf` will pause for the number of specified
   :setting:`~mongoperf.sleepMicros` divided by the
   :setting:`~mongoperf.nThreads` between each operation.

.. setting:: mongoperf.mmf

   *Type:* Boolean.

   *Default:* ``false``

   Set :setting:`~mongoperf.mmf` to ``true`` to use memory mapped
   files for the tests.

   Generally:

   - when :setting:`~mongoperf.mmf` is ``false``, :program:`mongoperf`
     tests direct, physical, I/O, without caching. Use a large file
     size to test heavy random I/O load and to avoid I/O coalescing.

   - when :setting:`~mongoperf.mmf` is ``true``, :program:`mongoperf`
     runs tests of the caching system, and can use normal file system
     cache. Use :setting:`~mongoperf.mmf` in this mode to test file system cache
     behavior with memory mapped files.

.. setting:: mongoperf.r

   *Type:* Boolean.

   *Default:* ``false``

   Set :setting:`~mongoperf.r` to ``true`` to perform reads as part of
   the tests.

   Either :setting:`~mongoperf.r` or :setting:`~mongoperf.w` must be ``true``.

.. setting:: mongoperf.w

   *Type:* Boolean.

   *Default:* ``false``

   Set :setting:`~mongoperf.w` to ``true`` to perform writes as part of
   the tests.

   Either :setting:`~mongoperf.r` or :setting:`~mongoperf.w` must be ``true``.

.. setting:: mongoperf.recSizeKB

   .. versionadded:: 2.4

   *Type:* Integer.

   *Default:* 4 kb

   The size of each write operation.

.. setting:: mongoperf.syncDelay

   *Type:* Integer.

   *Default:* 0

   Seconds between disk flushes. :setting:`mongoperf.syncDelay` is
   similar to :option:`--syncdelay <mongod --syncdelay>` for :program:`mongod`.

   The :setting:`~mongoperf.syncDelay` controls how frequently
   :program:`mongoperf` performs an asynchronous disk flush of the memory
   mapped file used for testing. By default, :program:`mongod`
   performs this operation every 60 seconds. Use
   :setting:`~mongoperf.syncDelay` to test basic system performance of
   this type of operation.

   Only use :setting:`~mongoperf.syncDelay` in conjunction with
   :setting:`~mongoperf.mmf` set to ``true``.

   The default value of ``0`` disables this.

Use
---

.. code-block:: javascript

   mongoperf < jsonconfigfile

Replace ``jsonconfigfile`` with the path to the :program:`mongoperf`
configuration. You may also invoke :program:`mongoperf` in the
following form:

.. code-block:: sh

   echo "{nThreads:16,fileSizeMB:1000,r:true}" | ./mongoperf

In this operation:

- :program:`mongoperf` tests direct physical random read io's, using
  16 concurrent reader threads.

- :program:`mongoperf`  uses a 1 gigabyte test file.

Consider using ``iostat``, as invoked in the following example to
monitor I/O performance during the test.

.. code-block:: sh

   iostat -xm 2
.. _mongorestore:

================
``mongorestore``
================

.. default-domain:: mongodb
.. |tool-binary| replace:: mongorestore

Synopsis
--------

The :program:`mongorestore` program writes data from a binary database
dump created by :program:`mongodump` to a MongoDB
instance. :program:`mongorestore` can create a new database or add
data to an existing database.

:program:`mongorestore` can write data to either `mongod` or :program:`mongos`
instances, in addition to writing directly to MongoDB data files
without an active :program:`mongod`.

Behavior
--------

If you restore to an existing database, :program:`mongorestore` will
only insert into the existing database, and does not perform updates
of any kind. If existing documents have the same value ``_id`` field
in the target database and collection,
:program:`mongorestore` will *not* overwrite those documents.

Remember the following properties of :program:`mongorestore` behavior:

- :program:`mongorestore` recreates indexes recorded by
  :program:`mongodump`.

- all operations are inserts, not updates.

- :program:`mongorestore` does not wait for a response from a
  :program:`mongod` to ensure that the MongoDB process has received or
  recorded the operation.

  The :program:`mongod` will record any errors to its log that occur
  during a restore operation, but :program:`mongorestore` will not
  receive errors.

.. include:: /includes/warning-mongodump-compatibility-2.2.rst

Required Access to Restore User Data
------------------------------------

.. include:: /includes/access-mongorestore.rst

Options
-------

.. binary:: mongorestore

.. program:: mongorestore

.. include:: /includes/option/option-mongorestore-help.rst

.. include:: /includes/option/option-mongorestore-verbose.rst

.. include:: /includes/option/option-mongorestore-quiet.rst

.. include:: /includes/option/option-mongorestore-version.rst

.. include:: /includes/option/option-mongorestore-host.rst

.. include:: /includes/option/option-mongorestore-port.rst

.. include:: /includes/option/option-mongorestore-ipv6.rst

.. include:: /includes/option/option-mongorestore-ssl.rst

.. include:: /includes/option/option-mongorestore-sslCAFile.rst

.. include:: /includes/option/option-mongorestore-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongorestore-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongorestore-sslCRLFile.rst

.. include:: /includes/option/option-mongorestore-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongorestore-sslFIPSMode.rst

.. include:: /includes/option/option-mongorestore-username.rst

.. include:: /includes/option/option-mongorestore-password.rst

.. include:: /includes/option/option-mongorestore-authenticationDatabase.rst

.. include:: /includes/option/option-mongorestore-authenticationMechanism.rst

.. include:: /includes/option/option-mongorestore-dbpath.rst

.. include:: /includes/option/option-mongorestore-directoryperdb.rst

.. include:: /includes/option/option-mongorestore-journal.rst

.. include:: /includes/option/option-mongorestore-db.rst

.. include:: /includes/option/option-mongorestore-collection.rst

.. include:: /includes/option/option-mongorestore-objcheck.rst

.. include:: /includes/option/option-mongorestore-noobjcheck.rst

.. include:: /includes/option/option-mongorestore-filter.rst

.. include:: /includes/option/option-mongorestore-drop.rst

.. include:: /includes/option/option-mongorestore-oplogReplay.rst

.. include:: /includes/option/option-mongorestore-oplogLimit.rst

.. include:: /includes/option/option-mongorestore-keepIndexVersion.rst

.. include:: /includes/option/option-mongorestore-noIndexRestore.rst

.. include:: /includes/option/option-mongorestore-noOptionsRestore.rst

.. this does not exist:
.. .. include:: /includes/option/option-mongorestore-restoreDbUserasAndRoles.rst

.. include:: /includes/option/option-mongorestore-w.rst

.. _mongorestore-path-option:

.. include:: /includes/option/option-mongorestore-<path>.rst

Use
---

See :doc:`/tutorial/backup-with-mongodump`
for a larger overview of :program:`mongorestore`
usage. Also see the :doc:`mongodump` document for an overview of the
:program:`mongodump`, which provides the related inverse
functionality.

Consider the following example:

.. code-block:: sh

   mongorestore --collection people --db accounts dump/accounts/people.bson

Here, :program:`mongorestore` reads the database dump in the ``dump/``
sub-directory of the current directory, and restores *only* the
documents in the collection named ``people`` from the database named
``accounts``. :program:`mongorestore` restores data to the instance
running on the localhost interface on port ``27017``.

In the next example, :program:`mongorestore` restores a backup of the
database instance located in ``dump`` to a database instance stored
in the ``/srv/mongodb`` on the local machine. This requires that there
are no active :program:`mongod` instances attached to ``/srv/mongodb``
data directory.

.. code-block:: sh

   mongorestore --dbpath /srv/mongodb

In the final example, :program:`mongorestore` restores a database
dump located at ``/opt/backup/mongodump-2011-10-24``, to a database
running on port ``37017`` on the host
``mongodb1.example.net``. The :program:`mongorestore` command authenticates to
the MongoDB instance using the username ``user`` and the
password ``pass``, as follows:

.. code-block:: sh

   mongorestore --host mongodb1.example.net --port 37017 --username user --password pass /opt/backup/mongodump-2011-10-24
==============
``mongos.exe``
==============

.. default-domain:: mongodb

Synopsis
--------

:program:`mongos.exe` is the build of the MongoDB Shard
(i.e. :program:`mongos`) for the Windows
platform. :program:`mongos.exe` has all of the features of
:program:`mongos` on Unix-like platforms and is completely compatible
with the other builds of :program:`mongos`. In addition,
:program:`mongos.exe` provides several options for interacting with
the Windows platform itself.

This document only references options that are unique to
:program:`mongos.exe`. All :program:`mongos` options are
available. See the :doc:`/reference/program/mongos` and the
:doc:`/reference/configuration-options` documents for more
information regarding :program:`mongos.exe`.

To install and use :program:`mongos.exe`, read the
:doc:`/tutorial/install-mongodb-on-windows` document.

Options
-------

.. binary:: mongos.exe

.. program:: mongos.exe

.. include:: /includes/option/option-mongos.exe-install.rst

.. include:: /includes/option/option-mongos.exe-remove.rst

.. include:: /includes/option/option-mongos.exe-reinstall.rst

.. include:: /includes/option/option-mongos.exe-serviceName.rst

.. include:: /includes/option/option-mongos.exe-serviceDisplayName.rst

.. include:: /includes/option/option-mongos.exe-serviceDescription.rst

.. include:: /includes/option/option-mongos.exe-serviceUser.rst

.. include:: /includes/option/option-mongos.exe-servicePassword.rst
.. _mongos:

==========
``mongos``
==========

.. default-domain:: mongodb

Synopsis
--------

:program:`mongos` for "MongoDB Shard," is a routing service for
MongoDB shard configurations that processes queries from the
application layer, and determines the location of this data in the
:term:`sharded cluster`, in order to complete these operations.
From the perspective of the application, a
:program:`mongos` instance behaves identically to any other MongoDB
instance.

Options
-------

.. only:: (not man)

   .. class:: hidden

      .. binary:: mongos

Core Options
~~~~~~~~~~~~

.. program:: mongos

.. include:: /includes/option/option-mongos-help.rst

.. include:: /includes/option/option-mongos-version.rst

.. include:: /includes/option/option-mongos-config.rst

.. include:: /includes/option/option-mongos-verbose.rst

.. include:: /includes/option/option-mongos-quiet.rst

.. include:: /includes/option/option-mongos-port.rst

.. include:: /includes/option/option-mongos-bind_ip.rst

.. include:: /includes/option/option-mongos-maxConns.rst

.. include:: /includes/option/option-mongos-syslog.rst

.. include:: /includes/option/option-mongos-syslogFacility.rst

.. include:: /includes/option/option-mongos-logpath.rst

.. include:: /includes/option/option-mongos-logappend.rst

.. include:: /includes/option/option-mongos-timeStampFormat.rst

.. include:: /includes/option/option-mongos-pidfilepath.rst

.. include:: /includes/option/option-mongos-keyFile.rst

.. include:: /includes/option/option-mongos-setParameter.rst

.. include:: /includes/option/option-mongos-httpinterface.rst

.. include:: /includes/option/option-mongos-clusterAuthMode.rst

.. include:: /includes/option/option-mongos-nounixsocket.rst

.. include:: /includes/option/option-mongos-unixSocketPrefix.rst

.. include:: /includes/option/option-mongos-fork.rst

Sharded Cluster Options
~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/option/option-mongos-configdb.rst

.. include:: /includes/option/option-mongos-localThreshold.rst

.. include:: /includes/option/option-mongos-upgrade.rst

.. include:: /includes/option/option-mongos-chunkSize.rst

.. include:: /includes/option/option-mongos-noAutoSplit.rst

SSL Options
~~~~~~~~~~~

.. see:: :doc:`/tutorial/configure-ssl` for full
   documentation of MongoDB's support.

.. include:: /includes/option/option-mongos-sslOnNormalPorts.rst

.. include:: /includes/option/option-mongos-sslMode.rst

.. include:: /includes/option/option-mongos-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongos-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongos-sslClusterFile.rst

.. include:: /includes/option/option-mongos-sslClusterPassword.rst

.. include:: /includes/option/option-mongos-sslCAFile.rst

.. include:: /includes/option/option-mongos-sslCRLFile.rst

.. include:: /includes/option/option-mongos-sslWeakCertificateValidation.rst

.. include:: /includes/option/option-mongos-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongos-sslFIPSMode.rst


Audit Options
~~~~~~~~~~~~~

.. include:: /includes/option/option-mongos-auditDestination.rst

.. include:: /includes/option/option-mongos-auditFormat.rst

.. include:: /includes/option/option-mongos-auditPath.rst

.. include:: /includes/option/option-mongos-auditFilter.rst


Additional Options
~~~~~~~~~~~~~~~~~~

.. this section will move into the top following the resolution of
   SERVER-12889

.. include:: /includes/option/option-mongos-ipv6.rst

.. include:: /includes/option/option-mongos-jsonp.rst

.. include:: /includes/option/option-mongos-noscripting.rst

.. pending removal with SERVER-12888

   .. .. include:: /includes/option/option-mongos-test.rst

.. The following options no longer appear in the help.

   .. objcheck is enabled by default

      .. .. include:: /includes/option/option-mongos-objcheck.rst

   .. ssl is replaced by sslMode

      .. .. include:: /includes/option/option-mongos-ssl.rst

   .. no longer shown in help because its the default

      .. .. include:: /includes/option/option-mongos-nohttpinterface.rst
.. _mongosniff:

==============
``mongosniff``
==============

.. default-domain:: mongodb

Synopsis
--------

:program:`mongosniff` provides a low-level operation tracing/sniffing view
into database activity in real time. Think of :program:`mongosniff` as a
MongoDB-specific analogue of ``tcpdump`` for TCP/IP network
traffic. Typically, :program:`mongosniff` is most frequently used in driver
development.

.. _mongosniff-libcap:

.. note::

   :program:`mongosniff` requires ``libpcap`` and is only available for
   Unix-like systems. Furthermore, the version distributed with the
   MongoDB binaries is dynamically linked against aversion 0.9 of
   ``libpcap``. If your system has a different version of ``libpcap``, you
   will need to compile :program:`mongosniff` yourself or create a
   symbolic link pointing to ``libpcap.so.0.9`` to your local version
   of ``libpcap``. Use an operation that resembles the following:

   .. code-block:: sh

      ln -s /usr/lib/libpcap.so.1.1.1 /usr/lib/libpcap.so.0.9

   Change the path's and name of the shared library as needed.

As an alternative to :program:`mongosniff`, Wireshark, a popular
network sniffing tool is capable of inspecting and parsing the MongoDB
wire protocol.

.. _mongosniff-options:

Options
-------

.. binary:: mongosniff

.. program:: mongosniff

.. include:: /includes/option/option-mongosniff-help.rst

.. include:: /includes/option/option-mongosniff-forward.rst

.. include:: /includes/option/option-mongosniff-source.rst

.. include:: /includes/option/option-mongosniff-objcheck.rst

.. include:: /includes/option/option-mongosniff-<port>.rst

Use
---

Use the following command to connect to a :program:`mongod` or
:program:`mongos` running on port 27017 *and* 27018 on the localhost
interface:

.. code-block:: sh

   mongosniff --source NET lo 27017 27018

Use the following command to only log invalid :term:`BSON` objects for
the :program:`mongod` or :program:`mongos` running on the localhost
interface and port 27018, for driver development and troubleshooting:

.. code-block:: sh

   mongosniff --objcheck --source NET lo 27018

Build ``mongosniff``
--------------------

To build ``mongosniff`` yourself, Linux users can use the following
procedure:

1. Obtain prerequisites using your operating
   systems package management software. Dependencies include:

   - ``libpcap`` - to capture network packets.
   - ``git`` - to download the MongoDB source code.
   - ``scons`` and a C++ compiler - to build :program:`mongosniff`.

2. Download a copy of the MongoDB source code using ``git``:

   .. code-block:: sh

      git clone git://github.com/mongodb/mongo.git

3. Issue the following sequence of commands to change to the
   ``mongo/`` directory and build :program:`mongosniff`:

   .. code-block:: sh

      cd mongo
      scons mongosniff

.. note::

   If you run ``scons mongosniff`` before installing ``libpcap`` you
   must run ``scons clean`` before you can build :program:`mongosniff`.
.. _mongostat:

=============
``mongostat``
=============

.. default-domain:: mongodb
.. |tool-binary| replace:: mongostat

Synopsis
--------

The :program:`mongostat` utility provides a quick overview of the
status of a currently running :program:`mongod`
or :program:`mongos`
instance. :program:`mongostat` is functionally similar to the
UNIX/Linux file system utility ``vmstat``, but provides data regarding
:program:`mongod` and :program:`mongos` instances.

.. seealso::

   For more information about monitoring MongoDB, see
   :doc:`/administration/monitoring`.

   For more background on various other MongoDB status outputs see:

   - :doc:`/reference/command/serverStatus`
   - :doc:`/reference/command/replSetGetStatus`
   - :doc:`/reference/command/dbStats`
   - :doc:`/reference/command/collStats`

   For an additional utility that provides MongoDB metrics see
   :doc:`mongotop </reference/program/mongotop>`.

:program:`mongostat` connects to the :program:`mongod` instance running
on the local host interface on TCP port ``27017``; however,
:program:`mongostat` can connect to any accessible remote :program:`mongod`
instance.


Options
-------

.. binary:: mongostat

.. program:: mongostat

.. include:: /includes/option/option-mongostat-help.rst

.. include:: /includes/option/option-mongostat-verbose.rst

.. include:: /includes/option/option-mongostat-version.rst

.. include:: /includes/option/option-mongostat-host.rst

.. include:: /includes/option/option-mongostat-port.rst

.. include:: /includes/option/option-mongostat-ipv6.rst

.. include:: /includes/option/option-mongostat-ssl.rst

.. include:: /includes/option/option-mongostat-sslCAFile.rst

.. include:: /includes/option/option-mongostat-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongostat-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongostat-sslCRLFile.rst

.. include:: /includes/option/option-mongostat-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongostat-sslFIPSMode.rst

.. include:: /includes/option/option-mongostat-username.rst

.. include:: /includes/option/option-mongostat-password.rst

.. include:: /includes/option/option-mongostat-authenticationDatabase.rst

.. include:: /includes/option/option-mongostat-authenticationMechanism.rst

.. include:: /includes/option/option-mongostat-noheaders.rst

.. include:: /includes/option/option-mongostat-rowcount.rst

.. include:: /includes/option/option-mongostat-http.rst

.. include:: /includes/option/option-mongostat-discover.rst

.. include:: /includes/option/option-mongostat-all.rst

.. include:: /includes/option/option-mongostat-<sleeptime>.rst

.. _mongostat-fields:

Fields
------

:program:`mongostat` returns values that reflect the operations over a
1 second period. When :command:`mongostat <sleeptime>` has a value
greater than 1, :program:`mongostat` averages the statistics to reflect
average operations per second.

:program:`mongostat` outputs the following fields:

.. describe:: inserts

   The number of objects inserted into the database per second. If
   followed by an asterisk (e.g. ``*``), the datum refers to a
   replicated operation.

.. describe:: query

   The number of query operations per second.

.. describe:: update

   The number of update operations per second.

.. describe:: delete

   The number of delete operations per second.

.. describe:: getmore

   The number of get more (i.e. cursor batch) operations per second.

.. describe:: command

   The number of commands per second. On :term:`slave` and
   :term:`secondary` systems, :program:`mongostat` presents two values
   separated by a pipe character (e.g. ``|``), in the form of
   ``local|replicated`` commands.

.. describe:: flushes

   The number of :term:`fsync` operations per second.

.. describe:: mapped

   The total amount of data mapped in megabytes. This is the total
   data size at the time of the last :program:`mongostat` call.

.. describe:: size

   The amount of virtual memory in megabytes used by the process at
   the time of the last :program:`mongostat` call.

.. describe:: non-mapped

   The total amount of virtual memory excluding all mapped memory at
   the time of the last :program:`mongostat` call.

.. describe:: res

   The amount of resident memory in megabytes used by the process at
   the time of the last :program:`mongostat` call.

.. describe:: faults

   .. versionchanged:: 2.1

   The number of page faults per second.

   Before version 2.1 this value was only provided for MongoDB
   instances running on Linux hosts.

.. describe:: locked

   The percent of time in a global write lock.

   .. versionchanged:: 2.2
      The ``locked db`` field replaces the ``locked %`` field to
      more appropriate data regarding the database specific locks in
      version 2.2.

.. describe:: locked db

   .. versionadded:: 2.2

   The percent of time in the per-database context-specific
   lock. :program:`mongostat` will report the database that has spent
   the most time since the last :program:`mongostat` call with a write
   lock.

   This value represents the amount of time that the listed database
   spent in a locked state *combined* with the time that the
   :program:`mongod` spent in the global lock. Because of this, and
   the sampling method, you may see some values greater than 100%.

.. describe:: idx miss

   The percent of index access attempts that required a page fault
   to load a btree node. This is a sampled value.

.. describe:: qr

   The length of the queue of clients waiting to read data from the
   MongoDB instance.

.. describe:: qw

   The length of the queue of clients waiting to write data from the
   MongoDB instance.

.. describe:: ar

   The number of active clients performing read operations.

.. describe:: aw

   The number of active clients performing write operations.

.. describe:: netIn

   The amount of network traffic, in *bytes*, received by the MongoDB instance.

   This includes traffic from :program:`mongostat` itself.

.. describe:: netOut

   The amount of network traffic, in *bytes*, sent by the MongoDB instance.

   This includes traffic from :program:`mongostat` itself.

.. describe:: conn

   The total number of open connections.

.. describe:: set

   The name, if applicable, of the replica set.

.. describe:: repl

   The replication status of the member.

   =========  ====================
   **Value**  **Replication Type**
   ---------  --------------------
   M          :term:`master`
   SEC        :term:`secondary`
   REC        recovering
   UNK        unknown
   SLV        :term:`slave`
   =========  ====================

Usage
-----

In the first example, :program:`mongostat` will return data every
second for 20 seconds. :program:`mongostat` collects data from the
:program:`mongod` instance running on the localhost interface on
port 27017. All of the following invocations produce identical
behavior:

.. code-block:: sh

   mongostat --rowcount 20 1
   mongostat --rowcount 20
   mongostat -n 20 1
   mongostat -n 20

In the next example, :program:`mongostat` returns data every 5 minutes
(or 300 seconds) for as long as the program runs. :program:`mongostat`
collects data from the :program:`mongod` instance running on the
localhost interface on port ``27017``. Both of the following
invocations produce identical behavior.

.. code-block:: sh

   mongostat --rowcount 0 300
   mongostat -n 0 300
   mongostat 300

In the following example, :program:`mongostat` returns data every 5
minutes for an hour (12 times.) :program:`mongostat` collects data
from the :program:`mongod` instance running on the localhost interface
on port 27017. Both of the following invocations produce identical
behavior.

.. code-block:: sh

   mongostat --rowcount 12 300
   mongostat -n 12 300

In many cases, using the :option:`--discover <mongostat --discover>`
will help provide a more complete snapshot of the state of an entire
group of machines. If a :program:`mongos` process connected to a
:term:`sharded cluster` is running on port ``27017`` of the local
machine, you can use the following form to return statistics from all
members of the cluster:

.. code-block:: sh

   mongostat --discover
.. _mongotop:

============
``mongotop``
============

.. default-domain:: mongodb

.. |tool-binary| replace:: mongotop

Synopsis
--------

:program:`mongotop` provides a method to track the amount of time a
MongoDB instance spends reading and writing data. :program:`mongotop`
provides statistics on a per-collection level. By default,
:program:`mongotop` returns values every second.

.. seealso::

   For more information about monitoring MongoDB, see
   :doc:`/administration/monitoring`.

   For additional background on various other MongoDB status outputs
   see:

   - :doc:`/reference/command/serverStatus`
   - :doc:`/reference/command/replSetGetStatus`
   - :doc:`/reference/command/dbStats`
   - :doc:`/reference/command/collStats`

   For an additional utility that provides MongoDB metrics
   see :doc:`mongostat </reference/program/mongostat>`.

.. _mongotop-options:

Options
-------

.. binary:: mongotop

.. program:: mongotop

.. include:: /includes/option/option-mongotop-help.rst

.. include:: /includes/option/option-mongotop-verbose.rst

.. include:: /includes/option/option-mongotop-quiet.rst

.. include:: /includes/option/option-mongotop-version.rst

.. include:: /includes/option/option-mongotop-host.rst

.. include:: /includes/option/option-mongotop-port.rst

.. include:: /includes/option/option-mongotop-ipv6.rst

.. include:: /includes/option/option-mongotop-ssl.rst

.. include:: /includes/option/option-mongotop-sslCAFile.rst

.. include:: /includes/option/option-mongotop-sslPEMKeyFile.rst

.. include:: /includes/option/option-mongotop-sslPEMKeyPassword.rst

.. include:: /includes/option/option-mongotop-sslCRLFile.rst

.. include:: /includes/option/option-mongotop-sslAllowInvalidCertificates.rst

.. include:: /includes/option/option-mongotop-sslFIPSMode.rst

.. include:: /includes/option/option-mongotop-username.rst

.. include:: /includes/option/option-mongotop-password.rst

.. include:: /includes/option/option-mongotop-authenticationDatabase.rst

.. include:: /includes/option/option-mongotop-authenticationMechanism.rst

.. include:: /includes/option/option-mongotop-locks.rst

.. include:: /includes/option/option-mongotop-<sleeptime>.rst

.. _mongotop-fields:

Fields
------

:program:`mongotop` returns time values specified in milliseconds
(ms.)

:program:`mongotop` only reports active namespaces or databases,
depending on the :option:`--locks` option. If you don't see a database
or collection, it has received no recent activity. You can issue a
simple operation in the :program:`mongo` shell to generate activity to
affect the output of :program:`mongotop`.

.. data:: mongotop.ns

   Contains the database namespace, which combines the database name
   and collection.

   .. versionchanged:: 2.2
      If you use the :option:`--locks`, the :data:`~mongotop.ns` field does not
      appear in the :program:`mongotop` output.

.. data:: mongotop.db

   .. versionadded:: 2.2

   Contains the name of the database. The database named ``.`` refers
   to the global lock, rather than a specific database.

   This field does not appear unless you have invoked
   :program:`mongotop` with the :option:`--locks` option.

.. data:: mongotop.total

   Provides the total amount of time that this :program:`mongod` spent
   operating on this namespace.

.. data:: mongotop.read

   Provides the amount of time that this :program:`mongod` spent
   performing read operations on this namespace.

.. data:: mongotop.write

   Provides the amount of time that this :program:`mongod` spent
   performing write operations on this namespace.

.. data:: mongotop.<timestamp>

   Provides a time stamp for the returned data.

Use
---

By default :program:`mongotop` connects to the MongoDB instance
running on the localhost port ``27017``. However, :program:`mongotop` can optionally
connect to remote :program:`mongod`
instances. See the :ref:`mongotop options <mongotop-options>` for more
information.

To force :program:`mongotop` to return less frequently specify a number, in
seconds at the end of the command. In this example, :program:`mongotop` will
return every 15 seconds.

.. code-block:: sh

   mongotop 15

This command produces the following output:

.. code-block:: sh

   connected to: 127.0.0.1

                       ns       total        read       write           2012-08-13T15:45:40
   test.system.namespaces         0ms         0ms         0ms
     local.system.replset         0ms         0ms         0ms
     local.system.indexes         0ms         0ms         0ms
     admin.system.indexes         0ms         0ms         0ms
                   admin.         0ms         0ms         0ms

                       ns       total        read       write           2012-08-13T15:45:55
   test.system.namespaces         0ms         0ms         0ms
     local.system.replset         0ms         0ms         0ms
     local.system.indexes         0ms         0ms         0ms
     admin.system.indexes         0ms         0ms         0ms
                   admin.         0ms         0ms         0ms


To return a :program:`mongotop` report every 5 minutes, use the
following command:

.. code-block:: sh

   mongotop 300

To report the use of per-database locks, use :option:`mongotop --locks`,
which produces the following output:

.. code-block:: sh

   $ mongotop --locks
   connected to: 127.0.0.1

                     db       total        read       write          2012-08-13T16:33:34
                  local         0ms         0ms         0ms
                  admin         0ms         0ms         0ms
                      .         0ms         0ms         0ms
==========================
MongoDB Package Components
==========================

.. default-domain:: mongodb

Core Processes
--------------

The core components in the MongoDB package are: :program:`mongod`,
the core database process; :program:`mongos` the controller and query
router for :term:`sharded clusters <sharded cluster>`; and
:program:`mongo` the interactive MongoDB Shell.

.. toctree::
   :maxdepth: 1

   /reference/program/mongod
   /reference/program/mongos
   /reference/program/mongo

Windows Services
----------------

The :program:`mongod.exe` and :program:`mongos.exe` describe the
options available for configuring MongoDB when running as a Windows
Service. The :program:`mongod.exe` and :program:`mongos.exe` binaries
provide a superset of the :program:`mongod` and :program:`mongos`
options.

.. toctree::
   :maxdepth: 1

   /reference/program/mongod.exe
   /reference/program/mongos.exe

Binary Import and Export Tools
------------------------------

:program:`mongodump` provides a method for creating :term:`BSON`
dump files from the :program:`mongod` instances, while
:program:`mongorestore` makes it possible to restore these
dumps. :program:`bsondump` converts BSON dump files into
:term:`JSON`. The :program:`mongooplog` utility provides the ability
to stream :term:`oplog` entries outside of normal replication.

.. toctree::
   :maxdepth: 1

   /reference/program/mongodump
   /reference/program/mongorestore
   /reference/program/bsondump
   /reference/program/mongooplog

Data Import and Export Tools
----------------------------

:program:`mongoimport` provides a method for taking data in :term:`JSON`,
:term:`CSV`, or :term:`TSV` and importing it into a :program:`mongod`
instance. :program:`mongoexport` provides a method to export data from
a :program:`mongod` instance into JSON, CSV, or TSV.

.. note::

   The conversion between BSON and other formats lacks full
   type fidelity. Therefore you cannot use :program:`mongoimport` and
   :program:`mongoexport` for round-trip import and export operations.

.. toctree::
   :maxdepth: 1

   /reference/program/mongoimport
   /reference/program/mongoexport

Diagnostic Tools
----------------

:program:`mongostat`, :program:`mongotop`, and :program:`mongosniff`
provide diagnostic information related to the current operation of a
:program:`mongod` instance.

.. note::

   Because :program:`mongosniff` depends on :term:`libpcap
   <pcap>`, most distributions of MongoDB do *not* include
   :program:`mongosniff`.

.. toctree::
   :maxdepth: 1

   /reference/program/mongostat
   /reference/program/mongotop
   /reference/program/mongosniff
   /reference/program/mongoperf

GridFS
------

:program:`mongofiles` provides a command-line interact to a MongoDB
:term:`GridFS` storage system.

.. toctree::
   :maxdepth: 1

   /reference/program/mongofiles
=========================
Read Preference Reference
=========================

.. default-domain:: mongodb

.. include:: /includes/introduction-read-preference.rst

.. include:: /includes/read-preference-modes-table.rst

.. index:: read preference; semantics
.. _replica-set-read-preference-semantics:
.. index:: read preference; modes
.. _replica-set-read-preference-modes:

Read Preference Modes
---------------------

.. readmode:: primary

   All read operations use only the current replica set :term:`primary`.
   This is the default. If the primary is unavailable,
   read operations produce an error or throw an exception.

   The :readmode:`primary` read preference mode is not compatible with
   read preference modes that use :ref:`tag sets
   <replica-set-read-preference-tag-sets>`. If you specify a tag set
   with :readmode:`primary`, the driver will produce an error.

.. readmode:: primaryPreferred

   In most situations, operations read from the :term:`primary` member
   of the set. However, if the primary is unavailable, as is the case
   during :term:`failover` situations, operations read from secondary
   members.

   When the read preference includes a :ref:`tag set
   <replica-set-read-preference-tag-sets>`, the client reads first from
   the primary, if available, and then from :term:`secondaries
   <secondary>` that match the specified tags. If no secondaries have
   matching tags, the read operation produces an error.

   Since the application may receive data from a secondary, read
   operations using the :readmode:`primaryPreferred` mode may return
   stale data in some situations.

   .. warning::

      .. versionchanged:: 2.2
         :program:`mongos` added full support for read preferences.

      When connecting to a :program:`mongos` instance older than 2.2,
      using a client that supports read preference modes,
      :readmode:`primaryPreferred` will send queries to secondaries.

.. readmode:: secondary

   Operations read *only* from the :term:`secondary` members of the set.
   If no secondaries are available, then this read operation produces an
   error or exception.

   Most sets have at least one secondary, but there are situations
   where there may be no available secondary. For example, a set with
   a primary, a secondary, and an :term:`arbiter` may not have any
   secondaries if a member is in recovering state or unavailable.

   When the read preference includes a :ref:`tag set
   <replica-set-read-preference-tag-sets>`, the client attempts to
   find secondary members that match the specified tag set and directs
   reads to a random secondary from among the :ref:`nearest group
   <replica-set-read-preference-behavior-nearest>`. If no secondaries
   have matching tags, the read operation produces an
   error. [#capacity-planning]_

   Read operations using the :readmode:`secondary` mode may return stale data.

.. readmode:: secondaryPreferred

   In most situations, operations read from :term:`secondary` members,
   but in situations where the set consists of a single
   :term:`primary` (and no other members), the read operation will use
   the set's primary.

   When the read preference includes a :ref:`tag set
   <replica-set-read-preference-tag-sets>`, the client attempts to find
   a secondary member that matches the specified tag set and directs
   reads to a random secondary from among the :ref:`nearest group
   <replica-set-read-preference-behavior-nearest>`. If no secondaries
   have matching tags, the client ignores tags and reads from the primary.

   Read operations using the :readmode:`secondaryPreferred` mode may
   return stale data.

.. readmode:: nearest

   The driver reads from the *nearest* member of the :term:`set <replica
   set>` according to the :ref:`member selection
   <replica-set-read-preference-behavior-nearest>` process. Reads in
   the :readmode:`nearest` mode do not consider the member's
   *type*. Reads in :readmode:`nearest` mode may read from both
   primaries and secondaries.

   Set this mode to minimize the effect of network latency
   on read operations without preference for current or stale data.

   If you specify a :ref:`tag set
   <replica-set-read-preference-tag-sets>`, the client attempts to
   find a replica set member that matches the specified tag set and
   directs reads to an arbitrary member from among the :ref:`nearest
   group <replica-set-read-preference-behavior-nearest>`.

   Read operations using the :readmode:`nearest` mode may return stale
   data.

   .. note::

      All operations read from a member of the nearest group of the
      replica set that matches the specified read preference mode. The
      :readmode:`nearest` mode prefers low latency reads over a
      member's :term:`primary` or :term:`secondary` status.

      For :readmode:`nearest`, the client assembles a list of
      acceptable hosts based on tag set and then narrows that list to
      the host with the shortest ping time and all other members of
      the set that are within the "local threshold," or acceptable
      latency. See :ref:`replica-set-read-preference-behavior-nearest`
      for more information.

.. [#capacity-planning] If your set has more than one secondary, and
   you use the :readmode:`secondary` read preference mode, consider
   the following effect. If you have a :ref:`three member replica set
   <replica-set-three-members>` with a primary and two secondaries,
   and if one secondary becomes unavailable, all :readmode:`secondary`
   queries must target the remaining secondary. This will double the
   load on this secondary. Plan and provide capacity to support this
   as needed.

Use Cases
---------

Depending on the requirements of an application, you can configure
different applications to use
different read preferences, or use different read preferences for different
queries in the same application. Consider the following applications
for different read preference strategies.

Maximize Consistency
~~~~~~~~~~~~~~~~~~~~

To avoid *stale* reads under all circumstances, use
:readmode:`primary`. This prevents all queries when the set has no
:term:`primary`, which happens during elections, or when a majority of
the replica set is not accessible.

Maximize Availability
~~~~~~~~~~~~~~~~~~~~~

To permit read operations when possible, Use
:readmode:`primaryPreferred`. When there's a primary you will get
consistent reads, but if there is no primary you can still query
:term:`secondaries <secondary>`.

Minimize Latency
~~~~~~~~~~~~~~~~

To always read from a low-latency node, use :readmode:`nearest`. The
driver or :program:`mongos` will read from the nearest member and
those no more than 15 milliseconds [#secondary-acceptable-latency]_
further away than the nearest member.

:readmode:`nearest` does *not* guarantee consistency. If the nearest
member to your application server is a secondary with some replication
lag, queries could return stale data. :readmode:`nearest` only
reflects network distance and does not reflect I/O or CPU load.

.. [#secondary-acceptable-latency] This threshold is configurable. See
   :setting:`localThreshold` for :program:`mongos` or your driver
   documentation for the appropriate setting.


Query From Geographically Distributed Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the members of a replica set are geographically distributed, you
can create replica tags based that reflect the location of the instance and
then configure your application to query the members nearby.

For example, if members in "east" and "west" data centers are
:ref:`tagged <replica-set-configuration-tag-sets>` ``{'dc': 'east'}`` and
``{'dc': 'west'}``, your application servers in the east data center can read
from nearby members with the following read preference:

.. code-block:: javascript

   db.collection.find().readPref( { mode: 'nearest',
                                    tags: [ {'dc': 'east'} ] } )

Although :readmode:`nearest` already favors members with low network latency,
including the tag makes the choice more predictable.

Reduce load on the primary
~~~~~~~~~~~~~~~~~~~~~~~~~~

To shift read load from the primary, use mode
:readmode:`secondary`. Although :readmode:`secondaryPreferred` is tempting for
this use case, it carries some risk: if all secondaries are unavailable and
your set has enough :term:`arbiters <arbiter>` to prevent the primary from
stepping down, then the primary will receive all traffic from clients. If the
primary is unable to handle this load, queries will compete with writes. For
this reason, use :readmode:`secondary` to distribute read load to replica sets,
not :readmode:`secondaryPreferred`.

Read Preferences for Database Commands
--------------------------------------

Because some :term:`database commands <database command>` read and
return data from the database, all of the official drivers support
full :ref:`read preference mode semantics <replica-set-read-preference-modes>`
for the following commands:

- :dbcommand:`group`
- :dbcommand:`mapReduce` [#inline-map-reduce]_
- :dbcommand:`aggregate` [#aggregation-out]_
- :dbcommand:`collStats`
- :dbcommand:`dbStats`
- :dbcommand:`count`
- :dbcommand:`distinct`
- :dbcommand:`geoNear`
- :dbcommand:`geoSearch`
- :dbcommand:`geoWalk`
- :dbcommand:`parallelCollectionScan`

.. versionadded:: 2.4
   :program:`mongos` adds support for routing commands to shards using
   read preferences. Previously :program:`mongos` sent all commands to
   shards' primaries.

.. [#inline-map-reduce] Only "inline" :dbcommand:`mapReduce`
   operations that do not write data support read preference,
   otherwise these operations must run on the :term:`primary`
   members.

.. [#aggregation-out] Using the ``$out`` pipeline operator forces the
   aggregation pipeline to run on the primary.
=========================
Replica Set Configuration
=========================

.. default-domain:: mongodb

Synopsis
--------

This reference provides an overview of replica set
configuration options and settings.

Use :method:`rs.conf()` in the :program:`mongo` shell to retrieve this
configuration. Note that default values are not explicitly displayed.

.. _replica-set-configuration-document:

Example Configuration Document
------------------------------

The following document provides a representation of a replica set
configuration document. Angle brackets (e.g. ``<`` and ``>``) enclose
all optional fields.

.. code-block:: javascript

   {
     _id : <setname>,
     version: <int>,
     members: [
       {
         _id : <ordinal>,
         host : hostname<:port>,
         <arbiterOnly : <boolean>,>
         <buildIndexes : <boolean>,>
         <hidden : <boolean>,>
         <priority: <priority>,>
         <tags: { <document> },>
         <slaveDelay : <number>,>
         <votes : <number>>
       }
       , ...
     ],
     <settings: {
       <getLastErrorDefaults : <lasterrdefaults>,>
       <chainingAllowed : <boolean>,>
       <getLastErrorModes : <modes>>
     }>
   }

.. _replica-set-configuration-variables:

Configuration Variables
-----------------------

.. data:: local.system.replset._id

   **Type**: string

   **Value**: <setname>

   An ``_id`` field holding the name of the replica set. This reflects
   the set name configured with :setting:`~replication.replSetName` or
   :option:`mongod --replSet`.

.. data:: local.system.replset.members

   **Type**: array

   Contains an array holding an embedded :term:`document` for each
   member of the replica set. The ``members`` document contains a
   number of fields that describe the configuration of each member of
   the replica set.

   The :data:`~local.system.replset.members` field in the replica set
   configuration document is a zero-indexed array.

.. data:: local.system.replset.members[n]._id

   **Type**: ordinal

   Provides the zero-indexed identifier of every member in the replica
   set.

   .. note::

      .. include:: /includes/fact-rs-conf-array-index.rst

.. data:: local.system.replset.members[n].host

   **Type**: <hostname><:port>

   Identifies the host name of the set member with a hostname and port
   number. This name must be resolvable for every host in the replica
   set.

   .. warning::

      :data:`~local.system.replset.members[n].host` cannot hold a value that resolves to
      ``localhost`` or the local interface unless *all* members of the
      set are on hosts that resolve to ``localhost``.

   .. |mongodb-package| replace:: :program:`mongod`

   .. include:: /includes/note-deb-and-rpm-default-to-localhost.rst

.. data:: local.system.replset.members[n].arbiterOnly

   *Optional*.

   **Type**: boolean

   **Default**: false

   Identifies an arbiter. For arbiters, this value is ``true``, and
   is automatically configured by :method:`rs.addArb()`".

.. data:: local.system.replset.members[n].buildIndexes

   *Optional*.

   **Type**: boolean

   **Default**: true

   Determines whether the :program:`mongod` builds :term:`indexes
   <index>` on this member. Do not set to ``false`` for instances that
   receive queries from clients.

   Omitting index creation, and thus this setting, may be useful,
   **if**:

   - You are only using this instance to perform backups using
     :program:`mongodump`,

   - this instance will receive no queries, *and*

   - index creation and maintenance overburdens the host
     system.

   If set to ``false``, secondaries configured with this option *do*
   build indexes on the ``_id`` field, to facilitate operations
   required for replication.

   .. warning::

      You may only set this value when adding a member to a replica
      set. You may not reconfigure a replica set to change the value of
      the :data:`~local.system.replset.members[n].buildIndexes` field
      after adding the member to the set.

      :data:`~local.system.replset.members[n].buildIndexes` is only
      valid when :term:`priority` is ``0`` to prevent these members from
      becoming :term:`primary`. Make all instances that do not build
      indexes hidden.

      Other secondaries cannot replicate from a members where
      :data:`~local.system.replset.members[n].buildIndexes` is
      false.

.. data:: local.system.replset.members[n].hidden

   *Optional*.

   **Type**: boolean

   **Default**: false

   When this value is ``true``, the replica set hides this instance,
   and does not include the member in the output of
   :method:`db.isMaster()` or :dbcommand:`isMaster`. This
   prevents read operations (i.e. queries) from ever reaching this
   host by way of secondary :term:`read preference`.

   .. seealso:: :ref:`Hidden Replica Set Members <replica-set-hidden-members>`

.. data:: local.system.replset.members[n].priority

   *Optional*.

   **Type**: Number, between 0 and 100.0 including decimals.

   **Default**: 1

   .. todo:: add/remove note for 2.4

      .. versionchanged:: 2.4
         :data:`~local.system.replset.members[n].priority` now accepts values between 0
         and 1000.

   Specify higher values to make a member *more* eligible to become
   :term:`primary`, and lower values to make the member *less* eligible
   to become primary. Priorities are only used in comparison to each
   other. Members of the set will veto election requests from members when
   another eligible member has a higher priority
   value. Changing the balance of priority in a replica set will trigger
   an election.

   A :data:`~local.system.replset.members[n].priority` of ``0`` makes it impossible for a
   member to become primary.

   .. seealso:: :data:`~local.system.replset.members[n].priority` and
      :ref:`Replica Set Elections <replica-set-elections>`.

.. data:: local.system.replset.members[n].tags

   *Optional*.

   **Type**: :term:`MongoDB Document <document>`

   **Default**: none

   Used to represent arbitrary values for describing or tagging members
   for the purposes of extending :term:`write concern`
   to allow configurable data center
   awareness.

   Use in conjunction with
   :data:`~local.system.replset.settings.getLastErrorModes` and
   :data:`~local.system.replset.settings.getLastErrorDefaults` and
   :method:`db.getLastError()` (i.e. :dbcommand:`getLastError`.)

   For procedures on configuring tag sets, see
   :doc:`/tutorial/configure-replica-set-tag-sets`.

   .. include:: /includes/fact-tag-sets-must-be-strings.rst

.. data:: local.system.replset.members[n].slaveDelay

   *Optional*.

   **Type**: Integer. (seconds.)

   **Default**: 0

   Describes the number of seconds "behind" the primary that this
   replica set member should "lag." Use this option to create
   :ref:`delayed members <replica-set-delayed-members>`, that
   maintain a copy of the data that reflects the state of the data set
   at some amount of time in the past, specified in seconds. Typically such delayed members
   help protect against human error, and provide some measure
   of insurance against the unforeseen consequences of changes and
   updates.

.. data:: local.system.replset.members[n].votes

   *Optional*.

   **Type**: Integer

   **Default**: 1

   Controls the number of votes a server will cast in a :ref:`replica set
   election <replica-set-elections>`. The number of votes each member
   has can be either ``1`` or ``0``.

   If you need more than 7 members in one replica set, set
   :data:`~local.system.replset.members[n].votes` to ``0`` for
   the additional non-voting members.

   .. include:: /includes/members-used-to-allow-multiple-votes.rst

.. data:: local.system.replset.settings

   *Optional*.

   **Type**: :term:`MongoDB Document <document>`

   The ``settings`` document configures options that apply to the whole
   replica set.

.. data:: local.system.replset.settings.chainingAllowed

   *Optional*.

   **Type**: boolean

   **Default**: true

   .. versionadded:: 2.2.4

   When :data:`~local.system.replset.settings.chainingAllowed` is
   ``true``, the replica set allows :term:`secondary` members to
   replicate from other secondary members. When
   :data:`~local.system.replset.settings.chainingAllowed` is
   ``false``, secondaries can replicate only from the :term:`primary`.

   When you run :method:`rs.config()` to view a replica set's
   configuration, the
   :data:`~local.system.replset.settings.chainingAllowed` field
   appears only when set to ``false``. If not set,
   :data:`~local.system.replset.settings.chainingAllowed` is ``true``.

   .. seealso:: :doc:`/tutorial/manage-chained-replication`

.. data:: local.system.replset.settings.getLastErrorDefaults

   *Optional*.

   **Type**: :term:`MongoDB Document <document>`

   Specify arguments to :dbcommand:`getLastError` that
   members of this replica set will use when
   :dbcommand:`getLastError` has no arguments. If you specify *any*
   arguments, :dbcommand:`getLastError`, ignores these defaults.

.. data:: local.system.replset.settings.getLastErrorModes

   *Optional*.

   **Type**: :term:`MongoDB Document <document>`

   Defines the names and combination of
   :data:`~local.system.replset.members` for use by the application layer
   to guarantee :term:`write concern` to database using the
   :dbcommand:`getLastError` command to provide :term:`data-center
   awareness`.

.. _replica-set-reconfiguration-usage:

Example Reconfiguration Operations
----------------------------------

Most modifications of :term:`replica set` configuration use the
:program:`mongo` shell. Consider the following reconfiguration
operation:

.. example::

   Given the following replica set configuration:

   .. code-block:: javascript

      {
          "_id" : "rs0",
          "version" : 1,
          "members" : [
                        {
                           "_id" : 0,
                           "host" : "mongodb0.example.net:27017"
                        },
                        {
                           "_id" : 1,
                           "host" : "mongodb1.example.net:27017"
                        },
                        {
                           "_id" : 2,
                           "host" : "mongodb2.example.net:27017"
                        }
           ]
      }

   The following reconfiguration operation updates the
   :data:`~local.system.replset.members[n].priority` of the replica set
   members:

   .. code-block:: javascript

      cfg = rs.conf()
      cfg.members[0].priority = 0.5
      cfg.members[1].priority = 2
      cfg.members[2].priority = 2
      rs.reconfig(cfg)

   First, this operation sets the local variable ``cfg`` to the current
   replica set configuration using the :method:`rs.conf()` method. Then
   it adds priority values to the ``cfg`` :term:`document` for the
   three sub-documents in the :data:`~local.system.replset.members`
   array, accessing each replica set member with the array index and
   **not** the replica set member's
   :data:`~local.system.replset.members[n]._id` field. Finally, it
   calls the :method:`rs.reconfig()` method with the argument of
   ``cfg`` to initialize this new configuration. The replica set
   configuration after this operation will resemble the following:

   .. code-block:: javascript

      {
          "_id" : "rs0",
          "version" : 1,
          "members" : [
                        {
                           "_id" : 0,
                           "host" : "mongodb0.example.net:27017",
                           "priority" : 0.5
                        },
                        {
                           "_id" : 1,
                           "host" : "mongodb1.example.net:27017",
                           "priority" : 2
                        },
                        {
                           "_id" : 2,
                           "host" : "mongodb2.example.net:27017",
                           "priority" : 1
                        }
           ]
      }


Using the "dot notation" demonstrated in the above example, you can
modify any existing setting or specify any of optional :ref:`replica
set configuration variables
<replica-set-configuration-variables>`. Until you run
``rs.reconfig(cfg)`` at the shell, no changes will take effect. You
can issue ``cfg = rs.conf()`` at any time before using
:method:`rs.reconfig()` to undo your changes and start from the current
configuration. If you issue ``cfg`` as an operation at any point, the
:program:`mongo` shell at any point will output the complete
:term:`document` with modifications for your review.

The :method:`rs.reconfig()` operation has a "force" option, to make it
possible to reconfigure a replica set if a majority of the replica set
is not visible, and there is no :term:`primary` member of the set.
use the following form:

.. code-block:: javascript

   rs.reconfig(cfg, { force: true } )

.. warning::

   Forcing a :method:`rs.reconfig()` can lead to :term:`rollback`
   situations and other difficult to recover from situations. Exercise
   caution when using this option.

.. note::

   The :method:`rs.reconfig()` shell method can force the current
   primary to step down and triggers an election in some
   situations. When the primary steps down, all clients will
   disconnect. This is by design. Since this typically takes 10-20
   seconds, attempt to make such changes during scheduled maintenance
   periods.
=========================
Replica Set Member States
=========================

.. default-domain:: mongodb

Members of replica sets have states that reflect the startup process, basic
operations, and potential error states.

.. include:: /includes/replica-states.rst

States
------

Core States
~~~~~~~~~~~
.. replstate:: PRIMARY

   Members in :replstate:`PRIMARY` state accept write operations. A replica set has only
   one primary at a time. A :replstate:`SECONDARY` member becomes primary
   after an :ref:`election <replica-set-elections>`. Members in the :replstate:`PRIMARY`
   state are eligible to vote.

.. replstate:: SECONDARY

   Members in :replstate:`SECONDARY` state replicate the primary's data set
   and can be configured to accept read operations. Secondaries are eligible to vote in
   elections, and may be elected to the :replstate:`PRIMARY` state if the
   primary becomes unavailable.

.. replstate:: ARBITER

   Members in :replstate:`ARBITER` state do not replicate data or accept write operations.
   They are eligible to vote, and exist solely to break a tie during
   elections. Replica sets should only have a member in the :replstate:`ARBITER` state
   if the set would otherwise have an even number of members, and could suffer
   from tied elections. Like primaries, there should only be at most one arbiter
   in any replica set.

See :doc:`/core/replica-set-members` for more information on core states.

Initialization States
~~~~~~~~~~~~~~~~~~~~~

.. replstate:: STARTUP

   Each member of a replica set starts up in :replstate:`STARTUP`
   state. :program:`mongod` then loads that member's replica set configuration,
   and transitions the member's state to :replstate:`STARTUP2`. Members in
   :replstate:`STARTUP` are not eligible to vote.

.. replstate:: STARTUP2

   Each member of a replica set enters the :replstate:`STARTUP2` state as
   soon as :program:`mongod` finishes loading that member's
   configuration. While in the :replstate:`STARTUP2` state, the member
   creates threads to handle internal replication operations. Members are in the
   :replstate:`STARTUP2` state for a short period of time before entering the :replstate:`RECOVERING` state.
   Members in the :replstate:`STARTUP2` state are not eligible to vote.

.. replstate:: RECOVERING

   A member of a replica set enters :replstate:`RECOVERING` state when
   it is not ready to accept reads. The :replstate:`RECOVERING` state
   can occur during normal operation, and doesn't necessarily reflect
   an error condition.  Members in the :replstate:`RECOVERING` state
   are eligible to vote in elections, but is not eligible to enter the
   :replstate:`PRIMARY` state.

   During startup, members transition through :replstate:`RECOVERING` after
   :replstate:`STARTUP2` and before becoming :replstate:`SECONDARY`.

   During normal operation, if a :term:`secondary` falls behind the
   other members of the replica set, it may need to :doc:`resync
   </tutorial/resync-replica-set-member>` with the
   rest of the set. While resyncing, the member enters the
   :replstate:`RECOVERING` state.

   Whenever the replica set replaces a :term:`primary` in an
   election, the old primary's data collection may contain documents
   that did not have time to replicate to the :term:`secondary`
   members. In this case the member rolls back those writes. During
   :doc:`rollback </core/replica-set-rollbacks>`, the member will have
   :replstate:`RECOVERING` state.

   On secondaries, the :dbcommand:`compact` and
   :dbcommand:`replSetMaintenance` commands force the secondary to enter
   :replstate:`RECOVERING` state. This prevents clients from reading
   during those operations.

Error States
~~~~~~~~~~~~

Members in any error state can't vote.

.. replstate:: FATAL

   Members that encounter an unrecoverable error enter the :replstate:`FATAL`
   state. Members in this state requires administrator intervention.

.. replstate:: UNKNOWN

   Members that have never communicated status information to the replica
   set are in the :replstate:`UNKNOWN` state.

.. replstate:: DOWN

   Members that lose their connection to the replica set enter the
   :replstate:`DOWN` state.

.. replstate:: SHUNNED

   Members that are removed from the replica set enter the :replstate:`SHUNNED`
   state.

.. replstate:: ROLLBACK

   When a :replstate:`SECONDARY` rolls back a write operation after
   transitioning from :replstate:`PRIMARY`, it enters the
   :replstate:`ROLLBACK` state. See
   :doc:`/core/replica-set-rollbacks`.
=====================
Replication Reference
=====================

.. default-domain:: mongodb

Replication Methods in the ``mongo`` Shell
------------------------------------------

.. include:: /includes/toc/table-method-rs.rst

Replication Database Commands
-----------------------------

.. include:: /includes/toc/table-command-replication.rst

Replica Set Reference Documentation
-----------------------------------

.. include:: /includes/toc/dfn-list-replica-set-reference.rst

.. include:: /includes/toc/replica-set-reference.rst
.. _resource-document:

=================
Resource Document
=================

.. default-domain:: mongodb

The resource document specifies the resources upon which a
privilege permits ``actions``.

Database and/or Collection Resource
-----------------------------------

To specify databases and/or collections, use the following syntax:

.. code-block:: javascript

   { db: <database>, collection: <collection> }

.. _resource-specific-db-collection:

Specify a Collection of a Database as Resource
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the resource document species both the ``db`` an ``collection``
fields as non-empty strings, the resource is the specified
collection in the specified database. For example, the following
document specifies a resource of the ``inventory`` collection in the
``products`` database:

.. code-block:: javascript

   { db: "products", collection: "inventory" }

.. include:: /includes/resource-document-facts.rst
   :end-before: admin-resources

.. _resource-specific-db:

Specify a Database as Resource
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If only the ``collection`` field is an empty string (``""``), the
resource is the specified database, excluding the :doc:`system
collections </reference/system-collections>`. For example, the
following resource document specifies the resource of the ``test``
database, excluding the system collections:

.. code-block:: javascript

   { db: "test", collection: "" }

.. include:: /includes/resource-document-facts.rst
   :end-before: admin-resources

.. note:: When you specify a database as the resource, the system
   collections are excluded, unless you name them explicitly, as in the
   following:

   .. code-block:: javascript

      { db: "test", collection: "system.namespaces" }

   System collections include but are not limited to the following:

   - :data:`<database>.system.profile`
   - :data:`<database>.system.namespaces`
   - :data:`<database>.system.indexes`
   - :data:`<database>.system.js`
   - :data:`local.system.replset`
   - :doc:`/reference/system-users-collection` in the ``admin`` database
   - :doc:`/reference/system-roles-collection` in the ``admin`` database

.. _resource-specific-collection:

Specify Collections Across Databases as Resource
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If only the ``db`` field is an empty string (``""``), the resource is
all collections with the specified name across all databases. For
example, the following document specifies the resource of all
the ``accounts`` collections across all the databases:

.. code-block:: javascript

   { db: "", collection: "accounts" }

.. include:: /includes/resource-document-facts.rst
   :start-after: admin-resources

.. _resource-all-but-system-collections:

Specify All Non-System Collections in All Databases
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If both the ``db`` and ``collection`` fields are empty strings
(``""``), the resource is all collections, excluding the :doc:`system
collections </reference/system-collections>`, in all the databases:

.. code-block:: javascript

   { db: "", collection: "" }

.. include:: /includes/resource-document-facts.rst
   :start-after: admin-resources

.. _resource-cluster:

Cluster Resource
----------------

To specify the cluster as the resource, use the following syntax:

.. code-block:: javascript

   { cluster : true }

Use the ``cluster`` resource for actions that affect the state of the
system rather than act on specific set of databases or collections.
Examples of such actions are ``shutdown``, ``replSetReconfig``, and
``addShard``. For example, the following document grants the action
``shutdown`` on the ``cluster``.

.. code-block:: javascript

   { resource: { cluster : true }, actions: [ "shutdown" ] }

.. include:: /includes/resource-document-facts.rst
   :start-after: admin-resources

.. _resource-anyresource:

anyResource
-----------

The internal resource ``anyResource`` gives access to every resource in
the system and is intended for internal use. **Do not** use this resource,
other than in exceptional circumstances. The syntax for this resource is
``{ anyResource: true }``.
==================
Security Reference
==================

.. default-domain:: mongodb

Security Methods in the ``mongo`` Shell
---------------------------------------

.. include:: /includes/toc/table-spec-security-methods.rst

User Management Methods
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-method-user-management.rst

Role Management Methods
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-method-role-management.rst

Security Commands in the ``mongo`` Shell
----------------------------------------

System Event Audit Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-command-audit.rst

User Management Commands
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-command-user-management.rst

Role Management Commands
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/toc/table-command-role-management.rst

Security Reference Documentation
--------------------------------

.. include:: /includes/toc/dfn-list-security-reference.rst

.. include:: /includes/toc/security-reference.rst

Security Release Notes Alerts
-----------------------------

.. include:: /includes/toc/dfn-list-security-release-note-alerts.rst

.. include:: /includes/toc/security-release-note-alerts.rst
:orphan:

====================
Server Status Output
====================

.. default-domain:: mongodb

This document provides a quick overview and example of the
:dbcommand:`serverStatus` command. The helper :method:`db.serverStatus()`
in the :program:`mongo` shell provides access to this output. For full
documentation of the content of this output, see
:doc:`/reference/command/serverStatus`.

.. .. When adding status fields to this document, make sure that the
   ``docs/source/reference/command/serverStatus.txt`` document also reflects
   those changes.

.. note::

   The fields included in this output vary slightly depending on the
   version of MongoDB, underlying operating system platform, and the
   kind of node, including :program:`mongos`, :program:`mongod` or
   :term:`replica set` member.

.. _server-status-example-instance-information:

The :ref:`server-status-instance-information` section displays
information regarding the specific :program:`mongod` and
:program:`mongos` and its state.

.. code-block:: javascript

   {
           "host" : "<hostname>",
           "version" : "<version>",
           "process" : "<mongod|mongos>",
           "pid" : <num>,
           "uptime" : <num>,
           "uptimeMillis" : <num>,
           "uptimeEstimate" : <num>,
           "localTime" : ISODate(""),

.. _server-status-example-locks:

The :ref:`server-status-locks` section reports data that reflect the
state and use of both global (i.e. ``.``) and database specific locks:

.. code-block:: javascript

           "locks" : {
                   "." : {
                           "timeLockedMicros" : {
                                   "R" : <num>,
                                   "W" : <num>
                           },
                           "timeAcquiringMicros" : {
                                   "R" : <num>,
                                   "W" : <num>
                           }
                   },
                   "admin" : {
                           "timeLockedMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           },
                           "timeAcquiringMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           }
                   },
                   "local" : {
                           "timeLockedMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           },
                           "timeAcquiringMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           }
                   },
                   "<database>" : {
                           "timeLockedMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           },
                           "timeAcquiringMicros" : {
                                   "r" : <num>,
                                   "w" : <num>
                           }
                   }
           },

.. _server-status-example-globallock:

The :ref:`server-status-globallock` field reports on MongoDB's
global system lock. In most cases the :ref:`locks <locks>` document
provides more fine grained data that reflects lock use:

.. code-block:: javascript

           "globalLock" : {
                   "totalTime" : <num>,
                   "lockTime" : <num>,
                   "currentQueue" : {
                           "total" : <num>,
                           "readers" : <num>,
                           "writers" : <num>
                   },
                   "activeClients" : {
                           "total" : <num>,
                           "readers" : <num>,
                           "writers" : <num>
                   }
           },

.. _server-status-example-memory:

The :ref:`server-status-memory` field reports on MongoDB's
current memory use:

.. code-block:: javascript

           "mem" : {
                   "bits" : <num>,
                   "resident" : <num>,
                   "virtual" : <num>,
                   "supported" : <boolean>,
                   "mapped" : <num>,
                   "mappedWithJournal" : <num>
           },

.. _server-status-example-connections:

The :ref:`server-status-connections` field reports on MongoDB's
current number of open connections:

.. versionchanged:: 2.4
   The :data:`~serverStatus.connections.totalCreated` field.

.. code-block:: javascript

           "connections" : {
                   "current" : <num>,
                   "available" : <num>,
                   "totalCreated" : NumberLong(<num>)

           },

.. _server-status-example-extrainfo:

The fields in the :ref:`server-status-extra-info` document provide
platform specific information. The following example block is from a
Linux-based system:

.. code-block:: javascript

           "extra_info" : {
                   "note" : "fields vary by platform",
                   "heap_usage_bytes" : <num>,
                   "page_faults" : <num>
           },

.. _server-status-example-indexcounters:

The :ref:`server-status-indexcounters` document reports on index
use:

.. code-block:: javascript

           "indexCounters" : {
                   "accesses" : <num>,
                   "hits" : <num>,
                   "misses" : <num>,
                   "resets" : <num>,
                   "missRatio" : <num>
           },

.. _server-status-example-backgroundflushing:

The :ref:`server-status-backgroundflushing` document reports on the
process MongoDB uses to write data to disk:

.. code-block:: javascript

           "backgroundFlushing" : {
                   "flushes" : <num>,
                   "total_ms" : <num>,
                   "average_ms" : <num>,
                   "last_ms" : <num>,
                   "last_finished" : ISODate("")
           },

.. _server-status-example-cursors:

The :ref:`server-status-cursors` document reports on current cursor
use and state:

.. code-block:: javascript

           "cursors" : {
                   "totalOpen" : <num>,
                   "clientCursors_size" : <num>,
                   "timedOut" : <num>
           },

.. _server-status-example-network:

The :ref:`server-status-network` document reports on network use and
state:

.. code-block:: javascript

           "network" : {
                   "bytesIn" : <num>,
                   "bytesOut" : <num>,
                   "numRequests" : <num>
           },

.. _server-status-example-repl:

The :ref:`server-status-repl` document reports on the state of
replication and the :term:`replica set`. This document only appears
for replica sets.

.. code-block:: javascript

           "repl" : {
                   "setName" : "<string>",
                   "ismaster" : <boolean>,
                   "secondary" : <boolean>,
                   "hosts" : [
                           <hostname>,
                           <hostname>,
                           <hostname>
                   ],
                   "primary" : <hostname>,
                   "me" : <hostname>

.. _server-status-example-opcountersrepl:

The :ref:`server-status-opcounters-repl` document reports the number
of replicated operations:

.. code-block:: javascript

           "opcountersRepl" : {
                   "insert" : <num>,
                   "query" : <num>,
                   "update" : <num>,
                   "delete" : <num>,
                   "getmore" : <num>,
                   "command" : <num>
           },

.. _server-status-example-opcounters:

The :ref:`server-status-opcounters` document reports the number of
operations this MongoDB instance has processed:

.. code-block:: javascript

           "opcounters" : {
                   "insert" : <num>,
                   "query" : <num>,
                   "update" : <num>,
                   "delete" : <num>,
                   "getmore" : <num>,
                   "command" : <num>
           },

.. _server-status-example-asserts:

The :ref:`server-status-asserts` document reports the number of
assertions or errors produced by the server:

.. code-block:: javascript

           "asserts" : {
                   "regular" : <num>,
                   "warning" : <num>,
                   "msg" : <num>,
                   "user" : <num>,
                   "rollovers" : <num>
           },

.. _server-status-example-writebacksqueued:

The :ref:`server-status-writebacksqueued` document reports the number of
:term:`writebacks`:

.. code-block:: javascript

           "writeBacksQueued" : <num>,

.. _server-status-example-journaling:

The :ref:`server-status-journaling` document reports on data that
reflect this :program:`mongod` instance's journaling-related operations
and performance during a :ref:`journal group commit interval
<journaling-journal-commit-interval>`:

.. code-block:: javascript

           "dur" : {
                   "commits" : <num>,
                   "journaledMB" : <num>,
                   "writeToDataFilesMB" : <num>,
                   "compression" : <num>,
                   "commitsInWriteLock" : <num>,
                   "earlyCommits" : <num>,
                   "timeMs" : {
                           "dt" : <num>,
                           "prepLogBuffer" : <num>,
                           "writeToJournal" : <num>,
                           "writeToDataFiles" : <num>,
                           "remapPrivateView" : <num>
                   }
           },

.. _server-status-example-recordstats:

The :ref:`server-status-recordstats` document reports data on
MongoDB's ability to predict page faults and yield write operations
when required data isn't in memory:

.. code-block:: javascript

           "recordStats" : {
                   "accessesNotInMemory" : <num>,
                   "pageFaultExceptionsThrown" : <num>,
                   "local" : {
                           "accessesNotInMemory" : <num>,
                           "pageFaultExceptionsThrown" : <num>
                   },
                   "<database>" : {
                           "accessesNotInMemory" : <num>,
                           "pageFaultExceptionsThrown" : <num>
                   }
           },

.. _server-status-example-workingset:

The :ref:`server-status-workingset` document provides an estimated
size of the MongoDB instance's working set. This data may not exactly
reflect the size of the working set in all cases. Additionally, the
:data:`~serverStatus.workingSet` document is only present in the
output of :dbcommand:`serverStatus` when explicitly enabled.

.. versionadded:: 2.4

.. code-block:: javascript

        "workingSet" : {
                "note" : "thisIsAnEstimate",
                "pagesInMemory" : <num>,
                "computationTimeMicros" : <num>,
                "overSeconds" : num
        },

.. _server-status-example-metrics:

The :ref:`server-status-metrics` document contains a number of
operational metrics that are useful for monitoring the state and
workload of a :program:`mongod` instance.

.. versionadded:: 2.4

.. code-block:: javascript

        "metrics" : {
                "document" : {
                        "deleted" : NumberLong(<num>),
                        "inserted" : NumberLong(<num>),
                        "returned" : NumberLong(<num>),
                        "updated" : NumberLong(<num>)
                },
                "getLastError" : {
                        "wtime" : {
                                "num" : <num>,
                                "totalMillis" : <num>
                        },
                        "wtimeouts" : NumberLong(<num>)
                },
                "operation" : {
                        "fastmod" : NumberLong(<num>),
                        "idhack" : NumberLong(<num>),
                        "scanAndOrder" : NumberLong(<num>)
                },
                "queryExecutor": {
                        "scanned" : NumberLong(<num>)
                },
                "record" : {
                        "moves" : NumberLong(<num>)
                },
                "repl" : {
                        "apply" : {
                                "batches" : {
                                        "num" : <num>,
                                        "totalMillis" : <num>
                                },
                                "ops" : NumberLong(<num>)
                        },
                        "buffer" : {
                                "count" : NumberLong(<num>),
                                "maxSizeBytes" : <num>,
                                "sizeBytes" : NumberLong(<num>)
                        },
                        "network" : {
                                "bytes" : NumberLong(<num>),
                                "getmores" : {
                                        "num" : <num>,
                                        "totalMillis" : <num>
                                },
                                "ops" : NumberLong(<num>),
                                "readersCreated" : NumberLong(<num>)
                        },
                        "oplog" : {
                                "insert" : {
                                        "num" : <num>,
                                        "totalMillis" : <num>
                                },
                                "insertBytes" : NumberLong(<num>)
                        },
                        "preload" : {
                                "docs" : {
                                        "num" : <num>,
                                        "totalMillis" : <num>
                                },
                                "indexes" : {
                                        "num" : <num>,
                                        "totalMillis" : <num>
                                }
                        }
                },
                "ttl" : {
                        "deletedDocuments" : NumberLong(<num>),
                        "passes" : NumberLong(<num>)
                }
        },

The final ``ok`` field holds the return status for the
:dbcommand:`serverStatus` command:

.. code-block:: javascript

           "ok" : 1
   }
==================
Sharding Reference
==================

.. default-domain:: mongodb

Sharding Methods in the ``mongo`` Shell
---------------------------------------

.. include:: /includes/toc/table-method-sh.rst

Sharding Database Commands
--------------------------

The following database commands support :term:`sharded clusters
<sharded cluster>`.

.. include:: /includes/toc/table-command-sharding.rst

Reference Documentation
-----------------------

.. include:: /includes/toc/dfn-list-sharding-reference.rst

.. include:: /includes/toc/sharding-reference.rst
================================
SQL to Aggregation Mapping Chart
================================

.. default-domain:: mongodb

.. all of the included table files are built from corresponding .yaml
   files in the includes directory. To change the content of the
   tables, edit those files.

The :doc:`aggregation pipeline </core/aggregation>` allows
MongoDB to provide native aggregation capabilities that corresponds to
many common data aggregation operations in SQL. If you're new to
MongoDB you might want to consider the :doc:`/faq` section for a
selection of common questions.

The following table provides an overview of common SQL aggregation
terms, functions, and concepts and the corresponding MongoDB
:ref:`aggregation operators <aggregation-pipeline-operator-reference>`:

.. include:: /includes/table/sql-to-agg-terms.rst

Examples
--------

The following table presents a quick reference of SQL aggregation
statements and the corresponding MongoDB statements. The examples in
the table assume the following conditions:

- The SQL examples assume *two* tables, ``orders`` and
  ``order_lineitem`` that join by the ``order_lineitem.order_id`` and
  the ``orders.id`` columns.

- The MongoDB examples assume *one* collection ``orders`` that contain
  documents of the following prototype:

  .. code-block:: javascript

     {
       cust_id: "abc123",
       ord_date: ISODate("2012-11-02T17:04:11.102Z"),
       status: 'A',
       price: 50,
       items: [ { sku: "xxx", qty: 25, price: 1 },
                { sku: "yyy", qty: 25, price: 1 } ]
     }

- The MongoDB statements prefix the names of the fields from the
  :term:`documents <document>` in the collection ``orders`` with a
  ``$`` character when they appear as operands to the aggregation
  operations.

.. include:: /includes/table/sql-to-agg-examples.rst
============================
SQL to MongoDB Mapping Chart
============================

.. default-domain:: mongodb

.. all of the included table files are built from corresponding .yaml
   files in the includes directory. To change the content of the
   tables, edit those files.

In addition to the charts that follow, you might want to consider the
:doc:`/faq` section for a selection of common questions about MongoDB.

Terminology and Concepts
------------------------

The following table presents the various SQL terminology and concepts
and the corresponding MongoDB terminology and concepts.

.. include:: /includes/table/sql-to-mongo-terms.rst

Executables
-----------

The following table presents the MySQL/Oracle executables and the
corresponding MongoDB executables.

.. include:: /includes/table/sql-to-mongo-executables.rst

Examples
--------

The following table presents the various SQL statements and the
corresponding MongoDB statements. The examples in the table assume the
following conditions:

- The SQL examples assume a table named ``users``.

- The MongoDB examples assume a collection named ``users`` that contain
  documents of the following prototype:

  .. code-block:: javascript

     {
       _id: ObjectId("509a8fb2f3f4948bd2f983a0"),
       user_id: "abc123",
       age: 55,
       status: 'A'
     }

Create and Alter
~~~~~~~~~~~~~~~~

The following table presents the various SQL statements related to
table-level actions and the corresponding MongoDB statements.

.. include:: /includes/table/sql-to-mongo-schema-examples.rst

For more information, see :method:`db.collection.insert()`,
:method:`db.createCollection()`, :method:`db.collection.update()`,
:update:`$set`, :update:`$unset`,
:method:`db.collection.ensureIndex()`, :doc:`indexes </core/indexes>`,
:method:`db.collection.drop()`, and :doc:`/core/data-models`.

Insert
~~~~~~

The following table presents the various SQL statements related to
inserting records into tables and the corresponding MongoDB statements.

.. include:: /includes/table/sql-to-mongo-insert-examples.rst

For more information, see :method:`db.collection.insert()`.

Select
~~~~~~

The following table presents the various SQL statements related to
reading records from tables and the corresponding MongoDB statements.

.. include:: /includes/table/sql-to-mongo-select-examples.rst

For more information, see
:method:`db.collection.find()`, :method:`db.collection.distinct()`,
:method:`db.collection.findOne()`, :query:`$ne`
:query:`$and`, :query:`$or`, :query:`$gt`, :query:`$lt`,
:query:`$exists`, :query:`$lte`, :query:`$regex`,
:method:`~cursor.limit()`, :method:`~cursor.skip()`,
:method:`~cursor.explain()`, :method:`~cursor.sort()`, and
:method:`~cursor.count()`.

Update Records
~~~~~~~~~~~~~~

The following table presents the various SQL statements related to
updating existing records in tables and the corresponding MongoDB
statements.

.. include:: /includes/table/sql-to-mongo-update-examples.rst

For more information, see :method:`db.collection.update()`,
:update:`$set`, :update:`$inc`, and :query:`$gt`.

Delete Records
~~~~~~~~~~~~~~

The following table presents the various SQL statements related to
deleting records from tables and the corresponding MongoDB statements.

.. include:: /includes/table/sql-to-mongo-delete-examples.rst

For more information, see :method:`db.collection.remove()`.
.. index:: collection; system
.. index:: system; collections
.. index:: system; namespace
.. index:: namespace; system
.. _metadata-system-collections:

==================
System Collections
==================

.. default-domain:: mongodb

Synopsis
--------

MongoDB stores system information in collections that use the
``<database>.system.*`` :term:`namespace`, which MongoDB reserves for
internal use. Do not create collections that begin with ``system``.

MongoDB also stores some additional instance-local metadata in the
:doc:`local database </reference/local-database>`, specifically for
replication purposes.

Collections
-----------

System collections include these collections stored in the ``admin`` database:

.. data:: admin.system.roles

   .. versionadded:: 2.6

   The :data:`admin.system.roles` collection stores custom roles that
   administrators create and assign to users to provide access to
   specific resources.

.. data:: admin.system.users

   .. versionchanged:: 2.6

   The :data:`admin.system.users` collection stores the user's
   authentication credentials as well as any roles assigned to the user.
   Users may define authorization roles in the
   :data:`admin.system.roles` collection.

.. data:: admin.system.version

   .. versionadded:: 2.6

   Stores the schema version of the user credential documents.

System collections also include these collections stored directly in
each database:

.. data:: <database>.system.namespaces

   The :data:`<database>.system.namespaces` collection contains
   information about all of the database’s collections. Additional
   namespace metadata exists in the ``database.ns`` files and is
   opaque to database users.

.. data:: <database>.system.indexes

   The :data:`<database>.system.indexes` collection lists all the
   indexes in the database. Add and remove data from this collection
   via the :method:`~db.collection.ensureIndex()` and
   :method:`~db.collection.dropIndex()`

.. data:: <database>.system.profile

   The :data:`<database>.system.profile` collection stores database
   profiling information. For information on profiling, see :ref:`database-profiling`.

.. data:: <database>.system.js

   The :data:`<database>.system.js` collection holds special JavaScript
   code for use in :doc:`server side JavaScript
   </core/server-side-javascript>`. See
   :doc:`/tutorial/store-javascript-function-on-server` for
   more information.
===========================
``system.roles`` Collection
===========================

.. versionadded:: 2.6

.. default-domain:: mongodb

The ``system.roles`` collection in the ``admin`` database stores the
user-defined roles. To create and manage these user-defined
roles, MongoDB provides :ref:`role management commands
<role-management-commands>`.

.. _admin-system-roles-collection:

``system.roles`` Schema
-----------------------

The documents in the ``system.roles`` collection have the following
schema:

.. code-block:: javascript

   {
     _id: <system-defined id>,
     role: "<role name>",
     db: "<database>",
     privileges:
         [
             {
                 resource: { <resource> },
                 actions: [ "<action>", ... ]
             },
             ...
         ],
     roles:
         [
             { role: "<role name>", db: "<database>" },
             ...
         ]
   }

A ``system.roles`` document has the following fields:

.. data:: admin.system.roles.role

   The :data:`~admin.system.roles.role` field is a string that
   specifies the name of the role.

.. data:: admin.system.roles.db

   The :data:`~admin.system.roles.db` field is a string that specifies
   the database to which the role belongs. MongoDB uniquely identifies
   each role by the pairing of its name (i.e.
   :data:`~admin.system.roles.role`) and its database.

.. data:: admin.system.roles.privileges

   The :data:`~admin.system.roles.privileges` array contains the
   privilege documents that define the :ref:`privileges <privileges>` for the role.

   A privilege document has the following syntax:

   .. code-block:: javascript

      {
        resource: { <resource> },
        actions: [ "<action>", ... ]
      }

   Each privilege document has the following fields:

   .. data:: admin.system.roles.privileges[n].resource

      A document that specifies the resources upon which the privilege
      :data:`~admin.system.roles.privileges[n].actions` apply. The document
      has one of the following form:

      .. code-block:: javascript

        { db: <database>, collection: <collection> }

      or

      .. code-block:: javascript

         { cluster : true }

      See :ref:`resource-document` for more details.

   .. data:: admin.system.roles.privileges[n].actions

      An array of actions permitted on the resource. For a list of
      actions, see :ref:`security-user-actions`.

.. data:: admin.system.roles.roles

   The :data:`~admin.system.roles.roles` array contains role documents
   that specify the roles from which this role :ref:`inherits <inheritance>` privileges.

   A role document has the following syntax:

   .. code-block:: javascript

      { role: "<role name>", db: "<database>" }

   A role document has the following fields:

   .. data:: admin.system.roles.roles[n].role

      The name of the role. A role can be a :ref:`built-in role
      <built-in-roles>` provided by MongoDB or a :ref:`user-defined
      role <user-defined-roles>`.

   .. data:: admin.system.roles.roles[n].db

      The name of the database where the role is defined.

Examples
--------

Consider the following sample documents found in ``system.roles``
collection of the ``admin`` database.

A User-Defined Role Specifies Privileges
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following is a sample document for a user-defined role ``appUser``
defined for the ``myApp`` database:

.. code-block:: javascript

   {
     _id: "myApp.appUser",
     role: "appUser",
     db: "myApp",
     privileges: [
          { resource: { db: "myApp" , collection: "" },
            actions: [ "find", "createCollection", "dbStats", "collStats" ] },
          { resource: { db: "myApp", collection: "logs" },
            actions: [ "insert" ] },
          { resource: { db: "myApp", collection: "data" },
            actions: [ "insert", "update", "remove", "compact" ] },
          { resource: { db: "myApp", collection: "system.indexes" },
            actions: [ "find" ] },
          { resource: { db: "myApp", collection: "system.namespaces" },
            actions: [ "find" ] },
     ],
     roles: []
   }

The ``privileges`` array lists the five privileges that the ``appUser``
role specifies:

- The first privilege permits its actions ( ``"find"``,
  ``"createCollection"``, ``"dbStats"``, ``"collStats"``) on all the
  collections in the ``myApp`` database *excluding* its system
  collections. See :ref:`resource-specific-db`.

- The next two privileges permits *additional* actions on specific
  collections, ``logs`` and ``data``, in the ``myApp`` database. See
  :ref:`resource-specific-db-collection`.

- The last two privileges permits actions on two :doc:`system
  collections </reference/system-collections>` in the ``myApp``
  database. While the first privilege gives database-wide permission
  for the ``find`` action, the action does not apply to ``myApp``'s
  system collections. To give access to a system collection, a
  privilege must explicitly specify the collection. See
  :doc:`/reference/resource-document`.

As indicated by the empty ``roles`` array, ``appUser`` inherits no
additional privileges from other roles.

User-Defined Role Inherits from Other Roles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following is a sample document for a user-defined role ``appAdmin``
defined for the ``myApp`` database: The document shows that the
``appAdmin`` role specifies privileges as well as inherits privileges
from other roles:

.. code-block:: javascript

   {
     _id: "myApp.appAdmin",
     role: "appAdmin",
     db: "myApp",
     privileges: [
                   {
                     resource: { db: "myApp", collection: "" },
                     actions: [ "insert", "dbStats", "collStats", "compact", "repairDatabase" ]
                   }
                 ],
     roles: [
              { role: "appUser", db: "myApp" }
            ]
   }

The ``privileges`` array lists the privileges that the ``appAdmin``
role specifies. This role has a single privilege that permits its
actions ( ``"insert"``, ``"dbStats"``, ``"collStats"``, ``"compact"``,
``"repairDatabase"``) on all the collections in the ``myApp`` database
*excluding* its system collections. See :ref:`resource-specific-db`.

The ``roles`` array lists the roles, identified by the role names and
databases, from which the role ``appAdmin`` inherits privileges.
===========================
``system.users`` Collection
===========================

.. versionchanged:: 2.6

.. default-domain:: mongodb

The ``system.users`` collection in the ``admin`` database stores user
:ref:`authentication <authentication>` and :ref:`authorization
<authorization>` information. To manage data in this collection,
MongoDB provides :ref:`user management commands
<user-management-commands>`.

.. _admin-system-users-collection:
.. _delegated-credentials:

``system.users`` Schema
-----------------------

The documents in the ``system.users`` collection have the following
schema:

.. code-block:: javascript

   {
     _id: <system defined id>,
     user: "<name>",
     db: "<database>",
     credentials: { <authentication credentials> },
     roles: [
              { role: "<role name>", db: "<database>" },
              ...
            ],
     customData: <custom information>
    }

Each ``system.users`` document has the following fields:

.. data:: admin.system.users.user

   The :data:`~admin.system.users.user` field is a string that
   identifies the user. A user exists in the context of a single logical
   database but can have access to other databases through roles
   specified in the :data:`~admin.system.users.roles` array.

.. data:: admin.system.users.db

   The :data:`~admin.system.users.db` field specifies the database
   associated with the user. The user's privileges are not necessarily
   limited to this database. The user can have privileges in additional
   databases through the :data:`~admin.system.users.roles` array.

.. data:: admin.system.users.credentials

   The :data:`~admin.system.users.credentials` field contains the
   user's authentication information. For users with externally stored
   authentication credentials, such as users that use :doc:`Kerberos
   </tutorial/control-access-to-mongodb-with-kerberos-authentication>`
   or x.509 certificates for authentication, the ``system.users``
   document for that user does not contain the
   :data:`~admin.system.users.credentials` field.

.. data:: admin.system.users.roles

   The :data:`~admin.system.users.roles` array contains role documents
   that specify the roles granted to the user. The array contains both
   :ref:`built-in roles <built-in-roles>` and :ref:`user-defined
   role <user-defined-roles>`.

   A role document has the following syntax:

   .. code-block:: javascript

      { role: "<role name>", db: "<database>" }

   A role document has the following fields:

   .. data:: admin.system.users.roles[n].role

      The name of a role. A role can be a :ref:`built-in role
      <built-in-roles>` provided by MongoDB or a :ref:`custom
      user-defined role <user-defined-roles>`.

   .. data:: admin.system.users.roles[n].db

      The name of the database where role is defined.

   When specifying a role using the :ref:`role management
   <role-management-commands>` or :ref:`user management
   <user-management-commands>` commands, you can specify the role name alone
   (e.g. ``"readWrite"``) if the role that exists on the database on which
   the command is run.

.. data:: admin.system.users.customData

   The :data:`~admin.system.users.customData` field contains optional
   custom information about the user.

Example
-------

Consider the following document in the ``system.users`` collection:

.. code-block:: javascript

   {
     _id: "home.Kari",
     user: "Kari",
     db: "home",
     credentials: { "MONGODB-CR" :"<hashed password>" },
     roles : [
               { role: "read", db: "home" },
               { role: "readWrite", db: "test" },
               { role: "appUser", db: "myApp" }
             ],
     customData: { zipCode: "64157" }
   }

The document shows that a user ``Kari`` is associated with the ``home``
database. ``Kari`` has the :authrole:`read` role in the ``home``
database, the :authrole:`readWrite` role in the ``test`` database, and
the ``appUser`` role in the ``myApp`` database.
.. _text-search-languages:

=====================
Text Search Languages
=====================

.. default-domain:: mongodb

The :ref:`text index <index-feature-text>`, the :query:`$text`
operator, and the :dbcommand:`text` command [#text-command]_ support
the following languages:

.. versionchanged:: 2.6

   MongoDB introduces version 2 of the text search feature. With
   version 2, text search feature supports using the two-letter
   language codes defined in ISO 639-1. Version 1 of text search only
   supported the long form of each language name.

- ``da`` or ``danish``
- ``nl`` or ``dutch``
- ``en`` or ``english``
- ``fi`` or ``finnish``
- ``fr`` or ``french``
- ``de`` or ``german``
- ``hu`` or ``hungarian``
- ``it`` or ``italian``
- ``no`` or ``norwegian``
- ``pt`` or ``portuguese``
- ``ro`` or ``romanian``
- ``ru`` or ``russian``
- ``es`` or ``spanish``
- ``sv`` or ``swedish``
- ``tr`` or ``turkish``

.. note::

   If you specify a language value of ``"none"``, then the text search
   has no list of stop words, and the text search does not stem or
   tokenize the search terms.

.. [#text-command] The :dbcommand:`text` command is deprecated in MongoDB 2.6.
=========================
UNIX ``ulimit`` Settings
=========================

.. default-domain:: mongodb

Most UNIX-like operating systems, including Linux and OS X, provide
ways to limit and control the usage of system resources such as
threads, files, and network connections on a per-process and per-user
basis. These "ulimits" prevent single users from using too many system
resources. Sometimes, these limits have low default values that can
cause a number of issues in the course of normal MongoDB operation.

.. note::

   Red Hat Enterprise Linux and CentOS 6 place a max process
   limitation of 1024 which overrides ``ulimit`` settings. Create a
   file named ``/etc/security/limits.d/99-mongodb-nproc.conf`` with
   new ``soft nproc`` and ``hard nproc`` values to increase the
   process limit. See ``/etc/security/limits.d/90-nproc.conf`` file as
   an example.

.. _system-resource-utilization:

Resource Utilization
--------------------

:program:`mongod` and :program:`mongos` each use threads and file
descriptors to track connections and manage internal operations. This
section outlines the general resource utilization patterns for MongoDB.
Use these figures in combination with the actual information about your
deployment and its use to determine ideal ``ulimit`` settings.

Generally, all :program:`mongod` and :program:`mongos` instances:

- track each incoming connection with a file descriptor *and* a
  thread.

- track each internal thread or *pthread* as a system process.

``mongod``
~~~~~~~~~~

- 1 file descriptor for each data file in use by the
  :program:`mongod` instance.

- 1 file descriptor for each journal file used by the
  :program:`mongod` instance when :setting:`storage.journal.enabled` is ``true``.

- In replica sets, each :program:`mongod` maintains a connection to
  all other members of the set.

:program:`mongod` uses background threads for a number of internal
processes, including :ref:`TTL collections <ttl-collections>`,
replication, and replica set health checks, which may require a small
number of additional resources.

.. _mongos-connection-use information:

``mongos``
~~~~~~~~~~

In addition to the threads and file descriptors for client
connections, :program:`mongos` must maintain connects to all config
servers and all shards, which includes all members of all replica
sets.

For :program:`mongos`, consider the following behaviors:

- :program:`mongos` instances maintain a connection pool to each shard
  so that the :program:`mongos` can reuse connections and quickly
  fulfill requests without needing to create new connections.

- You can limit the number of incoming connections using
  the :setting:`~net.maxIncomingConnections` run-time option.

  By restricting the number of incoming connections you can prevent a
  cascade effect where the :program:`mongos` creates too many
  connections on the :program:`mongod` instances.

  .. include:: /includes/note-max-conns-max.rst

Review and Set Resource Limits
------------------------------

``ulimit``
~~~~~~~~~~

.. note::

   Both the "hard" and the "soft" ``ulimit`` affect MongoDB's
   performance. The "hard" ``ulimit`` refers to the maximum number of
   processes that a user can have active at any time. This is the
   ceiling: no non-root process can increase the "hard" ``ulimit``. In
   contrast, the "soft" ``ulimit`` is the limit that is actually
   enforced for a session or process, but any process can increase it
   up to "hard" ``ulimit`` maximum.

   A low "soft" ``ulimit`` can cause ``can't create new thread,
   closing connection`` errors if the number of connections
   grows too high. For this reason, it is extremely important to set
   *both* ``ulimit`` values to the recommended values.

You can use the ``ulimit`` command at the system prompt to check
system limits, as in the following example:

.. code-block:: sh

   $ ulimit -a
   -t: cpu time (seconds)         unlimited
   -f: file size (blocks)         unlimited
   -d: data seg size (kbytes)     unlimited
   -s: stack size (kbytes)        8192
   -c: core file size (blocks)    0
   -m: resident set size (kbytes) unlimited
   -u: processes                  192276
   -n: file descriptors           21000
   -l: locked-in-memory size (kb) 40000
   -v: address space (kb)         unlimited
   -x: file locks                 unlimited
   -i: pending signals            192276
   -q: bytes in POSIX msg queues  819200
   -e: max nice                   30
   -r: max rt priority            65
   -N 15:                         unlimited

``ulimit`` refers to the per-*user* limitations for various
resources. Therefore, if your :program:`mongod` instance executes as a
user that is also running multiple processes, or multiple
:program:`mongod` processes, you might see contention for these
resources. Also, be aware that the ``processes`` value (i.e. ``-u``)
refers to the combined number of distinct processes and sub-process
threads.

You can change ``ulimit`` settings by issuing a command in the
following form:

.. code-block:: sh

   ulimit -n <value>

For many distributions of Linux you can change values by substituting
the ``-n`` option for any possible value in the output of ``ulimit
-a``. On OS X, use the ``launchctl limit`` command.  See your
operating system documentation for the precise procedure for changing
system limits on running systems.

.. note::

   After changing the ``ulimit`` settings, you *must* restart the
   process to take advantage of the modified settings. You can use the
   ``/proc`` file system to see the current limitations on a running
   process.

   Depending on your system's configuration, and default settings, any
   change to system limits made using ``ulimit`` may revert following
   system a system restart. Check your distribution and operating
   system documentation for more information.

.. _proc-file-system:

``/proc`` File System
~~~~~~~~~~~~~~~~~~~~~

.. note::

   This section applies only to Linux operating systems.

The ``/proc`` file-system stores the per-process limits in the
file system object located at ``/proc/<pid>/limits``, where ``<pid>``
is the process's :term:`PID` or process identifier. You can use the
following ``bash`` function to return the content of the ``limits``
object for a process or processes with a given name:

.. code-block:: sh

   return-limits(){

        for process in $@; do
             process_pids=`ps -C $process -o pid --no-headers | cut -d " " -f 2`

             if [ -z $@ ]; then
                echo "[no $process running]"
             else
                for pid in $process_pids; do
                      echo "[$process #$pid -- limits]"
                      cat /proc/$pid/limits
                done
             fi

        done

   }

You can copy and paste this function into a current shell session or
load it as part of a script. Call the function with one the following
invocations:

.. code-block:: sh

   return-limits mongod
   return-limits mongos
   return-limits mongod mongos

Recommended Settings
--------------------

Every deployment may have unique requirements and settings; however,
the following thresholds and settings are particularly important for
:program:`mongod` and :program:`mongos` deployments:

- ``-f`` (file size): ``unlimited``
- ``-t`` (cpu time): ``unlimited``
- ``-v`` (virtual memory): ``unlimited`` [#memory-size]_
- ``-n`` (open files): ``64000``
- ``-m`` (memory size): ``unlimited`` [#memory-size]_
- ``-u`` (processes/threads): ``64000``

Always remember to restart your :program:`mongod` and
:program:`mongos` instances after changing the ``ulimit`` settings to
make sure that the settings change takes effect.

.. [#memory-size] If you limit virtual or resident memory size on a
   system running MongoDB the operating system will refuse to honor
   additional allocation requests.
.. _write-concern-operation:
.. _write-concern-internals:

=======================
Write Concern Reference
=======================

.. default-domain:: mongodb

:doc:`Write concern </core/write-concern>` describes the guarantee that
MongoDB provides when reporting on the success of a write operation.

.. versionchanged:: 2.6
   A new protocol for :ref:`write operations
   <rel-notes-write-operations>` integrates write concerns with the
   write operations and eliminates the need to call the
   :dbcommand:`getLastError` command. Previous versions required a
   :dbcommand:`getLastError` command immediately after a write
   operation to specify the write concern.

Read Isolation Behavior
-----------------------

.. include:: /includes/fact-write-concern-read-uncommitted.rst

Available Write Concern
-----------------------

Write concern can include the :ref:`w <wc-w>` option to specify the
required number of acknowledgments before returning, the :ref:`j
<wc-j>` option to require writes to the journal before returning, and
:ref:`wtimeout <wc-wtimeout>` option to specify a time limit to prevent
write operations from blocking indefinitely.

In sharded clusters, :program:`mongos` instances will pass the write
concern on to the shard.

.. _wc-w:

``w`` Option
~~~~~~~~~~~~

The ``w`` option provides the ability to disable write concern entirely
*as well as* specify the write concern for :term:`replica sets <replica
set>`. 

MongoDB uses ``w: 1`` as the default write concern. ``w: 1``
provides basic receipt acknowledgment.

The ``w`` option accepts the following values:

.. list-table::
   :header-rows: 1
   :widths: 25 75

   * - Value

     - Description

   * - ``1``

     - Provides acknowledgment of write operations on a standalone
       :program:`mongod` or the :term:`primary` in a replica set.

       This is the default write concern for MongoDB.

   * - ``0``

     - Disables basic acknowledgment of write operations, but returns
       information about socket exceptions and networking errors to the
       application.

       If you disable basic write operation acknowledgment but require
       journal commit acknowledgment, the journal commit prevails, and
       the server will require that :program:`mongod` acknowledge
       the write operation.

   * - <Number greater than 1>

     - Guarantees that write operations have propagated successfully to
       the specified number of replica set members including the
       primary.

       For example, ``w: 2`` indicates acknowledgements from the
       primary and at least one secondary.

       If you set ``w`` to a number that is greater than the number of
       set members that hold data, MongoDB waits for the non-existent
       members to become available, which means MongoDB blocks
       indefinitely.

   * - ``"majority"``

     - Confirms that write operations have propagated to the majority
       of configured replica set: a majority of the set's configured
       members must acknowledge the write operation before it succeeds.
       This allows you to avoid hard coding assumptions about the size
       of your replica set into your application.

       .. include:: /includes/fact-master-slave-majority.rst

   * - <tag set>

     - By specifying a :ref:`tag set
       <replica-set-configuration-tag-sets>`, you can have fine-grained
       control over which replica set members must acknowledge a write
       operation to satisfy the required level of write concern.

.. _wc-j:

``j`` Option
~~~~~~~~~~~~

The ``j`` option confirms that the :program:`mongod` instance has
written the data to the on-disk journal. This ensures that data is not
lost if the :program:`mongod` instance shuts down unexpectedly. Set to
``true`` to enable.

.. versionchanged:: 2.6
   Specifying a write concern that includes ``j: true`` to
   a :program:`mongod` or :program:`mongos` running with
   :option:`--nojournal` option now errors. Previous versions would
   ignore the ``j: true``.

.. include:: /includes/note-write-concern-journaled-replication.rst

.. _wc-wtimeout:

``wtimeout``
~~~~~~~~~~~~

This option specifies a time limit, in milliseconds, for the write
concern. This option causes write operations to return upon reaching
this limit, even if the required write concern has yet to be
fulfilled.

If you do not specify the ``wtimeout`` option and the level of write
concern is unachievable, the write operation will block indefinitely.
Specifying a ``wtimeout`` value of ``0`` is equivalent to a write
concern without the ``wtimeout`` option.

.. seealso:: :doc:`Write Concern Introduction </core/write-concern>`,
   :ref:`Write Concern for Replica Sets <replica-set-write-concern>`
=========
Reference
=========

.. default-domain:: mongodb

.. include:: /includes/toc/dfn-list-reference-landing.rst

.. seealso:: The :ref:`genindex` may provide useful insight into the
   reference material in this manual. The :doc:`/reference/crud`
   :doc:`/reference/data-models`, :doc:`/reference/sharding`,
   :doc:`/reference/replication`, and :doc:`/reference/security`
   contain additional reference material.

.. include:: /includes/toc/reference-landing.rst
:orphan:

======================
Changes in MongoDB 1.2
======================

.. toctree::

   1.2
===============================
Release Notes for MongoDB 1.2.x
===============================

.. default-domain:: mongodb

New Features
------------

- More indexes per collection

- Faster index creation

- Map/Reduce

- Stored JavaScript functions

- Configurable fsync time

- Several small features and fixes

DB Upgrade Required
-------------------

There are some changes that will require doing an upgrade if your
previous version is <= 1.0.x. If you're already using a version >= 1.1.x
then these changes aren't required. There are 2 ways to do it:

- ``--upgrade``

  - stop your :program:`mongod` process

  - run ``./mongod --upgrade``

  - start :program:`mongod` again

- use a slave

  - start a slave on a different port and data directory

  - when its synced, shut down the master, and start the new slave on
    the regular port.

Ask in the forums or IRC for more help.

Replication Changes
-------------------

- There have been minor changes in replication. If you are upgrading a
  master/slave setup from <= 1.1.2 you have to update the slave first.

mongoimport
-----------

- ``mongoimportjson`` has been removed and is replaced with
  :doc:`mongoimport </reference/program/mongoimport>` that can do json/csv/tsv

field filter changing
---------------------

- We've changed the semantics of the field filter a little bit.
  Previously only objects with those fields would be returned. Now the
  field filter only changes the output, not which objects are returned.
  If you need that behavior, you can use :doc:`$exists </reference/operator/query/exists>`
:orphan:

======================
Changes in MongoDB 1.4
======================

.. toctree::

   1.4
=============================
Release Notes for MongoDB 1.4
=============================

.. default-domain:: mongodb

Upgrading
---------

We're pleased to announce the 1.4 release of MongoDB. 1.4 is a drop-in
replacement for 1.2. To upgrade you just need to shutdown
:program:`mongod`, then restart with the new binaries. (Users upgrading
from release 1.0 should review the :doc:`1.2 release notes </release-notes/1.2>`,
in particular the instructions for upgrading the DB format.)

Release 1.4 includes the following improvements over release 1.2:

Core Server Enhancements
------------------------

- :doc:`concurrency </faq/concurrency>` improvements

- indexing memory improvements

- :ref:`background index creation <index-creation-background>`

- better detection of regular expressions so the index can be used in
  more cases

Replication and Sharding
------------------------

- better handling for restarting slaves offline for a while

- fast new slaves from snapshots (``--fastsync``)

- configurable slave delay (``--slavedelay``)

- replication handles clock skew on master

- :doc:`$inc </reference/operator/update/inc>` replication fixes

- sharding alpha 3 - notably 2-phase commit on config servers

Deployment and Production
-------------------------

- :ref:`configure "slow threshold" for profiling <database-profiling-levels>`

- ability to do :doc:`fsync + lock </reference/command/fsync>` for backing up raw files

- option for separate directory per db (``--directoryperdb``)

- ``http://localhost:28017/_status`` to get serverStatus via http

- REST interface is off by default for security (``--rest`` to enable)

- can rotate logs with a db command, :doc:`logRotate </reference/command/logRotate>`

- enhancements to :doc:`serverStatus </reference/command/serverStatus/>`
  command (db.serverStatus()) - counters and :ref:`replication lag
  <replica-set-replication-lag>` stats

- new :doc:`mongostat </reference/program/mongostat>` tool

Query Language Improvements
---------------------------

- :doc:`$all </reference/operator/query/all>` with regex

- :doc:`$not </reference/operator/query/not>`

- partial matching of array elements :doc:`$elemMatch </reference/operator/projection/elemMatch>`

- $ operator for updating arrays

- :doc:`$addToSet </reference/operator/update/addToSet>`

- :doc:`$unset </reference/operator/update/unset>`

- :doc:`$pull </reference/operator/update/pull>` supports object matching

- :doc:`$set </reference/operator/update/set>` with array indexes

Geo
---

- :doc:`2d geospatial search </core/geospatial-indexes>`

- geo :doc:`$center </reference/operator/query/center>` and :doc:`$box
  </reference/operator/query/box>` searches
:orphan:

======================
Changes in MongoDB 1.6
======================

.. toctree::

   1.6
=============================
Release Notes for MongoDB 1.6
=============================

.. default-domain:: mongodb

Upgrading
---------

MongoDB 1.6 is a drop-in replacement for 1.4. To upgrade, simply
shutdown :program:`mongod` then restart with the new binaries.

*Please note that you should upgrade to the latest version of whichever
driver you're using. Certain drivers, including the Ruby driver, will
require the upgrade, and all the drivers will provide extra features for
connecting to replica sets.*

Sharding
--------

:doc:`/sharding` is now production-ready, making MongoDB horizontally
scalable, with no single point of failure. A single instance of
:program:`mongod` can now be upgraded to a distributed cluster with zero
downtime when the need arises.

- :doc:`/sharding`

- :doc:`/tutorial/deploy-shard-cluster`

- :doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`

Replica Sets
------------

:doc:`Replica sets </replication>`, which provide automated failover
among a cluster of ``n`` nodes, are also now available.

Please note that replica pairs are now deprecated; we strongly recommend
that replica pair users upgrade to replica sets.

- :doc:`/replication`

- :doc:`/tutorial/deploy-replica-set`

- :doc:`/tutorial/convert-standalone-to-replica-set`

Other Improvements
------------------

- The ``w`` option (and ``wtimeout``) forces writes to be propagated to ``n``
  servers before returning success (this works especially well with
  replica sets)

- :doc:`$or queries </reference/operator/query/or>`

- Improved concurrency

- :doc:`$slice </reference/operator/projection/slice>` operator for returning
  subsets of arrays

- 64 indexes per collection (formerly 40 indexes per collection)

- 64-bit integers can now be represented in the shell using NumberLong

- The :dbcommand:`findAndModify` command
  now supports upserts. It also allows you to specify fields to return

- $showDiskLoc option to see disk location of a document

- Support for IPv6 and UNIX domain sockets

Installation
------------

- Windows service improvements

- The C++ client is a separate tarball from the binaries

1.6.x Release Notes
-------------------

- `1.6.5 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/06_QCC05Fpk>`_

1.5.x Release Notes
-------------------

- `1.5.8 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/uJfF1QN6Thk>`_

- `1.5.7 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/OYvz40RWs90>`_

- `1.5.6 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/4l0N2U_H0cQ>`_

- `1.5.5 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/oO749nvTARY>`_

- `1.5.4 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/380V_Ec_q1c>`_

- `1.5.3 <https://groups.google.com/forum/?hl=en&fromgroups=#!topic/mongodb-user/hsUQL9CxTQw>`_

- `1.5.2 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/94EE3HVidAA>`_

- `1.5.1 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/7SBPQ2RSfdM>`_

- `1.5.0 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/VAhJcjDGTy0>`_

You can see a full list of all changes on
`JIRA <https://jira.mongodb.org/secure/IssueNavigator.jspa?mode=hide&requestId=10107>`_.

Thank you everyone for your support and suggestions!
:orphan:

======================
Changes in MongoDB 1.8
======================

.. toctree::

   1.8
=============================
Release Notes for MongoDB 1.8
=============================

.. default-domain:: mongodb

Upgrading
---------

MongoDB 1.8 is a standard, incremental production release and works as
a drop-in replacement for MongoDB 1.6, except:

- :term:`Replica set <replica set>` members should be upgraded in a
  particular order, as described in :ref:`1.8-upgrade-replica-set`.

- The :dbcommand:`mapReduce` command has changed in 1.8, causing
  incompatibility with previous releases. :dbcommand:`mapReduce` no
  longer generates temporary collections (thus, ``keepTemp`` has been
  removed). Now, you must always supply a value for ``out``. See the
  ``out`` field options in the :dbcommand:`mapReduce` document. If you
  use MapReduce, this also likely means you need a recent version of
  your client driver.

Preparation
~~~~~~~~~~~

Read through all release notes before upgrading and ensure that no
changes will affect your deployment.

.. _1.8-upgrade-standalone:

Upgrading a Standalone ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Download the v1.8.x binaries from the `MongoDB Download Page`_.

#. Shutdown your :program:`mongod` instance.

#. Replace the existing binary with the 1.8.x :program:`mongod` binary.

#. Restart MongoDB.

.. _`MongoDB Download Page`: http://downloads.mongodb.org/

.. _1.8-upgrade-replica-set:

Upgrading a Replica Set
~~~~~~~~~~~~~~~~~~~~~~~

1.8.x :term:`secondaries <secondary>` **can** replicate from 1.6.x
:term:`primaries <primary>`.

1.6.x secondaries **cannot** replicate from 1.8.x primaries.

Thus, to upgrade a :term:`replica set` you must replace all of your
secondaries first, then the primary.

For example, suppose you have a replica set with a primary, an
:term:`arbiter` and several secondaries. To upgrade the set, do the
following:

1. For the arbiter:

   a. Shut down the arbiter.

   #. Restart it with the 1.8.x binary from the `MongoDB Download Page`_.

#. Change your config (optional) to prevent election of a new primary.

   It is possible that, when you start shutting down members of the set,
   a new primary will be elected. To prevent this, you can give
   all of the secondaries a priority of ``0`` before
   upgrading, and then change them back afterwards. To do so:

   a. Record your current config. Run :method:`rs.config()` and paste the
      results into a text file.

   #. Update your config so that all secondaries have
      priority ``0``. For example:

      .. code-block:: javascript

         config = rs.conf()
         {
              "_id" : "foo",
              "version" : 3,
              "members" : [
                      {
                              "_id" : 0,
                              "host" : "ubuntu:27017"
                      },
                      {
                              "_id" : 1,
                              "host" : "ubuntu:27018"
                      },
                      {
                              "_id" : 2,
                              "host" : "ubuntu:27019",
                              "arbiterOnly" : true
                      }
                      {
                              "_id" : 3,
                              "host" : "ubuntu:27020"
                      },
                      {
                              "_id" : 4,
                              "host" : "ubuntu:27021"
                      },
              ]
         }
         config.version++
         3
         rs.isMaster()
         {
              "setName" : "foo",
              "ismaster" : false,
              "secondary" : true,
              "hosts" : [
                      "ubuntu:27017",
                      "ubuntu:27018"
              ],
              "arbiters" : [
                      "ubuntu:27019"
              ],
              "primary" : "ubuntu:27018",
              "ok" : 1
         }
         // for each secondary
         config.members[0].priority = 0
         config.members[3].priority = 0
         config.members[4].priority = 0
         rs.reconfig(config)

#. For each secondary:

   a. Shut down the secondary.

   #. Restart it with the 1.8.x binary from the `MongoDB Download Page`_.

#. If you changed the config, change it back to its original state:

   .. code-block:: javascript

      config = rs.conf()
      config.version++
      config.members[0].priority = 1
      config.members[3].priority = 1
      config.members[4].priority = 1
      rs.reconfig(config)

#. Shut down the primary (the final 1.6 server), and then restart it
   with the 1.8.x binary from the `MongoDB Download Page`_.

.. _1.8-upgrade-shard-cluster:
.. _1.8-upgrade-sharded-cluster:

Upgrading a Sharded Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Turn off the balancer:

   .. code-block:: javascript

      mongo <a_mongos_hostname>
      use config
      db.settings.update({_id:"balancer"},{$set : {stopped:true}}, true)

#. For each :term:`shard`:

   - If the shard is a :term:`replica set`, follow the directions above for
     :ref:`1.8-upgrade-replica-set`.

   - If the shard is a single :program:`mongod` process, shut it down
     and then restart it with the 1.8.x binary from the `MongoDB Download Page`_.

#. For each :program:`mongos`:

   a. Shut down the :program:`mongos` process.

   #. Restart it with the 1.8.x binary from the `MongoDB Download Page`_.

#. For each config server:

   a. Shut down the config server process.

   #. Restart it with the 1.8.x binary from the `MongoDB Download Page`_.

#. Turn on the balancer:

   .. code-block:: javascript

      use config
      db.settings.update({_id:"balancer"},{$set : {stopped:false}})

Returning to 1.6
~~~~~~~~~~~~~~~~

If for any reason you must move back to 1.6, follow the steps above in
reverse. Please be careful that you have not inserted any documents
larger than 4MB while running on 1.8 (where the max size has increased
to 16MB). If you have you will get errors when the server tries to read
those documents.

Journaling
``````````

Returning to 1.6 after using 1.8
:doc:`Journaling </core/journaling>` works
fine, as journaling does not change anything about the data file format.
Suppose you are running 1.8.x with journaling enabled and you decide to
switch back to 1.6. There are two scenarios:

- If you shut down cleanly with 1.8.x, just restart with the 1.6 mongod
  binary.

- If 1.8.x shut down uncleanly, start 1.8.x up again and let the journal
  files run to fix any damage (incomplete writes) that may have existed
  at the crash. Then shut down 1.8.x cleanly and restart with the 1.6
  mongod binary.

Changes
-------

Journaling
~~~~~~~~~~

MongoDB now supports write-ahead :doc:`/core/journaling` to
facilitate fast crash recovery and durability in the storage engine.
With journaling enabled, a :program:`mongod` can be quickly restarted
following a crash without needing to repair the :term:`collections
<collection>`. The aggregation framework makes it possible to do
aggregation

Sparse and Covered Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~

:ref:`Sparse Indexes <index-type-sparse>` are indexes that only include
documents that contain the fields specified in the index. Documents
missing the field will not appear in the index at all. This can
significantly reduce index size for indexes of fields that contain only a
subset of documents within a :term:`collection`.

:ref:`Covered Indexes <covered-queries>` enable MongoDB to answer
queries entirely from the index when the query only selects fields
that the index contains.

Incremental MapReduce Support
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :dbcommand:`mapReduce` command supports new options that enable
incrementally updating existing :term:`collections <collection>`.
Previously, a MapReduce job could output either to a temporary
collection or to a named permanent collection, which it would overwrite
with new data.

You now have several options for the output of your MapReduce jobs:

- You can merge MapReduce output into an existing collection. Output
  from the Reduce phase will replace existing keys in the output
  collection if it already exists. Other keys will remain in the
  collection.

- You can now re-reduce your output with the contents of an existing
  collection. Each key output by the reduce phase will be reduced with
  the existing document in the output collection.

- You can replace the existing output collection with the new results of
  the MapReduce job (equivalent to setting a permanent output
  collection in previous releases)

- You can compute MapReduce inline and return results to the caller
  without persisting the results of the job. This is similar to the
  temporary collections generated in previous releases, except results
  are limited to 8MB.

For more information, see the ``out`` field options in the
:dbcommand:`mapReduce` document.

Additional Changes and Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1.8.1
`````

- Sharding migrate fix when moving larger chunks.

- Durability fix with background indexing.

- Fixed mongos concurrency issue with many incoming connections.

1.8.0
`````

- All changes from 1.7.x series.

1.7.6
`````

- Bug fixes.

1.7.5
`````
- :doc:`Journaling </core/journaling>`.

- Extent allocation improvements.

- Improved :term:`replica set` connectivity for :program:`mongos`.

- :dbcommand:`getLastError` improvements for :term:`sharding`.

1.7.4
`````

- :program:`mongos` routes ``slaveOk`` queries to :term:`secondaries
  <secondary>` in :term:`replica sets <replica set>`.

- New :dbcommand:`mapReduce` output options.

- :ref:`index-type-sparse`.

1.7.3
`````

- Initial :ref:`covered index <covered-queries>` support.

- Distinct can use data from indexes when possible.

- :dbcommand:`mapReduce` can merge or reduce results into an existing collection.

- :program:`mongod` tracks and :program:`mongostat` displays network usage. See :ref:`mongostat`.

- Sharding stability improvements.

1.7.2
`````

- :update:`$rename` operator allows renaming of fields in a document.

- :method:`db.eval()` not to block.

- Geo queries with sharding.

- :option:`mongostat --discover` option

- Chunk splitting enhancements.

- Replica sets network enhancements for servers behind a nat.

1.7.1
`````

- Many sharding performance enhancements.

- Better support for :projection:`$elemMatch` on primitives in embedded arrays.

- Query optimizer enhancements on range queries.

- Window service enhancements.

- Replica set setup improvements.

- :update:`$pull` works on primitives in arrays.

1.7.0
`````

- Sharding performance improvements for heavy insert loads.

- Slave delay support for replica sets.

- :data:`~local.system.replset.settings.getLastErrorDefaults` for replica sets.

- Auto completion in the shell.

- Spherical distance for geo search.

- All fixes from 1.6.1 and 1.6.2.

Release Announcement Forum Pages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- `1.8.1 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/v09MbhEm62Y>`_,
  `1.8.0 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/JeHQOnam6Qk>`_

- `1.7.6 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/3t6GNZ1qGYc>`_,
  `1.7.5 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/S5R0Tx9wkEg>`_,
  `1.7.4 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/9Om3Vuw-y9c>`_,
  `1.7.3 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/DfNUrdbmflI>`_,
  `1.7.2 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/df7mwK6Xixo>`_,
  `1.7.1 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/HUR9zYtTpA8>`_,
  `1.7.0 <https://groups.google.com/forum/?fromgroups=#!topic/mongodb-user/TUnJCg9161A>`_

Resources
---------

- `MongoDB Downloads <http://mongodb.org/downloads>`_
- `All JIRA Issues resolved in 1.8 <https://jira.mongodb.org/secure/IssueNavigator.jspa?mode=hide&requestId=10172>`_
:orphan:

======================
Changes in MongoDB 2.0
======================

.. toctree::

   2.0
=============================
Release Notes for MongoDB 2.0
=============================

.. default-domain:: mongodb

Upgrading
---------

Although the major version number has changed, MongoDB 2.0 is a
standard, incremental production release and works as a drop-in
replacement for MongoDB 1.8.

Preparation
~~~~~~~~~~~

Read through all release notes before upgrading, and ensure that no
changes will affect your deployment.

If you create new indexes in 2.0, then downgrading to 1.8 is possible
but you must reindex the new collections.

:program:`mongoimport` and :program:`mongoexport` now correctly adhere to the CSV spec
for handling CSV input/output. This may break existing import/export
workflows that relied on the previous behavior. For more information see
:issue:`SERVER-1097`.

:wiki:`Journaling` is **enabled by default** in 2.0 for 64-bit builds.
If you still prefer to run without journaling, start :program:`mongod`
with the :option:`--nojournal <mongod --nojournal>` run-time option.
Otherwise, MongoDB creates journal files during startup. The first time you start :program:`mongod` with
journaling, you will see a delay as :program:`mongod` creates new files.
In addition, you may see reduced write throughput.

2.0 :program:`mongod` instances are interoperable with 1.8
:program:`mongod` instances; however, for best results, upgrade your
deployments using the following procedures:

.. _2.0-upgrade-standalone:

Upgrading a Standalone ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Download the v2.0.x binaries from the `MongoDB Download Page`_.

#. Shutdown your :program:`mongod` instance. Replace the existing
   binary with the 2.0.x :program:`mongod` binary and restart MongoDB.

.. _`MongoDB Download Page`: http://downloads.mongodb.org/

.. _2.0-upgrade-replica-set:

Upgrading a Replica Set
~~~~~~~~~~~~~~~~~~~~~~~

1. Upgrade the :term:`secondary` members of the set one at a time by
   shutting down the :program:`mongod` and replacing the 1.8 binary
   with the 2.0.x binary from the `MongoDB Download Page`_.

#. To avoid losing the last few updates on failover you can
   temporarily halt your application (failover should take less than 10
   seconds), or you can set :ref:`write concern <write-concern>` in your application
   code to confirm that each update reaches multiple servers.

#. Use the :method:`rs.stepDown()` to step down the primary to allow
   the normal :ref:`failover <replica-set-failover>` procedure.

   :method:`rs.stepDown()` and :dbcommand:`replSetStepDown` provide for
   shorter and more consistent failover procedures than simply
   shutting down the primary directly.

   When the primary has stepped down, shut down its instance and
   upgrade by replacing the :program:`mongod` binary with the 2.0.x
   binary.

.. _2.0-upgrade-shard-cluster:
.. _2.0-upgrade-sharded-cluster:

Upgrading a Sharded Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Upgrade all :term:`config server <config database>` instances
   *first*, in any order. Since config servers use two-phase commit,
   :term:`shard` configuration metadata updates will halt until all are
   up and running.

#. Upgrade :program:`mongos` routers in any order.

Changes
-------

Compact Command
~~~~~~~~~~~~~~~

A :dbcommand:`compact` command is now available for compacting a single
collection and its indexes. Previously, the only way to compact was to
repair the entire database.

Concurrency Improvements
~~~~~~~~~~~~~~~~~~~~~~~~

When going to disk, the server will yield the write lock when writing
data that is not likely to be in memory. The initial
implementation of this feature now exists:

See :issue:`SERVER-2563` for more information.

The specific operations yield in 2.0 are:

- Updates by ``_id``

- Removes

- Long cursor iterations

Default Stack Size
~~~~~~~~~~~~~~~~~~

MongoDB 2.0 reduces the default stack size. This change can reduce total memory
usage when there are many (e.g., 1000+) client connections, as there is
a thread per connection. While portions of a thread's stack can be
swapped out if unused, some operating systems do this slowly enough that
it might be an issue. The default stack size is lesser of the
system setting or 1MB.

.. _2.0-new-index-format:

Index Performance Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

v2.0 includes significant improvements to the
:doc:`index </tutorial/roll-back-to-v1.8-index>`.
Indexes are often 25% smaller and 25% faster (depends on the use case).
When upgrading from previous versions, the benefits of the new index
type are realized only if you create a new index or re-index an old one.

Dates are now signed, and the max index key size has increased slightly
from 819 to 1024 bytes.

All operations that create a new index will result in a 2.0 index by
default. For example:

- Reindexing results on an older-version index results in a 2.0 index.
  However, reindexing on a secondary does *not* work in versions prior
  to 2.0. Do not reindex on a secondary. For a workaround, see
  :issue:`SERVER-3866`.

- The :setting:`repairDatabase` command converts indexes to a 2.0
  indexes.

To convert all indexes for a given collection to the :ref:`2.0 type
<2.0-new-index-format>`, invoke the :dbcommand:`compact` command.

Once you create new indexes, downgrading to 1.8.x will require a
re-index of any indexes created using 2.0. See
:doc:`/tutorial/roll-back-to-v1.8-index`.

Sharding Authentication
~~~~~~~~~~~~~~~~~~~~~~~

Applications can now use authentication with :term:`sharded clusters <sharded cluster>`.

Replica Sets
~~~~~~~~~~~~

Hidden Nodes in Sharded Clusters
````````````````````````````````

In 2.0, :program:`mongos` instances can now determine when a member of
a replica set becomes "hidden" without requiring a restart. In 1.8,
:program:`mongos` if you reconfigured a
member as hidden, you *had* to restart :program:`mongos` to prevent
queries from reaching the hidden member.

Priorities
``````````

Each :term:`replica set` member can now have a priority value consisting
of a floating-point from 0 to 1000, inclusive. Priorities let you
control which member of the set you prefer to have as :term:`primary`
the member with the highest priority that can see a majority of the set
will be elected primary.

For example, suppose you have a replica set with three members, ``A``, ``B``, and
``C``, and suppose that their priorities are set as follows:

- ``A``'s priority is ``2``.

- ``B``'s priority is ``3``.

- ``C``'s priority is ``1``.

During normal operation, the set will always chose ``B`` as
primary. If ``B`` becomes unavailable, the set will elect ``A`` as primary.

For more information, see the
:data:`~local.system.replset.members[n].priority` documentation.

Data-Center Awareness
``````````````````````

You can now "tag" :term:`replica set` members to indicate their
location. You can use these tags to design custom :ref:`write rules <write-concern>`
across data centers, racks, specific servers, or any other architecture
choice.

For example, an administrator can define rules such as "very important write" or
``customerData`` or "audit-trail" to replicate to certain servers,
racks, data centers, etc. Then in the application code, the developer
would say:

.. code-block:: javascript

   db.foo.insert(doc, {w : "very important write"})

which would succeed if it fulfilled the conditions the DBA defined for
"very important write".

For more information, see
:wiki:`Tagging <Data+Center+Awareness#DataCenterAwareness-Tagging%28version2.0%29>`.

Drivers may also support tag-aware reads. Instead of
specifying ``slaveOk``, you specify ``slaveOk`` with tags indicating
which data-centers to read from. For details, see the
:doc:`/applications/drivers` documentation.

``w`` : ``majority``
````````````````````

You can also set ``w`` to ``majority`` to ensure that the write
propagates to a majority of nodes, effectively committing it. The
value for "majority" will automatically adjust as you add or
remove nodes from the set.

For more information, see :doc:`/core/write-concern`.

Reconfiguration with a Minority Up
``````````````````````````````````

If the majority of servers in a set has been permanently lost, you can
now force a reconfiguration of the set to bring it back online.

For more information see :doc:`/tutorial/reconfigure-replica-set-with-unavailable-members`.

Primary Checks for a Caught up Secondary before Stepping Down
`````````````````````````````````````````````````````````````

To minimize time without a :term:`primary`, the :method:`rs.stepDown()`
method will now fail if the primary does not see a :term:`secondary`
within 10 seconds of its latest optime. You can force the primary to
step down anyway, but by default it will return an error message.

See also :doc:`/tutorial/force-member-to-be-primary`.

Extended Shutdown on the Primary to Minimize Interruption
`````````````````````````````````````````````````````````

When you call the :dbcommand:`shutdown` command, the :term:`primary`
will refuse to shut down unless there is a :term:`secondary` whose
optime is within 10 seconds of the primary. If such a secondary isn't
available, the primary will step down and wait up to a minute for the
secondary to be fully caught up before shutting down.

Note that to get this behavior, you must issue the :dbcommand:`shutdown`
command explicitly; sending a signal to the process will not trigger
this behavior.

You can also force the primary to shut down, even without an up-to-date
secondary available.

Maintenance Mode
````````````````

When :dbcommand:`repair` or :dbcommand:`compact` runs on a :term:`secondary`, the
secondary will automatically drop into "recovering" mode until the
operation finishes. This prevents clients from trying to read from it
while it's busy.

Geospatial Features
~~~~~~~~~~~~~~~~~~~

Multi-Location Documents
````````````````````````

Indexing is now supported on documents which have multiple location
objects, embedded either inline or in nested sub-documents. Additional
command options are also supported, allowing results to return with
not only distance but the location used to generate the distance.

For more information, see :wiki:`Multi-location Documents <Geospatial+Indexing#GeospatialIndexing-MultilocationDocuments>`.

Polygon searches
````````````````

Polygonal :query:`$within` queries are also now supported for simple polygon
shapes. For details, see the :query:`$within` operator documentation.

Journaling Enhancements
~~~~~~~~~~~~~~~~~~~~~~~

- Journaling is now enabled by default for 64-bit platforms. Use the
  ``--nojournal`` command line option to disable it.

- The journal is now compressed for faster commits to disk.

- A new :option:`--journalCommitInterval <mongod --journalCommitInterval>` run-time option exists for
  specifying your own group commit interval. The default settings do
  not change.

- A new :dbcommand:`{ getLastError: { j: true } } <getLastError>` option is
  available to wait for the group commit. The group commit will happen
  sooner when a client is waiting on ``{j: true}``. If journaling is
  disabled, ``{j: true}`` is a no-op.

New ``ContinueOnError`` Option for Bulk Insert
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Set the ``continueOnError`` option for bulk inserts, in the
:doc:`driver </applications/drivers>`, so that bulk insert will
continue to insert any remaining documents even if an insert fails, as
is the case with  duplicate key exceptions or network interruptions. The :dbcommand:`getLastError`
command will report whether any inserts have failed, not just the
last one. If multiple errors occur, the client will only receive the
most recent :dbcommand:`getLastError` results.

See :wiki:`OP_INSERT <Mongo+Wire+Protocol#MongoWireProtocol-OPINSERT>`.

.. include:: /includes/note-bulk-inserts-on-sharded-clusters.rst

Map Reduce
~~~~~~~~~~

Output to a Sharded Collection
``````````````````````````````

Using the new ``sharded`` flag, it is possible to send the result of a
map/reduce to a sharded collection. Combined with the ``reduce`` or
``merge`` flags, it is possible to keep adding data to very large
collections from map/reduce jobs.

For more information, see :wiki:`MapReduce Output Options <MapReduce#MapReduce-Outputoptions>`
and :doc:`/reference/command/mapReduce`.

Performance Improvements
````````````````````````

Map/reduce performance will benefit from the following:

- Larger in-memory buffer sizes, reducing the amount of disk I/O needed
  during a job

- Larger javascript heap size, allowing for larger objects
  and less GC

- Supports pure JavaScript execution with the ``jsMode`` flag. See :doc:`/reference/command/mapReduce`.

New Querying Features
~~~~~~~~~~~~~~~~~~~~~

Additional regex options: ``s``
```````````````````````````````

Allows the dot (``.``) to match all characters including new lines. This is
in addition to the currently supported ``i``, ``m`` and ``x``. See
:wiki:`Regular Expressions <Advanced+Queries#AdvancedQueries-RegularExpressions>` and :query:`$regex`.

$and
````

A special boolean :query:`$and` query operator is now available.

Command Output Changes
~~~~~~~~~~~~~~~~~~~~~~

The output of the :dbcommand:`validate` command and the documents in the
``system.profile`` collection have both been enhanced to return
information as BSON objects with keys for each value rather than as
free-form strings.

Shell Features
~~~~~~~~~~~~~~

Custom Prompt
`````````````

You can define a custom prompt for the :program:`mongo` shell. You can
change the prompt at any time by setting the prompt variable to a string
or a custom JavaScript function returning a string. For examples, see
:wiki:`Custom Prompt <Overview+-+The+MongoDB+Interactive+Shell#Overview-TheMongoDBInteractiveShell-CustomPrompt>`.

Default Shell Init Script
`````````````````````````

On startup, the shell will check for a ``.mongorc.js`` file in the
user's home directory. The shell will execute this file after connecting
to the database and before displaying the prompt.

If you would like the shell not to run the ``.mongorc.js`` file
automatically, start the shell with :option:`--norc <mongo --norc>`.

For more information, see :doc:`/reference/program/mongo`.

Most Commands Require Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In 2.0, when running with authentication (e.g. :setting:`~security.authentication`) *all*
database commands require authentication, *except* the following
commands.

- :dbcommand:`isMaster`

- :dbcommand:`authenticate`

- :dbcommand:`getnonce`

- :dbcommand:`buildInfo`

- :dbcommand:`ping`

- :dbcommand:`isdbgrid`

Resources
---------

- `MongoDB Downloads <http://mongodb.org/downloads>`_
- `All JIRA Issues resolved in 2.0 <https://jira.mongodb.org/secure/IssueNavigator.jspa?mode=hide&requestId=11002>`_
- `All Backward Incompatible Changes <https://jira.mongodb.org/secure/IssueNavigator.jspa?requestId=11023>`_
:orphan:

======================
Changes in MongoDB 2.2
======================

.. toctree::

   2.2
=============================
Release Notes for MongoDB 2.2
=============================

.. default-domain:: mongodb

Upgrading
---------

MongoDB 2.2 is a production release series and succeeds the 2.0
production release series.

MongoDB 2.0 data files are compatible with 2.2-series binaries without any
special migration process. However, always perform the upgrade process for replica
sets and sharded clusters using the procedures that follow.

Synopsis
~~~~~~~~

- :program:`mongod`, 2.2 is a drop-in replacement for 2.0 and 1.8.

- Check your :doc:`driver </applications/drivers>` documentation for
  information regarding required compatibility upgrades, and always
  run the recent release of your driver.

  Typically, only users running with authentication, will need to
  upgrade drivers before continuing with the upgrade to 2.2.

- For all deployments using authentication, upgrade the
  drivers (i.e. client libraries), before upgrading the
  :program:`mongod` instance or instances.

- For all upgrades of sharded clusters:

  - turn off the balancer during the upgrade process. See the
    :ref:`sharding-balancing-disable-temporarily` section for more
    information.

  - upgrade all :program:`mongos` instances before upgrading any
    :program:`mongod` instances.

Other than the above restrictions, 2.2 processes can interoperate with
2.0 and 1.8 tools and processes. You can safely upgrade the
:program:`mongod` and :program:`mongos` components of a deployment
one by one while the deployment is otherwise operational.  Be sure to
read the detailed upgrade procedures below before upgrading production
systems.

.. [#secondaries-first] To minimize the interruption caused by
   :ref:`election process <replica-set-elections>`, always upgrade the
   secondaries of the set first, then :dbcommand:`step down
   <replSetStepDown>` the primary, and then upgrade the primary.

.. _2.2-upgrade-standalone:

Upgrading a Standalone ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. Download binaries of the latest release in the 2.2 series from the
   `MongoDB Download Page`_.

#. Shutdown your :program:`mongod` instance. Replace the existing
   binary with the 2.2 :program:`mongod` binary and restart MongoDB.

.. _`MongoDB Download Page`: http://downloads.mongodb.org/

.. _2.2-upgrade-replica-set:

Upgrading a Replica Set
~~~~~~~~~~~~~~~~~~~~~~~

You can upgrade to 2.2 by performing a "rolling"
upgrade of the set by upgrading the members individually while the
other members are available to minimize downtime. Use the following
procedure:

#. Upgrade the :term:`secondary` members of the set one at a time by
   shutting down the :program:`mongod` and replacing the 2.0 binary
   with the 2.2 binary.  After upgrading a :program:`mongod` instance,
   wait for the member to recover to ``SECONDARY`` state
   before upgrading the next instance.
   To check the member's state, issue :method:`rs.status()` in the
   :program:`mongo` shell.

#. Use the :program:`mongo` shell method :method:`rs.stepDown()` to
   step down the :term:`primary` to allow the normal :ref:`failover
   <replica-set-failover>` procedure.  :method:`rs.stepDown()`
   expedites the failover procedure and is preferable to shutting down
   the primary directly.

   Once the primary has stepped down and another member has assumed
   ``PRIMARY`` state, as observed in the output of
   :method:`rs.status()`, shut down the previous primary and replace
   :program:`mongod` binary with the 2.2 binary and start the new
   process.

   .. note:: Replica set failover is not instant but will
      render the set unavailable to read or accept writes
      until the failover process completes. Typically this takes
      10 seconds or more. You may wish to plan the upgrade during
      a predefined maintenance window.

.. _2.2-upgrade-shard-cluster:
.. _2.2-upgrade-sharded-cluster:

Upgrading a Sharded Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following procedure to upgrade a sharded cluster:

- :ref:`Disable the balancer <sharding-balancing-disable-temporarily>`.

- Upgrade all :program:`mongos` instances *first*, in any order.

- Upgrade all of the :program:`mongod` config server instances
  using the :ref:`stand alone <2.2-upgrade-standalone>` procedure.
  To keep the cluster online, be sure that at all times at least one config
  server is up.

- Upgrade each shard's replica set, using the :ref:`upgrade
  procedure for replica sets <2.2-upgrade-replica-set>` detailed above.

- re-enable the balancer.

.. note::

   Balancing is not currently supported in *mixed* 2.0.x and 2.2.0
   deployments. Thus you will want to reach a consistent version for all
   shards within a reasonable period of time, e.g. same-day.
   See :issue:`SERVER-6902` for more information.

Changes
-------

Major Features
~~~~~~~~~~~~~~

Aggregation Framework
`````````````````````

The aggregation framework makes it possible to do aggregation
operations without needing to use :term:`map-reduce`. The
:dbcommand:`aggregate` command exposes the aggregation framework, and the
:method:`~db.collection.aggregate()` helper in the :program:`mongo` shell
provides an interface to these operations. Consider the following
resources for background on the aggregation framework and its use:

- Documentation: :doc:`/core/aggregation`

- Reference: :doc:`/reference/aggregation`

- Examples: :doc:`/applications/aggregation`

TTL Collections
```````````````

TTL collections remove expired data from a collection, using a special
index and a background thread that deletes expired documents every
minute. These collections are useful as an alternative to
:term:`capped collections <capped collection>` in some cases, such as for data
warehousing and caching cases, including: machine generated event data,
logs, and session information that needs to persist in a database
for only a limited period of time.

For more information, see the :doc:`/tutorial/expire-data` tutorial.

Concurrency Improvements
````````````````````````

MongoDB 2.2 increases the server's capacity for concurrent
operations with the following improvements:

#. :issue:`DB Level Locking <SERVER-4328>`
#. :issue:`Improved Yielding on Page Faults <SERVER-3357>`
#. :issue:`Improved Page Fault Detection on Windows <SERVER-4538>`

To reflect these changes, MongoDB now provides changed and improved
reporting for concurrency and use, see :ref:`locks` and
:ref:`server-status-record-stats` in :doc:`server status
</reference/command/serverStatus>` and see
:method:`db.currentOp()`,
:doc:`mongotop </reference/program/mongotop>`, and :doc:`mongostat
</reference/program/mongostat>`.

.. todo:: add links to current op output documentation when it happens.

Improved Data Center Awareness with Tag Aware Sharding
``````````````````````````````````````````````````````

MongoDB 2.2 adds additional support for geographic distribution or
other custom partitioning for sharded collections in :term:`clusters
<sharded cluster>`. By using this "tag aware" sharding, you can
automatically ensure that data in a sharded database system is always
on specific shards. For example, with tag aware sharding, you can
ensure that data is closest to the application servers that use that
data most frequently.

.. todo When draft/core/write-operations.txt goes live, change this below:
   write-concern
   to
   write-operations-write-concern

Shard tagging controls data location, and is complementary but
separate from replica set tagging, which controls :doc:`read
preference </core/read-preference>` and :ref:`write concern
<write-concern>`. For example, shard tagging can pin all
"USA" data to one or more logical shards, while replica set tagging
can control which :program:`mongod` instances (e.g. "``production``"
or "``reporting``") the application uses to service requests.

See the documentation for the following helpers in the :program:`mongo`
shell that support tagged sharding configuration:

- :method:`sh.addShardTag()`
- :method:`sh.addTagRange()`
- :method:`sh.removeShardTag()`

Also, see :doc:`/core/tag-aware-sharding` and
:doc:`/tutorial/administer-shard-tags`.

Fully Supported Read Preference Semantics
`````````````````````````````````````````

All MongoDB clients and drivers now support full :doc:`read
preferences </core/read-preference>`, including consistent
support for a full range of :ref:`read preference modes
<replica-set-read-preference-modes>` and :ref:`tag sets
<replica-set-read-preference-tag-sets>`. This support extends to the
:program:`mongos` and applies identically to single replica sets and
to the replica sets for each shard in a :term:`sharded cluster`.

Additional read preference support now exists in the :program:`mongo`
shell using the :method:`~cursor.readPref()` cursor method.

.. including tagging

Compatibility Changes
~~~~~~~~~~~~~~~~~~~~~

Authentication Changes
``````````````````````

MongoDB 2.2 provides more reliable and robust support for
authentication clients, including drivers and :program:`mongos`
instances.

If your cluster runs with authentication:

- For all drivers, use the latest release of your driver and check
  its release notes.

- In sharded environments,
  to ensure that your cluster remains available during the upgrade
  process you **must** use the :ref:`upgrade procedure for sharded clusters
  <2.2-upgrade-shard-cluster>`.

.. _2.2-findandmodify-returns-null:

``findAndModify`` Returns Null Value for Upserts that Perform Inserts
`````````````````````````````````````````````````````````````````````

In version 2.2, for :term:`upsert` that perform inserts with the
``new`` option set to ``false``, :dbcommand:`findAndModify` commands will
now return the following output:

.. code-block:: javascript

   { 'ok': 1.0, 'value': null }

In the :program:`mongo` shell, upsert :dbcommand:`findAndModify`
operations that perform inserts (with ``new`` set to ``false``.)only output a ``null`` value.

In version 2.0 these operations would return an empty document,
e.g. ``{ }``.

See: :issue:`SERVER-6226` for more information.

``mongodump`` 2.2 Output Incompatible with Pre-2.2 ``mongorestore``
```````````````````````````````````````````````````````````````````

If you use the :program:`mongodump` tool from the 2.2 distribution to
create a dump of a database, you must use a 2.2 (or later) version of
:program:`mongorestore` to restore that dump.

See: :issue:`SERVER-6961` for more information.

.. _2.2-ObjectId-toString-valueOf-methods:

``ObjectId().toString()`` Returns String Literal ``ObjectId("...")``
````````````````````````````````````````````````````````````````````

In version 2.2, the :method:`~ObjectId.toString()` method returns the
string representation of the :ref:`ObjectId() <core-object-id-class>`
object and has the format ``ObjectId("...")``.

Consider the following example that calls the
:method:`~ObjectId.toString()` method on the
``ObjectId("507c7f79bcf86cd7994f6c0e")`` object:

.. code-block:: javascript

   ObjectId("507c7f79bcf86cd7994f6c0e").toString()

The method now returns the *string*
``ObjectId("507c7f79bcf86cd7994f6c0e")``.

Previously, in version 2.0, the method would return the *hexadecimal
string* ``507c7f79bcf86cd7994f6c0e``.

If compatibility between versions 2.0 and 2.2 is required, use
:ref:`ObjectId().str <core-object-id-class>`, which holds the
hexadecimal string value in both versions.

``ObjectId().valueOf()`` Returns hexadecimal string
```````````````````````````````````````````````````

In version 2.2, the :method:`~ObjectId.valueOf()` method returns the
value of the :ref:`ObjectId() <core-object-id-class>` object as a
lowercase hexadecimal string.

Consider the following example that calls the :method:`~ObjectId.valueOf()` method on the
``ObjectId("507c7f79bcf86cd7994f6c0e")`` object:

.. code-block:: javascript

   ObjectId("507c7f79bcf86cd7994f6c0e").valueOf()

The method now returns the *hexadecimal string*
``507c7f79bcf86cd7994f6c0e``.

Previously, in version 2.0, the method would return the *object*
``ObjectId("507c7f79bcf86cd7994f6c0e")``.

If compatibility between versions 2.0 and 2.2 is required, use
:ref:`ObjectId().str <core-object-id-class>` attribute, which holds the
hexadecimal string value in both versions.

Behavioral Changes
~~~~~~~~~~~~~~~~~~

.. _rn-2.2-collection-name-restriction:

Restrictions on Collection Names
````````````````````````````````

In version 2.2, collection names cannot:

- contain the ``$``.

- be an empty string (i.e. ``""``).

This change does not affect collections created with now illegal names
in earlier versions of MongoDB.

These new restrictions are in addition to the existing restrictions on
collection names which are:

- A collection name should begin with a letter or an underscore.

- A collection name cannot contain the null character.

- Begin with the ``system.`` prefix. MongoDB
  reserves ``system.``
  for system collections, such as the
  ``system.indexes`` collection.

- The maximum size of a collection name is 128 characters, including
  the name of the database. However, for maximum flexibility,
  collections should have names less than 80 characters.

Collections names may have any other valid UTF-8 string.

See the :issue:`SERVER-4442` and the
:ref:`faq-restrictions-on-collection-names` FAQ item.

.. _rn-2.2-database-name-restriction-windows:

Restrictions on Database Names for Windows
``````````````````````````````````````````

Database names running on Windows can no longer contain the following
characters:

.. code-block:: none

   /\. "*<>:|?

The names of the data files include the database name. If you attempt
to upgrade a database instance with one or more of these characters,
:program:`mongod` will refuse to start.

Change the name of these databases before upgrading. See
:issue:`SERVER-4584` and :issue:`SERVER-6729` for more information.

.. _2.2-id-indexes-capped-collections:

``_id`` Fields and Indexes on Capped Collections
````````````````````````````````````````````````

All :term:`capped collections <capped collection>` now have an ``_id``
field by default, *if* they exist outside of the ``local`` database,
and now have indexes on the ``_id`` field. This change only affects capped
collections created with 2.2 instances and does not affect existing
capped collections.

See: :issue:`SERVER-5516` for more information.

New ``$elemMatch`` Projection Operator
``````````````````````````````````````

The :projection:`$elemMatch` operator allows applications to narrow
the data returned from queries so that the query operation will only
return the first matching element in an array. See the
:doc:`/reference/operator/projection/elemMatch` documentation and the
:issue:`SERVER-2238` and :issue:`SERVER-828` issues for more
information.

Windows Specific Changes
~~~~~~~~~~~~~~~~~~~~~~~~

Windows XP is Not Supported
```````````````````````````

As of 2.2, MongoDB does not support Windows XP. Please upgrade to a
more recent version of Windows to use the latest releases of
MongoDB. See :issue:`SERVER-5648` for more information.

Service Support for ``mongos.exe``
``````````````````````````````````

You may now run :program:`mongos.exe` instances as a Windows
Service. See the :doc:`/reference/program/mongos.exe` reference and
:ref:`tutorial-mongod-as-windows-service` and :issue:`SERVER-1589` for
more information.

Log Rotate Command Support
``````````````````````````

MongoDB for Windows now supports log rotation by way of the
:dbcommand:`logRotate` database command. See :issue:`SERVER-2612` for
more information.

New Build Using SlimReadWrite Locks for Windows Concurrency
```````````````````````````````````````````````````````````

Labeled "2008+" on the `Downloads Page`_, this build for 64-bit
versions of Windows Server 2008 R2 and for Windows 7 or newer, offers
increased performance over the standard 64-bit Windows build of
MongoDB. See :issue:`SERVER-3844` for more information.

.. _`Downloads Page`: http://www.mongodb.org/downloads

Tool Improvements
~~~~~~~~~~~~~~~~~

Index Definitions Handled by ``mongodump`` and ``mongorestore``
```````````````````````````````````````````````````````````````

When you specify the :option:`--collection <mongodump --collection>`
option to :program:`mongodump`, :program:`mongodump` will now backup
the definitions for all indexes that exist on the source
database. When you attempt to restore this backup with
:program:`mongorestore`, the target :program:`mongod` will rebuild all
indexes. See :issue:`SERVER-808` for more information.

:program:`mongorestore` now includes the :option:`--noIndexRestore
<mongorestore --noIndexRestore>` option to provide the preceding
behavior. Use :option:`--noIndexRestore <mongorestore --noIndexRestore>`
to prevent :program:`mongorestore` from building
previous indexes.

``mongooplog`` for Replaying Oplogs
```````````````````````````````````

The :program:`mongooplog` tool makes it possible to pull :term:`oplog`
entries from :program:`mongod` instance and apply them to another
:program:`mongod` instance. You can use :program:`mongooplog` to
achieve point-in-time backup of a MongoDB data set. See the
:issue:`SERVER-3873` case and the :doc:`/reference/program/mongooplog`
documentation.

Authentication Support for ``mongotop`` and ``mongostat``
`````````````````````````````````````````````````````````

:program:`mongotop` and :program:`mongostat` now contain support for
username/password authentication. See :issue:`SERVER-3875` and
:issue:`SERVER-3871` for more information regarding this change. Also
consider the documentation of the following options for additional
information:

- :option:`mongotop --username`
- :option:`mongotop --password`
- :option:`mongostat --username`
- :option:`mongostat --password`

Write Concern Support for ``mongoimport`` and ``mongorestore``
``````````````````````````````````````````````````````````````

:program:`mongoimport` now provides an option to halt the import if
the operation encounters an error, such as a network interruption, a
duplicate key exception, or a write error.
The :option:`--stopOnError <mongoimport --stopOnError>` option
will
produce an error rather than silently continue importing data. See
:issue:`SERVER-3937` for more information.

In :program:`mongorestore`, the :option:`--w <mongorestore --w>`
option provides support for configurable write concern.

``mongodump`` Support for Reading from Secondaries
``````````````````````````````````````````````````

You can now run :program:`mongodump` when connected to a
:term:`secondary` member of a :term:`replica set`. See
:issue:`SERVER-3854` for more information.

``mongoimport`` Support for full 16MB Documents
```````````````````````````````````````````````

Previously, :program:`mongoimport` would only import documents that
were less than 4 megabytes in size. This issue is now corrected, and
you may use :program:`mongoimport` to import documents that are at
least 16 megabytes ins size. See :issue:`SERVER-4593` for more
information.

``Timestamp()`` Extended JSON format
````````````````````````````````````

MongoDB extended JSON now includes a new ``Timestamp()`` type to
represent the Timestamp type that MongoDB uses for timestamps in the
:term:`oplog` among other contexts.

This permits tools like :program:`mongooplog` and :program:`mongodump`
to query for specific timestamps. Consider the following
:program:`mongodump` operation:

.. code-block:: sh

   mongodump --db local --collection oplog.rs --query '{"ts":{"$gt":{"$timestamp" : {"t": 1344969612000, "i": 1 }}}}'  --out oplog-dump

See :issue:`SERVER-3483` for more information.

Shell Improvements
~~~~~~~~~~~~~~~~~~

Improved Shell User Interface
`````````````````````````````

2.2 includes a number of changes that improve the overall quality and
consistency of the user interface for the :program:`mongo` shell:

- Full Unicode support.

- Bash-like line editing features. See :issue:`SERVER-4312` for more
  information.

- Multi-line command support in shell history.
  See :issue:`SERVER-3470` for more information.

- Windows support for the ``edit`` command. See :issue:`SERVER-3998` for
  more information.

Helper to load Server-Side Functions
````````````````````````````````````

The :method:`db.loadServerScripts()` loads the contents of the current
database's ``system.js`` collection into the current :program:`mongo`
shell session. See :issue:`SERVER-1651` for more information.

Support for Bulk Inserts
````````````````````````

If you pass an array of :term:`documents <document>` to the
:method:`~db.collection.insert()` method, the :program:`mongo`
shell will now perform a bulk insert operation. See
:issue:`SERVER-3819` and :issue:`SERVER-2395` for more information.

.. include:: /includes/note-bulk-inserts-on-sharded-clusters.rst

Operations
~~~~~~~~~~

Support for Logging to Syslog
`````````````````````````````

See the :issue:`SERVER-2957` case and the documentation of
the :setting:`syslog` run-time option or the :option:`mongod --syslog`
and :option:`mongos --syslog` command line-options.

``touch`` Command
`````````````````

Added the :dbcommand:`touch` command to read the data and/or indexes
from a collection into memory. See: :issue:`SERVER-2023` and
:dbcommand:`touch` for more information.

``indexCounters`` No Longer Report Sampled Data
```````````````````````````````````````````````

:data:`indexCounters` now report actual counters that reflect index
use and state. In previous versions, these data were sampled. See
:issue:`SERVER-5784` and :data:`indexCounters` for more information.

Padding Specifiable on ``compact`` Command
``````````````````````````````````````````

See the documentation of the :dbcommand:`compact` and the
:issue:`SERVER-4018` issue for more information.

.. todo:: fix documentation and link

Added Build Flag to Use System Libraries
````````````````````````````````````````

The Boost library, version 1.49, is now embedded in the MongoDB
code base.

If you want to build MongoDB binaries using system Boost libraries,
you can pass ``scons`` using the ``--use-system-boost`` flag, as follows:

.. code-block:: sh

   scons --use-system-boost

When building MongoDB, you can also pass ``scons`` a flag to compile
MongoDB using only system libraries rather than the included versions
of the libraries. For example:

.. code-block:: sh

   scons --use-system-all

See the :issue:`SERVER-3829` and :issue:`SERVER-5172` issues for more
information.

Memory Allocator Changed to TCMalloc
````````````````````````````````````

To improve performance, MongoDB 2.2 uses the TCMalloc memory
allocator from Google Perftools. For more information about this
change see the :issue:`SERVER-188` and :issue:`SERVER-4683`. For more
information about TCMalloc, see the documentation of `TCMalloc`_ itself.

.. _`TCMalloc`: http://goog-perftools.sourceforge.net/doc/tcmalloc.html

Replication
~~~~~~~~~~~

Improved Logging for Replica Set Lag
````````````````````````````````````

When :term:`secondary` members of a replica set fall behind in
replication, :program:`mongod` now provides better reporting in the
log. This makes it possible to track replication in general and
identify what process may produce errors or halt replication. See
:issue:`SERVER-3575` for more information.

Replica Set Members can Sync from Specific Members
``````````````````````````````````````````````````

.. the following has been copied to source/administration/replica-sets.txt

The new :dbcommand:`replSetSyncFrom` command and new
:method:`rs.syncFrom()` helper in the :program:`mongo` shell make it
possible for you to manually configure from which member of the set a
replica will poll :term:`oplog` entries. Use these commands to
override the default selection logic if needed. Always exercise
caution with :dbcommand:`replSetSyncFrom` when overriding the default
behavior.

Replica Set Members will not Sync from Members Without Indexes Unless ``buildIndexes: false``
`````````````````````````````````````````````````````````````````````````````````````````````

.. the following has been copied to source/core/replication-internals.txt

To prevent inconsistency between members of replica sets, if the
member of a replica set has
:data:`~local.system.replset.members[n].buildIndexes` set to ``true``,
other members of the replica set will *not* sync from this member,
unless they also have
:data:`~local.system.replset.members[n].buildIndexes` set to
``true``. See :issue:`SERVER-4160` for more information.

New Option To Configure Index Pre-Fetching during Replication
`````````````````````````````````````````````````````````````

.. the following has been copied to source/core/replication-internals.txt

By default, when replicating options, :term:`secondaries <secondary>`
will pre-fetch :ref:`indexes` associated with a query to improve replication
throughput in most cases. The :setting:`replIndexPrefetch` setting and
:option:`--replIndexPrefetch <mongod --replIndexPrefetch>` option allow administrators to disable
this feature or allow the :program:`mongod` to pre-fetch only the
index on the ``_id`` field. See :issue:`SERVER-6718` for more information.

Map Reduce Improvements
~~~~~~~~~~~~~~~~~~~~~~~

In 2.2 Map Reduce received the following improvements:

- :issue:`Improved support for sharded MapReduce <SERVER-4521>`, and
- :issue:`MapReduce will retry jobs following a config error <SERVER-4158>`.

Sharding Improvements
~~~~~~~~~~~~~~~~~~~~~

Index on Shard Keys Can Now Be a Compound Index
```````````````````````````````````````````````

If your shard key uses the prefix of an existing index, then you do not
need to maintain a separate index for your shard key in addition to
your existing index. This index, however, cannot be a multi-key
index. See the :ref:`sharding-shard-key-indexes` documentation and
:issue:`SERVER-1506` for more information.

Migration Thresholds Modified
`````````````````````````````

The :ref:`migration thresholds <sharding-migration-thresholds>` have
changed in 2.2 to permit more even distribution of :term:`chunks
<chunk>` in collections that have smaller quantities of data. See the
:ref:`sharding-migration-thresholds` documentation for more
information.

.. Withholding from release notes. Internal feature.

   Option to Disable ``splitVector`` for :program:`mongos` Instances
   `````````````````````````````````````````````````````````````````

   By default, all :program:`mongos` instances are responsible for
   creating chunk splits. For deployments with large numbers of
   :program:`mongos` instances, this may impact the performance of the
   cluster.

   To ameliorate the additional load generated by these operations,
   the new :program:`mongos` option :option:`--noAutoSplit <mongos>`
   and the configuration file :setting:`noAutoSplit` setting both
   disable chunk splitting for individual :program:`mongos` instances.

   See :issue:`SERVER-6305` for more information.

Licensing Changes
-----------------

Added License notice for Google Perftools (TCMalloc Utility). See the
`License Notice <https://github.com/mongodb/mongo/blob/v2.2/distsrc/THIRD-PARTY-NOTICES#L231>`_
and the :issue:`SERVER-4683` for more information.

Resources
---------

- `MongoDB Downloads <http://mongodb.org/downloads>`_.
- `All JIRA issues resolved in 2.2 <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.1.0%22%2C+%222.1.1%22%2C+%222.1.2%22%2C+%222.2.0-rc0%22%2C+%222.2.0-rc1%22%2C+%222.2.0-rc2%22%29+ORDER+BY+component+ASC%2C+key+DESC>`_.
- `All backwards incompatible changes <https://jira.mongodb.org/secure/IssueNavigator.jspa?requestId=11225>`_.
- `All third party license notices <https://github.com/mongodb/mongo/blob/v2.2/distsrc/THIRD-PARTY-NOTICES>`_.
- `What's New in MongoDB 2.2 Online Conference <http://www.mongodb.com/events/webinar/mongodb-online-conference-sept>`_.
=============
2.4 Changelog
=============

.. default-domain:: mongodb

.. _2.4.10-changelog:

2.4.10 - Changes
----------------

- Indexes: Fixed issue that can cause index corruption when building indexes concurrently (:issue:`SERVER-12990`)
- Indexes: Fixed issue that can cause index corruption when shutting down secondary node during index build (:issue:`SERVER-12956`)
- Indexes: Mongod now recognizes incompatible “future” text and geo index versions and exits gracefully (:issue:`SERVER-12914`)
- Indexes: Fixed issue that can cause secondaries to fail replication when building the same index multiple times concurrently (:issue:`SERVER-12662`)
- Indexes: Fixed issue that can cause index corruption on the tenth index in a collection if the index build fails (:issue:`SERVER-12481`)
- Indexes: Introduced versioning for text and geo indexes to ensure backwards compatibility (:issue:`SERVER-12175`)
- Indexes: Disallowed building indexes on the system.indexes collection, which can lead to initial sync failure on secondaries (:issue:`SERVER-10231`)
- Sharding: Avoid frequent immediate balancer retries when config servers are out of sync (:issue:`SERVER-12908`)
- Sharding: Add indexes to locks collection on config servers to avoid long queries in case of large numbers of collections (:issue:`SERVER-12548`)
- Sharding: Fixed issue that can corrupt the config metadata cache when sharding collections concurrently (:issue:`SERVER-12515`)
- Sharding: Don't move chunks created on collections with a hashed shard key if the collection already contains data (:issue:`SERVER-9259`)
- Replication: Fixed issue where node appears to be down in a replica set during a compact operation (:issue:`SERVER-12264`)
- Replication: Fixed issue that could cause delays in elections when a node is not vetoing an election (:issue:`SERVER-12170`)
- Replication: Step down all primaries if multiple primaries are detected in replica set to ensure correct election result (:issue:`SERVER-10793`)
- Replication: Upon clock skew detection, secondaries will switch to sync directly from the primary to avoid sync cycles (:issue:`SERVER-8375`)
- Runtime: The SIGXCPU signal is now caught and mongod writes a log message and exits gracefully (:issue:`SERVER-12034`)
- Runtime: Fixed issue where mongod fails to start on Linux when /sys/dev/block directory is not readable (:issue:`SERVER-9248`)
- Windows: No longer zero-fill newly allocated files on systems other than Windows 7 or Windows Server 2008 R2 (:issue:`SERVER-8480`)
- GridFS: Chunk size is decreased to 255 KB (from 256 KB) to avoid overhead with usePowerOf2Sizes option (:issue:`SERVER-13331`)
- SNMP: Fixed MIB file validation under smilint (:issue:`SERVER-12487`)
- Shell: Fixed issue in V8 memory allocation that could cause long-running shell commands to crash (:issue:`SERVER-11871`)
- Shell: Fixed memory leak in the md5sumFile shell utility method (:issue:`SERVER-11560`)

Previous Releases
-----------------

- `All 2.4.9 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.9%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.8 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.8%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.7 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.7%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.6 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.6%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.5 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.5%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.4 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.4%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.3 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.3%22%20AND%20project%20%3D%20SERVER>`_.
- `All 2.4.2 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.2%22%20AND%20project%20%3D%20SERVER>`_
- `All 2.4.1 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.1%22%20AND%20project%20%3D%20SERVER>`_.
:orphan:

======================
Changes in MongoDB 2.4
======================

.. toctree::

   2.4
===================================================
Compatibility and Index Type Changes in MongoDB 2.4
===================================================

.. default-domain:: mongodb

In 2.4 MongoDB includes two new features related to indexes that users
upgrading to version 2.4 must consider, particularly with regard to
possible downgrade paths. For more information on downgrades, see
:ref:`2.4-downgrade`.

New Index Types
---------------

In 2.4 MongoDB adds two new index types: ``2dsphere`` and
``text``. These index types do not exist in 2.2, and for each
database, creating a ``2dsphere`` or ``text`` index, will upgrade the
data-file version and make that database incompatible with 2.2.

If you intend to downgrade, you should always drop all ``2dsphere``
and ``text`` indexes before moving to 2.2.

You can use the :ref:`downgrade procedure <2.4-downgrade>` to downgrade these
databases and run 2.2 if needed, however this will run a full database
repair (as with :dbcommand:`repairDatabase`) for all affected
databases.

.. _2.4-index-type-validation:

Index Type Validation
---------------------

In MongoDB 2.2 and earlier you could specify invalid index types that
did not exist. In these situations, MongoDB would create an ascending
(e.g. ``1``) index. Invalid indexes include index types specified by
strings that do not refer to an existing index type, and all numbers
other than ``1`` and ``-1``. [#grandfathered-indexes]_

In 2.4, creating any invalid index will result in an error.
Furthermore, you cannot create a ``2dsphere`` or ``text`` index on a
collection if its containing database has any invalid index types.
[#grandfathered-indexes]_

.. example::

   If you attempt to add an invalid index in MongoDB 2.4, as in the
   following:

   .. code-block:: javascript

      db.coll.ensureIndex( { field: "1" } )

   MongoDB will return the following error document:

   .. code-block:: javascript

      {
        "err" : "Unknown index plugin '1' in index { field: \"1\" }"
        "code": 16734,
        "n": <number>,
        "connectionId": <number>,
        "ok": 1
      }

.. [#grandfathered-indexes] In 2.4, indexes that specify a type of
   ``"1"`` or ``"-1"`` (the strings ``"1"`` and ``"-1"``) will continue
   to exist, despite a warning on start-up. **However**, a
   :term:`secondary` in a replica set cannot complete an initial sync
   from a primary that has a ``"1"`` or ``"-1"`` index. Avoid all
   indexes with invalid types.
=================================
JavaScript Changes in MongoDB 2.4
=================================

.. default-domain:: mongodb

Consider the following impacts of :ref:`2.4-release-javascript-change` in
MongoDB 2.4:

.. tip::

   Use the new :js:func:`interpreterVersion()` method in the
   :program:`mongo` shell and the :data:`~buildInfo.javascriptEngine`
   field in the output of :method:`db.serverBuildInfo()` to determine
   which JavaScript engine a MongoDB binary uses.

Improved Concurrency
--------------------

Previously, MongoDB operations that required the JavaScript interpreter
had to acquire a lock, and a single :program:`mongod` could only run a
single JavaScript operation at a time. The switch to V8 improves
concurrency by permitting multiple JavaScript operations to run at the
same time.

Modernized JavaScript Implementation (ES5)
------------------------------------------

The 5th edition of `ECMAscript
<http://www.ecma-international.org/publications/standards/Ecma-262.htm>`_,
abbreviated as ES5, adds many new language features, including:

- standardized `JSON
  <http://www.ecma-international.org/ecma-262/5.1/#sec-15.12.1>`_,

- `strict mode
  <http://www.ecma-international.org/ecma-262/5.1/#sec-4.2.2>`_,

- `function.bind()
  <http://www.ecma-international.org/ecma-262/5.1/#sec-15.3.4.5>`_,

- `array extensions
  <http://www.ecma-international.org/ecma-262/5.1/#sec-15.4.4.16>`_, and

- getters and setters.

With V8, MongoDB supports the ES5 implementation of Javascript with the
following exceptions.

.. note::

   The following features do not work as expected on documents
   **returned from MongoDB queries**:

   - ``Object.seal()`` throws an exception on documents returned from
     MongoDB queries.

   - ``Object.freeze()`` throws an exception on documents returned from
     MongoDB queries.

   - ``Object.preventExtensions()`` incorrectly allows the addition of
     new properties on documents returned from MongoDB queries.

   - ``enumerable`` properties, when added to documents returned from
     MongoDB queries, are not saved during write operations.

   See :issue:`SERVER-8216`, :issue:`SERVER-8223`,
   :issue:`SERVER-8215`, and :issue:`SERVER-8214` for more information.

   For objects that have not been returned from MongoDB queries, the
   features work as expected.

Removed Non-Standard SpiderMonkey Features
------------------------------------------

V8 does **not** support the following *non-standard* `SpiderMonkey
<https://developer.mozilla.org/en-US/docs/SpiderMonkey>`_ JavaScript
extensions, previously supported by MongoDB's use of SpiderMonkey as
its JavaScript engine.

E4X Extensions
~~~~~~~~~~~~~~

V8 does not support the *non-standard* `E4X
<https://developer.mozilla.org/en-US/docs/E4X>`_ extensions. E4X
provides a native `XML
<https://developer.mozilla.org/en-US/docs/E4X/Processing_XML_with_E4X>`_
object to the JavaScript language and adds the syntax for embedding
literal XML documents in JavaScript code.

You need to use alternative XML processing if you used any of the
following constructors/methods:

- ``XML()``

- ``Namespace()``

- ``QName()``

- ``XMLList()``

- ``isXMLName()``

Destructuring Assignment
~~~~~~~~~~~~~~~~~~~~~~~~

V8 does not support the non-standard destructuring assignments.
Destructuring assignment "extract[s] data from arrays or objects using
a syntax that mirrors the construction of array and object literals." -
`Mozilla docs
<https://developer.mozilla.org/en-US/docs/JavaScript/New_in_JavaScript/1.7#Destructuring_assignment_(Merge_into_own_page.2Fsection)>`_

.. example::

   The following destructuring assignment is **invalid** with V8 and
   throws a ``SyntaxError``:

   .. code-block:: javascript

      original = [4, 8, 15];
      var [b, ,c] = a;  // <== destructuring assignment
      print(b) // 4
      print(c) // 15

``Iterator()``, ``StopIteration()``, and Generators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

V8 does not support `Iterator(), StopIteration(), and generators
<https://developer.mozilla.org/en-US/docs/JavaScript/Guide/Iterators_and_Generators>`_.

``InternalError()``
~~~~~~~~~~~~~~~~~~~

V8 does not support ``InternalError()``. Use ``Error()`` instead.

``for each...in`` Construct
~~~~~~~~~~~~~~~~~~~~~~~~~~~

V8 does not support the use of `for each...in
<https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Statements/for_each...in>`_
construct. Use ``for (var x in y)`` construct
instead.

.. example::

   The following ``for each (var x in y)`` construct is **invalid**
   with V8:

   .. code-block:: javascript

      var o = { name: 'MongoDB', version: 2.4 };

      for each (var value in o) {
        print(value);
      }

   Instead, in version 2.4, you can use the ``for (var x in y)``
   construct:

   .. code-block:: javascript

      var o = { name: 'MongoDB', version: 2.4 };

      for (var prop in o) {
        var value = o[prop];
        print(value);
      }

   You can also use the array *instance* method ``forEach()`` with the
   ES5 method ``Object.keys()``:

   .. code-block:: javascript

      Object.keys(o).forEach(function (key) {
        var value = o[key];
        print(value);
      });

Array Comprehension
~~~~~~~~~~~~~~~~~~~

V8 does not support `Array comprehensions
<https://developer.mozilla.org/en-US/docs/JavaScript/Guide/Predefined_Core_Objects#Array_comprehensions>`_.

Use other methods such as the ``Array`` instance methods ``map()``,
``filter()``, or ``forEach()``.

.. example::

   With V8, the following array comprehension is **invalid**:

   .. code-block:: javascript

      var a = { w: 1, x: 2, y: 3, z: 4 }

      var arr = [i * i for each (i in a) if (i > 2)]
      printjson(arr)

   Instead, you can implement using the ``Array`` *instance* method
   ``forEach()`` and the ES5 method ``Object.keys()`` :

   .. code-block:: javascript

      var a = { w: 1, x: 2, y: 3, z: 4 }

      var arr = [];
      Object.keys(a).forEach(function (key) {
        var val = a[key];
        if (val > 2) arr.push(val * val);
      })
      printjson(arr)

   .. note::

      The new logic uses the ``Array`` *instance* method ``forEach()`` and
      not the *generic* method ``Array.forEach()``; V8 does **not**
      support ``Array`` *generic* methods. See :ref:`array-generics` for
      more information.

Multiple Catch Blocks
~~~~~~~~~~~~~~~~~~~~~

V8 does not support multiple ``catch`` blocks and will throw a
``SyntaxError``.

.. example::

   The following multiple catch blocks is **invalid** with V8 and will
   throw ``"SyntaxError: Unexpected token if"``:

   .. code-block:: javascript

      try {
        something()
      } catch (err if err instanceof SomeError) {
        print('some error')
      } catch (err) {
        print('standard error')
      }

Conditional Function Definition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

V8 will produce different outcomes than SpiderMonkey with `conditional
function definitions
<https://developer.mozilla.org/en-US/docs/JavaScript/Guide/Functions>`_.

.. example::

   The following conditional function definition produces different
   outcomes in SpiderMonkey versus V8:

   .. code-block:: javascript

      function test () {
         if (false) {
            function go () {};
         }
         print(typeof go)
      }

   With SpiderMonkey, the conditional function outputs ``undefined``,
   whereas with V8, the conditional function outputs ``function``.

   If your code defines functions this way, it is highly recommended
   that you refactor the code. The following example refactors the
   conditional function definition to work in both SpiderMonkey and V8.

   .. code-block:: javascript

      function test () {
        var go;
        if (false) {
          go = function () {}
        }
        print(typeof go)
      }

   The refactored code outputs ``undefined`` in both SpiderMonkey and V8.

.. note::

   ECMAscript prohibits conditional function definitions. To force V8
   to throw an ``Error``, `enable strict mode
   <http://www.nczonline.net/blog/2012/03/13/its-time-to-start-using-javascript-strict-mode/>`_.

   .. code-block:: javascript

      function test () {
        'use strict';

        if (false) {
          function go () {}
        }
      }

   The JavaScript code throws the following syntax error:

   .. code-block:: none

      SyntaxError: In strict mode code, functions can only be declared at top level or immediately within another function.

String Generic Methods
~~~~~~~~~~~~~~~~~~~~~~

V8 does not support `String generics
<https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/String#String_generic_methods>`_.
String generics are a set of methods on the ``String`` class that
mirror instance methods.

.. example::

   The following use of the generic method
   ``String.toLowerCase()`` is **invalid** with V8:

   .. code-block:: javascript

      var name = 'MongoDB';

      var lower = String.toLowerCase(name);

   With V8, use the ``String`` instance method ``toLowerCase()`` available
   through an *instance* of the ``String`` class instead:

   .. code-block:: javascript

      var name = 'MongoDB';

      var lower = name.toLowerCase();
      print(name + ' becomes ' + lower);

With V8, use the ``String`` *instance* methods instead of following
*generic* methods:

.. list-table::

   * - ``String.charAt()``
     - ``String.quote()``
     - ``String.toLocaleLowerCase()``

   * - ``String.charCodeAt()``
     - ``String.replace()``
     - ``String.toLocaleUpperCase()``

   * - ``String.concat()``
     - ``String.search()``
     - ``String.toLowerCase()``

   * - ``String.endsWith()``
     - ``String.slice()``
     - ``String.toUpperCase()``

   * - ``String.indexOf()``
     - ``String.split()``
     - ``String.trim()``

   * - ``String.lastIndexOf()``
     - ``String.startsWith()``
     - ``String.trimLeft()``

   * - ``String.localeCompare()``
     - ``String.substr()``
     - ``String.trimRight()``

   * - ``String.match()``
     - ``String.substring()``
     -

.. _array-generics:

Array Generic Methods
~~~~~~~~~~~~~~~~~~~~~

V8 does not support `Array generic methods
<https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Array#Array_generic_methods>`_.
Array generics are a set of methods on the ``Array`` class that mirror
instance methods.

.. example::

   The following use of the generic method ``Array.every()`` is
   **invalid** with V8:

   .. code-block:: javascript

      var arr = [4, 8, 15, 16, 23, 42];

      function isEven (val) {
         return 0 === val % 2;
      }

      var allEven = Array.every(arr, isEven);
      print(allEven);

   With V8, use the ``Array`` instance method ``every()`` available through
   an *instance* of the ``Array`` class instead:

   .. code-block:: javascript

      var allEven = arr.every(isEven);
      print(allEven);

With V8, use the ``Array`` *instance* methods instead of the following
*generic* methods:

.. list-table::

   * - ``Array.concat()``
     - ``Array.lastIndexOf()``
     - ``Array.slice()``

   * - ``Array.every()``
     - ``Array.map()``
     - ``Array.some()``

   * - ``Array.filter()``
     - ``Array.pop()``
     - ``Array.sort()``

   * - ``Array.forEach()``
     - ``Array.push()``
     - ``Array.splice()``

   * - ``Array.indexOf()``
     - ``Array.reverse()``
     - ``Array.unshift()``

   * - ``Array.join()``
     - ``Array.shift()``
     -

Array Instance Method ``toSource()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

V8 does not support the ``Array`` instance method `toSource()
<https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Array/toSource>`_.
Use the ``Array`` instance method ``toString()`` instead.

``uneval()``
~~~~~~~~~~~~

V8 does not support the non-standard method ``uneval()``. Use the
standardized `JSON.stringify()
<https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/JSON/stringify>`_
method instead.
======================
Upgrade MongoDB to 2.4
======================

.. default-domain:: mongodb

In the general case, the upgrade from MongoDB 2.2 to 2.4 is a
binary-compatible "drop-in" upgrade: shut down the :program:`mongod`
instances and replace them with :program:`mongod` instances running
2.4. **However**, before you attempt any upgrade please familiarize
yourself with the content of this document, particularly the procedure
for :ref:`upgrading sharded clusters <2.4-upgrade-cluster>` and the
considerations for :ref:`reverting to 2.2 after running 2.4
<2.4-downgrade>`.

Upgrade Recommendations and Checklist
-------------------------------------

When upgrading, consider the following:

- For all deployments using authentication, upgrade the
  drivers (i.e. client libraries), before upgrading the
  :program:`mongod` instance or instances.

- To upgrade to 2.4 sharded clusters *must* upgrade following the
  :ref:`meta-data upgrade procedure <2.4-upgrade-cluster>`.

- If you're using 2.2.0 and running with :setting:`~security.authentication` enabled, you
  will need to upgrade first to 2.2.1 and then upgrade to 2.4. See
  :ref:`2.4-upgrade-auth-limitation`.

- If you have :data:`system.users <<database>.system.users>` documents
  (i.e. for :setting:`~security.authentication`) that you created before 2.4 you *must*
  ensure that there are no duplicate values for the ``user`` field in
  the :data:`system.users <<database>.system.users>` collection in
  *any* database.  If you *do* have documents with duplicate user
  fields, you must remove them before upgrading.

  See :ref:`2.4-unique-users` for more information.

.. _2.4-upgrade-standalone:

Upgrade Standalone ``mongod`` Instance to MongoDB 2.4
-----------------------------------------------------

#. Download binaries of the latest release in the 2.4 series from the
   `MongoDB Download Page`_. See :doc:`/installation` for more
   information.

#. Shutdown your :program:`mongod` instance. Replace the existing
   binary with the 2.4 :program:`mongod` binary and restart :program:`mongod`.


.. _`MongoDB Download Page`: http://www.mongodb.org/downloads

.. _2.4-upgrade-replica-set:

Upgrade a Replica Set from MongoDB 2.2 to MongoDB 2.4
-----------------------------------------------------

You can upgrade to 2.4 by performing a "rolling"
upgrade of the set by upgrading the members individually while the
other members are available to minimize downtime. Use the following
procedure:

#. Upgrade the :term:`secondary` members of the set one at a time by
   shutting down the :program:`mongod` and replacing the 2.2 binary
   with the 2.4 binary.  After upgrading a :program:`mongod` instance,
   wait for the member to recover to ``SECONDARY`` state
   before upgrading the next instance.
   To check the member's state, issue :method:`rs.status()` in the
   :program:`mongo` shell.

#. Use the :program:`mongo` shell method :method:`rs.stepDown()` to
   step down the :term:`primary` to allow the normal :ref:`failover
   <replica-set-failover>` procedure.  :method:`rs.stepDown()`
   expedites the failover procedure and is preferable to shutting down
   the primary directly.

   Once the primary has stepped down and another member has assumed
   ``PRIMARY`` state, as observed in the output of
   :method:`rs.status()`, shut down the previous primary and replace
   :program:`mongod` binary with the 2.4 binary and start the new
   process.

   .. note:: Replica set failover is not instant but will
      render the set unavailable to read or accept writes
      until the failover process completes. Typically this takes
      10 seconds or more. You may wish to plan the upgrade during
      a predefined maintenance window.

.. _2.4-upgrade-cluster:

Upgrade a Sharded Cluster from MongoDB 2.2 to MongoDB 2.4
---------------------------------------------------------

.. important:: Only upgrade sharded clusters to 2.4 if **all** members
   of the cluster are currently running instances of 2.2. The only
   supported upgrade path for sharded clusters running 2.0 is via 2.2.

Overview
~~~~~~~~

Upgrading a :term:`sharded cluster` from MongoDB version 2.2 to 2.4
(or 2.3) requires that you run a 2.4 :program:`mongos`
with the :option:`--upgrade <mongos --upgrade>` option, described in
this procedure. The upgrade process does not require downtime.

The upgrade to MongoDB 2.4 adds epochs to the meta-data for all
collections and chunks in the existing cluster. MongoDB 2.2 processes
are capable of handling epochs, even though 2.2 did not require
them. This procedure applies only to upgrades from version 2.2. Earlier
versions of MongoDB do not correctly handle epochs. See
:ref:`2.4-sharded-cluster-meta-data-upgrade` for more information.

After completing the meta-data upgrade you can fully upgrade the
components of the cluster. With the balancer disabled:

- Upgrade all :program:`mongos` instances in the cluster.

- Upgrade all 3 :program:`mongod` config server instances.

- Upgrade the :program:`mongod` instances for each shard, one at a
  time.

See :ref:`2.4-finalize-shard-cluster-upgrade` for more information.

.. _2.4-sharded-cluster-meta-data-upgrade:

Cluster Meta-data Upgrade
~~~~~~~~~~~~~~~~~~~~~~~~~

Considerations
``````````````

Beware of the following properties of the cluster upgrade process:

- .. include:: /includes/fact-upgrade-sharded-cluster-prereq.rst

- While the upgrade is in progress, you cannot make changes to the
  collection meta-data. For example, during the upgrade, do **not**
  perform:

  - :method:`sh.enableSharding()`,

  - :method:`sh.shardCollection()`,

  - :method:`sh.addShard()`,

  - :method:`db.createCollection()`,

  - :method:`db.collection.drop()`,

  - :method:`db.dropDatabase()`,

  - any operation that creates a database, or

  - any other operation that modifies the cluster meta-data in any
    way. See :doc:`/reference/sharding` for a complete list
    of sharding commands. Note, however, that not all commands on
    the :doc:`/reference/sharding` page modifies the
    cluster meta-data.

- Once you upgrade to 2.4 and complete the upgrade procedure **do
  not** use 2.0 :program:`mongod` and :program:`mongos` processes in
  your cluster. 2.0 process may re-introduce old meta-data formats
  into cluster meta-data.

The upgraded config database will require more storage space than
before, to make backup and working copies of the
:data:`config.chunks` and :data:`config.collections` collections.
As always, if storage requirements increase, the :program:`mongod`
might need to pre-allocate additional data files. See
:ref:`faq-tools-for-measuring-storage-use` for more information.

.. _2.4-upgrade-meta-data:

Meta-data Upgrade Procedure
```````````````````````````

Changes to the meta-data format for sharded clusters, stored in the
:doc:`config database </reference/config-database>`, require a special
meta-data upgrade procedure when moving to 2.4.

Do not perform operations that modify meta-data while performing this
procedure. See :ref:`2.4-upgrade-cluster` for examples of prohibited
operations.

#. .. include:: /includes/fact-upgrade-sharded-cluster-prereq.rst

   To check the version of your indexes, use :method:`db.collection.getIndexes()`.

   If any index **on the config database** is ``{v:0}``, you should
   rebuild those indexes by connecting to the :program:`mongos` and
   either: rebuild all indexes using the
   :method:`db.collection.reIndex()` method, or drop and rebuild
   specific indexes using :method:`db.collection.dropIndex()` and then
   :method:`db.collection.ensureIndex()`.  If you need to upgrade the
   ``_id`` index to ``{v:1}`` use :method:`db.collection.reIndex()`.

   You may have ``{v:0}`` indexes on other databases in the cluster.

#. Turn off the :ref:`balancer <sharding-balancing-internals>` in the
   :term:`sharded cluster`, as described in
   :ref:`sharding-balancing-disable-temporarily`.

   .. optional::

      For additional security during the upgrade, you can make a
      backup of the config database using :program:`mongodump` or
      other backup tools.

#. Ensure there are no version 2.0 :program:`mongod` or
   :program:`mongos` processes still active in the sharded
   cluster. The automated upgrade process checks for 2.0 processes,
   but network availability can prevent a definitive check. Wait 5
   minutes after stopping or upgrading version 2.0 :program:`mongos`
   processes to confirm that none are still active.

#. Start a single 2.4 :program:`mongos` process with
   :setting:`~sharding.configDB` pointing to the sharded cluster's :ref:`config
   servers <sharding-config-server>` and with the :option:`--upgrade
   <mongos --upgrade>` option.  The upgrade process happens before the
   process becomes a daemon (i.e. before
   :option:`--fork <mongos --fork>`.)

   You can upgrade an existing
   :program:`mongos` instance to 2.4 or you can start a new `mongos`
   instance that can reach all config servers if you need to avoid
   reconfiguring a production :program:`mongos`.

   Start the :program:`mongos` with a command that resembles the
   following:

   .. code-block:: sh

      mongos --configdb <config servers> --upgrade

   Without the :option:`--upgrade <mongos --upgrade>` option 2.4
   :program:`mongos` processes will fail to start until the upgrade
   process is complete.

   The upgrade will prevent any chunk moves or splits from occurring
   during the upgrade process. If there are very many sharded
   collections or there are stale locks held by other failed processes,
   acquiring the locks for all collections can take
   seconds or minutes. See the log for progress updates.

#. When the :program:`mongos` process starts successfully, the upgrade is
   complete. If the :program:`mongos` process fails to start, check the
   log for more information.

   If the :program:`mongos` terminates or loses its connection to the
   config servers during the upgrade, you may always safely retry the
   upgrade.

   However, if the upgrade failed during the short critical section,
   the :program:`mongos` will exit and report that the upgrade will
   require manual intervention. To continue the upgrade process, you
   must follow the :ref:`upgrade-cluster-resync` procedure.

   .. optional::

      If the :program:`mongos` logs show the upgrade waiting for the
      upgrade lock, a previous upgrade process may still be active or
      may have ended abnormally.  After 15 minutes of no remote
      activity :program:`mongos` will force the upgrade lock. If you
      can verify that there are no running upgrade processes, you may
      connect to a 2.2 :program:`mongos` process and force the lock
      manually:

      .. code-block:: sh

         mongo <mongos.example.net>

      .. code-block:: javascript

         db.getMongo().getCollection("config.locks").findOne({ _id : "configUpgrade" })

      If the process specified in the ``process`` field of this document
      is *verifiably* offline, run the following operation to force the
      lock.

      .. code-block:: javascript

         db.getMongo().getCollection("config.locks").update({ _id : "configUpgrade" }, { $set : { state : 0 } })

      It is always more safe to wait for the :program:`mongos` to
      verify that the lock is inactive, if you have any doubts about
      the activity of another upgrade operation.  In addition to the
      ``configUpgrade``, the :program:`mongos` may need to wait for
      specific collection locks. Do not force the specific collection
      locks.

#. Upgrade and restart other :program:`mongos` processes in the
   sharded cluster, *without* the :option:`--upgrade <mongos --upgrade>`
   option.

   See :ref:`2.4-finalize-shard-cluster-upgrade` for more information.

#. :ref:`Re-enable the balancer
   <sharding-balancing-disable-temporarily>`. You can now perform
   operations that modify cluster meta-data.

Once you have upgraded, *do not* introduce version 2.0 MongoDB
processes into the sharded cluster. This can reintroduce old meta-data
formats into the config servers. The meta-data change made by this
upgrade process will help prevent errors caused by cross-version
incompatibilities in future versions of MongoDB.

.. _upgrade-cluster-resync:

Resync after an Interruption of the Critical Section
````````````````````````````````````````````````````

During the short critical section of the upgrade that applies changes
to the meta-data, it is unlikely but possible that a network
interruption can prevent all three config servers from verifying or
modifying data. If this occurs, the :ref:`config servers
<sharding-config-server>` must be re-synced, and there may be problems
starting new :program:`mongos` processes. The :term:`sharded cluster`
will remain accessible, but avoid all cluster meta-data changes until
you resync the config servers. Operations that change meta-data include:
adding shards, dropping databases, and dropping collections.

.. note::

   Only perform the following procedure *if* something (e.g. network,
   power, etc.) interrupts the upgrade process during the short
   critical section of the upgrade. Remember, you may always safely
   attempt the :ref:`meta data upgrade procedure
   <2.4-upgrade-meta-data>`.

To resync the config servers:

1. Turn off the :ref:`balancer <sharding-balancing-internals>` in the
   sharded cluster and stop all meta-data operations. If you are in the
   middle of an upgrade process (:ref:`2.4-upgrade-cluster`), you
   have already disabled the balancer.

#. Shut down two of the three config servers, preferably the last two listed
   in the :setting:`~sharding.configDB` string. For example, if your :setting:`~sharding.configDB`
   string is ``configA:27019,configB:27019,configC:27019``, shut down
   ``configB`` and ``configC``. Shutting down the last two config servers
   ensures that most :program:`mongos` instances will have
   uninterrupted access to cluster meta-data.

#. :program:`mongodump` the data files of the active config server
   (``configA``).

#. Move the data files of the deactivated config servers (``configB``
   and ``configC``) to a backup location.

#. Create new, empty :term:`data directories <dbpath>`.

#. Restart the disabled config servers with :option:`--dbpath <mongod --dbpath>`
   pointing to the now-empty data directory and :option:`--port <mongod --port>`
   pointing to an alternate port (e.g. ``27020``).

#. Use :program:`mongorestore` to repopulate the data files on the
   disabled documents from the active
   config server (``configA``) to the restarted config servers on the new
   port (``configB:27020,configC:27020``). These config servers are now
   re-synced.

#. Restart the restored config servers on the old port, resetting the
   port back to the old settings (``configB:27019`` and ``configC:27019``).

#. In some cases connection pooling may cause spurious failures, as
   the :program:`mongos` disables old connections only after attempted
   use. 2.4 fixes this problem, but to avoid this issue in version
   2.2, you can restart all :program:`mongos` instances (one-by-one,
   to avoid downtime) and use the :method:`rs.stepDown()` method
   before restarting each of the shard :term:`replica set`
   :term:`primaries <primary>`.

#. The sharded cluster is now fully resynced; however before you
   attempt the upgrade process again, you must manually reset the
   upgrade state using a version 2.2 :program:`mongos`. Begin by
   connecting to the 2.2 :program:`mongos` with the :program:`mongo`
   shell:

   .. code-block:: sh

      mongo <mongos.example.net>

   Then, use the following operation to reset the upgrade process:

   .. code-block:: javascript

      db.getMongo().getCollection("config.version").update({ _id : 1 }, { $unset : { upgradeState : 1 } })

#. Finally retry the upgrade process, as in
   :ref:`2.4-upgrade-cluster`.

.. _2.4-finalize-shard-cluster-upgrade:

Upgrade Sharded Cluster Components
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After you have successfully completed the meta-data upgrade process
described in :ref:`2.4-upgrade-meta-data`, and the 2.4
:program:`mongos` instance starts, you can upgrade the other processes
in your MongoDB deployment.

While the balancer is still disabled, upgrade the components of your
sharded cluster in the following order:

- Upgrade all :program:`mongos` instances in the cluster, in any
  order.

- Upgrade all 3 :program:`mongod` config server instances, upgrading
  the *first* system in the :option:`mongos --configdb` argument
  *last*.

- Upgrade each shard, one at a time, upgrading the :program:`mongod`
  secondaries before running :dbcommand:`replSetStepDown` and
  upgrading the primary of each shard.

When this process is complete, you can now :ref:`re-enable the
balancer <sharding-balancing-disable-temporarily>`.

.. _2.4-upgrade-auth-limitation:

Rolling Upgrade Limitation for 2.2.0 Deployments Running with ``auth`` Enabled
------------------------------------------------------------------------------

MongoDB *cannot* support deployments that mix 2.2.0 and 2.4.0, or
greater, components. MongoDB version 2.2.1 and later processes *can*
exist in mixed deployments with 2.4-series processes. Therefore you
cannot perform a rolling upgrade from MongoDB 2.2.0 to MongoDB
2.4.0. To upgrade a cluster with 2.2.0 components, use one of the
following procedures.

1. Perform a rolling upgrade of all 2.2.0 processes to the latest
   2.2-series release (e.g. 2.2.3) so that there are no processes in
   the deployment that predate 2.2.1. When there are no 2.2.0
   processes in the deployment, perform a rolling upgrade to 2.4.0.

2. Stop all processes in the cluster. Upgrade all processes to a
   2.4-series release of MongoDB, and start all processes at the same
   time.

Upgrade from 2.3 to 2.4
-----------------------

If you used a :program:`mongod` from the 2.3 or 2.4-rc (release
candidate) series, you can safely transition these databases to 2.4.0
or later; *however*, if you created ``2dsphere`` or ``text`` indexes
using a :program:`mongod` before v2.4-rc2, you will need to rebuild
these indexes. For example:

.. code-block:: javascript

   db.records.dropIndex( { loc: "2dsphere" } )
   db.records.dropIndex( "records_text" )

   db.records.ensureIndex( { loc: "2dsphere" } )
   db.records.ensureIndex( { records: "text" } )

.. _2.4-downgrade:

Downgrade MongoDB from 2.4 to Previous Versions
-----------------------------------------------

For some cases the on-disk format of data files used by 2.4 and 2.2
:program:`mongod` is compatible, and you can upgrade and downgrade if
needed. However, several new features in 2.4 are incompatible with
previous versions:

- ``2dsphere`` indexes are incompatible with 2.2 and earlier
  :program:`mongod` instances.

- ``text`` indexes are incompatible with 2.2 and earlier
  :program:`mongod` instances.

- using a ``hashed`` index as a shard key are incompatible with 2.2 and
  earlier :program:`mongos` instances.

- ``hashed`` indexes are incompatible with 2.0 and earlier
  :program:`mongod` instances.

.. important:: Collections sharded using hashed shard keys, should
   **not** use 2.2 :program:`mongod` instances, which cannot correctly
   support cluster operations for these collections.

   .. :issue:`SERVER_9776`

If you completed the :ref:`meta-data upgrade for a sharded cluster
<2.4-upgrade-cluster>`, you can safely downgrade to 2.2 MongoDB
processes. **Do not** use 2.0 processes after completing the upgrade
procedure.

.. note::

   In sharded clusters, once you have completed the :ref:`meta-data upgrade
   procedure <2.4-upgrade-cluster>`, you cannot use 2.0
   :program:`mongod` or :program:`mongos` instances in the same
   cluster.

   If you complete the meta-data upgrade, you can safely downgrade
   components in any order. When upgrade again, always
   upgrade :program:`mongos` instances before :program:`mongod` instances.

   **Do not** create ``2dsphere`` or ``text`` indexes in a cluster
   that has 2.2 components.

Considerations and Compatibility
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you upgrade to MongoDB 2.4, and then need to run MongoDB 2.2 with
the same data files, consider the following limitations.

- If you use a ``hashed`` index as the shard key index, which is only
  possible under 2.4 you will not be able to query data in this
  sharded collection. Furthermore, a 2.2 :program:`mongos` cannot
  properly route an insert operation for a collections sharded using a
  ``hashed`` index for the shard key index: any data that you insert
  using a 2.2 :program:`mongos`, will not arrive on the correct shard
  and will not be reachable by future queries.

- If you *never* create an ``2dsphere`` or ``text`` index, you can
  move between a 2.4 and 2.2 :program:`mongod` for a given data set;
  however, after you create the first ``2dsphere`` or ``text`` index
  with a 2.4 :program:`mongod` you will need to run a 2.2
  :program:`mongod` with the :option:`--upgrade <mongod --upgrade>`
  option and drop any ``2dsphere`` or ``text`` index.

Upgrade and Downgrade Procedures
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Basic Downgrade and Upgrade
```````````````````````````

**Except** as described below, moving between 2.2 and 2.4 is a drop-in
replacement:

- stop the existing :program:`mongod`, using the :option:`--shutdown
  <mongod --shutdown>` option as follows:

  .. code-block:: sh

     mongod --dbpath /var/mongod/data --shutdown

  Replace ``/var/mongod/data`` with your MongoDB :setting:`~storage.dbPath`.

- start the new :program:`mongod` processes with the same
  :setting:`~storage.dbPath` setting, for example:

  .. code-block:: sh

     mongod --dbpath /var/mongod/data

  Replace ``/var/mongod/data`` with your MongoDB :setting:`~storage.dbPath`.

.. _2.4-downgrade-pdfile:

Downgrade to 2.2 After Creating a ``2dsphere`` or ``text`` Index
````````````````````````````````````````````````````````````````

If you have created ``2dsphere`` or ``text`` indexes while running a
2.4 :program:`mongod` instance, you can downgrade at any time, by
starting the ``2.2`` :program:`mongod` with the :option:`--upgrade
<mongod --upgrade>` option as follows:

.. code-block:: sh

   mongod --dbpath /var/mongod/data/ --upgrade

Then, you will need to drop any existing ``2dsphere`` or ``text``
indexes using :method:`db.collection.dropIndex()`, for example:

.. code-block:: javascript

   db.records.dropIndex( { loc: "2dsphere" } )
   db.records.dropIndex( "records_text" )

.. warning::

   :option:`--upgrade <mongod --upgrade>` will run
   :dbcommand:`repairDatabase` on any database where you have created
   a ``2dsphere`` or ``text`` index, which will rebuild *all*
   indexes.

Troubleshooting Upgrade/Downgrade Operations
````````````````````````````````````````````

If you do not use :option:`--upgrade <mongod --upgrade>`, when you
attempt to start a 2.2 :program:`mongod` and you have created a
``2dsphere`` or ``text`` index, :program:`mongod` will return the
following message:

.. code-block:: none

   'need to upgrade database index_plugin_upgrade with pdfile version 4.6, new version: 4.5 Not upgrading, exiting'

While running 2.4, to check the data file version of a MongoDB
database, use the following operation in the shell:

.. code-block:: javascript

   db.getSiblingDB('<databasename>').stats().dataFileVersion

The major data file [#pdfile-version]_ version for both 2.2 and 2.4 is
``4``, the minor data file version for 2.2 is ``5`` and the minor data
file version for 2.4 is ``6`` **after** you create a ``2dsphere`` or
``text`` index.

.. [#pdfile-version] The data file version (i.e. ``pdfile version``)
   is independent and unrelated to the release version of MongoDB.
=============================
Release Notes for MongoDB 2.4
=============================

*March 19, 2013*

.. default-domain:: mongodb

MongoDB 2.4 includes enhanced geospatial support, switch to V8 JavaScript
engine, security enhancements, and text search (beta) and hashed index.

Minor Releases
--------------

.. class:: hidden

   .. toctree::

      /release-notes/2.4-changelog

2.4.10 -- April 4, 2014
~~~~~~~~~~~~~~~~~~~~~~~

- Performs fast file allocation on Windows when available :issue:`SERVER-8480`.

- Start elections if more than one primary is detected :issue:`SERVER-10793`.

- Changes to allow safe downgrading from v2.6 to v2.4 :issue:`SERVER-12914`, :issue:`SERVER-12175`.

- Fixes for edge cases in index creation :issue:`SERVER-12481`, :issue:`SERVER-12956`.

- :ref:`2.4.10 Changelog <2.4.10-changelog>`.

- `All 2.4.10 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.10%22%20AND%20project%20%3D%20SERVER>`_.

2.4.9 -- January 10, 2014
~~~~~~~~~~~~~~~~~~~~~~~~~

- Fix for instances where :program:`mongos` incorrectly reports a
  successful write :issue:`SERVER-12146`.

- Make non-primary read preferences consistent with ``slaveOK``
  versioning logic :issue:`SERVER-11971`.

- Allow new sharded cluster connections to read from secondaries when
  primary is down :issue:`SERVER-7246`.

- `All 2.4.9 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.9%22%20AND%20project%20%3D%20SERVER>`_.

2.4.8 -- November 1, 2013
~~~~~~~~~~~~~~~~~~~~~~~~~

- Increase future compatibility for 2.6 authorization features
  :issue:`SERVER-11478`.

- Fix :dbcommand:`dbhash` cache issue for config servers
  :issue:`SERVER-11421`.

- `All 2.4.8 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.8%22%20AND%20project%20%3D%20SERVER>`_.

2.4.7 -- October 21, 2013
~~~~~~~~~~~~~~~~~~~~~~~~~

- Fixed over-aggressive caching of V8 Isolates :issue:`SERVER-10596`.

- Removed extraneous initial count during mapReduce
  :issue:`SERVER-9907`.

- Cache results of dbhash command :issue:`SERVER-11021`.

- Fixed memory leak in aggregation :issue:`SERVER-10554`.

- `All 2.4.7 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.7%22%20AND%20project%20%3D%20SERVER>`_.

2.4.6 -- August 20, 2013
~~~~~~~~~~~~~~~~~~~~~~~~

- Fix for possible loss of documents during the chunk migration process
  if a document in the chunk is very large :issue:`SERVER-10478`.

- Fix for C++ client shutdown issues :issue:`SERVER-8891`.

- Improved replication robustness in presence of high network latency
  :issue:`SERVER-10085`.

- Improved Solaris support :issue:`SERVER-9832`, :issue:`SERVER-9786`,
  and :issue:`SERVER-7080`.

- `All 2.4.6 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.6%22%20AND%20project%20%3D%20SERVER>`_.

2.4.5 -- July 3, 2013
~~~~~~~~~~~~~~~~~~~~~

- Fix for CVE-2013-4650 Improperly grant user system privileges on
  databases other than local :issue:`SERVER-9983`.

- Fix for CVE-2013-3969 Remotely triggered segmentation fault in Javascript engine
  :issue:`SERVER-9878`.

- Fix to prevent identical background indexes from being built
  :issue:`SERVER-9856`.

- Config server performance improvements :issue:`SERVER-9864` and
  :issue:`SERVER-5442`.

- Improved initial sync resilience to network failure :issue:`SERVER-9853`.

- `All 2.4.5 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.5%22%20AND%20project%20%3D%20SERVER>`_.

2.4.4 -- June 4, 2013
~~~~~~~~~~~~~~~~~~~~~

- Performance fix for Windows version :issue:`SERVER-9721`

- Fix for config upgrade failure :issue:`SERVER-9661`.

- Migration to Cyrus SASL library for MongoDB Enterprise :issue:`SERVER-8813`.

- `All 2.4.4 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.4%22%20AND%20project%20%3D%20SERVER>`_.

2.4.3 -- April 23, 2013
~~~~~~~~~~~~~~~~~~~~~~~

- Fix for ``mongo`` shell ignoring modified object's ``_id`` field
  :issue:`SERVER-9385`.

- Fix for race condition in log rotation :issue:`SERVER-4739`.

- Fix for ``copydb`` command with authorization in a sharded cluster
  :issue:`SERVER-9093`.

- `All 2.4.3 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.3%22%20AND%20project%20%3D%20SERVER>`_.

2.4.2 -- April 17, 2013
~~~~~~~~~~~~~~~~~~~~~~~

- Several V8 memory leak and performance fixes :issue:`SERVER-9267` and
  :issue:`SERVER-9230`.

- Fix for upgrading sharded clusters :issue:`SERVER-9125`.

- Fix for high volume connection crash :issue:`SERVER-9014`.

- `All 2.4.2 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.2%22%20AND%20project%20%3D%20SERVER>`_

2.4.1 -- April 17, 2013
~~~~~~~~~~~~~~~~~~~~~~~

- Fix for losing index changes during initial sync :issue:`SERVER-9087`

- `All 2.4.1 improvements <https://jira.mongodb.org/issues/?jql=fixVersion%20%3D%20%222.4.1%22%20AND%20project%20%3D%20SERVER>`_.

Major New Features
------------------

The following changes in MongoDB affect both standard and Enterprise
editions:

.. _2.4-release-text-indexes:

Text Search
~~~~~~~~~~~

Add support for text search of content in MongoDB databases as a
*beta* feature. See :doc:`/core/index-text` for more information.

.. _2.4-release-geospatial:

Geospatial Support Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Add new :doc:`2dsphere index </core/2dsphere>`. The new index
  supports `GeoJSON <http://geojson.org/geojson-spec.html>`_ objects
  ``Point``, ``LineString``, and ``Polygon``. See
  :doc:`/core/2dsphere` and :doc:`/applications/geospatial-indexes`.

- Introduce operators :query:`$geometry`, :query:`$geoWithin` and
  :query:`$geoIntersects` to work with the GeoJSON data.

.. _2.4-release-hashed-indexes:

Hashed Index
~~~~~~~~~~~~

Add new :ref:`hashed index <index-type-hashed>` to index documents
using hashes of field values. When used to index a shard key, the
hashed index ensures an evenly distributed shard key. See also
:ref:`sharding-hashed-sharding`.

.. DOCS-752

Improvements to the Aggregation Framework
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Improve support for geospatial queries. See the
  :query:`$geoWithin` operator and the :pipeline:`$geoNear` pipeline
  stage.

- Improve sort efficiency when the :pipeline:`$sort` stage immediately
  precedes a :pipeline:`$limit` in the pipeline.

- Add new operators :expression:`$millisecond` and
  :expression:`$concat` and modify how :group:`$min` operator processes
  ``null`` values.

.. _2.4-fixed-size-arrays:

Changes to Update Operators
~~~~~~~~~~~~~~~~~~~~~~~~~~~

- Add new :update:`$setOnInsert` operator for use with
  :method:`upsert <db.collection.update()>` .

- Enhance functionality of the :update:`$push` operator, supporting
  its use with the :update:`$each`, the :update:`$sort`, and the
  :update:`$slice` modifiers.

Additional Limitations for Map-Reduce and ``$where`` Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :dbcommand:`mapReduce` command, :dbcommand:`group` command, and
the :query:`$where` operator expressions cannot access certain
global functions or properties, such as ``db``, that are available
in the :program:`mongo` shell. See the individual command or
operator for details.

.. _2.4-release-server-status:

Improvements to ``serverStatus`` Command
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Provide additional metrics and customization for the
:dbcommand:`serverStatus` command. See :method:`db.serverStatus()`
and :dbcommand:`serverStatus` for more information.

.. _2.4-privilege-documents:
.. _2.4-unique-users:

Security Enhancements
---------------------

- Introduce a role-based access control system
  :v2.4:`/reference/user-privileges` using new
  :doc:`/reference/privilege-documents`.

- Enforce uniqueness of the user in user privilege documents per
  database. Previous versions of MongoDB did not enforce this
  requirement, and existing databases may have duplicates.

- Support encrypted connections using SSL certificates signed by a
  Certificate Authority. See :doc:`/tutorial/configure-ssl`.

For more information on security and risk management strategies, see
:doc:`MongoDB Security Practices and Procedures </security>`.

Performance Improvements
------------------------

.. _2.4-release-javascript-change:

V8 JavaScript Engine
~~~~~~~~~~~~~~~~~~~~

.. class:: hidden

   .. toctree::

      /release-notes/2.4-javascript

Change default JavaScript engine from SpiderMonkey to V8. The change
provides improved concurrency for JavaScript operations, modernized
JavaScript implementation, and the removal of non-standard
SpiderMonkey features, and affects all JavaScript behavior including
the commands :dbcommand:`mapReduce`, :dbcommand:`group`, and
:dbcommand:`eval` and the query operator :query:`$where`.

See :doc:`/release-notes/2.4-javascript` for more information about
all changes .

BSON Document Validation Enabled by Default for ``mongod`` and ``mongorestore``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Enable basic :term:`BSON` object validation for :program:`mongod`
and :program:`mongorestore` when writing to MongoDB data files. See
:setting:`objcheck` for details.

Index Build Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~

- Add support for multiple concurrent index builds in the background by
  a single :program:`mongod` instance. See :ref:`building indexes in
  the background <index-creation-background>` for more information on
  background index builds.

- Allow the :method:`db.killOp()` method to terminate a foreground
  index build.

- Improve index validation during index creation. See
  :doc:`/release-notes/2.4-index-types` for more information.

Set Parameters as Command Line Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Provide ``--setParameter`` as a command line option for
:program:`mongos` and :program:`mongod`. See :program:`mongod` and
:program:`mongos` for list of available options for
:setting:`setParameter`.

Changed Replication Behavior for Chunk Migration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, each document move during :ref:`chunk migration
<sharding-chunk-migration>` in a :term:`sharded cluster` propagates to
at least one secondary before the balancer proceeds with its next
operation. See :ref:`chunk-migration-replication`.

Improved Chunk Migration Queue Behavior
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Increase performance for moving multiple chunks off an overloaded
shard. The balancer no longer waits for the current migration's
delete phase to complete before starting the next chunk migration.
See :ref:`chunk-migration-queuing` for details.

.. _mongodb-enterprise:

Enterprise
----------

The following changes are specific to MongoDB Enterprise Editions:

.. _2.4.4-sasl-change:

SASL Library Change
~~~~~~~~~~~~~~~~~~~

In 2.4.4, MongoDB Enterprise uses Cyrus SASL. Earlier 2.4 Enterprise
versions use GNU SASL (``libgsasl``). To upgrade to 2.4.4 MongoDB
Enterprise or greater, you **must** install all package dependencies
related to this change, including the appropriate Cyrus SASL ``GSSAPI``
library. See :doc:`/tutorial/install-mongodb-enterprise` for details of
the dependencies.

.. _kerberos-authentication:

New Modular Authentication System with Support for Kerberos
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In 2.4, the MongoDB Enterprise now supports authentication via a
Kerberos mechanism. See
:doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
for more information. For drivers that provide support for
Kerberos authentication to MongoDB, refer to :ref:`kerberos-and-drivers`.

For more information on security and risk management strategies, see
:doc:`MongoDB Security Practices and Procedures </security>`.

Additional Information
----------------------

Platform Notes
~~~~~~~~~~~~~~

For OS X, MongoDB 2.4 only supports OS X versions 10.6 (Snow Leopard)
and later. There are no other platform support changes in MongoDB
2.4. See the `downloads page <http://www.mongodb.org/downloads/>`_ for
more information on platform support.

Upgrade Process
~~~~~~~~~~~~~~~

.. class:: hidden

   .. toctree::

      /release-notes/2.4-upgrade
      /release-notes/2.4-index-types

See :doc:`/release-notes/2.4-upgrade` for full upgrade instructions.

Other Resources
~~~~~~~~~~~~~~~

- `MongoDB Downloads <http://mongodb.org/downloads>`_.
- `All JIRA issues resolved in 2.4 <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.3.2%22,+%222.3.1%22,+%222.3.0%22,+%222.4.0-rc0%22,+%222.4.0-rc1%22,+%222.4.0-rc2%22,+%222.4.0-rc3%22%29>`_.
- `All Backwards incompatible changes <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.3.2%22%2C+%222.3.1%22%2C+%222.3.0%22%2C+%222.4.0-rc0%22%2C+%222.4.0-rc1%22%2C+%222.4.0-rc2%22%2C+%222.4.0-rc3%22%29+AND+%22Backward+Breaking%22+in+%28+Rarely+%2C+sometimes%2C+yes+%29+ORDER+BY+votes+DESC%2C+issuetype+DESC%2C+key+DESC>`_.
- `All Third Party License Notices <https://github.com/mongodb/mongo/blob/v2.4/distsrc/THIRD-PARTY-NOTICES>`_.
:orphan:

======================
Changes in MongoDB 2.6
======================

.. toctree::

   2.6
====================================
Compatibility Changes in MongoDB 2.6
====================================

.. default-domain:: mongodb

The following 2.6 changes can affect the compatibility with older
versions of MongoDB. See :doc:`/release-notes/2.6` for the full list of
the 2.6 changes.

Index Changes
-------------

.. _2.6-index-key-length-incompatibility:

Enforce Index Key Length Limit
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   MongoDB 2.6 implements a stronger enforcement of the limit on
   :limit:`index key length <Index Key>`.

   Creating indexes will error if an index key in an existing document
   exceeds the limit:

   - :method:`db.collection.ensureIndex()`,
     :method:`db.collection.reIndex()`, :dbcommand:`compact`, and
     :dbcommand:`repairDatabase` will error and not create the index.
     Previous versions of MongoDB would create the index but not index
     such documents.

   - Because :method:`db.collection.reIndex()`, :dbcommand:`compact`,
     and :dbcommand:`repairDatabase` drop *all* the indexes from a
     collection and then recreate them sequentially, the error from the
     index key limit prevents these operations from rebuilding any
     remaining indexes for the collection and, in the case of the
     :dbcommand:`repairDatabase` command, from continuing with the
     remainder of the process.

   Inserts will error:

   - :method:`db.collection.insert()` and other operations that perform
     inserts (e.g. :method:`db.collection.save()` and
     :method:`db.collection.update()` with ``upsert`` that result in
     inserts) will fail to insert if the new document has an indexed
     field value that exceeds the limit. Previous versions of MongoDB
     would insert but not index such documents.

   - :program:`mongorestore` and :program:`mongoimport` will fail to
     insert if the new document has an indexed field value that exceeds
     the limit.

   Updates will error:

   - :method:`db.collection.update()` and
     :method:`db.collection.save()` operations on an indexed field will
     error if the new value for the field exceeds the limit.

   - If an existing document contains a value for the indexed field
     that exceeds the limit, updates on other fields that result in the
     relocation of a document on disk will error.

   Chunk Migration will fail:

   - Migrations will fail for a chunk that has a document with an
     indexed field that exceeds the index key length limit.

   - If left unfixed, the chunk will repeatedly fail migration,
     effectively ceasing chunk balancing for that collection. Or, if
     chunk splits occur in response to the migration failures, this
     response would lead to unnecessarily large number of chunks and an
     overly large config databases.

   Secondary members of replica sets will warn:

   - Secondaries will continue to replicate documents with an indexed
     field value that exceeds the limit on initial sync but will print
     warnings in the logs.

   - Secondaries allow index build and rebuild operations on a
     collection that contains an indexed field value that exceeds the
     limit but with warnings in the logs.

   - With *mixed version* replica sets where the secondaries are
     version 2.6 and the primary is version 2.4, secondaries will
     replicate documents inserted or updated on the 2.4 primary, but
     will print error messages in the log if the documents contain
     indexed field value that exceeds the limit.

Solution
   Run :method:`db.upgradeCheckAllDBs()` to find current keys that violate this
   limit and correct as appropriate. Preferably, run the test before
   upgrading; i.e. connect the 2.6 :program:`mongo` shell to your
   MongoDB 2.4 database and run the method.

If you have an existing data set and want to disable the default index
key length validation so that you can upgrade before resolving these
indexing issues, use the :parameter:`failIndexKeyTooLong` parameter.

Index Specifications Validate Field Names
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   In MongoDB 2.6, create and re-index operations fail when the index
   key refers to an empty field, e.g. ``"a..b" : 1`` or the field name
   starts with a dollar sign (``$``).

   - :method:`db.collection.ensureIndex()` will not create a new index
     with an invalid or empty key name.

   - :method:`db.collection.reIndex()`, :dbcommand:`compact`, and
     :dbcommand:`repairDatabase` will error if an index exists with an
     invalid or empty key name.

   Previous versions of MongoDB allow the index.

Solution
   Run :method:`db.upgradeCheckAllDBs()` to find current keys that violate this
   limit and correct as appropriate. Preferably, run the test before
   upgrading; i.e. connect the 2.6 :program:`mongo` shell to your
   MongoDB 2.4 database and run the method.

``ensureIndex`` and Existing Indexes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   :method:`db.collection.ensureIndex()` now errors:

   - if you try to create an existing index but with different
     options; e.g. in the following example, the second
     :method:`db.collection.ensureIndex()` will error.

     .. code-block:: javascript

        db.mycollection.ensureIndex( { x: 1 } )
        db.mycollection.ensureIndex( { x: 1 }, { unique: 1 } )

   - if you specify an index name that already exists but the key
     specifications differ; e.g. in the following example, the second
     :method:`db.collection.ensureIndex()` will error.

     .. code-block:: javascript

        db.mycollection.ensureIndex( { a: 1 }, { name: "myIdx" } )
        db.mycollection.ensureIndex( { z: 1 }, { name: "myIdx" } )

   Previous versions did not create the index but did not error.

.. _write-methods-incompatibility:

Write Method Acknowledgements
-----------------------------

Description
   The :program:`mongo` shell write methods
   :method:`db.collection.insert()`, :method:`db.collection.update()`,
   :method:`db.collection.save()` and :method:`db.collection.remove()`
   now integrate the :doc:`write concern </core/write-concern>`
   directly into the method rather than with a separate
   :dbcommand:`getLastError` command to provide :ref:`safe writes
   <write-concern-acknowledged>` whether run interactively in the
   :program:`mongo` shell or non-interactively in a script. In previous
   versions, these methods exhibited a "fire-and-forget" behavior.
   [#mongo-shell-gle-interactive]_

- Existing scripts for the :program:`mongo` shell that used these
  methods will now observe safe writes which take **longer** than
  the previous "fire-and-forget" behavior.

- The write methods now return a :method:`WriteResult` object that
  contains the results of the operation, including any write errors
  and write concern errors, and obviates the need to call
  :dbcommand:`getLastError` command to get the status of the results.
  See :method:`db.collection.insert()`,
  :method:`db.collection.update()`, :method:`db.collection.save()`
  and :method:`db.collection.remove()` for details.

.. [#mongo-shell-gle-interactive]
   In previous versions, when using the :program:`mongo` shell
   interactively, the :program:`mongo` shell automatically called the
   :dbcommand:`getLastError` command after a write method to provide
   "safe writes". Scripts, however, would observe "fire-and-forget"
   behavior in previous versions unless the scripts included an
   **explicit** call to the :dbcommand:`getLastError` command after a
   write method.

Solution
  Scripts that used these :program:`mongo` shell methods for bulk write
  operations with "fire-and-forget" behavior should use the
  :method:`Bulk()` methods.

  For example, instead of:

  .. code-block:: javascript

     for (var i = 1; i <= 1000000; i++) {
         db.test.insert( { x : i } );
     }

  In MongoDB 2.6, replace with :method:`Bulk()` operation:

  .. code-block:: javascript

     var bulk = db.test.initializeUnorderedBulkOp();

     for (var i = 1; i <= 1000000; i++) {
         bulk.insert( { x : i} );
     }

     bulk.execute( { w: 1 } );

  Bulk method returns a :ref:`bulk-write-result` object that contains
  the result of the operation.

  .. seealso::
     :ref:`rel-notes-write-operations`,
     :method:`Bulk()`,
     :method:`Bulk.execute()`,
     :method:`db.collection.initializeUnorderedBulkOp()`,
     :method:`db.collection.initializeOrderedBulkOp()`

``db.collection.aggregate()`` Change
------------------------------------

Description
  The :method:`db.collection.aggregate()` method in the
  :program:`mongo` shell defaults to returning a cursor to the results
  set. This change enables the aggregation pipeline to return result
  sets of any size and requires cursor iteration to access the result
  set. For example:

  .. code-block:: javascript

      var myCursor = db.orders.aggregate( [
          {
            $group: {
               _id: "$cust_id",
               total: { $sum: "$price" }
            }
          }
      ] );

      myCursor.forEach( function(x) { printjson (x); } );

  Previous versions returned a single document with a field ``results``
  that contained an array of the result set, subject to the :ref:`BSON
  Document size <limit-bson-document-size>` limit. Accessing the result
  set in the previous versions of MongoDB required accessing the
  ``results`` field and iterating the array. For example:

  .. code-block:: javascript

     var returnedDoc = db.orders.aggregate( [
         {
           $group: {
              _id: "$cust_id",
              total: { $sum: "$price" }
           }
         }
     ] );

     var myArray = returnedDoc.result; // access the result field

     myArray.forEach( function(x) { printjson (x); } );

Solution
  Update scripts that currently expect
  :method:`db.collection.aggregate()` to return a document with a
  ``results`` array to handle cursors instead.

.. seealso:: :ref:`rn-2.6-aggregation-cursor`,
   :method:`db.collection.aggregate()`,

Write Concern Validation
------------------------

Description
   Specifying a write concern that includes ``j: true`` to a
   :program:`mongod` or :program:`mongos` instance running with
   :option:`--nojournal` option now errors. Previous versions would
   ignore the ``j: true``.

Solution
   Either remove the ``j: true`` specification from the write concern when
   issued against a :program:`mongod` or :program:`mongos` instance
   with :option:`--nojournal` or run :program:`mongod` or
   :program:`mongos` with journaling.

Security Changes
----------------

.. _authentication-incompatibility:

New Authorization Model
~~~~~~~~~~~~~~~~~~~~~~~

Description
   MongoDB 2.6 :ref:`authorization model <authorization>` changes how
   MongoDB stores and manages user privilege information:

   - Before the upgrade, MongoDB 2.6 requires at least one user in the
     admin database.

   - MongoDB versions using older models cannot create/modify users or
     create user-defined roles.

Solution
   Ensure that at least one user exists in the admin database. If no
   user exists in the admin database, add a user. Then upgrade to
   MongoDB 2.6. Finally, upgrade the user privilege model. See
   :doc:`/release-notes/2.6-upgrade`.

   .. important::

      .. include:: /includes/important-upgrade-auth-model-prerequisites.rst

.. seealso:: :ref:`2.6-relnotes-security`

.. _ssl-hostname-validation:

SSL Certificate Hostname Validation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   The SSL certificate validation now checks the Common Name (``CN``)
   and the Subject Alternative Name (``SAN``) fields to ensure that
   either the ``CN`` or one of the ``SAN`` entries matches the hostname
   of the server. As a result, if you currently use SSL and *neither*
   the ``CN`` nor any of the ``SAN`` entries of your current SSL
   certificates match the hostnames, upgrading to version 2.6 will
   cause the SSL connections to fail.

Solution
   To allow for the continued use of these certificates, MongoDB
   provides the :setting:`~net.ssl.allowInvalidCertificates` setting. The
   setting is available for:

   - :program:`mongod` and :program:`mongos` to bypass the validation
     of SSL certificates on other servers in the cluster.

   - :program:`mongo` shell, :ref:`MongoDB tools that support SSL
     <mongodb-tools-support-ssl>`, and the C++ driver to bypass the
     validation of server certificates.

   When using the :setting:`~net.ssl.allowInvalidCertificates` setting, MongoDB
   logs as a warning the use of the invalid certificates.

   .. warning:: The :setting:`~net.ssl.allowInvalidCertificates` setting
      bypasses the other certificate validation, such as checks for
      expiration and valid signatures.

.. _2.6-2dsphere-version-incompatibility:

``2dsphere`` Index Version 2
----------------------------

Description
   MongoDB 2.6 introduces a version 2 of the :doc:`2dsphere index
   </core/2dsphere>`. If a document lacks a ``2dsphere``
   index field (or the field is ``null`` or an empty array), MongoDB
   does not add an entry for the document to the ``2dsphere`` index.
   For inserts, MongoDB inserts the document but does not add to the
   ``2dsphere`` index.

   Previous version would not insert documents where the ``2dsphere``
   index field is a ``null`` or an empty array. For documents that lack
   the ``2dsphere`` index field, previous versions would insert and
   index the document.

Solution
   To revert to old behavior, create the ``2dsphere`` index with ``{
   "2dsphereIndexVersion" : 1 }`` to create a version 1 index. However,
   version 1 index cannot use the new GeoJSON geometries.

.. seealso:: :ref:`2dsphere-v2`

Log Messages
------------

Timestamp Format Change
~~~~~~~~~~~~~~~~~~~~~~~

Description
  Each message now starts with the timestamp formatted in
  ``iso8601-local``, i.e. ``YYYY-MM-DDTHH:mm:ss.mmm<+/-Offset>``. For
  example, ``2014-03-04T20:13:38.944-0500``. Previous versions used
  ``ctime`` format.

Solution
  MongoDB adds a new option :option:`--timeStampFormat` which supports
  timestamp format in :option:`ctime <--timeStampFormat>`,
  :option:`iso8601-utc <--timeStampFormat>`, and :option:`iso8601-local
  <--timeStampFormat>` (new default).

.. Commenting out the others for now -- they're just placeholders and notes for self
.. Other
   ~~~~~
   - ``query`` log messages add new field ``planSummary`.
   - ``getmore`` log messages now omit the ``query`` field.
   - :program:`mongo` shell now calls :dbcommand:`isMaster` command
     instead of the ``admin`` command :dbcommand:`replSetGetStatus`
     during the construction of the shell prompt. Log messages change
     accordingly, as in the following
     .. code-block:: none
        command: { isMaster: 1.0, forShell: 1.0 } keyUpdates:0 numYields:0  reslen:178 0ms
   - In 2.6, log messages for write methods consist of a message for
     the method followed by a message for the associated write command.
     Previously, the messages consisted of a message for the method
     followed by a message for the ``getlasterror`` command. For example:
     .. code-block:: none
     2014-03-04T21:16:21.992-0500 [conn1] remove test.server12774 query: { a: { $lt: 3.0 } } ndeleted:2 keyUpdates:0 numYields:0 locks(micros) w:344 0ms
     2014-03-04T21:16:21.993-0500 [conn1] command test.$cmd command: { delete: "myCollection", deletes: [ { q: { a: { $lt: 3.0 } }, limit: 0.0 } ], ordered: true } keyUpdates:0 numYields:0  reslen:40 0ms
   - MongoDB removes the two blank lines preceding and following the:
     ``***** SERVER RESTARTED *****`` message.
   - ``writeback`` error message and ``FlushViewOfFile`` error message
     removes the space between the duration time and ``ms``.

Package Configuration Changes
-----------------------------

.. _2.6-default-bindip-incompatibility:

Default ``bind_ip`` for RPM/DEB Packages
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   In the official MongoDB packages in RPM (Red Hat, CentOS, Fedora
   Linux, and derivatives) and DEB (Debian, Ubuntu, and derivatives),
   the default :setting:`bind_ip` value attaches MongoDB components to
   the localhost interface *only*. These packages set this default in
   the default configuration file (i.e. ``/etc/mongodb.conf``.)

Solution
   If you use one of these packages and have *not* modified the default
   ``/etc/mongodb.conf`` file, you will need to set :setting:`bind_ip`
   before or during the upgrade.

There is no default ``bind_ip`` setting in any other official MongoDB
packages.

.. _2.6-snmp-iana-mib-incompatibility:

SNMP Changes
~~~~~~~~~~~~

Description
   - The IANA enterprise identifier for MongoDB changed from
     37601 to 34601.

   - MongoDB changed the MIB field name ``globalopcounts`` to
     ``globalOpcounts``.

Solution
   - Users of SNMP monitoring must modify their SNMP configuration
     (i.e. MIB) from 37601 to 34601.

   - Update references to ``globalopcounts`` to ``globalOpcounts``.

Remove Method Signature Change
------------------------------

Description
   :method:`db.collection.remove()` requires a query document as a
   parameter. In previous versions, the method invocation without a
   query document deleted all documents in a collection.

Solution
   For existing :method:`db.collection.remove()` invocations without a
   query document, modify the invocations to include an empty document
   :method:`db.collection.remove({})`.

.. _update-operations-incompatibility:

Updates Enforce Field Name Restrictions
---------------------------------------

Description
   - Updates cannot use :doc:`update operators (e.g $set)
     </reference/operator/update>` to target fields with empty field
     names (i.e. ``""``).

   - Updates no longer support saving field names that contain a dot
     (``.``) or a field name that starts with a dollar sign (``$``).

Solution
   - For existing documents that currently have fields with empty names
     ``""``, replace the whole document. See
     :method:`db.collection.update()` and
     :method:`db.collection.save()` for details on replacing an
     existing document.

   - :update:`Unset <$unset>` or :update:`rename <$rename>` existing
     fields with names that contain a dot (``.``) or that start with a
     dollar sign (``$``). Run :method:`db.upgradeCheckAllDBs()` to find fields
     whose names contain a dot or starts with a dollar sign.

See :ref:`rel-notes-write-operations` for the changes to the write
operation protocol, and :ref:`rel-notes-data-modification` for the
changes to the insert and update operations.

Query and Sort Changes
----------------------

.. _query-field-name-incompatibility:

Enforce Field Name Restrictions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   Queries cannot specify conditions on fields with names that start
   with a dollar sign (``$``).

Solution
   :update:`Unset <$unset>` or :update:`rename <$rename>` existing
   fields whose names start with a dollar sign (``$``). Run
   :method:`db.upgradeCheckAllDBs()` to find fields whose names start
   with a dollar sign.

.. _2.6-sparse-index-incompatibility:

Sparse Index and Incomplete Results
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   If a :doc:`sparse index </core/index-sparse>` results in an
   incomplete result set for queries and sort operations, MongoDB will
   not use that index unless a :method:`~cursor.hint()` explicitly
   specifies the index.

   For example, the query ``{ x: { $exists: false } }`` will no longer
   use a sparse index on the ``x`` field, unless explicitly hinted.

Solution
   To override the behavior to use the sparse index and return
   incomplete results, explicitly specify the index with a
   :method:`~cursor.hint()`.

See :ref:`sparse-index-incomplete-results` for an example that details
the new behavior.

.. _2.6-sort-value-incompatibility:

``sort()`` Specification Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   The :method:`~cursor.sort()` method **only** accepts the following
   values for the sort keys:

   - ``1`` to specify ascending order for a field,

   - ``-1`` to specify descending order for a field, or

   - :projection:`$meta` expression to specify sort by the text search
     score.

   Any other value will result in an error.

   Previous versions also accepted either ``true`` or ``false`` for
   ascending.

Solution
   Update sort key values that use ``true`` or ``false`` to ``1``.

.. _2.6-skip-incompatibility:

``skip()`` and ``_id`` Queries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   Equality match on the ``_id`` field obeys :method:`~cursor.skip()`.

   Previous versions ignored :method:`~cursor.skip()` when performing
   an equality match on the ``_id`` field.

.. _2.6-explain-query-plan-incompatibility:

``explain()`` Retains Query Plan Cache
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   :method:`~cursor.explain()` no longer clears the :doc:`query plans
   </core/query-plans>` cached for that :term:`query shape`.

   In previous versions, :method:`~cursor.explain()` would have the
   side effect of clearing the query plan cache for that query shape.

.. seealso:: :doc:`/reference/method/js-plan-cache`

Geospatial Changes
~~~~~~~~~~~~~~~~~~

.. _2.6-geo-maxDistance-incompatibility:

``$maxDistance`` Changes
````````````````````````

Description
   - For :query:`$near` queries on GeoJSON data, if the queries specify
     a :query:`$maxDistance`, :query:`$maxDistance` must be inside of
     the :query:`$near` document.

     In previous version, :query:`$maxDistance` could be either inside
     or outside the :query:`$near` document.

   - :query:`$maxDistance` must be a positive value.

Solution
   - Update any existing :query:`$near` queries on GeoJSON data that
     currently have the :query:`$maxDistance` outside the
     :query:`$near` document

   - Update any existing queries where :query:`$maxDistance` is a
     negative value.

.. _2.6-uniqueDocs-incompatibility:

Deprecated ``$uniqueDocs``
``````````````````````````

Description
   MongoDB 2.6 deprecates :query:`$uniqueDocs`, and geospatial queries
   no longer return duplicated results when a document matches the
   query multiple times.

.. _2.6-geospatial-validation-incompatibility:

Stronger Validation of Geospatial Queries
`````````````````````````````````````````

Description
   MongoDB 2.6 enforces a stronger validation of geospatial queries,
   such as validating the options or GeoJSON specifications, and errors
   if the geospatial query is invalid. Previous versions
   allowed/ignored invalid options.

Query Operator Changes
~~~~~~~~~~~~~~~~~~~~~~

.. _2.6-not-incompatibility:

``$not`` Query Behavior Changes
```````````````````````````````

Description
   - Queries with :query:`$not` expressions on an indexed field now match:

     - Documents that are missing the indexed field. Previous versions
       would not return these documents using the index.

     - Documents whose indexed field value is a different type than
       that of the specified value. Previous versions would not return
       these documents using the index.

     For example, if a collection ``orders`` contains the following
     documents:

     .. code-block:: javascript

        { _id: 1, status: "A", cust_id: "123", price: 40 }
        { _id: 2, status: "A", cust_id: "xyz", price: "N/A" }
        { _id: 3, status: "D", cust_id: "xyz" }

     If the collection has an index on the ``price`` field:

     .. code-block:: javascript

        db.orders.ensureIndex( { price: 1 } )

     The following query uses the index to search for documents where
     ``price`` is not greater than or equal to ``50``:

     .. code-block:: javascript

        db.orders.find( { price: { $not: { $gte: 50 } } } )

     In 2.6, the query returns the following documents:

     .. code-block:: javascript

         { "_id" : 3, "status" : "D", "cust_id" : "xyz" }
         { "_id" : 1, "status" : "A", "cust_id" : "123", "price" : 40 }
         { "_id" : 2, "status" : "A", "cust_id" : "xyz", "price" : "N/A" }

     In previous versions, indexed plans would only
     return matching documents where the type of the field matches the
     type of the query predicate:

     .. code-block:: javascript

        { "_id" : 1, "status" : "A", "cust_id" : "123", "price" : 40 }

     If using a collection scan, previous versions would return the
     same results as those in 2.6.

   - MongoDB 2.6 allows chaining of :query:`$not` expressions.

.. _2.6-null-incompatibility:

``null`` Comparison Queries
````````````````````````````

Description
   - :query:`$lt` and :query:`$gt` comparisons to null no
     longer match documents that are missing the field.

   - ``null`` equality conditions on array elements (e.g. ``"a.b":
     null``) no longer match document missing the nested field ``a.b``
     (e.g. ``a: [ 2, 3 ]``)

.. _2.6-all-incompatibility:

``$all`` Operator Behavior Change
`````````````````````````````````

Description
   - The :query:`$all` operator is now equivalent to an :query:`$and`
     operation of the specified values. This change in behavior can
     allow for more matches than previous versions when passed an array
     of a single nested array (e.g. ``[ [ "A" ] ]``). When passed an
     array of a nested array, :query:`$all` can now match documents
     where the field contains the nested array as an element (e.g.
     ``field: [ [ "A" ], ... ]``), *or* the field equals the nested
     array (e.g. ``field: [ "A", "B" ]``). Earlier version could only
     match documents where the field contains the nested array.

   - The :query:`$all` operator returns no match if the array field
     contains nested arrays (e.g. ``field: [ "a", ["b"] ]``) and
     :query:`$all` on the nested field is the element of the nested
     array (e.g. ``"field.1": { $all: [ "b" ] }``). Previous versions
     would return a match.

.. _2.6-mod-operator-incompatibility:

``$mod`` Operator Enforces Strict Syntax
````````````````````````````````````````

Description
   The :query:`$mod` operator now only accepts an array with exactly
   two elements, and errors when passed an array with fewer or more
   elements. See :ref:`mod-not-enough-elements` and :ref:`mod-too-many-elements`
   for details.

   In previous versions, if passed an array with one element, the
   :query:`$mod` operator uses ``0`` as the second element, and if
   passed an array with more than two elements, the :query:`$mod`
   ignores all but the first two elements. Previous versions do return
   an error when passed an empty array.

Solution
   Ensure that the array passed to :query:`$mod` contains exactly two
   elements:

   - If the array contains the a single element, add ``0`` as the
     second element.

   - If the array contains more than two elements, remove the extra
     elements.

.. _2.6-where-incompatibility:

``$where`` Must Be Top-Level
````````````````````````````

Description
   :query:`$where` expressions can now only be at top level and cannot be
   nested within another expression, such as :query:`$elemMatch`.

Solution
   Update existing queries that nest :query:`$where`.

``$exists`` and ``notablescan``
```````````````````````````````

If the MongoDB server has disabled collection scans, i.e.
:parameter:`notablescan`, then :query:`$exists` queries that have no
*indexed solution* will error.

.. _2.6-min-max-key-equality-match-incompatibility:

``MinKey`` and ``MaxKey`` Queries
`````````````````````````````````

Description
   Equality match for either ``MinKey`` or
   ``MaxKey`` no longer match documents missing the field.

.. COMMENT will wait to doc the following in case the resolution
   matches the 2.4 behavior. :query:`$elemMatch` against nested arrays
   - SERVER-12496

Text Search Compatibility
~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB does not support the use of the :query:`$text` query operator
in mixed sharded cluster deployments that contain both version 2.4
and version 2.6 shards. See :doc:`/release-notes/2.6-upgrade` for
upgrade instructions.

Replica Set/Sharded Cluster Validation
--------------------------------------

.. _2.6-metadata-check-incompatibility:

Shard Name Checks on Metadata Refresh
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   For sharded clusters, MongoDB 2.6 disallows a shard from refreshing
   the metadata if the shard name has not been explicitly set.

   For mixed sharded cluster deployments that contain both version 2.4
   and version 2.6 shards, this change can cause errors when migrating
   chunks **from** version 2.4 shards **to** version 2.6 shards if the
   shard name is unknown to the version 2.6 shards. MongoDB does not
   support migrations in mixed sharded cluster deployments.

Solution
   Upgrade all components of the cluster to 2.6. See
   :doc:`/release-notes/2.6-upgrade`.

.. _2.6-repl-set-vote-incompatibility:

Replica Set Vote Configuration Validation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Description
   MongoDB now deprecates giving any :term:`replica set` member more
   than a single vote. During configuration,
   :data:`local.system.replset.members[n].votes` should only have a
   value of 1 for voting members and 0 for non-voting members. MongoDB
   treats values other than 1 or 0 as a value of 1 and produces a
   warning message.

Solution
   Update :data:`local.system.replset.members[n].votes` with values
   other than 1 or 0 to 1 or 0 as appropriate.

.. _2.6-compatibility-other-resources:

Other Resources
---------------

- `All backwards incompatible changes (JIRA) <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.5.0%22%2C+%222.5.1%22%2C+%222.5.2%22%2C+%222.5.3%22%2C+%222.5.4%22%2C+%222.5.5%22%2C+%222.6.0-rc1%22%2C+%222.6.0-rc2%22%29+AND+%22Backward+Breaking%22+in+%28+Rarely+%2C+sometimes%2C+yes+%29+ORDER+BY+votes+DESC%2C+issuetype+DESC%2C+key+DESC>`_.

- :doc:`/release-notes/2.6`.

- :doc:`/release-notes/2.6-upgrade` for the upgrade process.
==========================
Downgrade MongoDB from 2.6
==========================

.. default-domain:: mongodb

Before you attempt any downgrade, familiarize yourself with the content
of this document, particularly the :ref:`2.6-downgrade-considerations`
and the procedure for :ref:`downgrading sharded clusters
<2.6-downgrade-cluster>`.

.. _2.6-downgrade-considerations:

Downgrade Recommendations and Checklist
---------------------------------------

When downgrading, consider the following:

Downgrade Path
~~~~~~~~~~~~~~

Once upgraded to MongoDB 2.6, you **cannot** downgrade to **any** version
earlier than MongoDB 2.4. If you created ``text`` or ``2dsphere``
indexes while running 2.6, you can only downgrade to MongoDB 2.4.10 or
later.

Preparedness
~~~~~~~~~~~~

- :ref:`Remove or downgrade version 2 text indexes
  <downgrade-text-index>` before downgrading MongoDB 2.6 to 2.4.

- :ref:`Remove or downgrade version 2 2dsphere indexes
  <downgrade-2dsphere-index>` before downgrading MongoDB 2.6 to 2.4.

- :ref:`downgrade-user-auth-model`. If you have upgraded to the 2.6
  user authorization model, you must downgrade the user model to 2.4
  before downgrading MongoDB 2.6 to 2.4.

Procedures
~~~~~~~~~~

Follow the downgrade procedures:

- To downgrade sharded clusters, see :ref:`2.6-downgrade-cluster`.

- To downgrade replica sets, see :ref:`2.6-downgrade-replica-set`.

- To downgrade a standalone MongoDB instance, see :ref:`2.6-downgrade-standalone`.

.. _downgrade-user-auth-model:

Downgrade 2.6 User Authorization Model
--------------------------------------

If you have upgraded to the 2.6 user authorization
model, you **must first** downgrade the user authorization model to
2.4 **before** before downgrading MongoDB 2.6 to 2.4.

Considerations
~~~~~~~~~~~~~~

- For a replica set, it is only necessary to run the downgrade process
  on the :term:`primary` as the changes will automatically replicate to
  the secondaries.

- For sharded clusters, although the procedure lists the downgrade of
  the cluster's authorization data first, you may downgrade the
  authorization data of the cluster or shards first.

- You *must* have the ``admin.system.backup_users`` and
  ``admin.system.new_users`` collections created during the upgrade
  process.

- **Important**. The downgrade process returns the user data to its
  state prior to upgrading to 2.6 authorization model. Any changes
  made to the user/role data using the 2.6 users model will be lost.

Procedure
~~~~~~~~~

The following downgrade procedure requires ``<database>.system.users``
collections used in version 2.4. to be intact for non-``admin``
databases:

.. include:: /includes/steps/2.6-downgrade-authorization.rst

For a sharded cluster, repeat the downgrade process by connecting to
the :term:`primary` replica set member for each shard.

.. note::

   The cluster's :program:`mongos` instances will fail to detect the
   authorization model downgrade until the user cache is refreshed. You
   can run :dbcommand:`invalidateUserCache` on each :program:`mongos`
   instance to refresh immediately, or you can wait until the cache is
   refreshed automatically at the end of the :parameter:`user cache
   invalidation interval <userCacheInvalidationIntervalSecs>`.

Result
~~~~~~

The downgrade process returns the user data to its state prior to
upgrading to 2.6 authorization model. Any changes made to the
user/role data using the 2.6 users model will be lost.

Downgrade Updated Indexes
-------------------------

.. _downgrade-text-index:

Text Index Version Check
~~~~~~~~~~~~~~~~~~~~~~~~

If you have *version 2* text indexes (i.e. the default version for text
indexes in MongoDB 2.6), drop the *version 2* text indexes before
downgrading MongoDB. After the downgrade, enable text search and
recreate the dropped text indexes.

To determine the version of your ``text`` indexes, run
:method:`db.collection.getIndexes()` to view index specifications. For
text indexes, the method returns the version information in the field
``textIndexVersion``. For example, the following shows that the
``text`` index on the ``quotes`` collection is version 2.

.. code-block:: javascript
   :emphasize-lines: 15

   {
      "v" : 1,
      "key" : {
         "_fts" : "text",
         "_ftsx" : 1
      },
      "name" : "quote_text_translation.quote_text",
      "ns" : "test.quotes",
      "weights" : {
         "quote" : 1,
         "translation.quote" : 1
      },
      "default_language" : "english",
      "language_override" : "language",
      "textIndexVersion" : 2
   }

.. _downgrade-2dsphere-index:

``2dsphere`` Index Version Check
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you have *version 2* ``2dsphere`` indexes (i.e. the default version
for ``2dsphere`` indexes in MongoDB 2.6), drop the *version 2*
``2dsphere`` indexes before downgrading MongoDB. After the downgrade,
recreate the ``2dsphere`` indexes.

To determine the version of your ``2dsphere`` indexes, run
:method:`db.collection.getIndexes()` to view index specifications. For
``2dsphere`` indexes, the method returns the version information in the field
``2dsphereIndexVersion``. For example, the following shows that the
``2dsphere`` index on the ``locations`` collection is version 2.

.. code-block:: javascript
   :emphasize-lines: 15

   {
      "v" : 1,
      "key" : {
         "geo" : "2dsphere"
      },
      "name" : "geo_2dsphere",
      "ns" : "test.locations",
      "sparse" : true,
      "2dsphereIndexVersion" : 2
   }

Downgrade MongoDB Processes
---------------------------

.. _2.6-downgrade-standalone:

Downgrade 2.6 Standalone ``mongod`` Instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following steps outline the procedure to downgrade a standalone
:program:`mongod` from version 2.6 to 2.4.

#. Download binaries of the latest release in the 2.4 series from the
   `MongoDB Download Page`_. See :doc:`/installation` for more
   information.

#. Shut down your :program:`mongod` instance. Replace the existing
   binary with the 2.4 :program:`mongod` binary and restart
   :program:`mongod`.

.. _`MongoDB Download Page`: http://www.mongodb.org/downloads

.. _2.6-downgrade-replica-set:

Downgrade a 2.6 Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following steps outline a "rolling" downgrade process for the
replica set. The "rolling" downgrade process minimizes downtime by
downgrading the members individually while the other members are
available:

.. include:: /includes/steps/2.6-downgrade-replica-set.rst

Replica set failover is not instant but will render the set unavailable to
writes and interrupt reads until the failover process
completes. Typically this takes 10 seconds or more. You may wish to
plan the downgrade during a predetermined maintenance window.

.. _2.6-downgrade-cluster:

Downgrade a 2.6 Sharded Cluster
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Requirements
````````````

While the downgrade is in progress, you cannot make changes to the
collection meta-data. For example, during the downgrade, do
**not** do any of the following:

- :method:`sh.enableSharding()`

- :method:`sh.shardCollection()`

- :method:`sh.addShard()`

- :method:`db.createCollection()`

- :method:`db.collection.drop()`

- :method:`db.dropDatabase()`

- any operation that creates a database

- any other operation that modifies the cluster meta-data in any
  way. See :doc:`/reference/sharding` for a complete list of
  sharding commands. Note, however, that not all commands on the
  :doc:`/reference/sharding` page modifies the cluster meta-data.

Procedure
`````````

The downgrade procedure for a sharded cluster reverses the order of
the upgrade procedure.

#. Turn off the :ref:`balancer <sharding-balancing-internals>` in the
   sharded cluster, as described in
   :ref:`sharding-balancing-disable-temporarily`.

#. Downgrade each shard, one at a time. For each shard,

   a. Downgrade the :program:`mongod` secondaries *before* downgrading
      the primary.

   #. To downgrade the primary, run :dbcommand:`replSetStepDown` and
      downgrade.

#. Downgrade all 3 :program:`mongod` config server instances, leaving
   the *first* system in the :option:`mongos --configdb` argument to
   downgrade *last*.

#. Downgrade and restart each :program:`mongos`, one at a time. The
   downgrade process is a binary drop-in replacement.

#. Turn on the balancer, as described in
   :ref:`sharding-balancing-enable`.


.. ###############   Downgrade Procedure    ###############

.. |action| replace:: Downgrade
.. |version-new| replace:: 2.4
.. |version-stop| replace:: 2.6

.. include:: /includes/2.4-2.6-upgrade-downgrade-procedure.rst
.. _2.6-upgrade-authorization-model:

=============================================
Upgrade User Authorization Data to 2.6 Format
=============================================

.. default-domain:: mongodb

MongoDB 2.6 includes significant changes to the authorization model,
which requires changes to the way that MongoDB stores users'
credentials. As a result, in addition to upgrading MongoDB processes,
if your deployment uses authentication and authorization, after
upgrading all MongoDB process to 2.6 you must also upgrade the
authorization model.

Considerations
~~~~~~~~~~~~~~

Complete all other Upgrade Requirements
```````````````````````````````````````

.. include:: /includes/important-upgrade-auth-model-prerequisites.rst

Timing
``````

.. include:: /includes/fact-auth-upgrade-recommendation.rst

If you decide to upgrade the user authorization
model immediately instead of waiting the recommended "burn in"
period, then for sharded clusters, you must wait at least 10 seconds
after upgrading the sharded clusters to run the authorization
upgrade script.

Replica Sets
````````````

For a replica set, it is only necessary to run the upgrade process on
the :term:`primary` as the changes will automatically replicate to
the secondaries.

Sharded Clusters
````````````````

For a sharded cluster, connect to a :program:`mongos` and run the
upgrade procedure to upgrade the cluster's authorization data. By
default, the procedure will upgrade the authorization data of the
shards as well.

To override this behavior, run the upgrade command with the
additional parameter ``upgradeShards: false``. If you choose to
override, you must run the upgrade procedure on the :program:`mongos`
first, and then run the procedure on the :term:`primary` members of
each shard.

For a sharded cluster, do **not** run the upgrade process directly
against the :doc:`config servers
</core/sharded-cluster-config-servers>`. Instead, perform the upgrade
process using one :program:`mongos` instance to interact with the
config database.

Requirements
~~~~~~~~~~~~

To upgrade the authorization model, you must have a user in the
``admin`` database with the role :authrole:`userAdminAnyDatabase`.

Procedure
~~~~~~~~~

.. include:: /includes/steps/2.6-upgrade-authorization.rst

Result
~~~~~~

All users a 2.6 system are stored in the :data:`admin.system.users`
collection. To manipulate these users use the :doc:`user management
methods </reference/method/js-user-management>`.

The upgrade procedure copies the version 2.4 ``admin.system.users``
collection to ``admin.system.backup_users``.

The upgrade procedure leaves the version 2.4
``<database>.system.users`` collection(s) intact.
======================
Upgrade MongoDB to 2.6
======================

.. default-domain:: mongodb

In the general case, the upgrade from MongoDB 2.4 to 2.6 is a
binary-compatible "drop-in" upgrade: shut down the :program:`mongod`
instances and replace them with :program:`mongod` instances running
2.6. **However**, before you attempt any upgrade, familiarize yourself
with the content of this document, particularly the
:ref:`2.6-upgrade-considerations`, the procedure for :ref:`upgrading
sharded clusters <2.6-upgrade-cluster>`, and the considerations for
:ref:`reverting to 2.4 after running 2.6
<2.6-downgrade-considerations>`.

.. _2.6-upgrade-considerations:

Upgrade Recommendations and Checklists
--------------------------------------

When upgrading, consider the following:

Upgrade Requirements
~~~~~~~~~~~~~~~~~~~~

To upgrade an existing MongoDB deployment to 2.6, you must be running
2.4. If you're running a version of MongoDB before 2.4, you *must*
upgrade to 2.4 before upgrading to 2.6. See
:doc:`/release-notes/2.4-upgrade` for the procedure to upgrade from
2.2 to 2.4.

If you use MMS Backup, ensure that you're running *at least* version
``v20131216.1`` of the Backup agent before upgrading.

Preparedness
~~~~~~~~~~~~

Before upgrading MongoDB always test your application in a staging
environment before deploying the upgrade to your production
environment.

To begin the upgrade procedure, connect a 2.6 :program:`mongo` shell
to your MongoDB 2.4 :program:`mongos` or :program:`mongod` and run the
:method:`db.upgradeCheckAllDBs()` to check your data set for
compatibility. This is a preliminary automated check. Assess and
resolve all issues identified by :method:`db.upgradeCheckAllDBs()`.

Some changes in MongoDB 2.6 require manual checks and
intervention. See :doc:`/release-notes/2.6-compatibility` for an
explanation of these changes. Resolve all incompatibilities in your
deployment before continuing.

Authentication
~~~~~~~~~~~~~~

MongoDB 2.6 includes significant changes to the authorization model,
which requires changes to the way that MongoDB stores users'
credentials. As a result, in addition to upgrading MongoDB processes,
if your deployment uses authentication and authorization, after
upgrading all MongoDB process to 2.6 you must also upgrade the
authorization model.

Before beginning the upgrade process for a deployment that uses
authentication and authorization:

- Ensure that at least one user exists in the ``admin`` database.

- If your application performs CRUD operations on the
  :data:`<database>.system.users` collection or uses a
  :method:`db.addUser()`\ -like method, then you **must**
  upgrade those drivers (i.e. client libraries) **before**
  :program:`mongod` or :program:`mongos` instances.

- You must fully complete the upgrade procedure for *all* MongoDB
  processes before upgrading the authorization model.

See :ref:`2.6-upgrade-authorization-model` for a complete discussion
of the upgrade procedure for the authorization model including
additional requirements and procedures.

Downgrade Limitations
~~~~~~~~~~~~~~~~~~~~~

Once upgraded to MongoDB 2.6, you **cannot** downgrade to **any** version
earlier than MongoDB 2.4. If you created ``text`` or ``2dsphere``
indexes while running 2.6, you can only downgrade to MongoDB 2.4.10 or
later.

Upgrade MongoDB Processes
-------------------------

.. _2.6-upgrade-standalone:

Upgrade Standalone ``mongod`` Instance to MongoDB 2.6
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following steps outline the procedure to upgrade a standalone
:program:`mongod` from version 2.4 to 2.6. To upgrade from version
2.2 to 2.6, :doc:`upgrade to version 2.4
</release-notes/2.4-upgrade>` *first*, and then follow the procedure to
upgrade from 2.4 to 2.6.

#. Download binaries of the latest release in the 2.6 series from the
   `MongoDB Download Page`_. See :doc:`/installation` for more
   information.

#. Shut down your :program:`mongod` instance. Replace the existing
   binary with the 2.6 :program:`mongod` binary and restart :program:`mongod`.

.. _`MongoDB Download Page`: http://www.mongodb.org/downloads

.. _2.6-upgrade-replica-set:

Upgrade a Replica Set to 2.6
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following steps outline the procedure to upgrade a replica set from
MongoDB 2.4 to MongoDB 2.6. To upgrade from MongoDB 2.2 to 2.6,
:doc:`upgrade all members of the replica set to version 2.4
</release-notes/2.4-upgrade>` *first*, and then follow the procedure to
upgrade from MongoDB 2.4 to 2.6.

You can upgrade from MongoDB 2.4 to 2.6 using a "rolling" upgrade to
minimize downtime by upgrading the members individually while the other
members are available:

.. include:: /includes/steps/2.6-upgrade-replica-set.rst

Replica set failover is not instant but will render the set
unavailable accept writes until the failover process
completes. Typically this takes 30 seconds or more: schedule the
upgrade procedure during a scheduled maintenance window.

.. _2.6-upgrade-cluster:

Upgrade a Sharded Cluster to 2.6
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Only upgrade sharded clusters to 2.6 if **all** members of the
cluster are currently running instances of 2.4. The only supported
upgrade path for sharded clusters running 2.2 is via 2.4. The upgrade
process checks all components of the cluster and will produce warnings
if any component is running version 2.2.

Considerations
``````````````

The upgrade process does not require any downtime. However, while you
upgrade the sharded cluster, ensure that clients do not make changes
to the collection meta-data. For example, during the upgrade, do **not**
do any of the following:

- :method:`sh.enableSharding()`

- :method:`sh.shardCollection()`

- :method:`sh.addShard()`

- :method:`db.createCollection()`

- :method:`db.collection.drop()`

- :method:`db.dropDatabase()`

- any operation that creates a database

- any other operation that modifies the cluster metadata in any
  way. See :doc:`/reference/sharding` for a complete list
  of sharding commands. Note, however, that not all commands on
  the :doc:`/reference/sharding` page modifies the
  cluster meta-data.

Upgrade Sharded Clusters
````````````````````````

*Optional but Recommended.* As a precaution, take a backup of the
``config`` database *before* upgrading the sharded cluster.

.. include:: /includes/steps/2.6-upgrade-sharded-cluster.rst

.. _2.6-finalize-shard-cluster-upgrade:

Complete Sharded Cluster Upgrade
````````````````````````````````

After you have successfully upgraded *all* :program:`mongos` instances,
you can upgrade the other instances in your MongoDB deployment.

.. warning::

   Do not upgrade :program:`mongod` instances until after you have
   upgraded *all* :program:`mongos` instances.

While the balancer is still disabled, upgrade the components of your
sharded cluster in the following order:

- Upgrade all 3 :program:`mongod` config server instances, leaving
  the *first* system in the :option:`mongos --configdb` argument to upgrade
  *last*.

- Upgrade each shard, one at a time, upgrading the :program:`mongod`
  secondaries before running :dbcommand:`replSetStepDown` and
  upgrading the primary of each shard.

When this process is complete, :ref:`re-enable the
balancer <sharding-balancing-enable>`.

.. |action| replace:: Upgrade
.. |version-new| replace:: 2.6
.. |version-stop| replace:: 2.4

.. include:: /includes/2.4-2.6-upgrade-downgrade-procedure.rst
:orphan:

=============================
Release Notes for MongoDB 2.6
=============================

.. default-domain:: mongodb

*April 8, 2014*

MongoDB 2.6 is now available. Key features include aggregation
enhancements, text-search integration, query-engine improvements, a new
write-operation protocol, and security enhancements.

MMS 1.4, which includes On-Prem Backup in addition to Monitoring, is
now also available. See `MMS 1.4 documentation
<https://mms.mongodb.com/help-hosted/v1.4/>`_ and the `MMS 1.4 release
notes <https://mms.mongodb.com/help-hosted/v1.4/management/changelog/>`_
for more information.

Major Changes
-------------

The following changes in MongoDB affect both the standard and Enterprise
editions:

.. _rn-2.6-aggregation-cursor:

Aggregation Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~

The aggregation pipeline adds the ability to return result sets of any
size, either by returning a cursor or writing the output to
a collection. Additionally, the aggregation pipeline supports
variables and adds new operations to handle sets and redact data.

- The :method:`db.collection.aggregate()` now returns a cursor, which
  enables the aggregation pipeline to return result sets of any size.

- Aggregation pipelines now support an ``explain`` operation to aid
  analysis of aggregation operations.

- Aggregation can now use a more efficient external-disk-based sorting
  process.

- New pipeline stages:

  - :pipeline:`$out` stage to output to a collection.

  - :pipeline:`$redact` stage to allow additional control to accessing
    the data.

- New or modified operators:

  - :doc:`set expression operators
    </reference/operator/aggregation-set>`.

  - :expression:`$let` and :expression:`$map` operators to allow for
    the use of variables.

  - :expression:`$literal` operator and :expression:`$size` operator.

  - :expression:`$cond` expression now accepts either an object or
    an array.

.. _rel-notes-text-query-operator:

Text Search Integration
~~~~~~~~~~~~~~~~~~~~~~~

Text search is now enabled by default, and the query system, including
the aggregation pipeline :pipeline:`$match` stage, includes the
:query:`$text` operator, which resolves text-search queries.

MongoDB 2.6 includes an updated :doc:`text index
</core/index-text>` format and deprecates the :dbcommand:`text`
command.

.. _rel-notes-data-modification:

Insert and Update Improvements
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Improvements to the update and insert systems include additional
operations and improvements that increase consistency of modified
data.

- .. include:: /includes/fact-update-field-order.rst
     :start-after: order-of-document-fields
     :end-before: .. versionchanged

- New or enhanced update operators:

  - :update:`$bit` operator supports bitwise ``xor`` operation.

  - :update:`$min` and :update:`$max` operators that perform
    conditional update depending on the relative size of the specified
    value and the current value of a field.

  - :update:`$push` operator has enhanced support for the
    :update:`$sort`, :update:`$slice`, and :update:`$each` modifiers
    and supports a new :update:`$position` modifier.

  - :update:`$currentDate` operator to set the value of a field to the
    current date.

- The :update:`$mul` operator for multiplicative increments for insert
  and update operations.

.. seealso:: :ref:`update-operations-incompatibility`

.. _rel-notes-write-operations:

New Write Operation Protocol
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A new write protocol integrates write operations with
write concerns. The protocol also provides improved
support for bulk operations.

MongoDB 2.6 adds the write commands :dbcommand:`insert`,
:dbcommand:`update`, and :dbcommand:`delete`, which provide the basis
for the improved bulk insert. All officially supported MongoDB drivers
support the new write commands.

The:program:`mongo` shell now includes
methods to perform bulk-write operations. See :method:`Bulk()` for
more information.

.. seealso:: :ref:`write-methods-incompatibility`

MSI Package for MongoDB Available for Windows
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB now distributes MSI packages for Microsoft Windows. This is the
recommended method for MongoDB installation under Windows.

.. _2.6-relnotes-security:

Security Improvements
---------------------

MongoDB 2.6 enhances support for secure deployments through improved SSL
support, x.509-based authentication, an improved authorization system
with more granular controls, as well as centralized credential storage, and
improved user management tools.

Specifically these changes include:

- A new :ref:`authorization model <authorization>` that provides the
  ability to create custom :ref:`user-defined-roles` and the ability to
  specify user privileges at a collection-level granularity.

- Global user management, which stores all user and
  user-defined role data in the ``admin`` database and provides a new
  set of commands for managing users and roles.

- :doc:`x.509 certificate authentication </tutorial/configure-x509>`
  for client authentication as well as for internal authentication of
  sharded and/or replica set cluster members. x.509 authentication is
  only available for deployments using SSL.

- Enhanced SSL Support:

  - :doc:`Rolling upgrades of clusters
    </tutorial/upgrade-cluster-to-ssl>` to use SSL.

  - :ref:`mongodb-tools-support-ssl` support connections to
    :program:`mongod` and :program:`mongos` instances using SSL
    connections.

  - :ref:`Prompt for passphrase <ssl-certificate-password>`
    by :program:`mongod` or :program:`mongos` at startup.

  - Require the use of strong SSL ciphers, with a minimum 128-bit key
    length for all connections. The strong-cipher requirement prevents
    an old or malicious client from forcing use of a weak cipher.

- MongoDB disables the http interface by default, limiting
  :doc:`network exposure </core/security-network>`. To enable the
  interface, see :setting:`httpinterface`.

.. seealso:: :ref:`authentication-incompatibility`,
   :ref:`ssl-hostname-validation`,
   and :doc:`/administration/security-checklist`.

Query Engine Improvements
-------------------------

- MongoDB can now use :doc:`index intersection
  </core/index-intersection>` to fulfill queries supported by more
  than one index.

- :ref:`index-filters` to limit which indexes can become the winning
  plan for a query.

- :doc:`/reference/method/js-plan-cache` methods to view and clear the
  :doc:`query plans </core/query-plans>` cached by the query optimizer.

- MongoDB can now use :method:`~cursor.count()` with
  :method:`~cursor.hint()`. See :method:`~cursor.count()` for details.

Improvements
------------

Geospatial Enhancements
~~~~~~~~~~~~~~~~~~~~~~~

- :ref:`2dsphere indexes version 2
  <2dsphere-v2>`.

- Support for :ref:`geojson-multipoint`,
  :ref:`geojson-multilinestring`, :ref:`geojson-multipolygon`, and
  :ref:`geojson-geometrycollection`.

- Support for geospatial query clauses in :query:`$or` expressions.

.. seealso::
   :ref:`2.6-2dsphere-version-incompatibility`,
   :ref:`2.6-geo-maxDistance-incompatibility`,
   :ref:`2.6-uniqueDocs-incompatibility`,
   :ref:`2.6-geospatial-validation-incompatibility`

Index Build Enhancements
~~~~~~~~~~~~~~~~~~~~~~~~

- :ref:`Background index build
  <index-creation-building-indexes-on-secondaries>` allowed on
  secondaries. If you initiate a background index build on a
  :term:`primary`, the secondaries will replicate the index build in
  the background.

- Automatic rebuild of interrupted index builds after a restart.

  - If a standalone or a primary instance terminates during an index
    build *without a clean shutdown*, :program:`mongod` now restarts
    the index build when the instance restarts. If the instance shuts
    down cleanly or if a user kills the index build, the interrupted
    index builds do not automatically restart upon the restart of the
    server.

  - If a secondary instance terminates during an index build, the
    :program:`mongod` instance will now restart the interrupted index
    build when the instance restarts.

  To disable this behavior, use the :option:`--noIndexBuildRetry`
  command-line option.

- :method:`~db.collection.ensureIndex()` now wraps a new
  :dbcommand:`createIndex` command.

- The ``dropDups`` option to :method:`~db.collection.ensureIndex()`
  and :dbcommand:`createIndex` is deprecated.

.. seealso:: :ref:`2.6-index-key-length-incompatibility`

Enhanced Sharding and Replication Administration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- New :dbcommand:`cleanupOrphaned` command to remove :term:`orphaned
  documents <orphaned document>` from a shard.

- New :dbcommand:`mergeChunks` command to combine contiguous chunks
  located on a single shard. See :dbcommand:`mergeChunks` and
  :doc:`/tutorial/merge-chunks-in-sharded-cluster`.

- New :method:`rs.printReplicationInfo()` and
  :method:`rs.printSlaveReplicationInfo()` methods to provide a
  formatted report of the status of a replica set from the perspective
  of the primary and the secondary, respectively.

Operational Changes
-------------------

Storage
~~~~~~~

:collflag:`usePowerOf2Sizes` is now the default allocation strategy for
all new collections. The new allocation strategy uses more storage
relative to total document size but results in lower levels of
storage fragmentation and more predictable storage capacity planning
over time.

To use the previous *exact-fit allocation strategy*:

- For a specific collection, use :dbcommand:`collMod` with
  :collflag:`usePowerOf2Sizes` set to ``false``.

- For all new collections on an entire :program:`mongod` instance,
  set :parameter:`newCollectionsUsePowerOf2Sizes` to ``false``.

See :doc:`/core/storage` for more information about MongoDB's storage system.

Networking
~~~~~~~~~~

- Removed upward limit for the :setting:`~net.maxIncomingConnections` for :program:`mongod`
  and :program:`mongos`. Previous versions capped the maximum possible
  :setting:`~net.maxIncomingConnections` setting at ``20,000`` connections.

- Connection pools for a :program:`mongos` instance may be used by multiple
  MongoDB servers. This can reduce the number of connections needed
  for high-volume workloads and reduce resource consumption in
  sharded clusters.

- The C++ driver now monitors :term:`replica set` health with the
  :dbcommand:`isMaster` command instead of
  :dbcommand:`replSetGetStatus`. This allows the C++ driver to support
  systems that require authentication.

- New :method:`cursor.maxTimeMS()` and corresponding ``maxTimeMS``
  option for commands to specify a time limit.

Tool Improvements
~~~~~~~~~~~~~~~~~

- :program:`mongo` shell supports a global :ref:`/etc/mongorc.js
  <mongo-global-mongorc-file>`.

- All MongoDB :doc:`executable files </reference/program>` now support
  the ``--quiet`` option to suppress all logging output except for
  error messages.

- :program:`mongoimport` uses the input filename, without the file
  extension if any, as the collection name if run without the ``-c`` or
  ``--collection`` specification.

- :program:`mongoexport` can now constrain export data using
  :option:`--skip` and :option:`--limit`, as well as order the documents
  in an export using the :option:`--sort` option.

- :program:`mongostat` can support the use of :option:`--rowcount`
  (:option:`-n`) with the :option:`--discover` option to produce the
  specified number of output lines.

- Add strict mode representation for :bsontype:`data_numberlong` for
  use by :program:`mongoexport` and :program:`mongoimport`.

MongoDB Enterprise Features
---------------------------

The following changes are specific to MongoDB Enterprise Editions:

MongoDB Enterprise for Windows
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:doc:`MongoDB Enterprise for Windows
</tutorial/install-mongodb-enterprise-on-windows>` is now available. It
includes support for Kerberos, SSL, and SNMP.

.. include:: /includes/admonition-mongodb-enterprise-windows-ldap.rst

MongoDB Enterprise for Windows includes with OpenSSL version 1.0.1g.

Auditing
~~~~~~~~

MongoDB Enterprise adds :doc:`auditing </core/auditing>` capability for
:program:`mongod` and :program:`mongos` instances. See :ref:`auditing`
for details.

.. _relnotes-ldap-authentication:

LDAP Support for Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB Enterprise provides support for proxy authentication of users.
This allows administrators to configure a MongoDB cluster to
authenticate users by proxying authentication requests
to a specified Lightweight Directory Access Protocol (LDAP) service.
See :doc:`/tutorial/configure-ldap-sasl-authentication` for details.

.. include:: /includes/admonition-mongodb-enterprise-windows-ldap.rst

MongoDB does **not** support LDAP authentication in mixed sharded
cluster deployments that contain both version 2.4 and version 2.6
shards. See :doc:`/release-notes/2.6-upgrade` for upgrade instructions.

Expanded SNMP Support
~~~~~~~~~~~~~~~~~~~~~

MongoDB Enterprise has greatly expanded its SNMP support to provide
SNMP access to nearly the full range of metrics provided by
:method:`db.serverStatus()`.

.. seealso:: :ref:`2.6-snmp-iana-mib-incompatibility`

Additional Information
----------------------

Changes Affecting Compatibility
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. class:: hidden

   .. toctree::

      /release-notes/2.6-compatibility

Some changes in 2.6 can affect :doc:`compatibility
</release-notes/2.6-compatibility>` and may require user actions. The
2.6 :program:`mongo` shell provides an
:method:`db.upgradeCheckAllDBs()` method to perform a check for upgrade
preparedness for some of these changes.

See :doc:`/release-notes/2.6-compatibility` for a detailed list of
compatibility changes.

.. seealso::
   `All Backwards incompatible changes (JIRA) <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.5.0%22%2C+%222.5.1%22%2C+%222.5.2%22%2C+%222.5.3%22%2C+%222.5.4%22%2C+%222.5.5%22%2C+%222.6.0-rc1%22%2C+%222.6.0-rc2%22%2C+%222.6.0-rc3%22%29+AND+%22Backward+Breaking%22+in+%28+Rarely+%2C+sometimes%2C+yes+%29+ORDER+BY+votes+DESC%2C+issuetype+DESC%2C+key+DESC>`_.

Upgrade Process
~~~~~~~~~~~~~~~

.. class:: hidden

   .. toctree::

      /release-notes/2.6-upgrade
      /release-notes/2.6-upgrade-authorization
      /release-notes/2.6-downgrade

See :doc:`/release-notes/2.6-upgrade` for full upgrade instructions.

Download
~~~~~~~~

To download MongoDB 2.6, go to the `downloads page`_.

.. TODO Should we add link to the installation guides?

.. _`downloads page`: http://www.mongodb.org/downloads

Other Resources
~~~~~~~~~~~~~~~

- `All JIRA issues resolved in 2.6 <https://jira.mongodb.org/secure/IssueNavigator.jspa?reset=true&jqlQuery=project+%3D+SERVER+AND+fixVersion+in+%28%222.5.0%22%2C+%222.5.1%22%2C+%222.5.2%22%2C+%222.5.3%22%2C+%222.5.4%22%2C+%222.5.5%22%2C+%222.6.0-rc1%22%2C+%222.6.0-rc2%22%2C+%222.6.0-rc3%22%29>`_.

- `All Third Party License Notices <https://github.com/mongodb/mongo/blob/v2.6/distsrc/THIRD-PARTY-NOTICES>`_.
.. _driver-write-concern-change:

============================
Default Write Concern Change
============================

.. default-domain:: mongodb

These release notes outline a change to all driver interfaces released
in November 2012. See release notes for specific drivers for
additional information.

.. _write-concern-change-notes:

Changes
-------

As of the releases listed below, there are two major changes to all
drivers:

#. All drivers will add a new top-level connection class that will
   increase consistency for all MongoDB client interfaces.

   This change is non-backward breaking: existing connection classes
   will remain in all drivers for a time, and will continue to operate
   as expected. However, those previous connection classes are now
   deprecated as of these releases, and will eventually be removed
   from the driver interfaces.

   The new top-level connection class is named ``MongoClient``, or
   similar depending on how host languages handle namespacing.

#. The default write concern on the new ``MongoClient`` class will be
   to acknowledge all write operations [#without-arguments]_. This
   will allow your application to receive acknowledgment of all write
   operations.

   See the documentation of :ref:`Write Concern <write-concern>` for
   more information about write concern in MongoDB.

   Please migrate to the new ``MongoClient`` class expeditiously.

.. [#without-arguments] The drivers will call
   :dbcommand:`getLastError` without arguments, which is logically
   equivalent to the ``w: 1`` option; however, this operation allows
   :term:`replica set` users to override the default write concern
   with the
   :data:`~local.system.replset.settings.getLastErrorDefaults`
   setting in the
   :doc:`/reference/replica-configuration`.

.. _write-concern-change-releases:

Releases
--------

The following driver releases will include the changes outlined in
:ref:`write-concern-change-notes`. See each driver's release notes for
a full account of each release as well as other related
driver-specific changes.

- C#, version 1.7
- Java, version 2.10.0
- Node.js, version 1.2
- Perl, version 0.501.1
- PHP, version 1.4
- Python, version 2.4
- Ruby, version 1.8
:orphan:

==============================================
Replica Set Features and Version Compatibility
==============================================

.. default-domain:: mongodb

.. note:: This table is for archival purposes and does not list all
   features of :term:`replica sets <replica set>`. Always use the latest stable
   release of MongoDB in production deployments.

.. list-table::
   :header-rows: 1
   :widths: 80 20

   * - **Features**
     - **Version**
   * - Slave Delay
     - 1.6.3
   * - Hidden
     - 1.7
   * - :dbcommand:`replSetFreeze` and :dbcommand:`replSetStepDown`
     - 1.7.3
   * - Replicated ops in :program:`mongostat`
     - 1.7.3
   * - Syncing from Secondaries
     - 1.8.0
   * - Authentication
     - 1.8.0
   * - Replication from Nearest Server (by ping Time)
     - 2.0
   * - :dbcommand:`replSetSyncFrom` support for replicating from specific
       members.
     - 2.2

Additionally:

- 1.8-series secondaries can replicate from 1.6-series primaries.

- 1.6-series secondaries cannot replicate from 1.8-series primaries.
======================
Security Release Notes
======================

.. default-domain:: mongodb

Access to ``system.users`` Collection
-------------------------------------

.. versionchanged:: 2.4

In 2.4, only users with the ``userAdmin`` role have access to the
``system.users`` collection.

In version 2.2 and earlier, the read-write users of a database all have
access to the ``system.users`` collection, which contains the user
names and user password hashes. [#read-and-write-system-users]_

.. [#read-and-write-system-users] Read-only users do not have access
   to the ``system.users`` collection.

.. _password-hashing-security:

Password Hashing Insecurity
---------------------------

If a user has the same password for multiple databases, the hash will
be the same. A malicious user could exploit this to gain access on a
second database using a different user's credentials.

As a result, always use unique username and password combinations
for each database.

.. example: NOT INCLUDED IN OUTPUT.

   Eve connects to the ``db1`` database and views the ``system.users``
   collection, with the following invocation of :program:`mongo`:

   .. code-block:: sh

      mongo <host>/db1 -u eve -p test

   Then, in the :program:`mongo` shell, issues the following
   operation:

   .. code-block:: javascript

      db.system.users.find()

   This operation returns the following documents:

   .. code-block:: javascript

      { "_id": ObjectId("5074202e032a960d16f4394e"), "user": "alice", "readOnly": false, "pwd": "ac2061b4a08ef8f2d60a07dc18ab4a0a" }
      { "_id": ObjectId("507420ba032a960d16f43951"), "user": "eve", "readOnly": false, "pwd": "5dcc2819b97e68d5cfe51da6cae8a7f6" }

   Alice has read and write accounts on both ``db1`` and ``db2`` and
   also has access on the ``admin`` database. Consider the following
   example where Alice authenticates to the ``admin`` db:

   .. code-block:: javascript

      use admin
      db.auth("alice", "pass")

   Consider the output of :method:`~db.collection.find()` on
   the ``system.users`` collection:

   .. code-block:: javascript

      db.system.users.find()

   The following output confirms that the user has the same password and hash:

   .. code-block:: javascript

      { "_id": ObjectId("50742045032a960d16f43950"), "user": "alice", "readOnly": false, "pwd": "ac2061b4a08ef8f2d60a07dc18ab4a0a" }

Thanks to Will Urbanski, from Dell SecureWorks, for identifying this issue.
.. start-include-here

Release Notes
-------------

Always install the latest, stable version of MongoDB. See
:ref:`release-version-numbers` for more information.

See the following release notes for an account of the changes in major
versions. Release notes also include instructions for upgrade.

Current Stable Release
~~~~~~~~~~~~~~~~~~~~~~

(*2.6-series*)

.. toctree::
   :maxdepth: 1

   /release-notes/2.6

Previous Stable Releases
~~~~~~~~~~~~~~~~~~~~~~~~

.. toctree::
   :maxdepth: 1

   /release-notes/2.4
   /release-notes/2.2
   /release-notes/2.0
   /release-notes/1.8
   /release-notes/1.6
   /release-notes/1.4
   /release-notes/1.2

Other MongoDB Release Notes
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. toctree::
   :maxdepth: 2

   /release-notes/drivers-write-concern

.. end-include-here

.. this page is primarily accessible via the /reference

.. _release-version-numbers:

MongoDB Version Numbers
~~~~~~~~~~~~~~~~~~~~~~~

For MongoDB ``2.4.1``, ``2.4`` refers to the release series and ``.1`` refers
to the revision. The second component of the release series (e.g. ``4``
in ``2.4.1``) describes the type of release series. Release series ending
with even numbers (e.g. ``4`` above) are *stable* and ready for
production, while odd numbers are for *development* and testing only.

Generally, changes in the release series (e.g. ``2.2`` to ``2.4``)
mark the introduction of new features that may break backwards
compatibility. Changes to the revision number mark the release bug
fixes and backwards-compatible changes.

.. important:: Always upgrade to the latest stable revision of your
   release series.

The version numbering system for MongoDB differs from the system
used for the MongoDB drivers. Drivers use only the first number to indicate
a major version. For details, see :ref:`drivers-version-numbers`.

.. example:: Version numbers

   - 2.0.0 : Stable release.

   - 2.0.1 : Revision.

   - 2.1.0 : Development release *for testing only*. Includes new features and changes for
     testing. Interfaces and stability may not be compatible in
     development releases.

   - 2.2.0 : Stable release. This is a culmination of the 2.1.x
     development series.
===========
Replication
===========

.. default-domain:: mongodb

A *replica set* in MongoDB is a group of :program:`mongod` processes
that maintain the same data set. Replica sets provide redundancy and
high availability, and are the basis for all production deployments.
This section introduces replication in MongoDB as well as the
components and architecture of replica sets. The section also provides
tutorials for common tasks related to replica sets.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`Replication
   and MongoDB <MongoDB-replication-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-replication-landing.rst

.. include:: /includes/toc/replication-landing.rst
========
Security
========

.. default-domain:: mongodb

This section outlines basic security and risk management strategies and
access control. The included tutorials outline specific tasks for
configuring firewalls, authentication, and system privileges.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`MongoDB
   Security <MongoDB-security-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-security-landing.rst

.. include:: /includes/toc/security-landing.rst
========
Sharding
========

.. default-domain:: mongodb

.. _sharding-background:

Sharding is the process of storing data records across multiple
machines and is MongoDB's approach to meeting the demands of data
growth. As the size of the data increases, a single machine may not be
sufficient to store the data nor provide an acceptable read and write
throughput. Sharding solves the problem with horizontal scaling. With
sharding, you add more machines to support data growth and the demands
of read and write operations.

.. only:: (website or singlehtml)

   You can download this section in PDF form as :hardlink:`Sharding
   and MongoDB <MongoDB-sharding-guide.pdf>`.

.. include:: /includes/toc/dfn-list-spec-sharding-landing.rst

.. include:: /includes/toc/sharding-landing.rst
.. _mongo-shell-help:

===========================================
Access the ``mongo`` Shell Help Information
===========================================

.. default-domain:: mongodb

In addition to the documentation in the :doc:`MongoDB Manual
</contents>`, the :program:`mongo` shell provides some additional
information in its "online" help system. This document provides an
overview of accessing this help information.

.. seealso::

   - :doc:`mongo Manual Page </reference/program/mongo>`
   - :doc:`/administration/scripting`, and
   - :doc:`/reference/mongo-shell`.

.. _mongo-shell-help-command-line:

Command Line Help
-----------------

To see the list of options and help for starting the :program:`mongo`
shell, use the :option:`--help <mongo --help>` option from the command line:

.. code-block:: sh

   mongo --help

Shell Help
----------

To see the list of help, in the :program:`mongo` shell, type ``help``:

.. code-block:: javascript

   help

.. _mongo-shell-help-db:

Database Help
-------------


- To see the list of databases on the server, use the ``show dbs``
  command:

  .. code-block:: javascript

     show dbs

  .. versionadded:: 2.4
     ``show databases`` is now an alias for ``show dbs``

- To see the list of help for methods you can use on the ``db``
  object, call the :method:`db.help()` method:

  .. code-block:: javascript

     db.help()

- To see the implementation of a method in the shell, type the
  ``db.<method name>`` without the parenthesis (``()``), as in the
  following example which will return the implementation of the method
  :method:`db.addUser()`:

  .. code-block:: javascript

     db.addUser

.. _mongo-shell-help-collection:

Collection Help
---------------

- To see the list of collections in the current database, use the
  ``show collections`` command:

  .. code-block:: javascript

     show collections

- To see the help for methods available on the collection objects
  (e.g. ``db.<collection>``), use the ``db.<collection>.help()``
  method:

  .. code-block:: javascript

     db.collection.help()

  ``<collection>`` can be the name of a collection that exists,
  although you may specify a collection that doesn't exist.

- To see the collection method implementation, type the
  ``db.<collection>.<method>`` name without the parenthesis (``()``),
  as in the following example which will return the implementation of
  the :method:`~db.collection.save()` method:

  .. code-block:: javascript

     db.collection.save

.. _mongo-shell-help-cursor:

Cursor Help
-----------

When you perform :ref:`read operations <read-operations-queries>` with
the :method:`~db.collection.find()` method in the
:program:`mongo` shell, you can use various cursor methods to modify
the :method:`~db.collection.find()` behavior and various
JavaScript methods to handle the cursor returned from the
:method:`~db.collection.find()` method.

- To list the available modifier and cursor handling methods, use the
  ``db.collection.find().help()`` command:

  .. code-block:: javascript

     db.collection.find().help()

  ``<collection>`` can be the name of a collection that exists,
  although you may specify a collection that doesn't exist.

- To see the implementation of the cursor method, type the
  ``db.<collection>.find().<method>`` name without the parenthesis
  (``()``), as in the following example which will return the
  implementation of the ``toArray()`` method:

  .. code-block:: javascript

     db.collection.find().toArray

Some useful methods for handling cursors are:

- :method:`~cursor.hasNext()` which checks whether the
  cursor has more documents to return.

- :method:`~cursor.next()` which returns the next document and
  advances the cursor position forward by one.

- :method:`forEach(\<function\>) <cursor.forEach()>` which iterates the
  whole cursor and applies the ``<function>`` to each document returned
  by the cursor. The ``<function>`` expects a single argument which
  corresponds to the document from each iteration.

For examples on iterating a cursor and retrieving the documents from
the cursor, see :doc:`cursor handling </core/cursors>`. See
also :ref:`js-query-cursor-methods` for all available cursor methods.

.. _mongo-shell-type-help:

Type Help
---------

To get a list of the wrapper classes available in the :program:`mongo`
shell, such as ``BinData()``, type ``help misc`` in the
:program:`mongo` shell:

.. code-block:: javascript

   help misc
======================================================
Create an Administrative User with Unrestricted Access
======================================================

.. default-domain:: mongodb

Overview
--------

Most users should have only the minimal set of privileges required for
their operations, in keeping with the policy of :term:`least privilege`.
However, some authorization architectures may
require a user with unrestricted access. To support these *super
users*, you can create users with access to all database :ref:`resources
<resource-document>` and :ref:`actions <security-user-actions>`.

.. link "creating a user" in the next sentence to the tutorial when
   it's published.

For many deployments, you may be able to avoid having *any* users with
unrestricted access by having an administrative user that with the
:authaction:`createUser` and :authaction:`grantRole` actions as needed
to support operations.

If users truly need unrestricted access to a MongoDB deployment,
MongoDB provides a :ref:`built-in role <built-in-roles>`
named :authrole:`root` that grants the combined privileges of all
built-in roles. This document describes how to create an administrative
user with the :authrole:`root` role.

For descriptions of the access each built-in role provides, see
the section on :ref:`built-in roles <built-in-roles>`.

.. _add-admin-prereq:

Prerequisites
-------------

.. include:: /includes/access-create-user.rst

Procedure
---------

.. include:: /includes/steps/add-admin-user.rst
=============================
Add an Arbiter to Replica Set
=============================

.. default-domain:: mongodb

Arbiters are :program:`mongod` instances that are part of
:term:`replica set` but do not hold data. Arbiters participate in
:ref:`elections <replica-set-elections>` in order to break ties.
If a replica set has an even number of members, add an arbiter.

Arbiters have minimal resource requirements and do not require
dedicated hardware. You can deploy an arbiter on an application
server, monitoring host.

.. important::

   Do not run an arbiter on the same system as a member of the replica
   set.

Considerations
--------------

An arbiter does not store data, but until the arbiter's :program:`mongod`
process is added to the replica set, the arbiter will act like any other
:program:`mongod` process and start up with a set of data files and with a
full-sized :term:`journal`.

To minimize the default creation of data, set the following in the
arbiter's :doc:`configuration file </reference/configuration-options>`:

- :setting:`journal.enabled <storage.journal.enabled>` to ``false``

  .. warning::

     Never set :setting:`journal.enabled <storage.journal.enabled>` to
     ``false`` on a data-bearing node.

- :setting:`~storage.smallFiles` to ``true``

- :setting:`~storage.preallocDataFiles` to ``false``

These settings are specific to arbiters. Do not set
:setting:`journal.enabled <storage.journal.enabled>` to ``false`` on a
data-bearing node. Similarly, do not set :setting:`~storage.smallFiles` or
:setting:`~storage.preallocDataFiles` unless specifically indicated.

Add an Arbiter
--------------

#. Create a data directory (e.g. :setting:`~storage.dbPath`) for the
   arbiter. The :program:`mongod` instance uses the directory for
   configuration data. The directory *will not* hold the data set. For
   example, create the ``/data/arb`` directory:

   .. code-block:: sh

      mkdir /data/arb

#. Start the arbiter. Specify the data directory and the replica set
   name. The following, starts an arbiter using the ``/data/arb``
   :setting:`~storage.dbPath` for the ``rs`` replica set:

   .. code-block:: sh

      mongod --port 30000 --dbpath /data/arb --replSet rs

#. Connect to the primary and add the arbiter to the replica set. Use
   the :method:`rs.addArb()` method, as in the following example:

   .. code-block:: javascript

      rs.addArb("m1.example.net:30000")

   This operation adds the arbiter running on port ``30000`` on the
   ``m1.example.net`` host.
.. _sharding-procedure-add-shard:

=======================
Add Shards to a Cluster
=======================

.. default-domain:: mongodb

You add shards to a :term:`sharded cluster` after you create the cluster
or anytime that you need to add capacity to the cluster. If you have not
created a sharded cluster, see :ref:`sharding-procedure-setup`.

When adding a shard to a cluster, you should always ensure that the
cluster has enough capacity to support the migration without affecting
legitimate production traffic.

In production environments, all shards should be :term:`replica sets
<replica set>`.

Add a Shard to a Cluster
------------------------

You interact with a sharded cluster by connecting to a :program:`mongos`
instance.

1. From a :program:`mongo` shell, connect to the :program:`mongos`
   instance. For example, if a :program:`mongos` is accessible at
   ``mongos0.example.net`` on port ``27017``, issue the following
   command:

   .. code-block:: sh

      mongo --host mongos0.example.net --port 27017

#. Add a shard to the cluster using the :method:`sh.addShard()`
   method, as shown in the examples below. Issue :method:`sh.addShard()`
   separately for each shard. If the shard is a replica set, specify the
   name of the replica set and specify a member of the set. In
   production deployments, all shards should be replica sets.

   .. optional:: You can instead use the :dbcommand:`addShard` database
      command, which lets you specify a name and maximum size for the
      shard. If you do not specify these, MongoDB automatically assigns
      a name and maximum size. To use the database command, see
      :dbcommand:`addShard`.

   The following are examples of adding a shard with
   :method:`sh.addShard()`:

   - To add a shard for a replica set named ``rs1`` with a member
     running on port ``27017`` on ``mongodb0.example.net``, issue the
     following command:

     .. code-block:: javascript

        sh.addShard( "rs1/mongodb0.example.net:27017" )

     .. versionchanged:: 2.0.3

     For MongoDB versions prior to 2.0.3, you must specify all members of the replica set. For
     example:

     .. code-block:: javascript

        sh.addShard( "rs1/mongodb0.example.net:27017,mongodb1.example.net:27017,mongodb2.example.net:27017" )

   - To add a shard for a standalone :program:`mongod` on port ``27017``
     of ``mongodb0.example.net``, issue the following command:

      .. code-block:: javascript

         sh.addShard( "mongodb0.example.net:27017" )

   .. note:: It might take some time for :term:`chunks <chunk>` to
      migrate to the new shard.
===========================
Create a User Administrator
===========================

.. default-domain:: mongodb

Overview
--------

User administrators create users and create and assigns roles. A
user administrator can grant any privilege in the database and can create
new ones. In a MongoDB deployment, create the user administrator as the
first user. Then let this user create all other users.

To provide user administrators, MongoDB has
:authrole:`userAdmin` and :authrole:`userAdminAnyDatabase` roles,
which grant access to :ref:`actions <security-user-actions>` that support user and
role management. Following the policy of :term:`least privilege`
:authrole:`userAdmin` and :authrole:`userAdminAnyDatabase`  confer no
additional privileges.

Carefully control access to these roles. A user with either of these roles can grant
*itself* unlimited additional privileges. Specifically, a user with the
:authrole:`userAdmin` role can grant itself any privilege in the database.
A user assigned either the :authrole:`userAdmin` role on the ``admin``
database or the :authrole:`userAdminAnyDatabase` can grant itself any
privilege *in the system*.

.. _add-user-admin-prereq:

Prerequisites
-------------

.. include:: /includes/access-create-user.rst

Procedure
---------

.. include:: /includes/steps/add-user-administrator.rst

Related Documents
-----------------

- :doc:`/core/authentication`

- :doc:`/core/security-introduction`

- :doc:`/tutorial/enable-authentication`

- :doc:`/administration/security-access-control`
========================
Add a User to a Database
========================

.. default-domain:: mongodb

.. versionchanged:: 2.6

Overview
--------

Each application and user of a MongoDB system should map to a
distinct application or administrator. This *access isolation*
facilitates access revocation and ongoing user maintenance. At the
same time users should have only the minimal set of privileges
required to ensure a system of :term:`least privilege`.

To create a user, you must define the user's credentials and assign that
user :ref:`roles <roles>`. Credentials verify the user's identity to a
database, and roles determine the user's access to database resources
and operations.

For an overview of credentials and roles in MongoDB see
:doc:`/core/security-introduction`.

Considerations
--------------

For users that authenticate using external mechanisms,
[#external-auth-mechanisms]_ you do not need to provide credentials
when creating users.

For all users, select the roles that have the exact required :ref:`privileges
<privileges>`. If the correct roles do not exist, :doc:`create roles
</tutorial/define-roles>`.

You can create a user without assigning roles, choosing instead to assign the
roles later. To do so, create the user with an empty
:data:`~admin.system.users.roles` array.

When adding a user to multiple databases, use unique username-and-password
combinations for each database, see :ref:`password-hashing-security`
for more information.

.. [#external-auth-mechanisms] :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`,
   :doc:`/tutorial/configure-ldap-sasl-authentication`,
   and x.509 certificates provide external authentication mechanisms.

.. _add-user-prereq:

Prerequisites
-------------

To create a user on a system that uses :ref:`authentication <authentication>`,
you must authenticate as a user administrator. If you have not yet created a
user administrator, do so as described in
:doc:`/tutorial/add-user-administrator`.

.. include:: /includes/access-create-user.rst

Procedures
----------

.. include:: /includes/steps/add-user-to-database.rst
======================================
Adjust Priority for Replica Set Member
======================================

.. default-domain:: mongodb

Overview
--------

The priority settings of replica set members affect the outcomes
of :doc:`elections </core/replica-set-elections>` for primary. Use this
setting to ensure that some members are more likely to become primary and
that others can never become primary.

The value of the member's
:data:`~local.system.replset.members[n].priority` setting determines the
member's priority in elections. The higher the number, the higher the
priority.

Considerations
--------------

To modify priorities, you update the :data:`~local.system.replset.members`
array in the replica configuration object. The array index begins with
``0``. Do **not** confuse this index value with the value of the replica
set member's :data:`~local.system.replset.members[n]._id` field in the
array.

The value of :data:`~local.system.replset.members[n].priority` can be any
floating point (i.e. decimal) number between ``0`` and ``1000``. The
default value for the :data:`~local.system.replset.members[n].priority`
field is ``1``.

To block a member from seeking election as primary, assign it a priority
of ``0``. :ref:`Hidden members <replica-set-hidden-members>`,
:ref:`delayed members <replica-set-delayed-members>`, and :ref:`arbiters
<replica-set-arbiters>` all have
:data:`~local.system.replset.members[n].priority` set to ``0``.

Adjust priority during a scheduled maintenance window. Reconfiguring
priority can force the current primary to step down, leading to an
election. Before an election the primary closes all open :term:`client`
connections.

Procedure
---------

.. include:: /includes/steps/adjust-replica-set-member-priority.rst
=================
Manage Shard Tags
=================

.. default-domain:: mongodb

In a sharded cluster, you can use tags to associate specific ranges of
a :term:`shard key` with a specific :term:`shard` or subset of shards.

Tag a Shard
-----------

Associate tags with a particular shard using the
:method:`sh.addShardTag()` method when connected to a :program:`mongos`
instance. A single shard may have multiple tags, and multiple shards
may also have the same tag.

.. example::

   The following example adds the tag ``NYC`` to two shards, and the tags
   ``SFO`` and ``NRT`` to a third shard:

   .. code-block:: javascript

      sh.addShardTag("shard0000", "NYC")
      sh.addShardTag("shard0001", "NYC")
      sh.addShardTag("shard0002", "SFO")
      sh.addShardTag("shard0002", "NRT")

You may remove tags from a particular shard using the
:method:`sh.removeShardTag()` method when connected to a
:program:`mongos` instance, as in the following example, which removes
the ``NRT`` tag from a shard:

.. code-block:: javascript

   sh.removeShardTag("shard0002", "NRT")

Tag a Shard Key Range
---------------------

To assign a tag to a range of shard keys use the
:method:`sh.addTagRange()` method when connected to a
:program:`mongos` instance. Any given shard key range may only have
*one* assigned tag. You cannot overlap defined ranges, or tag the same
range more than once.

.. example::

   Given a collection named ``users`` in the ``records`` database,
   sharded by the ``zipcode`` field. The following operations assign:

   - two ranges of zip codes in Manhattan and Brooklyn the ``NYC`` tag

   - one range of zip codes in San Francisco the ``SFO`` tag

   .. code-block:: javascript

      sh.addTagRange("records.users", { zipcode: "10001" }, { zipcode: "10281" }, "NYC")
      sh.addTagRange("records.users", { zipcode: "11201" }, { zipcode: "11240" }, "NYC")
      sh.addTagRange("records.users", { zipcode: "94102" }, { zipcode: "94135" }, "SFO")

.. note::

   Shard ranges are always inclusive of the lower value and exclusive
   of the upper boundary.

Remove a Tag From a Shard Key Range
-----------------------------------

The :program:`mongod` does not provide a helper for removing a tag
range. You may delete tag assignment from a shard key range by removing
the corresponding document from the :data:`~config.tags` collection of
the ``config`` database.

Each document in the :data:`~config.tags` holds the :term:`namespace`
of the sharded collection and a minimum shard key value.

.. example::

   The following example removes the ``NYC`` tag assignment for the
   range of zip codes within Manhattan:

   .. code-block:: javascript

      use config
      db.tags.remove({ _id: { ns: "records.users", min: { zipcode: "10001" }}, tag: "NYC" })

View Existing Shard Tags
------------------------

The output from :method:`sh.status()` lists tags associated with a
shard, if any, for each shard. A shard's tags exist in the shard's
document in the :data:`~config.shards` collection of the ``config``
database. To return all shards with a specific tag, use a sequence of
operations that resemble the following, which will return only those
shards tagged with ``NYC``:

.. code-block:: javascript

   use config
   db.shards.find({ tags: "NYC" })

You can find tag ranges for all :term:`namespaces <namespace>` in the
:data:`~config.tags` collection of the ``config`` database. The output
of :method:`sh.status()` displays all tag ranges. To return all shard
key ranges tagged with ``NYC``, use the following sequence of
operations:

.. code-block:: javascript

   use config
   db.tags.find({ tags: "NYC" })
=====================================
Aggregation with User Preference Data
=====================================

.. default-domain:: mongodb

Data Model
----------

Consider a hypothetical sports club with a database that contains a
``user`` collection that tracks the user's join dates, sport preferences,
and stores these data in documents that resemble the following:

.. code-block:: javascript

   {
     _id : "jane",
     joined : ISODate("2011-03-02"),
     likes : ["golf", "racquetball"]
   }
   {
     _id : "joe",
     joined : ISODate("2012-07-02"),
     likes : ["tennis", "golf", "swimming"]
   }

.. redacting following section, upon request of Mathias.

   Return a Single Field
   ---------------------

   The following operation uses :pipeline:`$project` to return only the
   ``_id`` field and to return it for all documents in the ``users``
   collection:

   .. code-block:: javascript

      db.users.aggregate(
        [
          { $project : { _id:1 } }
        ]
      )

   The operation returns results that resemble the following:

   .. code-block:: javascript

      {
        "_id" : "joe"
      },
      {
        "_id" : "jane"
      },
      {
        "_id" : "jill"
      }

   For basic query and projection operations, standard queries with the
   :method:`find() <db.collection.find()>` method are preferable. This
   operation appears as an hypothetical example only.

.. end-redaction

Normalize and Sort Documents
----------------------------

The following operation returns user names in upper case and in
alphabetical order. The aggregation includes user names for all documents in
the ``users`` collection. You might do this to normalize user names for
processing.

.. code-block:: javascript

   db.users.aggregate(
     [
       { $project : { name:{$toUpper:"$_id"} , _id:0 } },
       { $sort : { name : 1 } }
     ]
   )

All documents from the ``users`` collection pass through the
pipeline, which consists of the following operations:

- The :pipeline:`$project` operator:

  - creates a new field called ``name``.

  - converts the value of the ``_id`` to upper case, with the
    :expression:`$toUpper` operator. Then the
    :pipeline:`$project` creates a new field, named ``name``
    to hold this value.

  - suppresses the ``id`` field. :pipeline:`$project` will pass
    the ``_id`` field by default, unless explicitly suppressed.

- The :pipeline:`$sort` operator orders the results by the
  ``name`` field.

The results of the aggregation would resemble the following:

.. code-block:: javascript

   {
     "name" : "JANE"
   },
   {
     "name" : "JILL"
   },
   {
     "name" : "JOE"
   }

.. todo:: resolve the following commented block:

.. removed the following block until we can get it corrected. It would
   be ok to delete eventually.

   <bg> I think this example needs reworking. I don't think it returns
        the top 4 months that people tend to join the club, just the
        first four in the calendar year. For example, if people joined
        as follows: Jan 1 person, Feb 2, Mar 2, Apr 1, June 100, the
        query would still return Jan, Feb, Mar, Apr.

   <tycho> Agreed, it would need to group by month joined, and then
           sort those documents.

   Determine Most Common Join Month in Collection
   ----------------------------------------------

   .. code-block:: javascript

      db.users.aggregate(
        [
          { $project : { month_joined : { $month : "$joined" } } },
          { $sort : { month_joined : 1 } },
          { $limit : 4 }
        ]
      )

   The pipeline passes all documents in the ``users`` collection through
   the following operations:

   - The :pipeline:`$project` operator creates a new field called ``month_joined``.

   - The :expression:`$month` operator converts the ``joined`` field to
     integer representations of the month. Then the :pipeline:`$project` operator
     assigns the values to the ``month_joined`` field.

   - The :pipeline:`$sort` operator sorts the results by the ``month_joined`` field.

   - The :pipeline:`$limit` operator limits the results to the first 4 result documents.

   The operation returns results that resemble the following:

   .. code-block:: javascript

      {
        "_id" : "ruth",
        "month_joined" : 1
      },
      {
        "_id" : "harold",
        "month_joined" : 1
      },
      {
        "_id" : "kate",
        "month_joined" : 1
      },
      {
        "_id" : "jill",
        "month_joined" : 2
      }

.. end-comment

Return Usernames Ordered by Join Month
--------------------------------------

The following aggregation operation returns user names sorted by the
month they joined. This kind of aggregation could help generate
membership renewal notices.

.. code-block:: javascript

   db.users.aggregate(
     [
       { $project : { month_joined : {
                                       $month : "$joined"
                                     },
                      name : "$_id",
                      _id : 0
                    },
       { $sort : { month_joined : 1 } }
     ]
   )

The pipeline passes all documents in the ``users`` collection through
the following operations:

- The :pipeline:`$project` operator:

  - Creates two new fields: ``month_joined`` and ``name``.

  - Suppresses the ``id`` from the results. The :method:`aggregate()
    <db.collection.aggregate()>` method includes the ``_id``, unless
    explicitly suppressed.

- The :expression:`$month` operator converts the values of the
  ``joined`` field to integer representations of the month. Then the
  :pipeline:`$project` operator assigns those values to the
  ``month_joined`` field.

- The :pipeline:`$sort` operator sorts the results by the
  ``month_joined`` field.

The operation returns results that resemble the following:

.. code-block:: javascript

   {
     "month_joined" : 1,
     "name" : "ruth"
   },
   {
     "month_joined" : 1,
     "name" : "harold"
   },
   {
     "month_joined" : 1,
     "name" : "kate"
   }
   {
     "month_joined" : 2,
     "name" : "jill"
   }

Return Total Number of Joins per Month
--------------------------------------

The following operation shows how many people joined each month of the
year. You might use this aggregated data for recruiting and marketing
strategies.

.. code-block:: javascript

   db.users.aggregate(
     [
       { $project : { month_joined : { $month : "$joined" } } } ,
       { $group : { _id : {month_joined:"$month_joined"} , number : { $sum : 1 } } },
       { $sort : { "_id.month_joined" : 1 } }
     ]
   )

The pipeline passes all documents in the ``users`` collection through
the following operations:

- The :pipeline:`$project` operator creates a new field called
  ``month_joined``.

- The :expression:`$month` operator converts the values of the
  ``joined`` field to integer representations of the month. Then the
  :pipeline:`$project` operator assigns the values to the
  ``month_joined`` field.

- The :pipeline:`$group` operator collects all documents with a
  given ``month_joined`` value and counts how many documents there are
  for that value. Specifically, for each unique value,
  :pipeline:`$group` creates a new "per-month" document with two
  fields:

  - ``_id``, which contains a nested document with the
    ``month_joined`` field and its value.

  - ``number``, which is a generated field. The :group:`$sum`
    operator increments this field by 1 for every document containing
    the given ``month_joined`` value.

- The :pipeline:`$sort` operator sorts the documents created by
  :pipeline:`$group` according to the contents of the
  ``month_joined`` field.

The result of this aggregation operation would resemble the following:

.. code-block:: javascript

   {
     "_id" : {
       "month_joined" : 1
     },
     "number" : 3
   },
   {
     "_id" : {
       "month_joined" : 2
     },
     "number" : 9
   },
   {
     "_id" : {
       "month_joined" : 3
     },
     "number" : 5
   }

Return the Five Most Common "Likes"
-----------------------------------

The following aggregation collects top five most "liked" activities in
the data set. This type of analysis could help inform planning and
future development.

.. code-block:: javascript

   db.users.aggregate(
     [
       { $unwind : "$likes" },
       { $group : { _id : "$likes" , number : { $sum : 1 } } },
       { $sort : { number : -1 } },
       { $limit : 5 }
     ]
   )

The pipeline begins with all documents in the ``users`` collection,
and passes these documents through the following operations:

- The :pipeline:`$unwind` operator separates each value in the
  ``likes`` array, and creates a new version of the source document
  for every element in the array.

  .. example::

     Given the following document from the ``users`` collection:

     .. code-block:: javascript

        {
          _id : "jane",
          joined : ISODate("2011-03-02"),
          likes : ["golf", "racquetball"]
        }

     The :pipeline:`$unwind` operator would create the following
     documents:

     .. code-block:: javascript

        {
          _id : "jane",
          joined : ISODate("2011-03-02"),
          likes : "golf"
        }
        {
          _id : "jane",
          joined : ISODate("2011-03-02"),
          likes : "racquetball"
        }

- The :pipeline:`$group` operator collects all documents the same
  value for the ``likes`` field and counts each grouping. With this
  information, :pipeline:`$group` creates a new document with two
  fields:

  - ``_id``, which contains the ``likes`` value.

  - ``number``, which is a generated field. The :group:`$sum`
    operator increments this field by 1 for every document containing
    the given ``likes`` value.

- The :pipeline:`$sort` operator sorts these documents by the
  ``number`` field in reverse order.

- The :pipeline:`$limit` operator only includes the first 5 result
  documents.

The results of aggregation would resemble the following:

.. code-block:: javascript

   {
     "_id" : "golf",
     "number" : 33
   },
   {
     "_id" : "racquetball",
     "number" : 31
   },
   {
     "_id" : "swimming",
     "number" : 24
   },
   {
     "_id" : "handball",
     "number" : 19
   },
   {
     "_id" : "tennis",
     "number" : 18
   }
======================================
Aggregation with the Zip Code Data Set
======================================

.. default-domain:: mongodb

The examples in this document use the ``zipcode`` collection. This
collection is available at: `media.mongodb.org/zips.json
<http://media.mongodb.org/zips.json>`_. Use :program:`mongoimport` to
load this data set into your :program:`mongod` instance.

Data Model
----------

Each document in the ``zipcode`` collection has the following form:

.. code-block:: javascript

   {
     "_id": "10280",
     "city": "NEW YORK",
     "state": "NY",
     "pop": 5574,
     "loc": [
       -74.016323,
       40.710537
     ]
   }

The ``_id`` field holds the zip code as a string.

The ``city`` field holds the city.

The ``state`` field holds the two letter state abbreviation.

The ``pop`` field holds the population.

The ``loc`` field holds the location as a latitude longitude pair.

All of the following examples use the :method:`aggregate()
<db.collection.aggregate()>` helper in the :program:`mongo`
shell. :method:`aggregate() <db.collection.aggregate()>` provides a
wrapper around the :dbcommand:`aggregate` database command. See the
documentation for your :doc:`driver </applications/drivers>` for a
more idiomatic interface for data aggregation operations.

Return States with Populations above 10 Million
-----------------------------------------------

To return all states with a population greater than 10 million, use
the following aggregation operation:

.. code-block:: javascript

   db.zipcodes.aggregate( { $group :
                            { _id : "$state",
                              totalPop : { $sum : "$pop" } } },
                          { $match : {totalPop : { $gte : 10*1000*1000 } } } )

Aggregations operations using the :method:`aggregate()
<db.collection.aggregate()>` helper process all documents in the
``zipcodes`` collection. :method:`aggregate()
<db.collection.aggregate()>` connects a number of :ref:`pipeline
<aggregation-pipeline>` operators, which define the aggregation
process.

In this example, the pipeline passes all documents in the
``zipcodes`` collection through the following steps:

- the :pipeline:`$group` operator collects all documents and
  creates documents for each state.

  These new per-state documents have one field in addition to the
  ``_id`` field: ``totalPop`` which is a generated field using the
  :group:`$sum` operation to calculate the total value of all
  ``pop`` fields in the source documents.

  After the :pipeline:`$group` operation the documents in the
  pipeline resemble the following:

  .. code-block:: javascript

     {
       "_id" : "AK",
       "totalPop" : 550043
     }

- the :pipeline:`$match` operation filters these documents so that
  the only documents that remain are those where the value of
  ``totalPop`` is greater than or equal to 10 million.

  The :pipeline:`$match` operation does not alter the documents,
  which have the same format as the documents output by
  :pipeline:`$group`.

The equivalent :term:`SQL` for this operation is:

.. code-block:: sql

   SELECT state, SUM(pop) AS totalPop
          FROM zipcodes
          GROUP BY state
          HAVING totalPop >= (10*1000*1000)

Return Average City Population by State
---------------------------------------

To return the average populations for cities in each state, use the
following aggregation operation:

.. code-block:: javascript

   db.zipcodes.aggregate( { $group :
                            { _id : { state : "$state", city : "$city" },
                              pop : { $sum : "$pop" } } },
                          { $group :
                          { _id : "$_id.state",
                            avgCityPop : { $avg : "$pop" } } } )

Aggregations operations using the :method:`aggregate()
<db.collection.aggregate()>` helper process all documents in the
``zipcodes`` collection. :method:`aggregate()
<db.collection.aggregate()>`  connects a number of :ref:`pipeline
<aggregation-pipeline>` operators that define the aggregation
process.

In this example, the pipeline passes all documents in the
``zipcodes`` collection through the following steps:

- the :pipeline:`$group` operator collects all documents and
  creates new documents for every combination of the ``city`` and
  ``state`` fields in the source document.

  After this stage in the pipeline, the documents resemble the
  following:

  .. code-block:: javascript

     {
       "_id" : {
         "state" : "CO",
         "city" : "EDGEWATER"
       },
       "pop" : 13154
     }

- the second :pipeline:`$group` operator collects documents by the
  ``state`` field and use the :group:`$avg` expression to
  compute a value for the ``avgCityPop`` field.

The final output of this aggregation operation is:

.. code-block:: javascript

   {
     "_id" : "MN",
     "avgCityPop" : 5335
   },

.. commented until we get the SQL
   The equivalent :term:`SQL` for this operation is:

   .. code-block:: sql



Return Largest and Smallest Cities by State
-------------------------------------------

To return the smallest and largest cities by population for each
state, use the following aggregation operation:

.. code-block:: javascript

   db.zipcodes.aggregate( { $group:
                            { _id: { state: "$state", city: "$city" },
                              pop: { $sum: "$pop" } } },
                          { $sort: { pop: 1 } },
                          { $group:
                            { _id : "$_id.state",
                              biggestCity:  { $last: "$_id.city" },
                              biggestPop:   { $last: "$pop" },
                              smallestCity: { $first: "$_id.city" },
                              smallestPop:  { $first: "$pop" } } },

                          // the following $project is optional, and
                          // modifies the output format.

                          { $project:
                            { _id: 0,
                              state: "$_id",
                              biggestCity:  { name: "$biggestCity",  pop: "$biggestPop" },
                              smallestCity: { name: "$smallestCity", pop: "$smallestPop" } } } )

Aggregation operations using the :method:`aggregate()
<db.collection.aggregate()>` helper process all documents in the
``zipcodes`` collection. :method:`aggregate()
<db.collection.aggregate()>` combines a number of :ref:`pipeline
<aggregation-pipeline>` operators that define the aggregation
process.

All documents from the ``zipcodes`` collection pass into the pipeline,
which consists of the following steps:

- the :pipeline:`$group` operator collects all documents and
  creates new documents for every combination of the ``city`` and
  ``state`` fields in the source documents.

  By specifying the value of ``_id`` as a sub-document that contains
  both fields, the operation preserves the ``state`` field for use
  later in the pipeline. The documents produced by this stage of the
  pipeline have a second field, ``pop``, which uses the
  :group:`$sum` operator to provide the total of the ``pop``
  fields in the source document.

  At this stage in the pipeline, the documents resemble the following:

  .. code-block:: javascript

     {
       "_id" : {
         "state" : "CO",
         "city" : "EDGEWATER"
       },
       "pop" : 13154
     }

- :pipeline:`$sort` operator orders the documents in the pipeline
  based on the vale of the ``pop`` field from largest to
  smallest. This operation does not alter the documents.

- the second :pipeline:`$group` operator collects the documents in
  the pipeline by the ``state`` field, which is a field inside the
  nested ``_id`` document.

  Within each per-state document this :pipeline:`$group` operator
  specifies four fields: Using the :group:`$last` expression,
  the :pipeline:`$group` operator creates the ``biggestcity`` and
  ``biggestpop`` fields that store the city with the largest
  population and that population. Using the :group:`$first`
  expression, the :pipeline:`$group` operator creates the
  ``smallestcity`` and ``smallestpop`` fields that store the city with
  the smallest population and that population.

  The documents, at this stage in the pipeline resemble the following:

  .. code-block:: javascript

     {
       "_id" : "WA",
       "biggestCity" : "SEATTLE",
       "biggestPop" : 520096,
       "smallestCity" : "BENGE",
       "smallestPop" : 2
     }

- The final operation is :pipeline:`$project`, which renames the
  ``_id`` field to ``state`` and moves the ``biggestCity``,
  ``biggestPop``, ``smallestCity``, and ``smallestPop`` into
  ``biggestCity`` and ``smallestCity`` sub-documents.

The output of this aggregation operation is:

.. code-block:: javascript

   {
     "state" : "RI",
     "biggestCity" : {
       "name" : "CRANSTON",
       "pop" : 176404
     },
     "smallestCity" : {
       "name" : "CLAYVILLE",
       "pop" : 45
     }
   }

.. will uncomment this block when we get the SQL
   The equivalent :term:`SQL` for this operation is:

   .. code-block:: sql
=========================
Analyze Query Performance
=========================

.. default-domain:: mongodb

The :method:`~cursor.explain()` cursor method allows you to inspect the
operation of the query system. This method is useful for analyzing the
efficiency of queries, and for determining how the query uses the
index. The :method:`~cursor.explain()` method tests the query
operation, and *not* the timing of query performance. Because
:method:`~cursor.explain()` attempts multiple query plans, it does not
reflect an accurate timing of query performance.

Evaluate the Performance of a Query
-----------------------------------

To use the :method:`~cursor.explain()` method, call the method on a
cursor returned by :method:`~db.collection.find()`.

.. example:: Evaluate a query on the ``type`` field on the collection
   ``inventory`` that has an index on the ``type`` field.

   .. code-block:: javascript

      db.inventory.find( { type: 'food' } ).explain()

   Consider the results:

   .. code-block:: javascript

      {
        "cursor" : "BtreeCursor type_1",
        "isMultiKey" : false,
        "n" : 5,
        "nscannedObjects" : 5,
        "nscanned" : 5,
        "nscannedObjectsAllPlans" : 5,
        "nscannedAllPlans" : 5,
        "scanAndOrder" : false,
        "indexOnly" : false,
        "nYields" : 0,
        "nChunkSkips" : 0,
        "millis" : 0,
        "indexBounds" : { "type" : [
                                      [ "food",
                                        "food" ]
                                   ] },
        "server" : "mongodbo0.example.net:27017" }

   The ``BtreeCursor`` value of the :data:`~explain.cursor` field
   indicates that the query used an index.

   This query returned 5 documents, as indicated by the
   :data:`~explain.n` field.

   To return these 5 documents, the query scanned 5 documents from the
   index, as indicated by the :data:`~explain.nscanned` field, and then
   read 5 full documents from the collection, as indicated by the
   :data:`~explain.nscannedObjects` field.

   Without the index, the query would have scanned the whole collection
   to return the 5 documents.

   See :ref:`explain-results` method for full details on the output.

Compare Performance of Indexes
------------------------------

To manually compare the performance of a query using more than one
index, you can use the :method:`~cursor.hint()` and
:method:`~cursor.explain()` methods in conjunction.

.. example:: Evaluate a query using different indexes:

   .. code-block:: javascript

      db.inventory.find( { type: 'food' } ).hint( { type: 1 } ).explain()
      db.inventory.find( { type: 'food' } ).hint( { type: 1, name: 1 } ).explain()

   These return the statistics regarding the execution of the query
   using the respective index.

.. note::

   If you run :method:`explain() <cursor.explain()>` without including
   :method:`hint() <cursor.hint()>`, the query optimizer reevaluates
   the query and runs against multiple indexes before returning the
   query statistics.

For more detail on the explain output, see :ref:`explain-results`.
====================
Assign a User a Role
====================

.. default-domain:: mongodb

.. versionchanged:: 2.6

Overview
--------

A role provides a user privileges to perform a set of :ref:`actions
<security-user-actions>` on a :ref:`resource <resource-document>`. A
user can have multiple roles.

In MongoDB systems with :setting:`~security.authentication` enforced, you must grant a user a
role for the user to access a database resource. To assign a role, first
determine the privileges the user needs and then determine the role that
grants those privileges.

For an overview of roles and privileges, see :ref:`authorization`.
For descriptions of the access each built-in role provides, see
the section on :ref:`built-in roles <built-in-roles>`.

.. _assign-role-to-user-prereq:

Prerequisites
-------------

.. include:: /includes/access-grant-roles.rst

.. include:: /includes/access-roles-info.rst

Procedure
---------

.. include:: /includes/steps/assign-role-to-user.rst
=============================================
Authenticate to a MongoDB Instance or Cluster
=============================================

.. default-domain:: mongodb

Overview
--------

To authenticate to a running :program:`mongod` or :program:`mongos`
instance, you must have user credentials for a resource on that instance.
When you authenticate to MongoDB, you authenticate either to a database or
to a cluster. Your user privileges determine the resource you can
authenticate to.

You authenticate to a resource either by:

- using the authentication options when connecting to the
  :program:`mongod` or :program:`mongos` instance, or

- connecting first and then authenticating to the resource with the
  :dbcommand:`authenticate` command or the :method:`db.auth()` method.

This section describes both approaches.

In general, always use a trusted channel (VPN, SSL, trusted wired network)
for connecting to a MongoDB instance.

Prerequisites
-------------

You must have user credentials on the database or cluster to which you are
authenticating.

Procedures
----------

Authenticate When First Connecting to MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/steps/authenticate-as-client.rst

Authenticate After Connecting to MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/steps/authenticate-as-client-after-connection.rst
====================================
Create ``text`` Index with Long Name
====================================

.. default-domain:: mongodb

The default name for the index consists of each indexed field name
concatenated with ``_text``. For example, the following command creates
a ``text`` index on the fields ``content``, ``users.comments``, and
``users.profiles``:

.. code-block:: javascript

   db.collection.ensureIndex(
                              {
                                content: "text",
                                "users.comments": "text",
                                "users.profiles": "text"
                              }
                            )

The default name for the index is:

.. code-block:: javascript

   "content_text_users.comments_text_users.profiles_text"

To avoid creating an index with a name that exceeds the :limit:`index
name length limit <Index Name Length>`, you can pass the ``name``
option to the :method:`db.collection.ensureIndex()` method:

.. code-block:: javascript

   db.collection.ensureIndex(
                              {
                                content: "text",
                                "users.comments": "text",
                                "users.profiles": "text"
                              },
                              {
                                name: "MyTextIndex"
                              }
                            )

.. note::

   To drop the ``text`` index, use the index name. To get the name of
   an index, use :method:`db.collection.getIndexes()`.
=======================
Backup Cluster Metadata
=======================

.. default-domain:: mongodb

This procedure shuts down the :program:`mongod` instance of a
:ref:`config server <sharding-config-server>` in order to create a
backup of a :doc:`sharded cluster's </core/sharding-introduction>`
metadata. The cluster's config servers store all of the cluster's
metadata, most importantly the mapping from :term:`chunks <chunk>` to
:term:`shards <shard>`.

When you perform this procedure, the cluster remains operational
[#read-only]_.

#. Disable the cluster balancer process temporarily. See
   :ref:`sharding-balancing-disable-temporarily` for more information.

#. Shut down one of the config databases.

#. Create a full copy of the data files (i.e. the path specified by
   the :setting:`~storage.dbPath` option for the config instance.)

#. Restart the original configuration server.

#. Re-enable the balancer to allow the cluster to resume normal
   balancing operations. See the
   :ref:`sharding-balancing-disable-temporarily` section for more
   information on managing the balancer process.

.. seealso:: :doc:`/core/backups`.

.. [#read-only] While one of the three config servers is unavailable,
   the cluster cannot split any chunks nor can it migrate chunks
   between shards. Your application will be able to write data to the
   cluster. See :ref:`sharding-config-server` for more information.
============================================
Backup a Sharded Cluster with Database Dumps
============================================

.. default-domain:: mongodb

Overview
--------

This document describes a procedure for taking a backup of all
components of a sharded cluster. This procedure
uses :program:`mongodump` to create dumps of the :program:`mongod`
instance. An alternate procedure uses file system snapshots to capture
the backup data, and may be more efficient in some situations if your
system configuration allows file system backups. See
:doc:`/administration/backup-sharded-clusters` for more
information.

See :doc:`/core/backups` and
:doc:`/administration/backup-sharded-clusters` for complete
information on backups in MongoDB and backups of sharded clusters
in particular.

.. include:: /includes/note-shard-cluster-backup.rst

.. include:: /includes/access-mongodump-collections.rst

.. include:: /includes/access-mongodump-users.rst

Procedure
---------

In this procedure, you will stop the cluster balancer and take a backup
up of the :term:`config database`, and then take backups of each
shard in the cluster using :program:`mongodump` to capture the backup
data. If you need an exact moment-in-time snapshot of the system, you will
need to stop all application writes before taking the filesystem
snapshots; otherwise the snapshot will only approximate a moment of
time.

For approximate point-in-time snapshots, you can improve the quality
of the backup while minimizing impact on the cluster by taking the
backup from a secondary member of the replica set that provides each
shard.

1. Disable the :term:`balancer` process that equalizes the
   distribution of data among the :term:`shards <shard>`. To disable
   the balancer, use the :method:`sh.stopBalancer()` method in the
   :program:`mongo` shell. For example:

   .. code-block:: sh

      use config
      sh.setBalancerState(false)

   For more information, see the
   :ref:`sharding-balancing-disable-temporarily` procedure.

   .. warning::

      It is essential that you stop the balancer before creating
      backups. If the balancer remains active, your resulting backups
      could have duplicate data or miss some data, as :term:`chunks
      <chunk>` migrate while recording backups.

#. Lock one member of each replica set in each shard so that your
   backups reflect the state of your database at the nearest possible
   approximation of a single moment in time. Lock these
   :program:`mongod` instances in as short of an interval as possible.

   To lock or freeze a sharded cluster, you shut down one member of each
   replica set. Ensure that the :term:`oplog` has sufficient capacity to
   allow these secondaries to catch up to the state of the primaries after
   finishing the backup procedure. See :ref:`replica-set-oplog-sizing` for
   more information.

#. Use :program:`mongodump` to backup one of the :ref:`config servers
   <sharding-config-server>`. This backs up the cluster's
   metadata. You only need to back up one config server, as they all
   hold the same data.

   Use the :program:`mongodump` tool to capture the content of the
   config :program:`mongod` instances.

   Your config servers must run MongoDB 2.4 or later with the
   :option:`--configsvr <mongod --configsvr>` option and the
   :program:`mongodump` option must include the
   :option:`--oplog <mongodump --oplog>` to capture a consistent copy
   of the config database:

   .. code-block:: sh

      mongodump --oplog --db config

#. Back up the replica set members of the shards that shut down using
   :program:`mongodump` and specifying the :option:`--dbpath <mongodump --dbpath>`
   option. You may back up the shards in parallel. Consider the
   following invocation:

   .. code-block:: sh

      mongodump --journal --dbpath /data/db/ --out /data/backup/

   You must run this command on the system where the :program:`mongod`
   ran. This operation will use journaling and create a dump of the
   entire :program:`mongod` instance with data files stored in
   ``/data/db/``. :program:`mongodump` will write the output of this
   dump to the ``/data/backup/`` directory.

#. Restart all stopped replica set members of each shard as normal and
   allow them to catch up with the state of the primary.

#. Re-enable the balancer with the :method:`sh.setBalancerState()`
   method.

   Use the following command sequence when connected to the
   :program:`mongos` with the :program:`mongo` shell:

   .. code-block:: javascript

      use config
      sh.setBalancerState(true)
==================================================
Backup a Sharded Cluster with Filesystem Snapshots
==================================================

.. default-domain:: mongodb

Overview
--------

This document describes a procedure for taking a backup of all
components of a sharded cluster. This procedure uses file system
snapshots to capture a copy of the :program:`mongod` instance. An
alternate procedure uses :program:`mongodump` to create binary
database dumps when file-system snapshots are not available. See
:doc:`/tutorial/backup-sharded-cluster-with-database-dumps` for the
alternate procedure.

See :doc:`/core/backups` and
:doc:`/administration/backup-sharded-clusters` for complete
information on backups in MongoDB and backups of sharded clusters in
particular.

.. include:: /includes/note-shard-cluster-backup.rst

Procedure
---------

In this procedure, you will stop the cluster balancer and take a backup
up of the :term:`config database`, and then take backups of each
shard in the cluster using a file-system snapshot tool. If you need an
exact moment-in-time snapshot of the system, you will need to stop all
application writes before taking the filesystem snapshots; otherwise
the snapshot will only approximate a moment in time.

For approximate point-in-time snapshots, you can improve the quality
of the backup while minimizing impact on the cluster by taking the
backup from a secondary member of the replica set that provides each
shard.

1. Disable the :term:`balancer` process that equalizes the
   distribution of data among the :term:`shards <shard>`. To disable
   the balancer, use the :method:`sh.stopBalancer()` method in the
   :program:`mongo` shell. For example:

   .. code-block:: sh

      use config
      sh.setBalancerState(false)

   For more information, see the
   :ref:`sharding-balancing-disable-temporarily` procedure.

   .. warning::

      It is essential that you stop the balancer before creating
      backups. If the balancer remains active, your resulting backups
      could have duplicate data or miss some data, as :term:`chunks
      <chunk>` may migrate while recording backups.

#. Lock one secondary member of each replica set in each shard so that your
   backups reflect the state of your database at the nearest possible
   approximation of a single moment in time. Lock these
   :program:`mongod` instances in as short of an interval as possible.

   To lock a secondary, connect through the :program:`mongo` shell to the
   secondary member's :program:`mongod` instance and issue the
   :method:`db.fsyncLock()` method.

#. Back up one of the :ref:`config servers <sharding-config-server>`.
   Backing up a config server backs up the sharded cluster's metadata. You
   need back up only one config server, as they all hold the same data

   Do one of the following to back up one of the config servers:

   - Create a file-system snapshot of the config server. Use the procedure in
     :doc:`/tutorial/backup-with-filesystem-snapshots`.

     .. important:: This is only available if the config server has
        :term:`journaling <journal>` enabled. *Never*
        use  :method:`db.fsyncLock()` on config databases.

   - Use :program:`mongodump` to backup the config server. Issue
     :program:`mongodump` against one of the config :program:`mongod`
     instances or via the :program:`mongos`.

     If you are running MongoDB 2.4 or later with the
     :option:`--configsvr <mongod --configsvr>` option, then include the
     :option:`--oplog <mongod --oplog>` option when running
     :program:`mongodump` to ensure that the dump includes a partial oplog
     containing operations from the duration of the mongodump operation.
     For example:

     .. code-block:: sh

        mongodump --oplog --db config

#. Back up the replica set members of the shards that you locked. You
   may back up the shards in parallel. For each shard, create a
   snapshot. Use the procedure in
   :doc:`/tutorial/backup-with-filesystem-snapshots`.

#. Unlock all locked replica set members of each shard using the
   :method:`db.fsyncUnlock()` method in the :program:`mongo` shell.

#. Re-enable the balancer with the :method:`sh.setBalancerState()`
   method.

   Use the following command sequence when connected to the
   :program:`mongos` with the :program:`mongo` shell:

   .. code-block:: javascript

      use config
      sh.setBalancerState(true)
=================================================
Backup a Small Sharded Cluster with ``mongodump``
=================================================

.. default-domain:: mongodb

Overview
--------

If your :term:`sharded cluster` holds a small data set, you can
connect to a :program:`mongos` using :program:`mongodump`. You can
create backups of your MongoDB cluster, if your backup infrastructure
can capture the entire backup in a reasonable amount of time and if
you have a storage system that can hold the complete MongoDB data set.

See :doc:`/core/backups` and
:doc:`/administration/backup-sharded-clusters` for complete
information on backups in MongoDB and backups of sharded clusters in
particular.

.. important:: By default :program:`mongodump` issue its queries to
   the non-primary nodes.

.. include:: /includes/access-mongodump-collections.rst

.. include:: /includes/access-mongodump-users.rst

Considerations
--------------

If you use :program:`mongodump` without specifying a database
or collection, :program:`mongodump` will capture collection data
*and* the cluster meta-data from the :ref:`config servers
<sharding-config-server>`.

You cannot use the :option:`--oplog <mongodump --oplog>` option for
:program:`mongodump` when capturing data from
:program:`mongos`. As a result, if you need to capture a backup that
reflects a single moment in time, you must stop all writes to the
cluster for the duration of the backup operation.

Procedure
---------

Capture Data
~~~~~~~~~~~~

You can perform a backup of a :term:`sharded cluster` by connecting
:program:`mongodump` to a :program:`mongos`. Use the following
operation at your system's prompt:

.. code-block:: sh

   mongodump --host mongos3.example.net --port 27017

:program:`mongodump` will write :term:`BSON` files that hold a copy of
data stored in the :term:`sharded cluster` accessible via the
:program:`mongos` listening on port ``27017`` of the
``mongos3.example.net`` host.

Restore Data
~~~~~~~~~~~~

Backups created with :program:`mongodump` do not reflect the chunks or
the distribution of data in the sharded collection or
collections. Like all :program:`mongodump` output, these backups
contain separate directories for each database and :term:`BSON` files
for each collection in that database.

You can restore :program:`mongodump` output to any MongoDB instance,
including a standalone, a :term:`replica set`,  or a new
:term:`sharded cluster`. When restoring data to sharded cluster, you
must deploy and configure sharding before restoring data from the
backup. See :doc:`/tutorial/deploy-shard-cluster` for more information.
============================================
Backup and Restore with Filesystem Snapshots
============================================

.. default-domain:: mongodb

This document describes a procedure for creating backups of MongoDB
systems using system-level tools, such as :term:`LVM` or storage
appliance, as well as the corresponding restoration strategies.

These filesystem snapshots, or "block-level" backup methods use system
level tools to create copies of the device that holds MongoDB's data
files. These methods complete quickly and work reliably, but require
more system configuration outside of MongoDB.

.. seealso:: :doc:`/core/backups` and
   :doc:`/tutorial/backup-with-mongodump`.

.. _snapshots-overview:

Snapshots Overview
------------------

Snapshots work by creating pointers between the live data and a
special snapshot volume. These pointers are theoretically equivalent
to "hard links." As the working data diverges from the snapshot,
the snapshot process uses a copy-on-write strategy. As a result the snapshot
only stores modified data.

After making the snapshot, you mount the snapshot image on your
file system and copy data from the snapshot. The resulting backup
contains a full copy of all data.

Snapshots have the following limitations:

- The database must be valid when the
  snapshot takes place. This means that all writes accepted by the
  database need to be fully written to disk: either to the
  :term:`journal` or to data files.

  If all writes are not on disk when the backup occurs, the backup
  will not reflect these changes. If writes are *in progress* when the
  backup occurs, the data files will reflect an inconsistent
  state. With :term:`journaling <journal>` all data-file states
  resulting from in-progress writes are recoverable; without
  journaling you must flush all pending writes to disk before
  running the backup operation and must ensure that no writes occur during
  the entire backup procedure.

  If you do use journaling, the journal **must** reside on the same volume
  as the data.

- Snapshots create an image of an entire disk image. Unless you need
  to back up your entire system, consider isolating your MongoDB data
  files, journal (if applicable), and configuration on one logical
  disk that doesn't contain any other data.

  Alternately, store all MongoDB data files on a dedicated device
  so that you can make backups without duplicating extraneous data.

- Ensure that you copy data from snapshots and onto other systems to
  ensure that data is safe from site failures.

- Although different snapshots methods provide different capability, the
  LVM method outlined below does not provide any capacity for
  capturing incremental backups.

.. _backup-with-journaling:

Snapshots With Journaling
~~~~~~~~~~~~~~~~~~~~~~~~~

If your :program:`mongod` instance has journaling enabled, then you can
use any kind of file system or volume/block level snapshot tool to
create backups.

If you manage your own infrastructure on a Linux-based system, configure
your system with :term:`LVM` to provide your disk packages and provide
snapshot capability. You can also use LVM-based setups *within* a
cloud/virtualized environment.

.. note::

   Running :term:`LVM` provides additional flexibility and enables the
   possibility of using snapshots to back up MongoDB.

Snapshots with Amazon EBS in a RAID 10 Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If your deployment depends on Amazon's Elastic Block Storage (EBS) with
RAID configured within your instance, it is impossible to get a
consistent state across all disks using the platform's snapshot tool. As
an alternative, you can do one of the following:

- Flush all writes to disk and create a write lock to ensure
  consistent state during the backup process.

  If you choose this option see :ref:`backup-without-journaling`.

- Configure :term:`LVM` to run and hold your MongoDB data files on top of the
  RAID within your system.

  If you choose this option, perform the LVM backup operation described
  in :ref:`lvm-backup-operation`.

.. _lvm-backup-and-restore:

Backup and Restore Using LVM on a Linux System
----------------------------------------------

This section provides an overview of a simple backup process
using :term:`LVM` on a Linux system. While the tools, commands, and paths may
be (slightly) different on your system the following steps provide a
high level overview of the backup operation.

.. note::

   Only use the following procedure as a guideline for a backup system
   and infrastructure. Production backup systems must consider a
   number of application specific requirements and factors unique to
   specific environments.

.. _lvm-backup-operation:

Create a Snapshot
~~~~~~~~~~~~~~~~~

To create a snapshot with :term:`LVM`, issue a command as root in the
following format:

.. code-block:: sh

   lvcreate --size 100M --snapshot --name mdb-snap01 /dev/vg0/mongodb

This command creates an :term:`LVM` snapshot (with the ``--snapshot`` option)
named ``mdb-snap01`` of the ``mongodb`` volume in the ``vg0``
volume group.

This example creates a snapshot named ``mdb-snap01`` located at
``/dev/vg0/mdb-snap01``. The location and paths to your systems volume
groups and devices may vary slightly depending on your operating
system's :term:`LVM` configuration.

The snapshot has a cap of at 100 megabytes, because of the parameter
``--size 100M``. This size does not reflect the total amount of the
data on the disk, but rather the quantity of differences between the
current state of ``/dev/vg0/mongodb`` and the creation of the snapshot
(i.e. ``/dev/vg0/mdb-snap01``.)

.. warning::

   Ensure that you create snapshots with enough space to account for
   data growth, particularly for the period of time that it takes to copy
   data out of the system or to a temporary image.

   If your snapshot runs out of space, the snapshot image
   becomes unusable. Discard this logical volume and create another.

The snapshot will exist when the command returns. You can restore
directly from the snapshot at any time or by creating a new logical
volume and restoring from this snapshot to the alternate image.

While snapshots are great for creating high quality backups very
quickly, they are not ideal as a format for storing backup
data. Snapshots typically depend and reside on the same storage
infrastructure as the original disk images. Therefore, it's crucial
that you archive these snapshots and store them elsewhere.

Archive a Snapshot
~~~~~~~~~~~~~~~~~~

After creating a snapshot, mount the snapshot and move the data to
separate storage. Your system might try to compress the backup images as
you move the offline. The following procedure fully
archives the data from the snapshot:

.. code-block:: sh

   umount /dev/vg0/mdb-snap01
   dd if=/dev/vg0/mdb-snap01 | gzip > mdb-snap01.gz

The above command sequence does the following:

- Ensures that the ``/dev/vg0/mdb-snap01`` device is not mounted.

- Performs a block level copy of the entire snapshot image using the ``dd``
  command and compresses the result in a gzipped file in the
  current working directory.

  .. warning::

     This command will create a large ``gz`` file in your current
     working directory. Make sure that you run this command in a file
     system that has enough free space.

.. _backup-restore-snapshot:

Restore a Snapshot
~~~~~~~~~~~~~~~~~~

To restore a snapshot created with the above method, issue the following
sequence of commands:

.. code-block:: sh

   lvcreate --size 1G --name mdb-new vg0
   gzip -d -c mdb-snap01.gz | dd of=/dev/vg0/mdb-new
   mount /dev/vg0/mdb-new /srv/mongodb

The above sequence does the following:

- Creates a new logical volume named ``mdb-new``, in the ``/dev/vg0``
  volume group. The path to the new device will be ``/dev/vg0/mdb-new``.

  .. warning::

     This volume will have a maximum size of 1 gigabyte. The original
     file system must have had a total size of 1 gigabyte or smaller, or
     else the restoration will fail.

     Change ``1G`` to your desired volume size.

- Uncompresses and unarchives the ``mdb-snap01.gz`` into the
  ``mdb-new`` disk image.

- Mounts the ``mdb-new`` disk image to the ``/srv/mongodb`` directory.
  Modify the mount point to correspond to your MongoDB data file
  location, or other location as needed.

.. note::

   The restored snapshot will have a stale ``mongod.lock`` file. If
   you do not remove this file from the snapshot, and MongoDB may
   assume that the stale lock file indicates an unclean shutdown. If
   you're running with :setting:`storage.journal.enabled` enabled, and you *do not*
   use :method:`db.fsyncLock()`, you do not need to remove the
   ``mongod.lock`` file. If you use :method:`db.fsyncLock()` you will
   need to remove the lock.

.. _backup-restore-from-snapshot:

Restore Directly from a Snapshot
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To restore a backup without writing to a compressed ``gz`` file, use
the following sequence of commands:

.. code-block:: sh

   umount /dev/vg0/mdb-snap01
   lvcreate --size 1G --name mdb-new vg0
   dd if=/dev/vg0/mdb-snap01 of=/dev/vg0/mdb-new
   mount /dev/vg0/mdb-new /srv/mongodb

Remote Backup Storage
~~~~~~~~~~~~~~~~~~~~~

You can implement off-system backups using the :ref:`combined process
<backup-restore-from-snapshot>` and SSH.

This sequence is identical to procedures explained above, except that it
archives and compresses the backup on a remote system using SSH.

Consider the following procedure:

.. code-block:: sh

   umount /dev/vg0/mdb-snap01
   dd if=/dev/vg0/mdb-snap01 | ssh username@example.com gzip > /opt/backup/mdb-snap01.gz
   lvcreate --size 1G --name mdb-new vg0
   ssh username@example.com gzip -d -c /opt/backup/mdb-snap01.gz | dd of=/dev/vg0/mdb-new
   mount /dev/vg0/mdb-new /srv/mongodb

.. _backup-without-journaling:

Create Backups on Instances that do not have Journaling Enabled
---------------------------------------------------------------

If your :program:`mongod` instance does not run with journaling
enabled, or if your journal is on a separate volume, obtaining a
functional backup of a consistent state is more complicated.
As described in this section, you must flush all
writes to disk and lock the database to prevent writes during the
backup process. If you have a :term:`replica set` configuration,
then for your backup use a
:term:`secondary` which is not receiving reads (i.e. :term:`hidden
member`).

1. To flush writes to disk and to "lock" the database (to prevent
   further writes), issue the :method:`db.fsyncLock()` method in the
   :program:`mongo` shell:

   .. code-block:: javascript

      db.fsyncLock();

#. Perform the backup operation described in :ref:`lvm-backup-operation`.

#. To unlock the database after the snapshot has completed, use the
   following command in the :program:`mongo` shell:

   .. code-block:: javascript

      db.fsyncUnlock();

   .. note::

      .. versionchanged:: 2.0
         MongoDB 2.0 added :method:`db.fsyncLock()` and
         :method:`db.fsyncUnlock()` helpers to the :program:`mongo`
         shell.  Prior to this version, use the :dbcommand:`fsync`
         command with the ``lock`` option, as follows:

      .. code-block:: javascript

         db.runCommand( { fsync: 1, lock: true } );
         db.runCommand( { fsync: 1, lock: false } );

   .. include:: /includes/note-disable-profiling-fsynclock.rst

   .. include:: /includes/warning-fsync-lock-mongodump.rst
======================================
Back Up and Restore with MongoDB Tools
======================================

.. default-domain:: mongodb

This document describes the process for writing and restoring backups
to files in binary format  with the :program:`mongodump`  and
:program:`mongorestore` tools.

Use these tools for backups if other backup methods, such as the `MMS Backup Service
<https://mms.mongodb.com/?pk_campaign=mongodb-docs-tools>`_ or
:doc:`file system snapshots </tutorial/backup-with-filesystem-snapshots>`
are unavailable.

.. seealso:: :doc:`/core/backups`,
   :doc:`/reference/program/mongodump`, and
   :doc:`/reference/program/mongorestore`.

.. _backup-mongodump:

Backup a Database with ``mongodump``
------------------------------------

.. include:: /includes/fact-mongodump-local-database.rst

.. include:: /includes/access-mongodump-collections.rst

.. include:: /includes/access-mongodump-users.rst

Basic ``mongodump`` Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :program:`mongodump` utility can back up data by either:

- connecting to a running :program:`mongod` or
  :program:`mongos` instance, or

- accessing data files without an active instance.

The utility can create a backup for an entire server, database or collection,
or can use a query to backup just part of a collection.

When you run :program:`mongodump` without any arguments, the command
connects to the MongoDB instance on the local system
(e.g. ``127.0.0.1`` or ``localhost``) on port ``27017`` and creates a
database backup named ``dump/`` in the current directory.

To backup data from a :program:`mongod` or :program:`mongos` instance
running on the same machine and on the default port of ``27017``
use the following command:

.. code-block:: sh

   mongodump

.. include:: /includes/warning-mongodump-compatibility-2.2.rst

To limit the amount of data included in the database dump, you can
specify :option:`--db <mongodump --db>` and
:option:`--collection <mongodump --collection>` as options to the
:program:`mongodump` command. For example:

.. code-block:: sh

   mongodump --dbpath /data/db/ --out /data/backup/

.. code-block:: sh

   mongodump --host mongodb.example.net --port 27017

:program:`mongodump` will write :term:`BSON` files that hold a copy of
data accessible via the :program:`mongod` listening on port ``27017`` of
the ``mongodb.example.net`` host.

.. code-block:: sh

   mongodump --collection collection --db test

This command creates a dump of the collection named ``collection``
from the database ``test`` in a :file:`dump/` subdirectory of the current
working directory.

Point in Time Operation Using Oplogs
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the :option:`--oplog <mongodump --oplog>` option with
:program:`mongodump` to collect the :term:`oplog` entries to build a
point-in-time snapshot of a database within a replica set. With :option:`--oplog
<mongodump --oplog>`, :program:`mongodump` copies all the data from
the source database as well as all of the :term:`oplog` entries from
the beginning of the backup procedure to until the backup procedure
completes. This backup procedure, in conjunction with
:option:`mongorestore --oplogReplay <mongorestore --oplogReplay>`,
allows you to restore a backup that reflects the specific
moment in time that corresponds to when :program:`mongodump` completed
creating the dump file.

Create Backups Without a Running ``mongod`` Instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If your MongoDB instance is not running, you can use the
:option:`--dbpath <mongodump --dbpath>` option to specify the
location to your MongoDB instance's database files. :program:`mongodump`
reads from the data files directly with this operation. This
locks the data directory to prevent conflicting writes. The
:program:`mongod` process must *not* be running or attached to these
data files when you run :program:`mongodump` in this
configuration. Consider the following example:

.. example:: Backup a MongoDB Instance Without a Running ``mongod``

   Given a MongoDB instance that contains the ``customers``,
   ``products``, and ``suppliers`` databases, the following
   :program:`mongodump` operation backs up the databases using the
   :option:`--dbpath <mongodump --dbpath>` option, which specifies the
   location of the database files on the host:

   .. code-block:: javascript

      mongodump --dbpath /data -o dataout

   The :option:`--out <mongodump --out>` option allows you to specify the directory where
   :program:`mongodump` will save the backup. :program:`mongodump` creates
   a separate backup directory for each of the backed up databases:
   :file:`dataout/customers`, :file:`dataout/products`, and
   :file:`dataout/suppliers`.

Create Backups from Non-Local ``mongod`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :option:`--host <mongodump --host>` and
:option:`--port <mongodump --port>` options for
:program:`mongodump` allow you to connect to and backup from a remote host.
Consider the following example:

.. code-block:: sh

   mongodump --host mongodb1.example.net --port 3017 --username user --password pass --out /opt/backup/mongodump-2013-10-24

On any :program:`mongodump` command you may, as above, specify username
and password credentials to specify database authentication.

.. _backup-restore-dump:

Restore a Database with ``mongorestore``
----------------------------------------

.. include:: /includes/access-mongorestore.rst

Basic ``mongorestore`` Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The :program:`mongorestore` utility restores a binary backup created by
:program:`mongodump`. By default, :program:`mongorestore` looks for a
database backup in the :file:`dump/` directory.

The :program:`mongorestore` utility can restore data either by:

- connecting to a running :program:`mongod` or
  :program:`mongos` directly, or

- writing to a set of MongoDB data files without use of a running
  :program:`mongod`.

:program:`mongorestore` can restore either an entire database backup
or a subset of the backup.

To use :program:`mongorestore` to connect to an active
:program:`mongod` or :program:`mongos`, use a command with the following prototype form:

.. code-block:: sh

   mongorestore --port <port number> <path to the backup>

To use :program:`mongorestore` to write to data files
without using a running :program:`mongod`, use a command with the following prototype
form:

.. code-block:: sh

   mongorestore --dbpath <database path> <path to the backup>

Consider the following example:

.. code-block:: sh

   mongorestore dump-2013-10-25/

Here, :program:`mongorestore` imports the database backup in
the :file:`dump-2013-10-25` directory to the :program:`mongod` instance
running on the localhost interface.

.. _backup-restore-oplogreplay:

Restore Point in Time Oplog Backup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you created your database dump using the :option:`--oplog
<mongodump --oplog>` option to ensure a point-in-time snapshot, call
:program:`mongorestore` with the
:option:`--oplogReplay <mongorestore --oplogReplay>`
option, as in the following example:

.. code-block:: sh

   mongorestore --oplogReplay

You may also consider using the :option:`mongorestore --objcheck`
option to check the integrity of objects while inserting them into the
database, or you may consider the :option:`mongorestore --drop` option to drop each
collection from the database before restoring from
backups.

.. _backup-restore-filter:

Restore a Subset of data from a Binary Database Dump
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:program:`mongorestore` also includes the ability to a filter
to all input before inserting it into the new database. Consider the
following example:

.. code-block:: sh

   mongorestore --filter '{"field": 1}'

Here, :program:`mongorestore` only adds documents to the database from
the dump located in the :file:`dump/` folder *if* the documents have a
field name ``field`` that holds a value of ``1``. Enclose the
filter in single quotes (e.g. ``'``) to prevent the filter from
interacting with your shell environment.

.. _backup-restore-dbpath:

Restore Without a Running ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:program:`mongorestore` can write data to MongoDB data files without
needing to connect to a :program:`mongod` directly.

.. example:: Restore a Database Without a Running ``mongod``

   Given a set of backed up databases in the ``/data/backup/`` directory:

   - :file:`/data/backup/customers`,
   - :file:`/data/backup/products`, and
   - :file:`/data/backup/suppliers`

   The following :program:`mongorestore` command restores the
   ``products`` database. The command uses the :option:`--dbpath
   <mongorestore --dbpath>` option to specify the path to the MongoDB
   data files:

   .. code-block:: javascript

      mongorestore --dbpath /data/db --journal /data/backup/products

   The :program:`mongorestore` imports the database backup in the
   :file:`/data/backup/products` directory to the :program:`mongod` instance
   that runs on the localhost interface. The :program:`mongorestore`
   operation imports the backup even if the :program:`mongod` is not
   running.

   The :option:`--journal <mongorestore --journal>` option ensures that
   :program:`mongorestore` records all operation in the durability
   :term:`journal`. The journal prevents data file corruption if
   anything (e.g. power failure, disk failure, etc.) interrupts the
   restore operation.

.. seealso:: :doc:`/reference/program/mongodump` and
   :doc:`/reference/program/mongorestore`.

Restore Backups to Non-Local ``mongod`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, :program:`mongorestore` connects to a MongoDB instance
running on the localhost interface (e.g. ``127.0.0.1``) and on the
default port (``27017``). If you want to restore to a different host or
port, use the :option:`--host <mongorestore --host>` and :option:`--port
<mongorestore --port>` options.

Consider the following example:

.. code-block:: sh

   mongorestore --host mongodb1.example.net --port 3017 --username user --password pass /opt/backup/mongodump-2013-10-24

As above, you may specify username and password connections if your
:program:`mongod` requires authentication.
.. _geospatial-create-2d-index:

=====================
Create a ``2d`` Index
=====================

.. default-domain:: mongodb

To build a geospatial ``2d`` index, use the :method:`ensureIndex()
<db.collection.ensureIndex()>` method and specify ``2d``. Use the
following syntax:

.. code-block:: javascript

   db.<collection>.ensureIndex( { <location field> : "2d" ,
                                  <additional field> : <value> } ,
                                { <index-specification options> } )

The ``2d`` index uses the following optional index-specification
options:

.. code-block:: javascript

   { min : <lower bound> , max : <upper bound> ,
     bits : <bit precision> }



.. _geospatial-indexes-range:

Define Location Range for a ``2d`` Index
----------------------------------------

By default, a ``2d`` index assumes longitude and latitude and has boundaries
of -180 inclusive and 180 non-inclusive (i.e. ``[ -180 , 180 )``). If
documents contain coordinate data outside of the specified range,
MongoDB returns an error.

.. important:: The default boundaries allow applications to insert
   documents with invalid latitudes greater than 90 or less than -90.
   The behavior of geospatial queries with such invalid points is not
   defined.

On ``2d`` indexes you can change the location range.

You can build a ``2d`` geospatial index with a location range other than
the default. Use the ``min`` and ``max`` options when creating the
index. Use the following syntax:

.. code-block:: javascript

   db.collection.ensureIndex( { <location field> : "2d" } ,
                              { min : <lower bound> , max : <upper bound> } )

.. _geospatial-indexes-precision:

Define Location Precision for a ``2d`` Index
--------------------------------------------

By default, a ``2d`` index on legacy coordinate pairs uses 26 bits of
precision, which is roughly equivalent to 2 feet or 60 centimeters of
precision using the default range of -180 to 180. Precision is measured
by the size in bits of the :term:`geohash` values used to store location
data. You can configure geospatial indexes with up to 32 bits of
precision.

Index precision does not affect query accuracy. The actual grid coordinates
are always used in the final query processing. Advantages to lower
precision are a lower processing overhead for insert operations and use
of less space. An advantage to higher precision is that queries scan
smaller portions of the index to return results.

To configure a location precision other than the default, use the
``bits`` option when creating the index. Use following syntax:

.. code-block:: javascript

   db.<collection>.ensureIndex( {<location field> : "<index type>"} ,
                                { bits : <bit precision> } )

For information on the internals of geohash values, see
:ref:`geospatial-indexes-geohash`.
.. _geospatial-indexes-create-2dsphere:

===========================
Create a ``2dsphere`` Index
===========================

.. default-domain:: mongodb

To create a geospatial index for GeoJSON-formatted data, use the
:method:`~db.collection.ensureIndex()` method and set the
value of the location field for your collection to ``2dsphere``. A
``2dsphere`` index can be a :ref:`compound index <index-type-compound>`
and does not require the location field to be the first field indexed.

To create the index use the following syntax:

.. code-block:: javascript

   db.points.ensureIndex( { <location field> : "2dsphere" } )

The following are four example commands for creating a ``2dsphere`` index:

.. code-block:: javascript

   db.points.ensureIndex( { loc : "2dsphere" } )
   db.points.ensureIndex( { loc : "2dsphere" , type : 1 } )
   db.points.ensureIndex( { rating : 1 , loc : "2dsphere" } )
   db.points.ensureIndex( { loc : "2dsphere" , rating : 1 , category : -1 } )

The first example creates a simple geospatial index on the location
field ``loc``. The second example creates a compound index where the
second field contains non-location data. The third example creates an
index where the location field is not the primary field: the location
field does not have to be the first field in a ``2dsphere`` index. The
fourth example creates a compound index with three fields. You can
include as many fields as you like in a ``2dsphere`` index.
.. _geospatial-indexes-haystack-index:

=======================
Create a Haystack Index
=======================

.. default-domain:: mongodb

To build a haystack index, use the ``bucketSize`` option when creating
the index. A ``bucketSize`` of ``5`` creates an index that groups
location values that are within 5 units of the specified longitude and
latitude. The ``bucketSize`` also determines the granularity of the
index. You can tune the parameter to the distribution of your data so
that in general you search only very small regions. The areas defined by
buckets can overlap. A document can exist in multiple buckets.

A haystack index can reference two fields: the location field and a
second field. The second field is used for exact matches. Haystack
indexes return documents based on location and an exact match on a
single additional criterion. These indexes are not necessarily suited
to returning the closest documents to a particular location.

To build a haystack index, use the following syntax:

.. code-block:: javascript

   db.coll.ensureIndex( { <location field> : "geoHaystack" ,
                          <additional field> : 1 } ,
                        { bucketSize : <bucket value> } )

.. example::

   If you have a collection with documents that contain fields similar to
   the following:

   .. code-block:: javascript

      { _id : 100, pos: { lng : 126.9, lat : 35.2 } , type : "restaurant"}
      { _id : 200, pos: { lng : 127.5, lat : 36.1 } , type : "restaurant"}
      { _id : 300, pos: { lng : 128.0, lat : 36.7 } , type : "national park"}

   The following operations create a haystack index with buckets that
   store keys within 1 unit of longitude or latitude.

   .. code-block:: javascript

      db.places.ensureIndex( { pos : "geoHaystack", type : 1 } ,
                             { bucketSize : 1 } )

   This index stores the document with an ``_id`` field that has the
   value ``200`` in two different buckets:

   - In a bucket that includes the document where the ``_id`` field has
     a value of ``100``

   - In a bucket that includes the document where the ``_id`` field has
     a value of ``300``

To query using a haystack index you use the :dbcommand:`geoSearch`
command. See :ref:`geospatial-indexes-haystack-queries`.

By default, queries that use a haystack index return 50 documents.
.. index:: index; create in background
.. _index-create-in-background:

===============================
Build Indexes in the Background
===============================

.. default-domain:: mongodb

By default, MongoDB builds indexes in the foreground, which prevents all
read and write operations to the database while the index
builds. Also, no operation that requires a read or write lock on all
databases (e.g. :command:`listDatabases`) can occur during a
foreground index build.

:ref:`Background index construction <index-creation-background>` allows
read and write operations to continue while building the index.


.. seealso:: :doc:`/core/indexes` and :doc:`/administration/indexes`
   for more information.

Considerations
--------------

Background index builds take longer to complete and result
in an index that is *initially* larger, or less compact, than an index
built in the foreground. Over time, the compactness of indexes built in
the background will approach foreground-built indexes.

After MongoDB finishes building the index, background-built indexes
are functionally identical to any other index.

Procedure
---------

To create an index in the background, add the ``background`` argument
to the :method:`~db.collection.ensureIndex()` operation, as in the
following index:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1 }, { background: true } )

Consider the section on :ref:`background index construction
<index-creation-background>` for more information about these indexes
and their implications.
.. index:: index; replica set
.. index:: replica set; index
.. _index-build-on-replica-sets:
.. _index-building-replica-sets:

=============================
Build Indexes on Replica Sets
=============================

.. default-domain:: mongodb

:ref:`Background index creation operations
<index-creation-background>` become *foreground* indexing operations
on :term:`secondary` members of replica sets. The foreground index
building process blocks all replication and read operations on the
secondaries while they build the index.

Secondaries will begin building indexes *after* the
:term:`primary` finishes building the index. In :term:`sharded clusters
<sharded cluster>`, the :program:`mongos` will send :method:`ensureIndex()
<db.collection.ensureIndex()>` to the primary members of the replica
set for each shard, which then replicate to the secondaries after the
primary finishes building the index.

To minimize the impact of building an index on your replica set, use
the following procedure to build indexes on secondaries:

.. see:: :doc:`/administration/indexes` and :doc:`/core/indexes` for
   more information.

Considerations
--------------

- Ensure that your :term:`oplog` is large enough to permit the
  indexing or re-indexing operation to complete without falling
  too far behind to catch up. See the :ref:`oplog sizing
  <replica-set-oplog-sizing>` documentation for additional
  information.

- This procedure *does* take one member out of the replica set at a
  time. However, this procedure will only affect one member of the
  set at a time rather than *all* secondaries at the same time.

- Do **not** use this procedure when building a :ref:`unique index
  <index-type-unique>` with the ``dropDups`` option.

Procedure
---------

.. note::

   If you need to build an index in a :term:`sharded cluster`, repeat
   the following procedure for each replica set that provides each
   :term:`shard`.

.. _tutorial-index-on-replica-sets-stop-one-member:

Stop One Secondary
~~~~~~~~~~~~~~~~~~

Stop the :program:`mongod` process on one secondary. Restart the
:program:`mongod` process *without* the :option:`--replSet <mongod --replSet>`
option and running on a different port. [#different-port]_ This
instance is now in "standalone" mode.

For example, if your :program:`mongod` *normally* runs with on the
default port of ``27017`` with the :option:`--replSet
<mongod --replSet>` option you would use the following invocation:

.. code-block:: sh

   mongod --port 47017

.. [#different-port] By running the :program:`mongod` on a different
   port, you ensure that the other members of the replica set and all
   clients will not contact the member while you are building the
   index.

.. _tutorial-index-on-replica-sets-build-index:

Build the Index
~~~~~~~~~~~~~~~

Create the new index using the :method:`~db.collection.ensureIndex()`
in the :program:`mongo` shell, or comparable method in your
driver. This operation will create or rebuild the index on this
:program:`mongod` instance

For example, to create an ascending index on the ``username`` field of
the ``records`` collection, use the following :program:`mongo` shell
operation:

.. code-block:: sh

   db.records.ensureIndex( { username: 1 } )

.. seealso:: :doc:`/tutorial/create-an-index` and
   :doc:`/tutorial/create-a-compound-index` for more information.

.. _tutorial-index-on-replica-sets-restart-mongod:

Restart the Program ``mongod``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When the index build completes, start the :program:`mongod` instance
with the :option:`--replSet <mongod --replSet>` option on its usual port:

.. code-block:: sh

   mongod --port 27017 --replSet rs0

Modify the port number (e.g. ``27017``) or the replica set name
(e.g. ``rs0``) as needed.

Allow replication to catch up on this member.

Build Indexes on all Secondaries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For each secondary in the set, build an index according to the
following steps:

#. :ref:`tutorial-index-on-replica-sets-stop-one-member`
#. :ref:`tutorial-index-on-replica-sets-build-index`
#. :ref:`tutorial-index-on-replica-sets-restart-mongod`

Build the Index on the Primary
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To build an index on the primary you can either:

#. :doc:`Build the index in the background
   </tutorial/build-indexes-in-the-background>` on the primary.

#. Step down the primary using the :method:`rs.stepDown()` method in
   the :program:`mongo` shell to cause the current primary to become a
   secondary graceful and allow the set to elect another member as
   primary.

   Then repeat the index building procedure, listed below, to build the
   index on the primary:

   #. :ref:`tutorial-index-on-replica-sets-stop-one-member`

   #. :ref:`tutorial-index-on-replica-sets-build-index`

   #. :ref:`tutorial-index-on-replica-sets-restart-mongod`

Building the index on the background, takes longer than the foreground
index build and results in a less compact index structure. Additionally,
the background index build may impact write performance on the
primary. However, building the index in the background allows the set to
be continuously up for write operations during while MongoDB builds the
index.
===========================================
Calculate Distance Using Spherical Geometry
===========================================

.. default-domain:: mongodb

.. note:: While basic queries using spherical distance are supported by
   the ``2d`` index, consider moving to a ``2dsphere`` index if your
   data is primarily longitude and latitude.

The ``2d`` index supports queries that calculate distances on a
Euclidean plane (flat surface). The index also supports the following
query operators and command that calculate distances using spherical
geometry:

- :query:`$nearSphere`

- :query:`$centerSphere`

- :query:`$near`

- :dbcommand:`geoNear` command with the ``{ spherical: true }`` option.

.. important:: These three queries use radians for distance. Other query
   types do not.

   For spherical query operators to function properly, you must convert
   distances to radians, and convert from radians to the distances units
   used by your application.

   To convert:

   - *distance to radians*: divide the distance by the radius of the
     sphere (e.g. the Earth) in the same units as the distance
     measurement.

   - *radians to distance*: multiply the radian measure by the radius
     of the sphere (e.g. the Earth) in the units system that you want to
     convert the distance to.

   The radius of the Earth is approximately ``3,959`` miles or
   ``6,371`` kilometers.

The following query would return documents from the ``places``
collection within the circle described by the center ``[ -74, 40.74 ]``
with a radius of ``100`` miles:

.. code-block:: javascript

   db.places.find( { loc: { $geoWithin: { $centerSphere: [ [ -74, 40.74 ] ,
                                                        100 / 3959 ] } } } )

You may also use the ``distanceMultiplier`` option to the
:dbcommand:`geoNear` to convert radians in the :program:`mongod`
process, rather than in your application code. See :ref:`distance
multiplier <geospatial-indexes-distance-multiplier>`.

The following spherical query, returns all documents in the
collection ``places`` within ``100`` miles from the point ``[ -74,
40.74 ]``.

.. code-block:: javascript

   db.runCommand( { geoNear: "places",
                    near: [ -74, 40.74 ],
                    spherical: true
                  }  )

The output of the above command would be:

.. code-block:: javascript

   {
      // [ ... ]
      "results" : [
         {
            "dis" : 0.01853688938212826,
            "obj" : {
               "_id" : ObjectId( ... )
               "loc" : [
                  -73,
                  40
               ]
            }
         }
      ],
      "stats" : {
         // [ ... ]
         "avgDistance" : 0.01853688938212826,
         "maxDistance" : 0.01853714811400047
      },
      "ok" : 1
   }

.. warning::

   Spherical queries that wrap around the poles or at the transition
   from ``-180`` to ``180`` longitude raise an error.

.. note::

   While the default Earth-like bounds for geospatial indexes are
   between ``-180`` inclusive, and ``180``, valid values for latitude
   are between ``-90`` and ``90``.

.. _geospatial-indexes-distance-multiplier:

Distance Multiplier
-------------------

The ``distanceMultiplier`` option of the :dbcommand:`geoNear` command returns
distances only after multiplying the results by an assigned value. This allows
MongoDB to return converted values, and removes the requirement to
convert units in application logic.

Using ``distanceMultiplier`` in spherical queries provides results from
the :dbcommand:`geoNear` command that do not need radian-to-distance
conversion. The following example uses ``distanceMultiplier`` in the
:dbcommand:`geoNear` command with a :doc:`spherical
</tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes>` example:

.. code-block:: javascript

   db.runCommand( { geoNear: "places",
                    near: [ -74, 40.74 ],
                    spherical: true,
                    distanceMultiplier: 3959
                  }  )

The output of the above operation would resemble the following:

.. code-block:: javascript

   {
      // [ ... ]
      "results" : [
         {
            "dis" : 73.46525170413567,
            "obj" : {
               "_id" : ObjectId( ... )
               "loc" : [
                  -73,
                  40
               ]
            }
         }
      ],
      "stats" : {
         // [ ... ]
         "avgDistance" : 0.01853688938212826,
         "maxDistance" : 0.01853714811400047
      },
      "ok" : 1
   }
=================================
Change Hostnames in a Replica Set
=================================

.. default-domain:: mongodb

For most :term:`replica sets <replica set>`, the hostnames in the
:data:`~local.system.replset.members[n].host` field never change.
However, if organizational needs change, you might need to migrate some
or all host names.

.. note:: Always use resolvable hostnames for the value of the
   :data:`~local.system.replset.members[n].host` field in the replica
   set configuration to avoid confusion and complexity.

Overview
--------

This document provides two separate procedures for changing the
hostnames in the :data:`~local.system.replset.members[n].host`
field. Use either of the following approaches:

- :ref:`Change hostnames without disrupting availability
  <replica-set-change-hostname-no-downtime>`. This approach ensures your
  applications will always be able to read and write data to the replica
  set, but the approach can take a long time and may incur downtime at
  the application layer.

  If you use the first procedure, you must configure your applications
  to connect to the replica set at both the old and new locations, which
  often requires a restart and reconfiguration at the application layer
  and which may affect the availability of your applications.
  Re-configuring applications is beyond the scope of this document.

- :ref:`Stop all members running on the old hostnames at once
  <replica-set-change-hostname-downtime>`. This approach has a shorter
  maintenance window, but the replica set will be unavailable during
  the operation.

.. seealso:: :ref:`Replica Set Reconfiguration Process
   <replica-set-reconfiguration-usage>`,
   :doc:`/tutorial/deploy-replica-set`, and
   :doc:`/tutorial/expand-replica-set`.

.. _procedure-assumption-change-hostnames-replica-set:

Assumptions
-----------

Given a :term:`replica set` with three members:

- ``database0.example.com:27017`` (the :term:`primary`)

- ``database1.example.com:27017``

- ``database2.example.com:27017``

And with the following :method:`rs.conf()` output:

.. code-block:: javascript

   {
       "_id" : "rs",
       "version" : 3,
       "members" : [
           {
               "_id" : 0,
               "host" : "database0.example.com:27017"
           },
           {
               "_id" : 1,
               "host" : "database1.example.com:27017"
           },
           {
               "_id" : 2,
               "host" : "database2.example.com:27017"
           }
       ]
   }

The following procedures change the members' hostnames as follows:

- ``mongodb0.example.net:27017`` (the primary)

- ``mongodb1.example.net:27017``

- ``mongodb2.example.net:27017``

Use the most appropriate procedure for your deployment.

.. _replica-set-change-hostname-no-downtime:

Change Hostnames while Maintaining Replica Set Availability
-----------------------------------------------------------

This procedure uses the above :ref:`assumptions <procedure-assumption-change-hostnames-replica-set>`.

#. For each :term:`secondary` in the replica set, perform the
   following sequence of operations:

   a. Stop the secondary.

   #. Restart the secondary at the new location.

   #. Open a :program:`mongo` shell connected to the replica set's
      primary. In our example, the primary runs on port ``27017`` so you
      would issue the following command:

      .. code-block:: sh

         mongo --port 27017

   #. Use :method:`rs.reconfig()` to update the :doc:`replica set
      configuration document </reference/replica-configuration>` with
      the new hostname.

      For example, the following sequence of commands updates the
      hostname for the secondary at the array index ``1`` of the
      ``members`` array (i.e. ``members[1]``) in the replica set
      configuration document:

      .. code-block:: javascript

         cfg = rs.conf()
         cfg.members[1].host = "mongodb1.example.net:27017"
         rs.reconfig(cfg)

      For more information on updating the configuration document, see
      :ref:`replica-set-reconfiguration-usage`.

   #. Make sure your client applications are able to access the
      set at the new location and that the secondary has a chance to
      catch up with the other members of the set.

      Repeat the above steps for each non-primary member of the set.

#. Open a :program:`mongo` shell connected to the primary and step
   down the primary using the :method:`rs.stepDown()` method:

   .. code-block:: javascript

      rs.stepDown()

   The replica set elects another member to the become primary.

#. When the step down succeeds, shut down the old primary.

#. Start the :program:`mongod` instance that will become the new primary
   in the new location.

#. Connect to the current primary, which was just elected, and update
   the :doc:`replica set configuration document
   </reference/replica-configuration>` with the hostname of the node that
   is to become the new primary.

   For example, if the old primary was at position ``0`` and the new
   primary's hostname is ``mongodb0.example.net:27017``, you would run:

   .. code-block:: javascript

      cfg = rs.conf()
      cfg.members[0].host = "mongodb0.example.net:27017"
      rs.reconfig(cfg)

#. Open a :program:`mongo` shell connected to the new primary.

#. To confirm the new configuration, call :method:`rs.conf()` in the
   :program:`mongo` shell.

   Your output should resemble:

   .. code-block:: javascript

      {
          "_id" : "rs",
          "version" : 4,
          "members" : [
              {
                  "_id" : 0,
                  "host" : "mongodb0.example.net:27017"
              },
              {
                  "_id" : 1,
                  "host" : "mongodb1.example.net:27017"
              },
              {
                  "_id" : 2,
                  "host" : "mongodb2.example.net:27017"
              }
          ]
      }

.. _replica-set-change-hostname-downtime:

Change All Hostnames at the Same Time
-------------------------------------

This procedure uses the above :ref:`assumptions  <procedure-assumption-change-hostnames-replica-set>`.

#. Stop all members in the :term:`replica set`.

#. Restart each member *on a different port* and *without* using the
   :option:`--replSet <mongod --replSet>` run-time option. Changing
   the port number during maintenance prevents clients from connecting
   to this host while you perform maintenance. Use the member's usual
   :option:`--dbpath <mongod --dbpath>`, which in this
   example is ``/data/db1``. Use a command that resembles the following:

   .. code-block:: sh

      mongod --dbpath /data/db1/ --port 37017

#. For each member of the replica set, perform the following sequence
   of operations:

   a. Open a :program:`mongo` shell connected to the :program:`mongod`
      running on the new, temporary port. For example, for a member
      running on a temporary port of ``37017``, you would issue this
      command:

      .. code-block:: sh

         mongo --port 37017

   #. Edit the replica set configuration manually. The replica set
      configuration is the only document in the ``system.replset``
      collection in the ``local`` database. Edit the replica set
      configuration with the new hostnames and correct ports for all
      the members of the replica set. Consider the following sequence of
      commands to change the hostnames in a three-member set:

      .. code-block:: javascript

         use local

         cfg = db.system.replset.findOne( { "_id": "rs" } )

         cfg.members[0].host = "mongodb0.example.net:27017"

         cfg.members[1].host = "mongodb1.example.net:27017"

         cfg.members[2].host = "mongodb2.example.net:27017"

         db.system.replset.update( { "_id": "rs" } , cfg )

   #. Stop the :program:`mongod` process on the member.

#. After re-configuring all members of the set, start each
   :program:`mongod` instance in the normal way: use the usual port
   number and use the :option:`--replSet <mongod --replSet>` option. For
   example:

   .. code-block:: sh

      mongod --dbpath /data/db1/ --port 27017 --replSet rs

#. Connect to one of the :program:`mongod` instances
   using the :program:`mongo` shell. For example:

   .. code-block:: sh

      mongo --port 27017

#. To confirm the new configuration, call :method:`rs.conf()` in the
   :program:`mongo` shell.

   Your output should resemble:

   .. code-block:: javascript

      {
          "_id" : "rs",
          "version" : 4,
          "members" : [
              {
                  "_id" : 0,
                  "host" : "mongodb0.example.net:27017"
              },
              {
                  "_id" : 1,
                  "host" : "mongodb1.example.net:27017"
              },
              {
                  "_id" : 2,
                  "host" : "mongodb2.example.net:27017"
              }
          ]
      }
============================
Change the Size of the Oplog
============================

.. default-domain:: mongodb

The :term:`oplog` exists internally as a :term:`capped collection`, so
you cannot modify its size in the course of normal operations. In most
cases the :ref:`default oplog size <replica-set-oplog-sizing>` is an
acceptable size; however, in some situations you may need a larger or
smaller oplog. For example, you might need to change the oplog size
if your applications perform large numbers of multi-updates or
deletes in short periods of time.

This tutorial describes how to resize the oplog. For a detailed
explanation of oplog sizing, see :ref:`replica-set-oplog-sizing`.  For
details how oplog size affects :term:`delayed members <delayed
member>` and affects :term:`replication lag`, see
:ref:`replica-set-delayed-members`.

Overview
--------

To change the size of the oplog, you must perform maintenance on each
member of the replica set in turn. The procedure requires: stopping
the :program:`mongod` instance and starting as a standalone instance,
modifying the oplog size, and restarting the member.

.. important:: Always start rolling replica set maintenance with the
   secondaries, and finish with the maintenance on primary member.

Procedure
---------

- Restart the member in standalone mode.

  .. tip:: Always use :method:`rs.stepDown()` to force the primary to
     become a secondary, before stopping the server. This facilitates
     a more efficient election process.

- Recreate the oplog with the new size and with an old oplog entry as
  a seed.

- Restart the :program:`mongod` instance as a member of the replica
  set.

Restart a Secondary in Standalone Mode on a Different Port
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Shut down the :program:`mongod` instance for one of the non-primary
members of your replica set. For example, to shut down, use the
:method:`db.shutdownServer()` method:

.. code-block:: sh

   db.shutdownServer()

Restart this :program:`mongod` as a standalone instance
running on a different port and *without*
the :option:`--replSet <mongod --replSet>` parameter. Use a command
similar to the following:

.. code-block:: sh

   mongod --port 37017 --dbpath /srv/mongodb

Create a Backup of the Oplog (Optional)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Optionally, backup the existing oplog on the standalone instance, as
in the following example:

.. code-block:: sh

   mongodump --db local --collection 'oplog.rs' --port 37017


Recreate the Oplog with a New Size and a Seed Entry
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Save the last entry from the oplog. For example, connect to the instance
using the :program:`mongo` shell, and enter the following command to
switch to the ``local`` database:

.. code-block:: javascript

   use local

In :program:`mongo` shell scripts you can use the following operation
to set the ``db`` object:

.. code-block:: javascript

   db = db.getSiblingDB('local')

Use the :method:`db.collection.save()` method and a sort on
reverse :term:`natural order` to find the last entry and save it
to a temporary collection:

.. _last-oplog-entry:

.. code-block:: javascript

   db.temp.save( db.oplog.rs.find( { }, { ts: 1, h: 1 } ).sort( {$natural : -1} ).limit(1).next() )

To see this oplog entry, use the following operation:

.. code-block:: javascript

   db.temp.find()

Remove the Existing Oplog Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Drop the old ``oplog.rs`` collection in the ``local`` database. Use
the following command:

.. code-block:: javascript

   db = db.getSiblingDB('local')
   db.oplog.rs.drop()

This returns ``true`` in the shell.

Create a New Oplog
~~~~~~~~~~~~~~~~~~

Use the :dbcommand:`create` command to create a new oplog of a
different size. Specify the ``size`` argument in bytes. A value of
``2 * 1024 * 1024 * 1024`` will create a new oplog that's 2 gigabytes:

.. code-block:: javascript

   db.runCommand( { create: "oplog.rs", capped: true, size: (2 * 1024 * 1024 * 1024) } )

Upon success, this command returns the following status:

.. code-block:: javascript

   { "ok" : 1 }

Insert the Last Entry of the Old Oplog into the New Oplog
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Insert the previously saved last entry from the old oplog into the
new oplog. For example:

.. code-block:: javascript

   db.oplog.rs.save( db.temp.findOne() )

To confirm the entry is in the new oplog, use the following operation:

.. code-block:: javascript

   db.oplog.rs.find()

Restart the Member
~~~~~~~~~~~~~~~~~~

Restart the :program:`mongod` as a member of the replica set on its
usual port. For example:

.. code-block:: javascript

   db.shutdownServer()
   mongod --replSet rs0 --dbpath /srv/mongodb

The replica set member will recover and "catch up" before it is
eligible for election to primary.

Repeat Process for all Members that may become Primary
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Repeat this procedure for all members you want to change the size of
the oplog. Repeat the procedure for the primary as part of the
following step.

Change the Size of the Oplog on the Primary
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To finish the rolling maintenance operation, step down the primary with the
:method:`rs.stepDown()` method and repeat the oplog resizing procedure
above.
====================================
Change Your Password and Custom Data
====================================

.. versionchanged:: 2.6

.. default-domain:: mongodb

Overview
--------

Users with appropriate privileges can change their own passwords and
custom data. :data:`Custom data <admin.system.users.customData>` stores
optional user information.

Considerations
--------------

To generate a strong password for use in this procedure, you can use the
``openssl`` utility's ``rand`` command. For example, issue ``openssl
rand`` with the following options to create a base64-encoded string of 48
pseudo-random bytes:

.. code-block:: sh

   openssl rand -base64 48

.. _change-own-password-prereq:

Prerequisites
-------------

.. include:: /includes/access-change-own-password-and-custom-data.rst

Procedure
---------

.. include:: /includes/steps/change-own-password-and-custom-data.rst
========================
Change a User's Password
========================

.. versionchanged:: 2.6

.. default-domain:: mongodb

Overview
--------

Strong passwords help prevent unauthorized access, and all users
should have strong passwords. You can use the ``openssl`` program to
generate unique strings for use in passwords, as in the following
command:

.. code-block:: sh

   openssl rand -base64 48

.. _change-password-prereq:

Prerequisites
-------------

.. include:: /includes/access-change-password.rst

Procedure
---------

.. include:: /includes/steps/change-user-password.rst
======================
Modify a User's Access
======================

.. default-domain:: mongodb

Overview
--------

When a user's responsibilities change, modify the user's access to include
only those roles the user requires. This follows the policy of :term:`least
privilege`.

To change a user's access, first determine the privileges the user needs and
then determine the roles that grants those privileges. Grant and revoke roles
using the method:`db.grantRolesToUser()` and :method:`db.revokeRolesFromUser`
methods.

For an overview of roles and privileges, see :ref:`authorization`. For
descriptions of the access each built-in role provides, see the section on
:ref:`built-in roles <built-in-roles>`.

.. _change-user-privileges-prereq:

Prerequisites
-------------

.. include:: /includes/access-grant-roles.rst

.. include:: /includes/access-revoke-roles.rst

.. include:: /includes/access-roles-info.rst

Procedure
---------

.. include:: /includes/steps/change-user-privileges.rst
=======================================
Considerations for Selecting Shard Keys
=======================================

.. TODO the content is from 3 separate sources. Combine.

.. _sharding-internals-operations-and-reliability:

.. _sharding-internals-choose-shard-key:

Choosing a Shard Key
~~~~~~~~~~~~~~~~~~~~

For many collections there may be no single, naturally occurring key
that possesses all the qualities of a good shard key. The following
strategies may help construct a useful shard key from existing data:

#. Compute a more ideal shard key in your application layer,
   and store this in all of your documents, potentially in the
   ``_id`` field.

#. Use a compound shard key that uses two or three values from all
   documents that provide the right mix of cardinality with scalable
   write operations and query isolation.

#. Determine that the impact of using a less than ideal shard key
   is insignificant in your use case, given:

   - limited write volume,
   - expected data size, or
   - application query patterns.

#. .. versionadded:: 2.4
      Use a :term:`hashed shard key`. Choose a field that has high
      cardinality and create a :ref:`hashed index <index-hashed-index>`
      on that field. MongoDB uses these hashed index values as shard key
      values, which ensures an even distribution of documents across the
      shards.

   .. include:: /includes/tip-applications-do-not-need-to-compute-hashes.rst

.. _sharding-shard-key-selection:

Considerations for Selecting Shard Key
--------------------------------------

Choosing the correct shard key can have a great impact on the
performance, capability, and functioning of your database and cluster.
Appropriate shard key choice depends on the schema of your data and the
way that your applications query and write data.

Create a Shard Key that is Easily Divisible
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

An easily divisible shard key makes it easy for MongoDB to distribute
content among the shards. Shard keys that have a limited number of
possible values can result in chunks that are "unsplittable."

.. seealso:: :ref:`sharding-shard-key-cardinality`

Create a Shard Key that has High Degree of Randomness
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A shard key with high degree of randomness prevents any single shard
from becoming a bottleneck and will distribute write operations among
the cluster.

.. seealso:: :ref:`sharding-shard-key-write-scaling`

Create a Shard Key that Targets a Single Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A shard key that targets a single shard makes it possible for the
:program:`mongos` program to return most query operations directly
from a single *specific* :program:`mongod` instance. Your shard key
should be the primary field used by your queries. Fields with a high
degree of "randomness" make it difficult to target operations to
specific shards.

.. seealso:: :ref:`sharding-shard-key-query-isolation`

Shard Using a Compound Shard Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The challenge when selecting a shard key is that there is not always
an obvious choice. Often, an existing field in your collection may not
be the optimal key. In those situations, computing a special purpose
shard key into an additional field or using a compound shard key may
help produce one that is more ideal.

.. index:: shard key; cardinality
.. _sharding-shard-key-cardinality:

Cardinality
~~~~~~~~~~~

Cardinality in the context of MongoDB, refers to the ability of the
system to :term:`partition` data into :term:`chunks <chunk>`. For
example, consider a collection of data such as an "address book" that
stores address records:

- Consider the use of a ``state`` field as a shard key:

  The state key's value holds the US state for a given address document.
  This field has a *low cardinality* as all documents that have the
  same value in the ``state`` field *must* reside on the same shard,
  even if a particular state's chunk exceeds the maximum chunk size.

  Since there are a limited number of possible values for the ``state``
  field, MongoDB may distribute data unevenly among a small
  number of fixed chunks. This may have a number of effects:

  - If MongoDB cannot split a chunk because all of its documents
    have the same shard key, migrations involving these un-splittable
    chunks will take longer than other migrations, and it will be more
    difficult for your data to stay balanced.

  - If you have a fixed maximum number of chunks, you will never be
    able to use more than that number of shards for this collection.

- Consider the use of a ``zipcode`` field as a shard key:

  While this field has a large number of possible values, and thus has
  potentially higher cardinality, it's possible that a large number of users
  could have the same value for the shard key, which would make this
  chunk of users un-splittable.

  In these cases, cardinality depends on the data. If your address book
  stores records for a geographically distributed contact list
  (e.g. "Dry cleaning businesses in America,") then a value like
  zipcode would be sufficient. However, if your address book is
  more geographically concentrated (e.g "ice cream stores in Boston
  Massachusetts,") then you may have a much lower cardinality.

- Consider the use of a ``phone-number`` field as a shard key:

  Phone number has a *high cardinality,* because users will generally
  have a unique value for this field, MongoDB will be able to split as
  many chunks as needed.

While "high cardinality," is necessary for ensuring an even
distribution of data, having a high cardinality does not guarantee
sufficient :ref:`query isolation <sharding-shard-key-query-isolation>`
or appropriate :ref:`write scaling <sharding-shard-key-write-scaling>`.
======================================
Configure a Delayed Replica Set Member
======================================

.. default-domain:: mongodb

.. start-configuration

To configure a delayed secondary member, set its
:data:`~local.system.replset.members[n].priority` value to ``0``, its
:data:`~local.system.replset.members[n].hidden` value to ``true``, and
its :data:`~local.system.replset.members[n].slaveDelay` value to the
number of seconds to delay.

.. important::

   The length of the secondary
   :data:`~local.system.replset.members[n].slaveDelay` must
   fit within the window of the oplog. If the oplog is shorter than
   the :data:`~local.system.replset.members[n].slaveDelay`
   window, the delayed member cannot successfully replicate
   operations.

When you configure a delayed member, the delay
applies both to replication and to the member's :term:`oplog`. For
details on delayed members and their uses, see
:doc:`/core/replica-set-delayed-member`.

Example
-------

The following example sets a 1-hour delay on a secondary member
currently at the index ``0`` in the
:data:`~local.system.replset.members` array. To set the delay, issue
the following sequence of operations in a :program:`mongo` shell
connected to the primary:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.members[0].priority = 0
   cfg.members[0].hidden = true
   cfg.members[0].slaveDelay = 3600
   rs.reconfig(cfg)

After the replica set reconfigures, the delayed secondary member cannot
become :term:`primary` and is hidden from applications. The
:data:`~local.system.replset.members[n].slaveDelay` value delays both
replication and the member's :term:`oplog` by 3600 seconds (1 hour).

.. include:: /includes/fact-rs-conf-array-index.rst

.. include:: /includes/warning-rs-reconfig.rst

Related Documents
-----------------

- :data:`~local.system.replset.members[n].slaveDelay`

- :ref:`Replica Set Reconfiguration
  <replica-set-reconfiguration-usage>`

- :ref:`replica-set-oplog-sizing`

- :doc:`/tutorial/change-oplog-size` tutorial

- :doc:`/core/replica-set-elections`
=====================================
Configure a Hidden Replica Set Member
=====================================

.. default-domain:: mongodb

Hidden members are part of a :term:`replica set` but cannot become
:term:`primary` and are invisible to client applications. Hidden members
do vote in :ref:`elections <replica-set-elections>`. For a
more information on hidden members and their uses, see
:doc:`/core/replica-set-hidden-member`.

Considerations
--------------

The most common use of hidden nodes is to support :doc:`delayed
members </core/replica-set-delayed-member>`. If you only need to prevent a member from
becoming primary, configure a :doc:`priority 0 member
</core/replica-set-priority-0-member>`.

.. include:: /includes/fact-replica-set-sync-prefers-non-hidden.rst

.. versionchanged:: 2.0
   For :term:`sharded clusters <sharded cluster>` running with replica
   sets before 2.0, if you reconfigured a member as hidden, you *had*
   to restart :program:`mongos` to prevent queries from reaching the
   hidden member.

Examples
--------

Member Configuration Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To configure a secondary member as hidden, set its
:data:`~local.system.replset.members[n].priority` value to ``0`` and
set its :data:`~local.system.replset.members[n].hidden` value to
``true`` in its member configuration:

.. code-block:: javascript
   :emphasize-lines: 4-5

   {
     "_id" : <num>
     "host" : <hostname:port>,
     "priority" : 0,
     "hidden" : true
   }


Configuration Procedure
~~~~~~~~~~~~~~~~~~~~~~~

The following example hides the secondary member currently at the index
``0`` in the :data:`~local.system.replset.members` array. To configure
a :term:`hidden member`, use the following sequence of operations in a
:program:`mongo` shell connected to the primary, specifying the member
to configure by its array index in the
:data:`~local.system.replset.members` array:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.members[0].priority = 0
   cfg.members[0].hidden = true
   rs.reconfig(cfg)

After re-configuring the set, this secondary member has a priority of
``0`` so that it cannot become primary and is hidden. The other members
in the set will not advertise the hidden member in the
:dbcommand:`isMaster` or :method:`db.isMaster()` output.

.. include:: /includes/fact-rs-conf-array-index.rst

.. include:: /includes/warning-rs-reconfig.rst

Related Documents
-----------------

- :ref:`Replica Set Reconfiguration <replica-set-reconfiguration-usage>`

- :doc:`/core/replica-set-elections`

- :ref:`Read Preference <replica-set-read-preference>`
=======================================
Configure Non-Voting Replica Set Member
=======================================

.. default-domain:: mongodb

Non-voting members allow you to add additional members for read
distribution beyond the maximum seven voting members. To configure a
member as non-voting, set its
:data:`~local.system.replset.members[n].votes` value to ``0``.

Example
-------

To disable the ability to vote in elections for the fourth, fifth, and
sixth replica set members, use the following command sequence in the
:program:`mongo` shell connected to the primary. You identify each
replica set member by its array index in the
:data:`~local.system.replset.members` array:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.members[3].votes = 0
   cfg.members[4].votes = 0
   cfg.members[5].votes = 0
   rs.reconfig(cfg)

This sequence gives ``0`` votes to the fourth, fifth, and sixth members
of the set according to the order of the
:data:`~local.system.replset.members` array in the output of
:method:`rs.conf()`. This setting allows the set to elect these members
as :term:`primary` but does not allow them to vote in elections. Place
voting members so that your designated primary or primaries can reach a
majority of votes in the event of a network partition.

.. include:: /includes/fact-rs-conf-array-index.rst

.. include:: /includes/warning-rs-reconfig.rst

In general and when possible, all members should have only 1 vote. This
prevents intermittent ties, deadlocks, or the wrong members from
becoming primary. Use :data:`~local.system.replset.members[n].priority`
to control which members are more likely to become primary.

Related Documents
-----------------

- :data:`~local.system.replset.members[n].votes`

- :ref:`Replica Set Reconfiguration <replica-set-reconfiguration-usage>`

- :doc:`/core/replica-set-elections`
================================
Configure System Events Auditing
================================

.. default-domain:: mongodb

.. versionadded:: 2.6

MongoDB Enterprise supports :ref:`auditing <auditing>` of various
operations. A complete auditing solution must involve all
:program:`mongod` server and :program:`mongos` router processes.

The audit facility can write audit events to the console, the
:term:`syslog` (option is unavailable on Windows), a JSON file, or a
BSON file. For details on the audited operations and the audit log
messages, see :doc:`/reference/audit-message`.

Enable and Configure Audit Output
---------------------------------

Use the :option:`--auditDestination` option to enable
auditing and specify where to output the audit events.

Output to Syslog
~~~~~~~~~~~~~~~~

To enable auditing and print audit events to the syslog (option
is unavailable on Windows) in JSON format, specify ``syslog`` for the
:option:`--auditDestination` setting. For example:

.. code-block:: sh

   mongod --dbpath data/db --auditDestination syslog

.. warning::

   The syslog message limit can result in the truncation of the audit
   messages. The auditing system will neither detect the truncation nor
   error upon its occurrence.

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   dbpath=data/db
   auditDestination=syslog

Output to Console
~~~~~~~~~~~~~~~~~

To enable auditing and print the audit events to standard
output (i.e. ``stdout``), specify ``console`` for the
:option:`--auditDestination` setting. For example:

.. code-block:: sh

   mongod --dbpath data/db --auditDestination console

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   dbpath=data/db
   auditDestination=console

Output to JSON File
~~~~~~~~~~~~~~~~~~~

To enable auditing and print audit events to a file in JSON
format, specify ``file`` for the :option:`--auditDestination` setting,
``JSON`` for the :option:`--auditFormat` setting, and
the output filename for the :option:`--auditPath`. The
:option:`--auditPath` option accepts either full path name or relative
path name. For example, the following enables auditing and records
audit events to a file with the relative path name of
``data/db/auditLog.json``:

.. code-block:: sh

   mongod --dbpath data/db --auditDestination file --auditFormat JSON --auditPath data/db/auditLog.json

The audit file rotates at the same time as the server log file.

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   dbpath=data/db
   auditDestination=file
   auditFormat=JSON
   auditPath=data/db/auditLog.json

.. note:: Printing audit events to a file in JSON format degrades
   server performance more than printing to a file in BSON format.

Output to BSON File
~~~~~~~~~~~~~~~~~~~

To enable auditing and print audit events to a file in BSON binary
format, specify ``file`` for the :option:`--auditDestination` setting,
``BSON`` for the :option:`--auditFormat` setting, and the output
filename for the :option:`--auditPath`. The :option:`--auditPath`
option accepts either full path name or relative path name. For
example, the following enables auditing and records audit events to a
BSON file with the relative path name of ``data/db/auditLog.bson``:

.. code-block:: sh

   mongod --dbpath data/db --auditDestination file --auditFormat BSON --auditPath data/db/auditLog.bson

The audit file rotates at the same time as the server log file.

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   dbpath=data/db
   auditDestination=file
   auditFormat=BSON
   auditPath=data/db/auditLog.bson

To view the contents of the file, pass the file to the MongoDB utility
:program:`bsondump`. For example, the following converts the audit log
into a human-readable form and output to the terminal:

.. code-block:: sh

   bsondump data/db/auditLog.bson

.. _audit-filter:

Filter Events
-------------

By default, the audit facility records all :ref:`auditable operations
<audit-action-details-results>`. The audit feature has an
:option:`--auditFilter` option to determine which events to record. The
:option:`--auditFilter` option takes a document of the form:

.. code-block:: javascript

   { atype: <expression> }

The ``<expression>`` is a :ref:`query condition expression
<query-selectors>` to match on :ref:`various actions
<audit-action-details-results>` .

Filter for a Single Operation Type
----------------------------------

For example, to audit only the :authaction:`createCollection` action, use the
filter ``{ atype: "createCollection" }``:

.. tip::
   To specify the filter as a command-line option, enclose the filter document
   in single quotes to pass the document as a string.

.. code-block:: javascript

   mongod --dbpath data/db --auditDestination file --auditFilter '{ atype: "createCollection" }' --auditFormat JSON --auditPath data/db/auditLog.json

Filter for Multiple Operation Types
-----------------------------------

To match on multiple operations, use the :operator:`$in` operator in
the ``<expression>`` as in the following:

.. tip::
   To specify the filter as a command-line option, enclose the filter document
   in single quotes to pass the document as a string.

.. code-block:: javascript

   mongod --dbpath data/db --auditDestination file --auditFilter '{ atype: { $in: [ "createCollection", "dropCollection" ] } }' --auditFormat JSON --auditPath data/db/auditLog.json

Filter on Authentication Operations on a Single Database
--------------------------------------------------------

For authentication operations, you can also specify a specific
database with the ``param.db`` field:

.. code-block:: javascript

   { atype: <expression>, "param.db": <database> }

For example, to audit only ``authenticate`` operations that occur
against the ``test`` database, use the filter ``{ atype:
"authenticate", "param.db": "test" }``:

.. tip:: To specify the filter as a command-line option, enclose the
   filter document in single quotes to pass the document as a string.

.. code-block:: javascript

   mongod --dbpath data/db --auth --auditDestination file --auditFilter '{ atype: "authenticate", "param.db": "test" }' --auditFormat JSON --auditPath data/db/auditLog.json

To filter on all :authaction:`authenticate` operations across
databases, use the filter ``{ atype: "authenticate" }``.
================================
Authenticate Using SASL and LDAP
================================

.. default-domain:: mongodb

MongoDB Enterprise provides support for proxy authentication of users.
This allows administrators to configure a MongoDB cluster to
authenticate users by proxying authentication requests to a specified
Lightweight Directory Access Protocol (LDAP) service.

.. include:: /includes/admonition-mongodb-enterprise-windows-ldap.rst

MongoDB does **not** support LDAP authentication in mixed sharded
cluster deployments that contain both version 2.4 and version 2.6
shards. See :doc:`/release-notes/2.6-upgrade` for upgrade instructions.

.. _ldap-password-in-plaintext:

Considerations
--------------

Because the transmission of the username and password from the client
to the MongoDB server, from the MongoDB server to ``saslauthd``, and
from ``saslauthd`` to the LDAP server uses ``SASL PLAIN`` mechanism,
i.e. in **plain text**, you should, in general, use only on a trusted
channel (VPN, SSL, trusted wired network).

Configure ``saslauthd``
-----------------------

LDAP support for user authentication requires proper configuration of
the ``saslauthd`` daemon process as well as the MongoDB server.

.. include:: /includes/steps/configure-ldap-saslauthd.rst

Configure MongoDB
-----------------

.. include:: /includes/steps/configure-ldap-mongodb.rst
=================================================
Configure Linux ``iptables`` Firewall for MongoDB
=================================================

.. default-domain:: mongodb

On contemporary Linux systems, the ``iptables`` program provides
methods for managing the Linux Kernel's ``netfilter`` or network
packet filtering capabilities. These firewall rules make it possible
for administrators to control what hosts can connect to the system,
and limit risk exposure by limiting the hosts that can connect to a
system.

This document outlines basic firewall configurations for ``iptables``
firewalls on Linux. Use these approaches as a starting point for your
larger networking organization. For a detailed overview of security
practices and risk management for MongoDB, see :doc:`/core/security`.

.. seealso:: For MongoDB deployments on Amazon's web services, see the
   :ecosystem:`Amazon EC2 </platforms/amazon-ec2>` page, which addresses Amazon's
   Security Groups and other EC2-specific security features.

Overview
--------

Rules in ``iptables`` configurations fall into chains, which describe
the process for filtering and processing specific streams of
traffic. Chains have an order, and packets must pass through earlier
rules in a chain to reach later rules. This document addresses only the
following two chains:

``INPUT``
   Controls all incoming traffic.

``OUTPUT``
   Controls all outgoing traffic.

Given the :ref:`default ports <security-port-numbers>` of all MongoDB
processes, you must configure networking rules that permit *only*
required communication between your application and the appropriate
:program:`mongod` and :program:`mongos` instances.

Be aware that, by default, the default policy of ``iptables`` is to
allow all connections and traffic unless explicitly disabled. The
configuration changes outlined in this document will create rules that
explicitly allow traffic from specific addresses and on specific
ports, using a default policy that drops all traffic that is not
explicitly allowed. When you have properly configured your
``iptables`` rules to allow only the traffic that you want to permit,
you can :ref:`iptables-change-default-policy-to-drop`.

Patterns
--------

This section contains a number of patterns and examples for
configuring ``iptables`` for use with MongoDB deployments. If you have
configured different ports using the :setting:`~net.port` configuration
setting, you will need to modify the rules accordingly.

.. _iptables-basic-rule-set:

Traffic to and from ``mongod`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pattern is applicable to all :program:`mongod` instances running
as standalone instances or as part of a :term:`replica set`.

The goal of this pattern is to explicitly allow traffic to the
:program:`mongod` instance from the application server. In the
following examples, replace ``<ip-address>`` with the IP address of
the application server:

.. code-block:: sh

   iptables -A INPUT -s <ip-address> -p tcp --destination-port 27017 -m state --state NEW,ESTABLISHED -j ACCEPT
   iptables -A OUTPUT -d <ip-address> -p tcp --source-port 27017 -m state --state ESTABLISHED -j ACCEPT

The first rule allows all incoming traffic from ``<ip-address>`` on
port ``27017``, which allows the application server to connect to the
:program:`mongod` instance. The second rule, allows outgoing traffic
from the :program:`mongod` to reach the application server.

.. optional::

   If you have only one application server, you can replace
   ``<ip-address>`` with either the IP address itself, such as:
   ``198.51.100.55``. You can also express this using CIDR notation as
   ``198.51.100.55/32``. If you want to permit a larger block of
   possible IP addresses you can allow traffic from a ``/24`` using
   one of the following specifications for the ``<ip-address>``, as
   follows:

   .. code-block:: sh

      10.10.10.10/24
      10.10.10.10/255.255.255.0

Traffic to and from ``mongos`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:program:`mongos` instances provide query routing for :term:`sharded
clusters <sharded cluster>`. Clients connect to :program:`mongos` instances, which
behave from the client's perspective as :program:`mongod`
instances. In turn, the :program:`mongos` connects to all
:program:`mongod` instances that are components of the sharded
cluster.

Use the same ``iptables`` command to allow traffic to and from these
instances as you would from the :program:`mongod` instances that are
members of the replica set. Take the configuration outlined in the
:ref:`iptables-basic-rule-set` section as an example.

Traffic to and from a MongoDB Config Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Config servers, host the :term:`config database` that stores metadata
for sharded clusters. Each production cluster has three config
servers, initiated using the :option:`mongod --configsvr`
option. [#config-option]_ Config servers listen for connections on port
``27019``. As a result, add the following ``iptables`` rules to the
config server to allow incoming and outgoing connection on port
``27019``, for connection to the other config servers.

.. code-block:: sh

   iptables -A INPUT -s <ip-address> -p tcp --destination-port 27019 -m state --state NEW,ESTABLISHED -j ACCEPT
   iptables -A OUTPUT -d <ip-address> -p tcp --source-port 27019 -m state --state ESTABLISHED -j ACCEPT

Replace ``<ip-address>`` with the address or address space of *all*
the :program:`mongod` that provide config servers.

Additionally, config servers need to allow incoming connections from
all of the :program:`mongos` instances in the cluster *and* all
:program:`mongod` instances in the cluster. Add rules that
resemble the following:

.. code-block:: sh

   iptables -A INPUT -s <ip-address> -p tcp --destination-port 27019 -m state --state NEW,ESTABLISHED -j ACCEPT

Replace ``<ip-address>`` with the address of the
:program:`mongos` instances and the shard :program:`mongod`
instances.

.. [#config-option] You can also run a config server by setting the
   :setting:`configsvr` option in a configuration file.

Traffic to and from a MongoDB Shard Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For shard servers, running as :option:`mongod --shardsvr`
[#shard-option]_ Because the default port number when running with
:setting:`shardsvr` is ``27018``,  you must configure the following
``iptables`` rules to allow traffic to and from each shard:

.. code-block:: sh

   iptables -A INPUT -s <ip-address> -p tcp --destination-port 27018 -m state --state NEW,ESTABLISHED -j ACCEPT
   iptables -A OUTPUT -d <ip-address> -p tcp --source-port 27018 -m state --state ESTABLISHED -j ACCEPT

Replace the ``<ip-address>`` specification with the IP address of all
:program:`mongod`. This allows you to permit incoming and outgoing
traffic between all shards including constituent replica set members,
to:

- all :program:`mongod` instances in the shard's replica sets.

- all :program:`mongod` instances in other shards. [#migrations]_

Furthermore, shards need to be able make outgoing connections to:

- all :program:`mongos` instances.

- all :program:`mongod` instances in the config servers.

Create a rule that resembles the following, and replace the
``<ip-address>`` with the address of the config servers and the
:program:`mongos` instances:

.. code-block:: sh

   iptables -A OUTPUT -d <ip-address> -p tcp --source-port 27018 -m state --state ESTABLISHED -j ACCEPT

.. [#shard-option] You can also specify the shard server option using
   the :setting:`shardsvr` setting in the configuration file. Shard
   members are also often conventional replica sets using the default
   port.

.. [#migrations] All shards in a cluster need to be able to
   communicate with all other shards to facilitate :term:`chunk` and
   balancing operations.

Provide Access For Monitoring Systems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. The :program:`mongostat` diagnostic tool, when running with the
   :option:`--discover <mongostat --discover>` needs to be able to
   reach all components of a cluster, including the config servers,
   the shard servers, and the :program:`mongos` instances.

#. If your monitoring system needs access the HTTP interface, insert
   the following rule to the chain:

   .. code-block:: sh

      iptables -A INPUT -s <ip-address> -p tcp --destination-port 28017 -m state --state NEW,ESTABLISHED -j ACCEPT

   Replace ``<ip-address>`` with the address of the instance that
   needs access to the HTTP or REST interface. For *all* deployments,
   you should restrict access to this port to *only* the monitoring
   instance.

   .. optional::

      For shard server :program:`mongod` instances running with
      :setting:`shardsvr`, the rule would resemble the following:

      .. code-block:: sh

         iptables -A INPUT -s <ip-address> -p tcp --destination-port 28018 -m state --state NEW,ESTABLISHED -j ACCEPT

      For config server :program:`mongod` instances running with
      :setting:`configsvr`, the rule would resemble the following:

      .. code-block:: sh

         iptables -A INPUT -s <ip-address> -p tcp --destination-port 28019 -m state --state NEW,ESTABLISHED -j ACCEPT

.. _iptables-change-default-policy-to-drop:

Change Default Policy to ``DROP``
---------------------------------

The default policy for ``iptables`` chains is to allow all
traffic. After completing all ``iptables`` configuration changes, you
*must* change the default policy to ``DROP`` so that all traffic that
isn't explicitly allowed as above will not be able to reach components
of the MongoDB deployment. Issue the following commands to change this
policy:

.. code-block:: sh

   iptables -P INPUT DROP

   iptables -P OUTPUT DROP

Manage and Maintain ``iptables`` Configuration
----------------------------------------------

This section contains a number of basic operations for managing and
using ``iptables``. There are various front end tools that automate
some aspects of ``iptables`` configuration, but at the core all
``iptables`` front ends provide the same basic functionality:

.. _iptables-make-all-rules-persistent:

Make all ``iptables`` Rules Persistent
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default all ``iptables`` rules are only stored in memory. When
your system restarts, your firewall rules will revert to their
defaults. When you have tested a rule set and have guaranteed that it
effectively controls traffic you can use the following operations to
you should make the rule set persistent.

On Red Hat Enterprise Linux, Fedora Linux, and related distributions
you can issue the following command:

.. code-block:: sh

   service iptables save

On Debian, Ubuntu, and related distributions, you can use the
following command to dump the ``iptables`` rules to the
``/etc/iptables.conf`` file:

.. code-block:: sh

   iptables-save > /etc/iptables.conf

Run the following operation to restore the network rules:

.. code-block:: sh

   iptables-restore < /etc/iptables.conf

Place this command in your ``rc.local`` file, or in the
``/etc/network/if-up.d/iptables`` file with other similar operations.

List all ``iptables`` Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To list all of currently applied ``iptables`` rules, use the following
operation at the system shell.

.. code-block:: sh

   iptables --L

Flush all ``iptables`` Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you make a configuration mistake when entering ``iptables`` rules
or simply need to revert to the default rule set, you can use the
following operation at the system shell to flush all rules:

.. code-block:: sh

   iptables --F

If you've already made your ``iptables`` rules persistent, you will
need to repeat the appropriate procedure in the
:ref:`iptables-make-all-rules-persistent` section.
===================================
Configure a Secondary's Sync Target
===================================

.. default-domain:: mongodb

To override the default sync target selection logic, you may manually
configure a :term:`secondary` member's sync target for pulling
:term:`oplog` entries temporarily. The following operations provide
access to this functionality:

- :dbcommand:`replSetSyncFrom` command, or

- :method:`rs.syncFrom()` helper in the :program:`mongo` shell

Only modify the default sync logic as needed, and always exercise
caution.  :method:`rs.syncFrom()` will not affect an in-progress
initial sync operation. To affect the sync target for the initial sync, run
:method:`rs.syncFrom()` operation *before* initial sync.

If you run :method:`rs.syncFrom()` during initial sync, MongoDB
produces no error messages, but the sync target will not change until
after the initial sync operation.

.. note::

   .. include:: /includes/fact-replica-set-sync-from-is-temporary.rst
.. index:: replica set; tag sets
.. index:: read preference; tag sets
.. index:: tag sets; configuration
.. _replica-set-configuration-tag-sets:

==============================
Configure Replica Set Tag Sets
==============================

.. default-domain:: mongodb

Tag sets let you customize :term:`write concern` and :term:`read
preferences <read preference>` for a :term:`replica set`. MongoDB
stores tag sets in the replica set configuration object, which is the
document returned by :method:`rs.conf()`, in the :data:`members[n].tags
<local.system.replset.members[n].tags>` sub-document.

This section introduces the configuration of tag sets. For an
overview on tag sets and their use, see
:ref:`Replica Set Write Concern <replica-set-write-concern>` and
:ref:`replica-set-read-preference-tag-sets`.

Differences Between Read Preferences and Write Concerns
-------------------------------------------------------

Custom read preferences and write concerns evaluate tags sets in
different ways:

- Read preferences consider the value of a tag when selecting a
  member to read from.

- Write concerns do not use the value of a tag to select a member
  except to consider whether or not the value is unique.

For example, a tag set for a read operation may resemble the following
document:

.. code-block:: javascript

   { "disk": "ssd", "use": "reporting" }

To fulfill such a read operation, a member would need to have both of these tags.
Any of the following tag sets would satisfy this requirement:

.. code-block:: javascript

   { "disk": "ssd", "use": "reporting" }
   { "disk": "ssd", "use": "reporting", "rack": "a" }
   { "disk": "ssd", "use": "reporting", "rack": "d" }
   { "disk": "ssd", "use": "reporting", "mem": "r"}

The following tag sets would *not* be able to fulfill this query:

.. code-block:: javascript

   { "disk": "ssd" }
   { "use": "reporting" }
   { "disk": "ssd", "use": "production" }
   { "disk": "ssd", "use": "production", "rack": "k" }
   { "disk": "spinning", "use": "reporting", "mem": "32" }

Add Tag Sets to a Replica Set
-----------------------------

Given the following replica set configuration:

.. code-block:: javascript

   {
       "_id" : "rs0",
       "version" : 1,
       "members" : [
                {
                        "_id" : 0,
                        "host" : "mongodb0.example.net:27017"
                },
                {
                        "_id" : 1,
                        "host" : "mongodb1.example.net:27017"
                },
                {
                        "_id" : 2,
                        "host" : "mongodb2.example.net:27017"
                }
        ]
   }

You could add tag sets to the members of this replica set
with the following command sequence in the :program:`mongo` shell:

.. code-block:: javascript

   conf = rs.conf()
   conf.members[0].tags = { "dc": "east", "use": "production"  }
   conf.members[1].tags = { "dc": "east", "use": "reporting"  }
   conf.members[2].tags = { "use": "production"  }
   rs.reconfig(conf)

After this operation the output of :method:`rs.conf()` would
resemble the following:

.. code-block:: javascript

    {
        "_id" : "rs0",
        "version" : 2,
        "members" : [
                 {
                         "_id" : 0,
                         "host" : "mongodb0.example.net:27017",
                         "tags" : {
                                 "dc": "east",
                                 "use": "production"
                         }
                 },
                 {
                         "_id" : 1,
                         "host" : "mongodb1.example.net:27017",
                         "tags" : {
                                 "dc": "east",
                                 "use": "reporting"
                         }
                 },
                 {
                         "_id" : 2,
                         "host" : "mongodb2.example.net:27017",
                         "tags" : {
                                 "use": "production"
                         }
                 }
         ]
    }

.. include:: /includes/fact-tag-sets-must-be-strings.rst

Custom Multi-Datacenter Write Concerns
--------------------------------------

Given a five member replica set with members in two data centers:

1. a facility ``VA``  tagged ``dc.va``

2. a facility ``GTO`` tagged ``dc.gto``

Create a custom write concern to require confirmation from two
data centers using replica set tags, using the following sequence
of operations in the :program:`mongo` shell:

#. Create a replica set configuration JavaScript object ``conf``:

   .. code-block:: javascript

      conf = rs.conf()

#. Add tags to the replica set members reflecting their locations:

   .. code-block:: javascript

      conf.members[0].tags = { "dc.va": "rack1"}
      conf.members[1].tags = { "dc.va": "rack2"}
      conf.members[2].tags = { "dc.gto": "rack1"}
      conf.members[3].tags = { "dc.gto": "rack2"}
      conf.members[4].tags = { "dc.va": "rack1"}
      rs.reconfig(conf)

#. Create a custom
   :data:`~local.system.replset.settings.getLastErrorModes` setting to
   ensure that the write operation will propagate to at least one member
   of each facility:

   .. code-block:: javascript

      conf.settings = { getLastErrorModes: { MultipleDC : { "dc.va": 1, "dc.gto": 1}}

#. Reconfigure the replica set using the modified ``conf`` configuration
   object:

   .. code-block:: javascript

      rs.reconfig(conf)

To ensure that a write operation propagates to at least one member of
the set in both data centers, use the ``MultipleDC`` write concern
mode as follows:

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "MultipleDC" } )

Alternatively, if you want to ensure that each write operation
propagates to at least 2 racks in each facility, reconfigure the
replica set as follows in the :program:`mongo` shell:

#. Create a replica set configuration object ``conf``:

   .. code-block:: javascript

      conf = rs.conf()

#. Redefine the
   :data:`~local.system.replset.settings.getLastErrorModes` value to
   require two different values of both ``dc.va`` and ``dc.gto``:

   .. code-block:: javascript

      conf.settings = { getLastErrorModes: { MultipleDC : { "dc.va": 2, "dc.gto": 2}}

#. Reconfigure the replica set using the modified ``conf`` configuration
   object:

   .. code-block:: javascript

      rs.reconfig(conf)

Now, the following write concern operation will only return after the
write operation propagates to at least two different racks in the
each facility:

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "MultipleDC" } )

Configure Tag Sets for Functional Segregation of Read and Write Operations
--------------------------------------------------------------------------

Given a replica set with tag sets that reflect:

- data center facility,

- physical rack location of instance, and

- storage system (i.e. disk) type.

Where each member of the set has a tag set that resembles one of the
following: [#read-and-write-tags]_

.. code-block:: javascript

   {"dc.va": "rack1", disk:"ssd", ssd: "installed" }
   {"dc.va": "rack2", disk:"raid"}
   {"dc.gto": "rack1", disk:"ssd", ssd: "installed" }
   {"dc.gto": "rack2", disk:"raid"}
   {"dc.va": "rack1", disk:"ssd", ssd: "installed" }

To target a read operation to a member of the replica set with a
disk type of ``ssd``, you could use the following tag set:

.. code-block:: javascript

   { disk: "ssd" }

However, to create comparable write concern modes, you would specify a
different set of
:data:`~local.system.replset.settings.getLastErrorModes`
configuration. Consider the following sequence of operations in
the :program:`mongo` shell:

#. Create a replica set configuration object ``conf``:

   .. code-block:: javascript

      conf = rs.conf()

#. Redefine the
   :data:`~local.system.replset.settings.getLastErrorModes` value to
   configure two write concern modes:

   .. code-block:: javascript

      conf.settings = {
                       "getLastErrorModes" : {
                               "ssd" : {
                                          "ssd" : 1
                               },
                               "MultipleDC" : {
                                       "dc.va" : 1,
                                      "dc.gto" : 1
                               }
                       }
                     }

#. Reconfigure the replica set using the modified ``conf`` configuration
   object:

   .. code-block:: javascript

      rs.reconfig(conf)

Now you can specify the ``MultipleDC`` write concern mode, as in the
following operation, to ensure that a write operation propagates to
each data center.

.. code-block:: javascript

   db.runCommand( { getLastError: 1, w: "MultipleDC" } )

Additionally, you can specify the ``ssd`` write concern mode to ensure that a write operation propagates
to at least one instance with an SSD.

.. [#read-and-write-tags] Since read preferences and write concerns
   use the value of fields in tag sets differently, larger
   deployments may have some redundancy.
=======================================
Prevent Secondary from Becoming Primary
=======================================

.. default-domain:: mongodb

To prevent a :term:`secondary` member from ever becoming a
:term:`primary` in a :term:`failover`, assign the secondary a priority
of ``0``, as described here. You can set this "secondary-only mode" for
any member of the :term:`replica set`, except the current primary. For a
detailed description of secondary-only members and their purposes, see
:doc:`/core/replica-set-priority-0-member`.

To configure a member as secondary-only, set its
:data:`~local.system.replset.members[n].priority` value to ``0`` in
the :data:`~local.system.replset.members` document in its replica set
configuration. Any member with a
:data:`~local.system.replset.members[n].priority` equal to ``0`` will
never seek :ref:`election <replica-set-elections>` and cannot become
primary in any situation.

.. code-block:: javascript
   :emphasize-lines: 4

   {
      "_id" : <num>,
      "host" : <hostname:port>,
      "priority" : 0
   }

MongoDB does not permit the current :term:`primary` to have a priority
of ``0``. To prevent the current primary from again becoming a primary,
you must first step down the current primary using
:method:`rs.stepDown()`, and then you must :ref:`reconfigure the replica
set <replica-set-reconfiguration-usage>` with :method:`rs.conf()` and
:method:`rs.reconfig()`.

Example
-------

As an example of modifying member priorities, assume a four-member
replica set. Use the following sequence of operations to modify member
priorities in the :program:`mongo` shell connected to the primary.
Identify each member by its array index in the
:data:`~local.system.replset.members` array:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.members[0].priority = 2
   cfg.members[1].priority = 1
   cfg.members[2].priority = 0.5
   cfg.members[3].priority = 0
   rs.reconfig(cfg)

The sequence of operations reconfigures the set with the following
priority settings:

- Member at ``0`` has a priority of ``2`` so that it becomes primary under
  most circumstances.

- Member at ``1`` has a priority of ``1``, which is the default value.
  Member ``1`` becomes primary if no member with a *higher* priority is
  eligible.

- Member at ``2`` has a priority of ``0.5``, which makes it less likely to
  become primary than other members but doesn't prohibit the
  possibility.

- Member at ``3`` has a priority of ``0``.
  Member at ``3`` **cannot** become the :term:`primary` member under any
  circumstances.

.. include:: /includes/fact-rs-conf-array-index.rst

.. include:: /includes/warning-rs-reconfig.rst

Related Documents
-----------------

- :data:`~local.system.replset.members[n].priority`

- :doc:`/tutorial/adjust-replica-set-member-priority`

- :ref:`Replica Set Reconfiguration <replica-set-reconfiguration-usage>`

- :doc:`/core/replica-set-elections`
.. index:: balancing; configure

==========================================================
Configure Behavior of Balancer Process in Sharded Clusters
==========================================================

.. default-domain:: mongodb

The balancer is a process that runs on *one* of the :program:`mongos`
instances in a cluster and ensures that :term:`chunks <chunk>` are
evenly distributed throughout a sharded cluster. In most deployments,
the default balancer configuration is sufficient for normal
operation. However, administrators might need to modify balancer
behavior depending on application or operational requirements. If you
encounter a situation where you need to modify the behavior of the
balancer, use the procedures described in this document.

For conceptual information about the balancer, see
:ref:`sharding-balancing` and :ref:`sharding-balancing-internals`.

.. _sharded-cluster-config-balancing-window:

Schedule a Window of Time for Balancing to Occur
------------------------------------------------

You can schedule a window of time during which the balancer can
migrate chunks, as described in the following procedures:

- :ref:`sharding-schedule-balancing-window`

- :ref:`sharding-balancing-remove-window`.

The :program:`mongos` instances user their own local timezones to when
respecting balancer window.

.. _sharded-cluster-config-default-chunk-size:

Configure Default Chunk Size
----------------------------

The default chunk size for a sharded cluster is 64 megabytes. In most
situations, the default size is appropriate for splitting and migrating
chunks. For information on how chunk size affects deployments, see
details, see :ref:`sharding-chunk-size`.

Changing the default chunk size affects chunks that are processes during
migrations and auto-splits but does not retroactively affect all chunks.

To configure default chunk size, see :doc:`modify-chunk-size-in-sharded-cluster`.

.. _sharded-cluster-config-max-shard-size:

Change the Maximum Storage Size for a Given Shard
-------------------------------------------------

The ``maxSize`` field in the :data:`~config.shards` collection in the
:ref:`config database <config-database>` sets the maximum size for a
shard, allowing you to control whether the balancer will migrate chunks
to a shard. If :data:`~serverStatus.mem.mapped` size [#local-limitation]_ is above a shard's
``maxSize``, the balancer will not move chunks to the shard. Also, the
balancer will not move chunks off an overloaded shard. This must happen
manually. The ``maxSize`` value only affects the balancer's selection of
destination shards.

By default, ``maxSize`` is not specified, allowing shards to consume the
total amount of available space on their machines if necessary.

You can set ``maxSize`` both when adding a shard and once a shard is
running.

To set ``maxSize`` when adding a shard, set the :dbcommand:`addShard`
command's ``maxSize`` parameter to the maximum size in megabytes. For
example, the following command run in the :program:`mongo` shell adds a
shard with a maximum size of 125 megabytes:

.. code-block:: javascript

   db.runCommand( { addshard : "example.net:34008", maxSize : 125 } )

To set ``maxSize`` on an existing shard, insert or update the
``maxSize`` field in the :data:`~config.shards` collection in the
:ref:`config database <config-database>`. Set the ``maxSize`` in
megabytes.

.. example::

   Assume you have the following shard without a ``maxSize`` field:

   .. code-block:: javascript

      { "_id" : "shard0000", "host" : "example.net:34001" }

   Run the following sequence of commands in the :program:`mongo` shell
   to insert a ``maxSize`` of 125 megabytes:

   .. code-block:: javascript

      use config
      db.shards.update( { _id : "shard0000" }, { $set : { maxSize : 125 } } )

   To later increase the ``maxSize`` setting to 250 megabytes, run the
   following:

   .. code-block:: javascript

      use config
      db.shards.update( { _id : "shard0000" }, { $set : { maxSize : 250 } } )

.. [#local-limitation] This value includes the mapped size of all data
   files including the``local`` and ``admin`` databases. Account for
   this when setting ``maxSize``.

.. index:: balancing; secondary throttle
.. index:: secondary throttle
.. _sharded-cluster-config-secondary-throttle:

Change Replication Behavior for Chunk Migration (Secondary Throttle)
--------------------------------------------------------------------

The ``_secondaryThrottle`` parameter of the balancer and the
:dbcommand:`moveChunk` command affects the replication behavior during
:ref:`chunk migration <chunk-migration-replication>`. By default,
``_secondaryThrottle`` is ``true``, which means each document move
during chunk migration propagates to at least one secondary before the
balancer proceeds with its next operation. For more information on the
replication behavior during various steps of chunk migration, see
:ref:`chunk-migration-replication`.

To change the balancer's ``_secondaryThrottle`` value, connect to a
:program:`mongos` instance and directly update the
``_secondaryThrottle`` value in the :data:`~config.settings` collection
of the :ref:`config database <config-database>`. For example, from a
:program:`mongo` shell connected to a :program:`mongos`, issue the
following command:

.. code-block:: javascript

   use config
   db.settings.update(
      { "_id" : "balancer" },
      { $set : { "_secondaryThrottle" : true } },
      { upsert : true }
   )

The effects of changing the ``_secondaryThrottle`` value may not be
immediate. To ensure an immediate effect, stop the balancer and restart
it with the selected value of ``_secondaryThrottle``. See
:doc:`/tutorial/manage-sharded-cluster-balancer` for details.
===========================
Connect to MongoDB with SSL
===========================

.. default-domain:: mongodb

This document outlines the use and operation of MongoDB's SSL
support. SSL allows MongoDB clients to support encrypted connections
to :program:`mongod` instances.

.. note:: The `default distribution of MongoDB`_ does **not** contain
   support for SSL. To use SSL, you must either build MongoDB locally
   passing the ``--ssl`` option to ``scons`` or use `MongoDB
   Enterprise`_.

These instructions outline the process for getting started with SSL and
assume that you have already installed a build of MongoDB that includes
SSL support and that your client driver supports SSL. For instructions
on upgrading a cluster currently not using SSL to using SSL, see
:doc:`/tutorial/upgrade-cluster-to-ssl`.

.. versionchanged:: 2.6

   - MongoDB's SSL encryption only allows use of strong SSL ciphers
     with a minimum of 128-bit key length for all connections.

   - MongoDB Enterprise for Windows includes support for SSL.

.. _`default distribution of MongoDB`: http://www.mongodb.org/downloads
.. _`MongoDB Enterprise`: http://www.mongodb.com/products/mongodb-enterprise

Configure ``mongod`` and ``mongos`` for SSL
-------------------------------------------

Combine SSL Certificate and Key File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before you can use SSL, you must have a :file:`.pem` file that
contains the public key certificate and private key. MongoDB can use
any valid SSL certificate. To generate a self-signed certificate and
private key, use a command that resembles the following:

.. code-block:: sh

   cd /etc/ssl/
   openssl req -new -x509 -days 365 -nodes -out mongodb-cert.crt -keyout mongodb-cert.key

This operation generates a new, self-signed certificate with no
passphrase that is valid for 365 days. Once you have the certificate,
concatenate the certificate and private key to a :file:`.pem` file, as
in the following example:

.. code-block:: sh

   cat mongodb-cert.key mongodb-cert.crt > mongodb.pem

.. _ssl-mongod-ssl-cert-key:

Set Up ``mongod`` and ``mongos`` with SSL Certificate and Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To use SSL in your MongoDB deployment, include the following run-time
options with :program:`mongod` and :program:`mongos`:

- :setting:`net.ssl.mode` set to ``requireSSL``. This setting restricts each
  server to use only SSL encrypted connections. You can also specify
  either the value ``allowSSL`` or ``preferSSL`` to set up the use of
  mixed SSL modes on a port. See :setting:`net.ssl.mode` for details.

- :setting:`~net.ssl.PEMKeyfile` with the :file:`.pem` file that contains the
  SSL certificate and key.

Consider the following syntax for :program:`mongod`:

.. code-block:: sh

   mongod --sslMode requireSSL --sslPEMKeyFile <pem>

For example, given an SSL certificate located at
:file:`/etc/ssl/mongodb.pem`, configure :program:`mongod` to use SSL
encryption for all connections with the following command:

.. code-block:: sh

   mongod --sslMode requireSSL --sslPEMKeyFile /etc/ssl/mongodb.pem

.. note::

   - Specify ``<pem>`` with the full path name to the certificate.

     ..  relative paths aren't supported with --fork because of a
         server issue.

   - If the private key portion of the ``<pem>`` is encrypted, specify
     the passphrase. See :ref:`ssl-certificate-password`.

   - You may also specify these options in the :doc:`configuration file
     </reference/configuration-options>`, as in the following example:

     .. code-block:: ini

        sslMode = requireSSL
        sslPEMKeyFile = /etc/ssl/mongodb.pem

To connect, to :program:`mongod` and :program:`mongos` instances using
SSL, the :program:`mongo` shell and MongoDB tools must include the
``--ssl`` option. See :ref:`ssl-clients` for more information on
connecting to :program:`mongod` and :program:`mongos` running with SSL.

.. seealso:: :doc:`/tutorial/upgrade-cluster-to-ssl`

.. _ssl-mongod-ca-signed-ssl-cert-key:

Set Up ``mongod`` and ``mongos`` with Certificate Validation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To set up :program:`mongod` or :program:`mongos` for SSL encryption
using an SSL certificate signed by a certificate authority, include the
following run-time options during startup:

- :setting:`net.ssl.mode` set to ``requireSSL``. This setting restricts each
  server to use only SSL encrypted connections. You can also specify
  either the value ``allowSSL`` or ``preferSSL`` to set up the use of
  mixed SSL modes on a port. See :setting:`net.ssl.mode` for details.

- :setting:`~net.ssl.PEMKeyfile` with the name of the :file:`.pem` file that
  contains the signed SSL certificate and key.

- :setting:`~net.ssl.CAFile` with the name of the :file:`.pem` file that
  contains the root certificate chain from the Certificate Authority.

Consider the following syntax for :program:`mongod`:

.. code-block:: sh

   mongod --sslMode requireSSL --sslPEMKeyFile <pem> --sslCAFile <ca>

For example, given a signed SSL certificate located at
:file:`/etc/ssl/mongodb.pem` and the certificate authority file at
:file:`/etc/ssl/ca.pem`, you can configure :program:`mongod` for SSL
encryption as follows:

.. code-block:: sh

   mongod --sslMode requireSSL --sslPEMKeyFile /etc/ssl/mongodb.pem --sslCAFile /etc/ssl/ca.pem

.. note::

   - Specify the ``<pem>`` file and the ``<ca>`` file with either the
     full path name or the relative path name.

   - If the ``<pem>`` is encrypted, specify the passphrase. See
     :ref:`ssl-certificate-password`.

   - You may also specify these options in the :doc:`configuration file
     </reference/configuration-options>`, as in the following example:

     .. code-block:: ini

        sslMode = requireSSL
        sslPEMKeyFile = /etc/ssl/mongodb.pem
        sslCAFile = /etc/ssl/ca.pem

To connect, to :program:`mongod` and :program:`mongos` instances using
SSL, the :program:`mongo` tools must include the both the
:option:`--ssl <mongo --ssl>` and
:option:`--sslPEMKeyFile <mongo --sslPEMKeyFile>` option.
See :ref:`ssl-clients` for more information on connecting to
:program:`mongod` and :program:`mongos` running with SSL.

.. seealso:: :doc:`/tutorial/upgrade-cluster-to-ssl`

Block Revoked Certificates for Clients
``````````````````````````````````````

To prevent clients with revoked certificates from connecting, include
the :setting:`sslCRLFile` to specify a :file:`.pem` file that contains
revoked certificates.

For example, the following :program:`mongod` with SSL configuration
includes the :setting:`sslCRLFile` setting:

.. code-block:: sh

   mongod --sslMode requireSSL --sslCRLFile /etc/ssl/ca-crl.pem --sslPEMKeyFile /etc/ssl/mongodb.pem --sslCAFile /etc/ssl/ca.pem

Clients with revoked certificates in the :file:`/etc/ssl/ca-crl.pem`
will not be able to connect to this :program:`mongod` instance.

.. _ssl-mongod-weak-certification:

Validate Only if a Client Presents a Certificate
````````````````````````````````````````````````

In most cases it is important to ensure that clients present valid
certificates. However, if you have clients that cannot present a
client certificate, or are transitioning to using a certificate
authority you may only want to validate certificates from clients that
present a certificate.

If you want to bypass validation for clients that don't present
certificates, include the :setting:`~net.ssl.weakCertificateValidation`
run-time option with :program:`mongod` and :program:`mongos`. If the
client does not present a certificate, no validation occurs. These
connections, though not validated, are still encrypted using SSL.

For example, consider the following :program:`mongod` with an SSL
configuration that includes the :setting:`~net.ssl.weakCertificateValidation`
setting:

.. code-block:: sh

   mongod --sslMode requireSSL --sslWeakCertificateValidation --sslPEMKeyFile /etc/ssl/mongodb.pem --sslCAFile /etc/ssl/ca.pem

Then, clients can connect either with the option :option:`--ssl <mongo --ssl>`
and **no** certificate or with the option :option:`--ssl <mongo --ssl>`
and a **valid** certificate. See :ref:`ssl-clients` for more
information on SSL connections for clients.

.. note::

   If the client presents a certificate, the certificate must be a
   valid certificate.

   All connections, including those that have not presented
   certificates are encrypted using SSL.

Run in FIPS Mode
~~~~~~~~~~~~~~~~

`MongoDB Enterprise`_ supports running in FIPS mode.

If your :program:`mongod` or :program:`mongos` is running on a system
with an OpenSSL library configured with the FIPS 140-2 module, you can
run :program:`mongod` or :program:`mongos` in FIPS mode, with the
:setting:`~net.ssl.FIPSMode` setting.

For Red Hat Enterprise Linux 6.x (RHEL 6.x) or its derivatives such as
CentOS 6.x, the OpenSSL toolkit must be at least
``openssl-1.0.1e-16.el6_5`` to run in FIPS mode. To upgrade the toolkit
for these platforms, issue the following command:

.. code-block:: javascript

   yum update openssl

.. _ssl-certificate-password:

SSL Certificate Passphrase
~~~~~~~~~~~~~~~~~~~~~~~~~~

The PEM files for :setting:`~net.ssl.PEMKeyfile` and
:setting:`~net.ssl.ClusterFile` may be encrypted. With encrypted PEM files,
you must specify the passphrase at startup with a command-line or a
configuration file option or enter the passphrase when prompted.

.. versionchanged:: 2.6

   In previous versions, you can only specify the passphrase with a
   command-line or a configuration file option.

To specify the passphrase in clear text on the command line or in a
configuration file, use the :setting:`~net.ssl.PEMKeyPassword` and/or the
:setting:`~net.ssl.ClusterPassword` option.

To have MongoDB prompt for the passphrase at the start of
:program:`mongod` or :program:`mongos` and avoid specifying the
passphrase in clear text, omit the :setting:`~net.ssl.PEMKeyPassword` and/or
the :setting:`~net.ssl.ClusterPassword` option. MongoDB will prompt for each
passphrase as necessary.

.. important:: The passphrase prompt option is available if you run the
   MongoDB instance in the foreground with a connected terminal. If you
   run :program:`mongod` or :program:`mongos` in a non-interactive
   session (e.g. without a terminal or as a service on Windows),
   you cannot use the passphrase prompt option.

.. _ssl-clients:

SSL Configuration for Clients
-----------------------------

Clients must have support for SSL to work with a :program:`mongod` or a
:program:`mongos` instance that has SSL support enabled. The current
versions of the Python, Java, Ruby, Node.js, .NET, and C++ drivers have
support for SSL, with full support coming in future releases of other
drivers.

.. _mongo-shell-ssl-connect:

``mongo`` SSL Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~~~

For SSL connections, you must use the :program:`mongo` shell built with
SSL support or distributed with MongoDB Enterprise. To support SSL,
:program:`mongo` has the following settings:

- :option:`--ssl`

- :setting:`--sslPEMKeyFile <sslPEMKeyFile>` with the name of the
  :file:`.pem` file that contains the SSL certificate and key.

- :setting:`--sslCAFile <sslCAFile>` with the name of the :file:`.pem`
  file that contains the certificate from the Certificate Authority.

- :setting:`--sslPEMKeyPassword <sslPEMKeyPassword>` option if the
  client certificate-key file is encrypted.

Connect to MongoDB Instance with SSL Encryption
```````````````````````````````````````````````

To connect to a :program:`mongod` or :program:`mongos` instance that
requires :ref:`only a SSL encryption mode <ssl-mongod-ssl-cert-key>`,
start :program:`mongo` shell with :option:`--ssl <mongo --ssl>`, as in
the following:

.. code-block:: sh

   mongo --ssl

Connect to MongoDB Instance that Requires Client Certificates
`````````````````````````````````````````````````````````````

To connect to a :program:`mongod` or :program:`mongos` that requires
:ref:`CA-signed client certificates
<ssl-mongod-ca-signed-ssl-cert-key>`, start the :program:`mongo` shell with
:option:`--ssl <mongo --ssl>` and the :setting:`--sslPEMKeyFile
<sslPEMKeyFile>` option to specify the signed certificate-key file, as
in the following:

.. code-block:: sh

   mongo --ssl --sslPEMKeyFile /etc/ssl/client.pem

Connect to MongoDB Instance that Validates when Presented with a Certificate
````````````````````````````````````````````````````````````````````````````

To connect to a :program:`mongod` or :program:`mongos` instance that
:ref:`only requires valid certificates when the client presents a certificate
<ssl-mongod-weak-certification>`, start :program:`mongo` shell either
with the :option:`--ssl <mongo --ssl>` ssl and **no** certificate or
with the :option:`--ssl <mongo --ssl>` ssl and a **valid** signed
certificate.

For example, if :program:`mongod` is running with weak certificate
validation, both of the following :program:`mongo` shell clients can
connect to that :program:`mongod`:

.. code-block:: sh

   mongo --ssl
   mongo --ssl --sslPEMKeyFile /etc/ssl/client.pem

.. important:: If the client presents a certificate, the certificate
   must be valid.

MMS Monitoring Agent
~~~~~~~~~~~~~~~~~~~~

The Monitoring agent will also have to connect via SSL in order to gather its
stats.  Because the agent already utilizes SSL for its communications
to the MMS servers, this is just a matter of enabling SSL support in
MMS itself on a per host basis.

Use the "Edit" host button (i.e. the pencil) on the Hosts page in the
MMS console to enable SSL.

Please see the `MMS documentation <http://mms.mongodb.com/help>`_ for more
information about MMS configuration.

PyMongo
~~~~~~~

Add the "``ssl=True``" parameter to a PyMongo
:py:class:`MongoClient <pymongo:pymongo.mongo_client.MongoClient>`
to create a MongoDB connection to an SSL MongoDB instance:

.. code-block:: python

   from pymongo import MongoClient
   c = MongoClient(host="mongodb.example.net", port=27017, ssl=True)

To connect to a replica set, use the following operation:

.. code-block:: python

   from pymongo import MongoReplicaSetClient
   c = MongoReplicaSetClient("mongodb.example.net:27017",
                             replicaSet="mysetname", ssl=True)

PyMongo also supports an "``ssl=true``" option for the MongoDB URI:

.. code-block:: none

   mongodb://mongodb.example.net:27017/?ssl=true

Java
~~~~

Consider the following example "``SSLApp.java``" class file:

.. code-block:: java

    import com.mongodb.*;
    import javax.net.ssl.SSLSocketFactory;

    public class SSLApp {

        public static void main(String args[])  throws Exception {

            MongoClientOptions o = new MongoClientOptions.Builder()
                    .socketFactory(SSLSocketFactory.getDefault())
                    .build();

            MongoClient m = new MongoClient("localhost", o);

            DB db = m.getDB( "test" );
            DBCollection c = db.getCollection( "foo" );

            System.out.println( c.findOne() );
        }
    }

Ruby
~~~~

The recent versions of the Ruby driver have support for connections
to SSL servers. Install the latest version of the driver with the
following command:

.. code-block:: sh

   gem install mongo

Then connect to a standalone instance, using the following form:

.. code-block:: javascript

   require 'rubygems'
   require 'mongo'

   connection = MongoClient.new('localhost', 27017, :ssl => true)

Replace ``connection`` with the following if you're connecting to a
replica set:

.. code-block:: ruby

   connection = MongoReplicaSetClient.new(['localhost:27017'],
                                          ['localhost:27018'],
                                          :ssl => true)

Here, :program:`mongod` instance run on "``localhost:27017``" and
"``localhost:27018``".

Node.JS (``node-mongodb-native``)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the `node-mongodb-native`_ driver, use the following invocation to
connect to a :program:`mongod` or :program:`mongos` instance via SSL:

.. code-block:: javascript

   var db1 = new Db(MONGODB, new Server("127.0.0.1", 27017,
                                        { auto_reconnect: false, poolSize:4, ssl:ssl } );

To connect to a replica set via SSL, use the following form:

.. code-block:: javascript

   var replSet = new ReplSetServers( [
       new Server( RS.host, RS.ports[1], { auto_reconnect: true } ),
       new Server( RS.host, RS.ports[0], { auto_reconnect: true } ),
       ],
     {rs_name:RS.name, ssl:ssl}
   );

.. _`node-mongodb-native`: https://github.com/mongodb/node-mongodb-native

.NET
~~~~

As of release 1.6, the .NET driver supports SSL connections with
:program:`mongod` and :program:`mongos` instances. To connect using
SSL, you must add an option to the connection string, specifying
``ssl=true`` as follows:

.. code-block:: csharp

    var connectionString = "mongodb://localhost/?ssl=true";
    var server = MongoServer.Create(connectionString);

The .NET driver will validate the certificate against the local
trusted certificate store, in addition to providing encryption of the
server. This behavior may produce issues during testing if the server
uses a self-signed certificate. If you encounter this issue, add the
``sslverifycertificate=false`` option to the connection string to
prevent the .NET driver from validating the certificate, as follows:

.. code-block:: csharp

    var connectionString = "mongodb://localhost/?ssl=true&sslverifycertificate=false";
    var server = MongoServer.Create(connectionString);

.. _mongodb-tools-support-ssl:

MongoDB Tools
~~~~~~~~~~~~~

.. versionchanged:: 2.6

Various MongoDB utility programs supports SSL. These tools include:

- :program:`mongodump`
- :program:`mongoexport`
- :program:`mongofiles`
- :program:`mongoimport`
- :program:`mongooplog`
- :program:`mongorestore`
- :program:`mongostat`
- :program:`mongotop`

.. tip:: To use SSL connections with these tools, use the same SSL
   options as the :program:`mongo` shell. See
   :ref:`mongo-shell-ssl-connect`.
================================================
Configure Windows ``netsh`` Firewall for MongoDB
================================================

.. default-domain:: mongodb

On Windows Server systems, the ``netsh`` program provides
methods for managing the :guilabel:`Windows Firewall`. These firewall rules make it possible
for administrators to control what hosts can connect to the system,
and limit risk exposure by limiting the hosts that can connect to a
system.

This document outlines basic :guilabel:`Windows Firewall` configurations.
Use these approaches as a starting point for your
larger networking organization.
For a detailed over view of security
practices and risk management for MongoDB, see
:doc:`/core/security`.

.. seealso:: `Windows Firewall <http://technet.microsoft.com/en-us/network/bb545423.aspx>`_
   documentation from Microsoft.

Overview
--------

:guilabel:`Windows Firewall` processes rules in an ordered determined
by rule type, and parsed in the following order:

#. ``Windows Service Hardening``
#. ``Connection security rules``
#. ``Authenticated Bypass Rules``
#. ``Block Rules``
#. ``Allow Rules``
#. ``Default Rules``

By default, the policy in :guilabel:`Windows Firewall` allows all outbound connections
and blocks all incoming connections.

Given the :ref:`default ports <security-port-numbers>` of all MongoDB
processes, you must configure networking rules that permit *only*
required communication between your application and the appropriate
:program:`mongod.exe` and :program:`mongos.exe` instances.

The configuration changes outlined in this document will create rules
which explicitly allow traffic from specific addresses and on specific
ports, using a default policy that drops all traffic that is not
explicitly allowed.

You can configure the :guilabel:`Windows Firewall` with using the ``netsh`` command line
tool or through a windows application.  On Windows Server 2008 this
application is :guilabel:`Windows Firewall With Advanced Security` in :guilabel:`Administrative Tools`.
On previous versions of Windows Server, access the
:guilabel:`Windows Firewall` application in the :guilabel:`System and Security` control panel.

The procedures in this document use the ``netsh`` command line tool.

Patterns
--------

This section contains a number of patterns and examples for
configuring :guilabel:`Windows Firewall` for use with MongoDB deployments.
If you have configured different ports using the :setting:`~net.port` configuration
setting, you will need to modify the rules accordingly.

.. _wfirewall-basic-rule-set:

Traffic to and from ``mongod.exe`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This pattern is applicable to all :program:`mongod.exe` instances running
as standalone instances or as part of a :term:`replica set`.
The goal of this pattern is to explicitly allow traffic to the
:program:`mongod.exe` instance from the application server.

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod port 27017" dir=in action=allow protocol=TCP localport=27017

This rule allows all incoming traffic to port ``27017``, which
allows the application server to connect to the
:program:`mongod.exe` instance.

:guilabel:`Windows Firewall` also allows enabling network access for
an entire application rather than to a specific port, as in the
following example:

.. code-block:: bat

   netsh advfirewall firewall add rule name="Allowing mongod" dir=in action=allow program=" C:\mongodb\bin\mongod.exe"

You can allow all access for a :program:`mongos.exe` server, with the
following invocation:

.. code-block:: bat

   netsh advfirewall firewall add rule name="Allowing mongos" dir=in action=allow program=" C:\mongodb\bin\mongos.exe"

Traffic to and from ``mongos.exe`` Instances
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:program:`mongos.exe` instances provide query routing for
:term:`sharded clusters <sharded cluster>`. Clients connect to :program:`mongos.exe`
instances, which behave from the client's perspective as
:program:`mongod.exe` instances. In turn, the :program:`mongos.exe`
connects to all :program:`mongod.exe` instances that are components of
the sharded cluster.

Use the same :guilabel:`Windows Firewall` command to allow traffic to
and from these instances as you would from the :program:`mongod.exe`
instances that are members of the replica set.

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod shard port 27018" dir=in action=allow protocol=TCP localport=27018

Traffic to and from a MongoDB Config Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Configuration servers, host the :term:`config database` that stores metadata
for sharded clusters. Each production cluster has three configuration
servers, initiated using the :option:`mongod --configsvr`
option. [#config-option]_ Configuration servers listen for connections on port
``27019``. As a result, add the following :guilabel:`Windows Firewall` rules to the
config server to allow incoming and outgoing connection on port
``27019``, for connection to the other config servers.

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod config svr port 27019" dir=in action=allow protocol=TCP localport=27019

Additionally, config servers need to allow incoming connections from
all of the :program:`mongos.exe` instances in the cluster *and* all
:program:`mongod.exe` instances in the cluster. Add rules that
resemble the following:

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod config svr inbound" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=27019

Replace ``<ip-address>`` with the addresses of the
:program:`mongos.exe` instances and the shard :program:`mongod.exe`
instances.

.. [#config-option] You can also run a config server by setting the
   :setting:`configsvr` option in a configuration file.

Traffic to and from a MongoDB Shard Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For shard servers, running as :option:`mongod --shardsvr`
[#shard-option]_ Because the default port number when running with
:setting:`shardsvr` is ``27018``,  you must configure the following
:guilabel:`Windows Firewall` rules to allow traffic to and from each shard:

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod shardsvr inbound" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=27018
   netsh advfirewall firewall add rule name="Open mongod shardsvr outbound" dir=out action=allow protocol=TCP remoteip=<ip-address> localport=27018

Replace the ``<ip-address>`` specification with the IP address of all
:program:`mongod.exe` instances. This allows you to permit incoming
and outgoing traffic between all shards including constituent replica
set members to:

- all :program:`mongod.exe` instances in the shard's replica sets.

- all :program:`mongod.exe` instances in other shards. [#migrations]_

Furthermore, shards need to be able make outgoing connections to:

- all :program:`mongos.exe` instances.

- all :program:`mongod.exe` instances in the config servers.

Create a rule that resembles the following, and replace the
``<ip-address>`` with the address of the config servers and the
:program:`mongos.exe` instances:

.. code-block:: bat

   netsh advfirewall firewall add rule name="Open mongod config svr outbound" dir=out action=allow protocol=TCP remoteip=<ip-address> localport=27018

.. [#shard-option] You can also specify the shard server option using
   the :setting:`shardsvr` setting in the configuration file. Shard
   members are also often conventional replica sets using the default
   port.

.. [#migrations] All shards in a cluster need to be able to
   communicate with all other shards to facilitate :term:`chunk` and
   balancing operations.

Provide Access For Monitoring Systems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#. The :program:`mongostat` diagnostic tool, when running with the
   :option:`--discover <mongostat --discover>` needs to be able to
   reach all components of a cluster, including the config servers,
   the shard servers, and the :program:`mongos.exe` instances.

#. If your monitoring system needs access the HTTP interface, insert
   the following rule to the chain:

   .. code-block:: bat

      netsh advfirewall firewall add rule name="Open mongod HTTP monitoring inbound" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=28017

   Replace ``<ip-address>`` with the address of the instance that
   needs access to the HTTP or REST interface. For *all* deployments,
   you should restrict access to this port to *only* the monitoring
   instance.

   .. optional::

      For shard server :program:`mongod.exe` instances running with
      :setting:`shardsvr`, the rule would resemble the following:

      .. code-block:: bat

         netsh advfirewall firewall add rule name="Open mongos HTTP monitoring inbound" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=28018

      For config server :program:`mongod.exe` instances running with
      :setting:`configsvr`, the rule would resemble the following:

      .. code-block:: bat

         netsh advfirewall firewall add rule name="Open mongod configsvr HTTP monitoring inbound" dir=in action=allow protocol=TCP remoteip=<ip-address> localport=28019

Manage and Maintain *Windows Firewall* Configurations
-----------------------------------------------------

This section contains a number of basic operations for managing and
using ``netsh``. While you can use the GUI front ends to manage the
:guilabel:`Windows Firewall`, all core functionality is accessible is
accessible from ``netsh``.

Delete all *Windows Firewall* Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To delete the firewall rule allowing :program:`mongod.exe` traffic:

.. code-block:: bat

   netsh advfirewall firewall delete rule name="Open mongod port 27017" protocol=tcp localport=27017

   netsh advfirewall firewall delete rule name="Open mongod shard port 27018" protocol=tcp localport=27018

List All *Windows Firewall* Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To return a list of all :guilabel:`Windows Firewall` rules:

.. code-block:: bat

   netsh advfirewall firewall show rule name=all

Reset *Windows Firewall*
~~~~~~~~~~~~~~~~~~~~~~~~

To reset the :guilabel:`Windows Firewall` rules:

.. code-block:: bat

   netsh advfirewall reset

Backup and Restore *Windows Firewall* Rules
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To simplify administration of larger collection of systems, you can export or
import firewall systems from different servers) rules very easily on Windows:

Export all firewall rules with the following command:

.. code-block:: bat

   netsh advfirewall export "C:\temp\MongoDBfw.wfw"

Replace ``"C:\temp\MongoDBfw.wfw"`` with a path of your choosing. You
can use a command in the following form to import a file created using
this operation:

.. code-block:: bat

   netsh advfirewall import "C:\temp\MongoDBfw.wfw"
===================================
Authenticate with x.509 Certificate
===================================

.. default-domain:: mongodb

.. versionadded:: 2.6

MongoDB supports x.509 certificate authentication for use with a secure
:doc:`SSL connection </tutorial/configure-ssl>`. The x.509
authentication allows :ref:`clients to authenticate to servers with
certificates <x509-client-authentication>` instead of with username and
password. The x.509 authentication also allows sharded cluster members
and replica set members to use x.509 certificates to :ref:`verify their
membership to the cluster or the replica set
<x509-internal-authentication>` instead of using :doc:`keyfiles
</core/authentication>`. The membership authentication is
an internal process.

.. _`default distribution of MongoDB`: http://www.mongodb.org/downloads
.. _`MongoDB Enterprise`: http://www.mongodb.com/products/mongodb-enterprise

.. _x509-client-authentication:

Use x.509 for Client Authentication
-----------------------------------

Client x.509 Certificate
~~~~~~~~~~~~~~~~~~~~~~~~

The client certificate must have the following
properties:

- A single Certificate Authority (CA) must issue the certificates
  for both the client and the server.

- Client certificates must contain the following fields:

  .. code-block:: none

     keyUsage = digitalSignature
     extendedKeyUsage = clientAuth

Configure MongoDB Server
~~~~~~~~~~~~~~~~~~~~~~~~

Configure the MongoDB server from the command line, as in the following:

.. code-block:: sh

   mongod --sslMode requireSSL --sslPEMKeyFile <path to SSL certificate and key PEM file> --sslCAFile <path to root CA PEM file>

You may also specify these options in the :doc:`configuration file
</reference/configuration-options>`:

.. code-block:: none

   sslMode = requireSSL
   sslPEMKeyFile = <path to SSL certificate and key PEM file>
   sslCAFile = <path to the root CA PEM file>

Include any additional options, SSL or otherwise, that are required for
your specific configuration.

.. _addX509SubjectUser:

Add x.509 Certificate ``subject`` as a User
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To authenticate with a client certificate, you must first add the value
of the ``subject`` from the client certificate as a MongoDB user.

#. You can retrieve the ``subject`` from the client certificate with
   the following command:

   .. code-block:: sh

      openssl x509 -in <pathToClient PEM> -inform PEM -subject -nameopt RFC2253

   The command returns the ``subject`` string as well as certificate:

   .. code-block:: sh

      subject= CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry
      -----BEGIN CERTIFICATE-----
      # ...
      -----END CERTIFICATE-----

#. Add the value of the ``subject``, omitting the spaces, from the
   certificate as a user.

   For example, in the :program:`mongo` shell, to add the user with
   both the ``readWrite`` role in the ``test`` database and the
   ``userAdminAnyDatabase`` role which is defined only in the ``admin``
   database:

   .. code-block:: javascript

      db.getSiblingDB("$external").runCommand(
        {
          createUser: "CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry",
          roles: [
                   { role: 'readWrite', db: 'test' },
                   { role: 'userAdminAnyDatabase', db: 'admin' }
                 ],
          writeConcern: { w: "majority" , wtimeout: 5000 }
        }
      )

   In the above example, to add the user with the ``readWrite`` role in
   the ``test`` database, the role specification document specified
   ``'test'`` in the ``db`` field. To add ``userAdminAnyDatabase``
   role for the user, the above example specified ``'admin'`` in the
   ``db`` field.

   .. note::
      Some roles are defined only in the ``admin`` database, including:
      ``clusterAdmin``, ``readAnyDatabase``, ``readWriteAnyDatabase``,
      ``dbAdminAnyDatabase``, and ``userAdminAnyDatabase``. To add a
      user with these roles, specify ``'admin'`` in the ``db``.

See :doc:`/tutorial/add-user-to-database` for details on adding a user
with roles.

Authenticate with a x.509 Certificate
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To authenticate with a client certificate, you must first add a MongoDB
user that corresponds to the client certificate. See
:ref:`addX509SubjectUser`.

To authenticate, use the :method:`db.auth()` method in the
``$external`` database, specifying ``"MONGODB-X509"`` for the
``mechanism`` field, and the :ref:`user that corresponds to the client
certificate <addX509SubjectUser>` for the ``user`` field.

For example, if using the :program:`mongo` shell,

1. Connect :program:`mongo` shell to the :program:`mongod` set up for
   SSL:

   .. code-block:: sh

      mongo --ssl --sslPEMKeyFile <path to CA signed client PEM file>

#. To perform the authentication, use the :method:`db.auth()` method in
   the ``$external`` database. For the ``mechanism`` field, specify
   ``"MONGODB-X509"``, and for the ``user`` field, specify the user, or
   the ``subject``, that corresponds to the client certificate.

   .. code-block:: javascript

      db.getSiblingDB("$external").auth(
        {
          mechanism: "MONGODB-X509",
          user: "CN=myName,OU=myOrgUnit,O=myOrg,L=myLocality,ST=myState,C=myCountry"
        }
      )

.. _x509-internal-authentication:

Use x.509 for Replica Set/Sharded Cluster Member Authentication
---------------------------------------------------------------

Member x.509 Certificate
~~~~~~~~~~~~~~~~~~~~~~~~

The member certificate, used for internal authentication to verify
membership to the sharded cluster or a replica set, must have the
following properties:

- A single Certificate Authority (CA) must issue all the x.509
  certificates for the members of a sharded cluster or a replica set.

- The member certificate's ``subject``, which contains the
  Distinguished Name (``DN``), must match the ``subject`` of the
  certificate on the other servers in the cluster, *starting from and
  including* the Organizational Unit (``OU``) of the certificate on the
  server.

- Either the Common Name (``CN``) or one of the Subject Alternative
  Name (``SAN``) entries must match the hostname of the server, used by
  the other members of the cluster.

For example, the certificates for a cluster could have the following
subjects:

.. code-block:: sh

   subject= CN=<myhostname1>,OU=Dept1,O=MongoDB,ST=NY,C=US
   subject= CN=<myhostname2>,OU=Dept1,O=MongoDB,ST=NY,C=US
   subject= CN=<myhostname3>,OU=Dept1,O=MongoDB,ST=NY,C=US

Configure Clusters
~~~~~~~~~~~~~~~~~~

To specify the x.509 certificate for internal cluster member
authentication, append the additional SSL options
:option:`--clusterAuthMode` and :option:`--sslClusterFile`, as in the
following example for a member of a replica set:

.. code-block:: sh

   mongod --replSet <name> --sslMode requireSSL --clusterAuthMode x509 --sslClusterFile <path to membership certificate and key PEM file> --sslPEMKeyFile <path to SSL certificate and key PEM file> --sslCAFile <path to root CA PEM file>

Include any additional options, SSL or otherwise, that are required for
your specific configuration. For instance, if the membership key is
encrypted, set the :option:`--sslClusterPassword` to the passphrase to
decrypt the key or have MongoDB prompt for the passphrase. See
:ref:`ssl-certificate-password` for details.

.. note::
   You may also specify these options in the :doc:`configuration file
   </reference/configuration-options>`, as in the following example:

   .. code-block:: ini

      sslMode = requireSSL
      sslPEMKeyFile = <path to SSL certificate and key PEM file>
      sslCAFile = <path to root CA PEM file>
      clusterAuthMode = x509
      sslClusterFile = <path to membership certificate and key PEM file>

.. _upgrade-to-x509-internal-authentication:

Upgrade from Keyfile Authentication to to x.509 Authentication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To upgrade clusters that are currently using keyfile authentication to
x.509 authentication, use a rolling upgrade process.

Clusters Currently Using SSL
````````````````````````````

For clusters using SSL and keyfile authentication, to upgrade to x.509
cluster authentication, use the following rolling upgrade process:

#. For each node of a cluster, start the node with the option
   :option:`--clusterAuthMode` set to ``sendKeyFile`` and the option
   :option:`--sslClusterFile` set to the appropriate path of the node's
   certificate. Include other :doc:`SSL options
   </tutorial/configure-ssl>` as well as any other options that are
   required for your specific configuration. For example:

   .. code-block:: sh

      mongod --replSet <name> --sslMode requireSSL --clusterAuthMode sendKeyFile --sslClusterFile <path to membership certificate and key PEM file> --sslPEMKeyFile <path to SSL Certificate and key PEM file>  --sslCAFile <path to root CA PEM file>

   With this setting, each node continues to use its keyfile to
   authenticate itself as a member. However, each node can now accept
   either a keyfile or an x.509 certificate from other members to
   authenticate those members. Upgrade all nodes of the cluster to
   this setting.

#. Then, for each node of a cluster, connect to the node and use the
   :dbcommand:`setParameter` command to update the :parameter:`clusterAuthMode`
   to ``sendX509``. [#update-mode-alternative]_ For example,

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, clusterAuthMode: "sendX509" } )

   With this setting, each node uses its x.509 certificate, specified
   with the :option:`--sslClusterFile` option in the previous step, to
   authenticate itself as a member. However, each node continues to
   accept either a keyfile or an x.509 certificate from other members
   to authenticate those members. Upgrade all nodes of the cluster to
   this setting.

#. Optional but recommended. Finally, for each node of the cluster,
   connect to the node and use the :dbcommand:`setParameter` command to
   update the :parameter:`clusterAuthMode` to ``x509`` to only use the
   x.509 certificate for authentication. [#update-mode-alternative]_
   For example:

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, clusterAuthMode: "x509" } )

#. After the upgrade of all nodes, edit the :doc:`configuration file
   </reference/configuration-options>` with the appropriate x.509
   settings to ensure that upon subsequent restarts, the cluster uses
   x.509 authentication.

See :option:`--clusterAuthMode` for the various modes and their
descriptions.

Clusters Currently Not Using SSL
````````````````````````````````

For clusters using keyfile authentication but not SSL, to upgrade to
x.509 authentication, use the following rolling upgrade process:

#. For each node of a cluster, start the node with the option
   :option:`--sslMode` set to ``allowSSL``, the option
   :option:`--clusterAuthMode` set to ``sendKeyFile`` and the option
   :option:`--sslClusterFile` set to the appropriate path of the node's
   certificate. Include other :doc:`SSL options
   </tutorial/configure-ssl>` as well as any other options that are
   required for your specific configuration. For example:

   .. code-block:: sh

      mongod --replSet <name> --sslMode allowSSL --clusterAuthMode sendKeyFile --sslClusterFile <path to membership certificate and key PEM file> --sslPEMKeyFile <path to SSL certificate and key PEM file>  --sslCAFile <path to root CA PEM file>

   The :option:`--sslMode allowSSL <--sslMode>` setting allows the
   node to accept both SSL and non-SSL incoming connections. Its
   outgoing connections do not use SSL.

   The :option:`--clusterAuthMode sendKeyFile <--clusterAuthMode>`
   setting allows each node continues to use its keyfile to
   authenticate itself as a member. However, each node can now accept
   either a keyfile or an x.509 certificate from other members to
   authenticate those members.

   Upgrade all nodes of the cluster to these settings.

#. Then, for each node of a cluster, connect to the node and use the
   :dbcommand:`setParameter` command to update the :parameter:`sslMode`
   to ``preferSSL`` and the :parameter:`clusterAuthMode` to
   ``sendX509``. [#update-mode-alternative]_ For example:

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, sslMode: "preferSSL", clusterAuthMode: "sendX509" } )

   With the :parameter:`sslMode` set to ``preferSSL``, the node accepts
   both SSL and non-SSL incoming connections, and its outgoing
   connections use SSL.

   With the :parameter:`clusterAuthMode` set to ``sendX509``, each node
   uses its x.509 certificate, specified with the
   :option:`--sslClusterFile` option in the previous step, to
   authenticate itself as a member. However, each node continues to
   accept either a keyfile or an x.509 certificate from other members
   to authenticate those members.

   Upgrade all nodes of the cluster to these settings.

#. Optional but recommended. Finally, for each node of the cluster,
   connect to the node and use the :dbcommand:`setParameter` command to
   update the :parameter:`sslMode` to ``requireSSL`` and the
   :parameter:`clusterAuthMode` to ``x509``. [#update-mode-alternative]_
   For example:

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, sslMode: "requireSSL", clusterAuthMode: "x509" } )

   With the :parameter:`sslMode` set to ``requireSSL``, the node only uses
   SSL connections.

   With the :parameter:`clusterAuthMode` set to ``x509``, the node only
   uses the x.509 certificate for authentication.

#. After the upgrade of all nodes, edit the :doc:`configuration file
   </reference/configuration-options>` with the appropriate SSL and
   x.509 settings to ensure that upon subsequent restarts, the cluster
   uses x.509 authentication.

See :option:`--clusterAuthMode` for the various modes and their
descriptions.

.. [#update-mode-alternative] As an alternative to using the
   :dbcommand:`setParameter` command, you can also
   restart the nodes with the appropriate SSL and x509 options and
   values.
======================================================
Deploy MongoDB on Windows with Kerberos Authentication
======================================================

.. default-domain:: mongodb

.. versionadded:: 2.6

Overview
--------

MongoDB Enterprise supports authentication using a :doc:`Kerberos
service </core/kerberos>`. Kerberos is an industry standard
authentication protocol for large client/server system. Kerberos allows
MongoDB and applications to take advantage of existing authentication
infrastructure and processes.

Prerequisites
-------------

Setting up and configuring a Kerberos deployment is beyond the scope of
this document. This tutorial assumes have configured a :ref:`Kerberos
service principal <kerberos-service-principal>` for each
:program:`mongod.exe` and :program:`mongos.exe` instance as well as
:ref:`initialize a credential cache <linux-init-mongodb-clients>` for
the :program:`mongo` shell.

Procedures
----------

.. include:: /includes/steps/control-access-to-mongodb-windows-with-kerberos-authentication.rst

Additional Considerations
-------------------------

Configure ``mongos.exe`` for Kerberos
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To start :program:`mongos.exe` with Kerberos support, set the
:program:`mongos.exe` parameter :parameter:`authenticationMechanisms`
to ``GSSAPI``. You must start :program:`mongos.exe` as the
:ref:`service principal account <assign-service-principal-name>`.:

.. code-block:: sh

   mongos.exe --setParameter authenticationMechanisms=GSSAPI <additional mongos options>

For example, the following starts a :program:`mongos` instance with
Kerberos support:

.. code-block:: sh

   mongos.exe --setParameter authenticationMechanisms=GSSAPI --configdb shard0.example.net, shard1.example.net,shard2.example.net --keyFile C:\<path>\mongos.keyfile

Modify or include any additional :program:`mongos.exe` options as required
for your configuration. For example, instead of using
:option:`--keyFile` for for internal authentication of sharded cluster
members, you can use :ref:`x.509 member authentication
<x509-internal-authentication>` instead.

.. _assign-service-principal-name:

Assign Service Principal Name to MongoDB Windows Service
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use ``setspn.exe`` to assign the service principal name (SPN) to the
account running the ``mongod.exe`` and the ``mongos.exe`` service:

.. code-block:: sh

   setspn.exe -A <service>/<fully qualified domain name> <service account name>

For example, if :program:`mongod.exe` runs as a service named
``mongodb`` on ``testserver.mongodb.com`` with the service account name
``mongodtest``, assign the SPN as follows:

.. code-block:: sh

   setspn.exe -A mongodb/testserver.mongodb.com mongodtest

.. _enable-mixed-kerberos-and-cr-windows:

Incorporate Additional Authentication Mechanisms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Kerberos authentication (``GSSAPI``) can work alongside MongoDB's
challenge/response authentication mechanism (``MONGODB-CR``), MongoDB's
authentication mechanism for LDAP (``PLAIN``), and MongoDB's
authentication mechanism for x.509 (``MONGODB-X509``). Specify the
mechanisms, as follows:

.. code-block:: sh

   --setParameter authenticationMechanisms=GSSAPI,MONGODB-CR

Only add the other mechanisms if in use. This parameter setting does
not affect MongoDB's internal authentication of cluster members.
=============================================
Deploy MongoDB with Kerberos Support on Linux
=============================================

.. default-domain:: mongodb

.. versionadded:: 2.4

Overview
--------

MongoDB Enterprise supports authentication using a :doc:`Kerberos
service </core/kerberos>`. Kerberos is an industry standard
authentication protocol for large client/server system.

Prerequisites
-------------

Setting up and configuring a Kerberos deployment is beyond the scope of
this document. This tutorial assumes you have have configured a
:ref:`Kerberos service principal <kerberos-service-principal>` for each
:program:`mongod` and :program:`mongos` instance in your MongoDB
deployment, and you have a valid :ref:`keytab file <keytab-files>` for
for each :program:`mongod` and :program:`mongos` instance.

Procedure
---------

The following procedure outlines the steps to add a Kerberos user
principal to MongoDB, configure a standalone :program:`mongod` instance
for Kerberos support, and connect using the :program:`mongo` shell and
authenticate the user principal.

.. include:: /includes/steps/control-access-to-mongodb-with-kerberos-authentication.rst

Additional Considerations
-------------------------

.. _setting-krb5_ktname:

KRB5_KTNAME
~~~~~~~~~~~

If you installed MongoDB Enterprise using one of the official ``.deb``
or ``.rpm`` packages, and you use the included init/upstart scripts to
control the :program:`mongod` instance, you can set the ``KR5_KTNAME``
variable in the default environment settings file instead of setting
the variable each time.

For ``.rpm`` packages, the default environment settings file is
:file:`/etc/sysconfig/mongod`.

For ``.deb`` packages, the file is :file:`/etc/default/mongodb`.

Set the ``KRB5_KTNAME`` value in a line that resembles the following:

.. code-block:: javascript

   export KRB5_KTNAME="<path to keytab>"

Configure ``mongos`` for Kerberos
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To start :program:`mongos` with Kerberos support, set the environmental
variable ``KRB5_KTNAME`` to the path of its :ref:`keytab file
<keytab-files>` and the :program:`mongos` parameter
:parameter:`authenticationMechanisms` to ``GSSAPI`` in the following form:

.. code-block:: sh

   env KRB5_KTNAME=<path to keytab file> \
   mongos \
   --setParameter authenticationMechanisms=GSSAPI \
   <additional mongos options>

For example, the following starts a :program:`mongos` instance with
Kerberos support:

.. code-block:: sh

   env KRB5_KTNAME=/opt/mongodb/mongos.keytab \
   mongos \
   --setParameter authenticationMechanisms=GSSAPI \
   --configdb shard0.example.net, shard1.example.net,shard2.example.net \
   --keyFile /opt/mongodb/mongos.keyfile

The path to your :program:`mongos` as well as your :ref:`keytab file
<keytab-files>` may differ. The :ref:`keytab file <keytab-files>` must
be only accessible to the owner of the :program:`mongos` process.

Modify or include any additional :program:`mongos` options as required
for your configuration. For example, instead of using
:option:`--keyFile` for for internal authentication of sharded cluster
members, you can use :ref:`x.509 member authentication
<x509-internal-authentication>` instead.

Use a Config File
~~~~~~~~~~~~~~~~~

To configure :program:`mongod` or :program:`mongos` for Kerberos
support using a :doc:`configuration file
</reference/configuration-options>`, specify the
:parameter:`authenticationMechanisms` setting in the configuration file:

.. code-block:: sh

   setParameter=authenticationMechanisms=GSSAPI

Modify or include any additional :program:`mongod` options as required
for your configuration.

For example, if :file:`/opt/mongodb/mongod.conf` contains the following configuration settings for
a standalone :program:`mongod`:

.. code-block:: sh

   auth = true
   setParameter=authenticationMechanisms=GSSAPI
   dbpath=/opt/mongodb/data

To start :program:`mongod` with Kerberos support, use the following
form:

.. code-block:: sh

   env KRB5_KTNAME=/opt/mongodb/mongod.keytab \
   /opt/mongodb/bin/mongod --config /opt/mongodb/mongod.conf

The path to your :program:`mongod`, :ref:`keytab file <keytab-files>`,
and configuration file may differ. The
:ref:`keytab file <keytab-files>` must be only accessible to the owner
of the :program:`mongod` process.

Troubleshoot Kerberos Setup for MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you encounter problems when starting :program:`mongod` or
:program:`mongos` with Kerberos authentication, see
:doc:`/tutorial/troubleshoot-kerberos`.

.. _enable-mixed-kerberos-and-cr:

Incorporate Additional Authentication Mechanisms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Kerberos authentication (``GSSAPI``) can work alongside MongoDB's
challenge/response authentication mechanism (``MONGODB-CR``), MongoDB's
authentication mechanism for LDAP (``PLAIN``), and MongoDB's
authentication mechanism for x.509 (``MONGODB-X509``). Specify the
mechanisms, as follows:

.. code-block:: sh

   --setParameter authenticationMechanisms=GSSAPI,MONGODB-CR

Only add the other mechanisms if in use. This parameter setting does
not affect MongoDB's internal authentication of cluster members.
===================================
Control Search Results with Weights
===================================

.. default-domain:: mongodb


This document describes how to create a ``text`` index with specified
weights for results fields.

For a ``text`` index, the *weight* of an indexed field denotes the
significance of the field relative to the other indexed fields in terms
of the score. The score for a given word in a document is derived from
the weighted sum of the frequency for each of the indexed fields in
that document. See :projection:`$meta` operator for details on
returning and sorting by text scores.

The default weight is 1 for the indexed fields. To adjust the weights
for the indexed fields, include the ``weights`` option in the
:method:`db.collection.ensureIndex()` method.

.. warning::

   Choose the weights carefully in order to prevent the need to reindex.

A collection ``blog`` has the following documents:

.. code-block:: javascript

   { _id: 1,
     content: "This morning I had a cup of coffee.",
     about: "beverage",
     keywords: [ "coffee" ]
   }

   { _id: 2,
     content: "Who doesn't like cake?",
     about: "food",
     keywords: [ "cake", "food", "dessert" ]
   }

To create a ``text`` index with different field weights for the
``content`` field and the ``keywords`` field, include the ``weights``
option to the :method:`~db.collection.ensureIndex()` method. For
example, the following command creates an index on three fields and
assigns weights to two of the fields:

.. code-block:: javascript

   db.blog.ensureIndex(
                        {
                          content: "text",
                          keywords: "text",
                          about: "text"
                        },
                        {
                          weights: {
                                     content: 10,
                                     keywords: 5,
                                   },
                          name: "TextIndex"
                        }
                      )

The ``text`` index has the following fields and weights:

- ``content`` has a weight of 10,

- ``keywords`` has a weight of 5, and

- ``about`` has the default weight of 1.

These weights denote the relative significance of the indexed fields to
each other. For instance, a term match in the ``content`` field has:

- ``2`` times (i.e. ``10:5``) the impact as a term match in the
  ``keywords`` field and

- ``10`` times (i.e. ``10:1``) the impact as a term match in the
  ``about`` field.
=====================================================
Convert a Replica Set to a Replicated Sharded Cluster
=====================================================

.. default-domain:: mongodb

Overview
--------

Following this tutorial, you will convert a single 3-member
replica set to a cluster that consists of 2 shards. Each shard
will consist of an independent 3-member replica set.

The tutorial uses a test environment running on a local system
UNIX-like system. You should feel encouraged to "follow along at
home." If you need to perform this process in a production
environment, notes throughout the document indicate procedural
differences.

The procedure, from a high level, is as follows:

#. Create or select a 3-member replica set and insert some data into a collection.

#. Start the config databases and create a cluster with a single
   shard.

#. Create a second replica set with three new :program:`mongod` instances.

#. Add the second replica set as a shard in the cluster.

#. Enable sharding on the desired collection or collections.

Process
-------

Install MongoDB according to the instructions in the :ref:`MongoDB Installation Tutorial
<tutorials-installation>`.

Deploy a Replica Set with Test Data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If have an existing MongoDB :term:`replica set` deployment, you can
omit the this step and continue from
:ref:`convert-replica-set-to-shard-cluster-deploy-sharding-infrastructure`.

Use the following sequence of steps to configure and deploy a replica
set and to insert test data.

#. Create the following directories for the first replica set instance, named ``firstset``:

   - ``/data/example/firstset1``
   - ``/data/example/firstset2``
   - ``/data/example/firstset3``

   To create directories, issue the following command:

   .. code-block:: sh

      mkdir -p /data/example/firstset1 /data/example/firstset2 /data/example/firstset3

#. In a separate terminal window or GNU Screen
   window, start three :program:`mongod` instances by running each of the
   following commands:

   .. code-block:: sh

      mongod --dbpath /data/example/firstset1 --port 10001 --replSet firstset --oplogSize 700 --rest
      mongod --dbpath /data/example/firstset2 --port 10002 --replSet firstset --oplogSize 700 --rest
      mongod --dbpath /data/example/firstset3 --port 10003 --replSet firstset --oplogSize 700 --rest

   .. note::

      The :option:`--oplogSize 700 <mongod --oplogSize>`
      option restricts the size of the operation log (i.e. oplog) for
      each :program:`mongod` instance to 700MB. Without the
      :option:`--oplogSize <mongod --oplogSize>` option, each
      :program:`mongod` reserves approximately 5% of the free disk
      space on the volume. By limiting the size of the oplog, each
      instance starts more quickly. Omit this setting in production
      environments.

#. In a :program:`mongo` shell session in a new terminal, connect to the
   mongodb instance on port 10001 by running the following command. If you
   are in a production environment, first read the note below.

   .. code-block:: sh

      mongo localhost:10001/admin

   .. note::

      Above and hereafter, if you are running in a production
      environment or are testing this process with :program:`mongod`
      instances on multiple systems, replace "localhost" with a
      resolvable domain, hostname, or the IP address of your system.

#. In the :program:`mongo` shell, initialize the first replica set by issuing the following command:

   .. code-block:: javascript

      db.runCommand({"replSetInitiate" :
                          {"_id" : "firstset", "members" : [{"_id" : 1, "host" : "localhost:10001"},
                                                            {"_id" : 2, "host" : "localhost:10002"},
                                                            {"_id" : 3, "host" : "localhost:10003"}
                   ]}})
      {
              "info" : "Config now saved locally.  Should come online in about a minute.",
              "ok" : 1
      }

#. In the :program:`mongo` shell, create and populate a new collection
   by issuing the following sequence of JavaScript operations:

   .. code-block:: javascript

      use test
      switched to db test
      people = ["Marc", "Bill", "George", "Eliot", "Matt", "Trey", "Tracy", "Greg", "Steve", "Kristina", "Katie", "Jeff"];
      for(var i=0; i<1000000; i++){
                                   name = people[Math.floor(Math.random()*people.length)];
                                   user_id = i;
                                   boolean = [true, false][Math.floor(Math.random()*2)];
                                   added_at = new Date();
                                   number = Math.floor(Math.random()*10001);
                                   db.test_collection.save({"name":name, "user_id":user_id, "boolean": boolean, "added_at":added_at, "number":number });
                                  }

   The above operations add one million documents to the collection
   ``test_collection``. This can take several minutes, depending on your
   system.

   The script adds the documents in the following form:

.. code-block:: javascript

   { "_id" : ObjectId("4ed5420b8fc1dd1df5886f70"), "name" : "Greg", "user_id" : 4, "boolean" : true, "added_at" : ISODate("2011-11-29T20:35:23.121Z"), "number" : 74 }

.. _convert-replica-set-to-shard-cluster-deploy-sharding-infrastructure:

Deploy Sharding Infrastructure
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This procedure creates the three config databases that
store the cluster's metadata.

.. note::

   For development and testing environments, a single config database is
   sufficient. In production environments, use three config
   databases. Because config instances store only the *metadata* for the
   sharded cluster, they have minimal resource requirements.

#. Create the following data directories for three :term:`config database`
   instances:

   - ``/data/example/config1``
   - ``/data/example/config2``
   - ``/data/example/config3``

   Issue the following command at the system prompt:

   .. code-block:: sh

      mkdir -p /data/example/config1 /data/example/config2 /data/example/config3

#. In a
   separate terminal window or GNU Screen window,
   start the config databases by running the following commands:

   .. code-block:: sh

      mongod --configsvr --dbpath /data/example/config1 --port 20001
      mongod --configsvr --dbpath /data/example/config2 --port 20002
      mongod --configsvr --dbpath /data/example/config3 --port 20003

#. In a separate terminal window or GNU Screen
   window,
   start :program:`mongos` instance by running the following
   command:

   .. code-block:: sh

      mongos --configdb localhost:20001,localhost:20002,localhost:20003 --port 27017 --chunkSize 1

   .. note::

      If you are using the collection created earlier or are just
      experimenting with sharding, you can use a small
      :option:`--chunkSize <mongos --chunkSize>` (1MB works well.) The
      default :setting:`~sharding.chunkSize` of 64MB means that your
      cluster must have 64MB of data before the MongoDB's
      automatic sharding begins working.

      In production environments,
      do not use a small shard size.

   The :setting:`~sharding.configDB` options specify the *configuration databases*
   (e.g. ``localhost:20001``, ``localhost:20002``, and
   ``localhost:2003``). The :program:`mongos` instance runs on the default
   "MongoDB" port (i.e. ``27017``), while the databases themselves
   are running on ports in the ``30001`` series. In the
   this example, you may omit
   the :option:`--port 27017 <mongos --port>` option, as ``27017`` is the default port.

#. Add the first shard in :program:`mongos`. In a new terminal window
   or GNU Screen session, add the first shard, according to the
   following procedure:

   1. Connect to the :program:`mongos` with the following
      command:

      .. code-block:: sh

         mongo localhost:27017/admin

   2. Add the first shard to the cluster by issuing
      the :dbcommand:`addShard` command:

      .. code-block:: javascript

         db.runCommand( { addShard : "firstset/localhost:10001,localhost:10002,localhost:10003" } )

   3. Observe the following message, which denotes success:

      .. code-block:: javascript

         { "shardAdded" : "firstset", "ok" : 1 }

Deploy a Second Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~

This procedure deploys a second replica set. This
closely mirrors the process used to establish the first replica set
above, omitting the test data.

#. Create the following  data directories for the members of the
   second replica set, named ``secondset``:

   - ``/data/example/secondset1``
   - ``/data/example/secondset2``
   - ``/data/example/secondset3``

#. In three new terminal windows, start three instances of :program:`mongod`
   with the following commands:

   .. code-block:: sh

      mongod --dbpath /data/example/secondset1 --port 10004 --replSet secondset --oplogSize 700 --rest
      mongod --dbpath /data/example/secondset2 --port 10005 --replSet secondset --oplogSize 700 --rest
      mongod --dbpath /data/example/secondset3 --port 10006 --replSet secondset --oplogSize 700 --rest

   .. note::

      As above, the second replica set uses the smaller
      :setting:`~replication.oplogSizeMB` configuration. Omit this setting in
      production environments.

#. In the :program:`mongo` shell, connect to one mongodb instance by issuing
   the following command:

   .. code-block:: sh

      mongo localhost:10004/admin

#. In the :program:`mongo` shell, initialize the second replica set by issuing
   the following command:

   .. code-block:: javascript

      db.runCommand({"replSetInitiate" :
                          {"_id" : "secondset",
                           "members" : [{"_id" : 1, "host" : "localhost:10004"},
                                        {"_id" : 2, "host" : "localhost:10005"},
                                        {"_id" : 3, "host" : "localhost:10006"}
                   ]}})

      {
           "info" : "Config now saved locally.  Should come online in about a minute.",
           "ok" : 1
      }

#. Add the second replica set to the cluster. Connect to the :program:`mongos` instance created
   in the previous procedure and issue the following sequence of commands:

   .. code-block:: javascript

      use admin
      db.runCommand( { addShard : "secondset/localhost:10004,localhost:10005,localhost:10006" } )

   This command returns the following success message:

   .. code-block:: javascript

      { "shardAdded" : "secondset", "ok" : 1 }


#. Verify that both shards are properly configured by running the
   :dbcommand:`listShards` command. View this and example output
   below:

   .. code-block:: javascript

      db.runCommand({listShards:1})
      {
             "shards" : [
                    {
                           "_id" : "firstset",
                           "host" : "firstset/localhost:10001,localhost:10003,localhost:10002"
                    },
                    {
                           "_id" : "secondset",
                           "host" : "secondset/localhost:10004,localhost:10006,localhost:10005"
                    }
            ],
           "ok" : 1
      }


Enable Sharding
~~~~~~~~~~~~~~~

MongoDB must have :term:`sharding` enabled on *both* the database and
collection levels.



Enabling Sharding on the Database Level
```````````````````````````````````````

Issue the :dbcommand:`enableSharding` command. The following example
enables sharding on the "test" database:

.. code-block:: javascript

   db.runCommand( { enableSharding : "test" } )
   { "ok" : 1 }


Create an Index on the Shard Key
````````````````````````````````

MongoDB uses the shard key to
distribute documents between shards. Once selected, you cannot change
the shard key. Good shard keys:

- have values that are evenly distributed among all documents,

- group documents that are often accessed at the same time into
  contiguous chunks, and

- allow for effective distribution of activity among shards.

Typically shard keys are compound, comprising of some sort of hash and
some sort of other primary key. Selecting a shard key depends on your
data set, application architecture, and usage pattern, and is beyond
the scope of this document. For the purposes of this example, we will
shard the "number" key. This typically would
*not* be a good shard key for production deployments.

Create the index with the following procedure:

.. code-block:: javascript

   use test
   db.test_collection.ensureIndex({number:1})

.. seealso:: The :ref:`Shard Key Overview <sharding-shard-key>` and
   :ref:`Shard Key <sharding-internals-shard-keys>` sections.


Shard the Collection
````````````````````

Issue the following command:

.. code-block:: javascript

   use admin
   db.runCommand( { shardCollection : "test.test_collection", key : {"number":1} })
   { "collectionsharded" : "test.test_collection", "ok" : 1 }

The collection ``test_collection`` is now sharded!

Over the next few minutes the Balancer begins to redistribute
chunks of documents. You can confirm this activity by switching to the
``test`` database and running :method:`db.stats()` or
:method:`db.printShardingStatus()`.

As clients insert additional documents into this collection,
:program:`mongos` distributes the documents evenly between the shards.

In the :program:`mongo` shell, issue the following commands to return
statics against each cluster:

.. code-block:: javascript

   use test
   db.stats()
   db.printShardingStatus()

Example output of the :method:`db.stats()` command:

.. code-block:: javascript

   {
        "raw" : {
                "firstset/localhost:10001,localhost:10003,localhost:10002" : {
                        "db" : "test",
                        "collections" : 3,
                        "objects" : 973887,
                        "avgObjSize" : 100.33173458522396,
                        "dataSize" : 97711772,
                        "storageSize" : 141258752,
                        "numExtents" : 15,
                        "indexes" : 2,
                        "indexSize" : 56978544,
                        "fileSize" : 1006632960,
                        "nsSizeMB" : 16,
                        "ok" : 1
                },
                "secondset/localhost:10004,localhost:10006,localhost:10005" : {
                        "db" : "test",
                        "collections" : 3,
                        "objects" : 26125,
                        "avgObjSize" : 100.33286124401914,
                        "dataSize" : 2621196,
                        "storageSize" : 11194368,
                        "numExtents" : 8,
                        "indexes" : 2,
                        "indexSize" : 2093056,
                        "fileSize" : 201326592,
                        "nsSizeMB" : 16,
                        "ok" : 1
                }
        },
        "objects" : 1000012,
        "avgObjSize" : 100.33176401883178,
        "dataSize" : 100332968,
        "storageSize" : 152453120,
        "numExtents" : 23,
        "indexes" : 4,
        "indexSize" : 59071600,
        "fileSize" : 1207959552,
        "ok" : 1
   }

Example output of the :method:`db.printShardingStatus()` command:

.. code-block:: javascript

   --- Sharding Status ---
   sharding version: { "_id" : 1, "version" : 3 }
   shards:
          {  "_id" : "firstset",  "host" : "firstset/localhost:10001,localhost:10003,localhost:10002" }
          {  "_id" : "secondset",  "host" : "secondset/localhost:10004,localhost:10006,localhost:10005" }
   databases:
          {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
          {  "_id" : "test",  "partitioned" : true,  "primary" : "firstset" }
                     test.test_collection chunks:
                                                  secondset	5
                                                  firstset	186

   [...]

In a few moments you can run these commands for a second time to
demonstrate that :term:`chunks <chunk>` are migrating from
``firstset`` to ``secondset``.

When this procedure is complete, you will have converted a replica set
into a cluster where each shard is itself a replica set.
=================================
Convert a Secondary to an Arbiter
=================================

.. default-domain:: mongodb

If you have a :term:`secondary` in a :term:`replica set` that no
longer needs to hold data but that needs to remain in the set to
ensure that the set can :ref:`elect a primary
<replica-set-elections>`, you may convert the secondary to an
:ref:`arbiter <replica-set-arbiters>` using either procedure in this
tutorial. Both procedures are operationally equivalent:

- You may operate the arbiter on the same port as the former secondary.
  In this procedure, you must shut down the secondary and remove its
  data before restarting and reconfiguring it as an arbiter.

  For this procedure, see :ref:`replica-set-convert-secondary-to-arbiter-same-port`.

- Run the arbiter on a new port. In this procedure, you can reconfigure
  the server as an arbiter before shutting down the instance running as
  a secondary.

  For this procedure, see :ref:`replica-set-convert-secondary-to-arbiter`.

.. _replica-set-convert-secondary-to-arbiter-same-port:

Convert Secondary to Arbiter and Reuse the Port Number
------------------------------------------------------

#. If your application is connecting directly to the secondary,
   modify the application so that MongoDB queries don't reach
   the secondary.

#. Shut down the secondary.

#. Remove the :term:`secondary` from the :term:`replica set` by calling
   the :method:`rs.remove()` method. Perform this operation while connected to the current
   :term:`primary` in the :program:`mongo` shell:

   .. code-block:: javascript

      rs.remove("<hostname><:port>")

#. Verify that the replica set no longer includes the secondary by
   calling the :method:`rs.conf()` method in the :program:`mongo` shell:

   .. code-block:: javascript

      rs.conf()

#. Move the secondary's data directory to an archive folder. For example:

   .. code-block:: sh

      mv /data/db /data/db-old

   .. optional:: You may remove the data instead.

#. Create a new, empty data directory to point to when restarting the
   :program:`mongod` instance. You can reuse the previous name. For
   example:

   .. code-block:: sh

      mkdir /data/db

#. Restart the :program:`mongod` instance for the secondary, specifying
   the port number, the empty data directory, and the replica set. You
   can use the same port number you used before. Issue a command similar
   to the following:

   .. code-block:: sh

      mongod --port 27021 --dbpath /data/db --replSet rs

#. In the :program:`mongo` shell convert the secondary to an arbiter
   using the :method:`rs.addArb()` method:

   .. code-block:: javascript

      rs.addArb("<hostname><:port>")

#. Verify the arbiter belongs to the replica set by calling the
   :method:`rs.conf()` method in the :program:`mongo` shell.

   .. code-block:: javascript

      rs.conf()

   The arbiter member should include the following:

   .. code-block:: javascript

      "arbiterOnly" : true

.. _replica-set-convert-secondary-to-arbiter:

Convert Secondary to Arbiter Running on a New Port Number
---------------------------------------------------------

#. If your application is connecting directly to the secondary
   or has a connection string referencing the secondary,
   modify the application so that MongoDB queries don't reach
   the secondary.

#. Create a new, empty data directory to be used with the new port
   number. For example:

   .. code-block:: sh

      mkdir /data/db-temp

#. Start a new :program:`mongod` instance on the new port number,
   specifying the new data directory and the existing replica
   set. Issue a command similar to the following:

   .. code-block:: sh

      mongod --port 27021 --dbpath /data/db-temp --replSet rs

#. In the :program:`mongo` shell connected to the current primary,
   convert the new :program:`mongod` instance to an arbiter using the :method:`rs.addArb()`
   method:

   .. code-block:: javascript

      rs.addArb("<hostname><:port>")

#. Verify the arbiter has been added to the replica set by calling the
   :method:`rs.conf()` method in the :program:`mongo` shell.

   .. code-block:: javascript

      rs.conf()

   The arbiter member should include the following:

   .. code-block:: javascript

      "arbiterOnly" : true

#. Shut down the secondary.

#. Remove the :term:`secondary` from the :term:`replica set` by calling
   the :method:`rs.remove()` method in the :program:`mongo` shell:

   .. code-block:: javascript

      rs.remove("<hostname><:port>")

#. Verify that the replica set no longer includes the old secondary by
   calling the :method:`rs.conf()` method in the :program:`mongo` shell:

   .. code-block:: javascript

      rs.conf()

#. Move the secondary's data directory to an archive folder. For example:

   .. code-block:: sh

      mv /data/db /data/db-old

   .. optional:: You may remove the data instead.
======================================
Convert Sharded Cluster to Replica Set
======================================

.. default-domain:: mongodb

This tutorial describes the process for converting a :term:`sharded
cluster` to a non-sharded :term:`replica set`. To convert a replica
set into a sharded cluster
:doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`. See the
:doc:`/sharding` documentation for more information on sharded
clusters.

Convert a Cluster with a Single Shard into a Replica Set
--------------------------------------------------------

In the case of a :term:`sharded cluster` with only one shard, that shard
contains the full data set. Use the following procedure to convert that
cluster into a non-sharded :term:`replica set`:

1. Reconfigure the application to connect to the primary member of the
   replica set hosting the single shard that system will be the new replica
   set.

#. Optionally remove the :option:`--shardsrv <mongod --shardsvr>`
   option, if your :program:`mongod` started with this option.

   .. tip:: Changing the :option:`--shardsrv <mongod --shardsrv>`
      option will change the port that :program:`mongod` listens for
      incoming connections on.

The single-shard cluster is now a non-sharded :term:`replica set` that
will accept read and write operations on the data set.

You may now decommission the remaining sharding infrastructure.

Convert a Sharded Cluster into a Replica Set
--------------------------------------------

Use the following procedure to transition from a :term:`sharded cluster`
with more than one shard to an entirely new :term:`replica set`.

1. With the :term:`sharded cluster` running, :doc:`deploy a new replica
   set <deploy-replica-set>` in addition to your sharded cluster. The
   replica set must have sufficient capacity to hold all of the data
   files from all of the current shards combined. Do not configure the
   application to connect to the new replica set until the data
   transfer is complete.

#. Stop all writes to the :term:`sharded cluster`. You may reconfigure
   your application or stop all :program:`mongos` instances. If you
   stop all :program:`mongos` instances, the applications will not be
   able to read from the database. If you stop all :program:`mongos`
   instances, start a temporary :program:`mongos` instance on that
   applications cannot access for the data migration procedure.

#. Use :doc:`mongodump and mongorestore
   </tutorial/backup-with-mongodump/>` to migrate
   the data from the :program:`mongos` instance to the new
   :term:`replica set`.

   .. note:: Not all collections on all databases are necessarily
      sharded. Do not solely migrate the sharded collections. Ensure that
      all databases and all collections migrate correctly.

#. Reconfigure the application to use the non-sharded :term:`replica
   set` instead of the :program:`mongos` instance.

The application will now use the un-sharded :term:`replica set` for
reads and writes. You may now decommission the remaining unused
sharded cluster infrastructure.
=====================================
Convert a Standalone to a Replica Set
=====================================

.. default-domain:: mongodb

This tutorial describes the process for converting a
:term:`standalone` :program:`mongod` instance into a three-member
:term:`replica set`.  Use standalone instances for testing and
development, but always use replica sets in production. To install a
standalone instance, see the :ref:`installation tutorials
<tutorials-installation>`.

To deploy a replica set without using a pre-existing :program:`mongod`
instance, see :doc:`/tutorial/deploy-replica-set`.

Procedure
---------

#. Shut down the :term:`standalone` :program:`mongod` instance.

#. Restart the instance. Use the :option:`--replSet <mongod --replSet>`
   option to specify the name of the new replica set.

   For example, the following command starts a standalone instance as a
   member of a new replica set named ``rs0``. The command uses the
   standalone's existing database path of ``/srv/mongodb/db0``:

   .. code-block:: sh

      mongod --port 27017 --dbpath /srv/mongodb/db0 --replSet rs0

   .. include:: /includes/fact-unique-replica-set-names.rst

   For more information on configuration options, see
   :doc:`/reference/configuration-options` and the :program:`mongod`
   manual page.

#. Connect to the :program:`mongod` instance.

#. Use :method:`rs.initiate()` to initiate the new replica set:

   .. code-block:: javascript

      rs.initiate()

   The replica set is now operational.

   To view the replica set configuration, use :method:`rs.conf()`. To
   check the status of the replica set, use :method:`rs.status()`.

.. _expand-the-replica-set:

Expand the Replica Set
~~~~~~~~~~~~~~~~~~~~~~

Add additional replica set members by doing the following:

#. On two distinct systems, start two new standalone :program:`mongod`
   instances. For information on starting a standalone instance, see
   the :ref:`installation tutorial <tutorials-installation>` specific
   to your environment.

#. On your connection to the original :program:`mongod` instance (the
   former standalone instance), issue a command in the following form
   for each new instance to add to the replica set:

   .. code-block:: javascript

      rs.add("<hostname><:port>")

   Replace ``<hostname>`` and ``<port>`` with the resolvable hostname
   and port of the :program:`mongod` instance to add to the set. For
   more information on adding a host to a replica set, see
   :doc:`/tutorial/expand-replica-set`.

Sharding Considerations
~~~~~~~~~~~~~~~~~~~~~~~

If the new replica set is part of a :term:`sharded cluster`, change
the shard host information in the :term:`config database` by doing
the following:

#. Connect to one of the sharded cluster's :program:`mongos`
   instances and issue a command in the following form:

   .. code-block:: javascript

      db.getSiblingDB("config").shards.save( {_id: "<name>", host: "<replica-set>/<member,><member,><...>" } )

   Replace ``<name>`` with the name of the shard. Replace
   ``<replica-set>`` with the name of the replica set. Replace
   ``<member,><member,><>`` with the list of the members of the
   replica set.

#. Restart all :program:`mongos` instances. If possible, restart all
   components of the replica sets (i.e., all :program:`mongos` and
   all shard :program:`mongod` instances).
.. index:: index; create
.. index:: index; compound
.. _index-create-compound-index:

=======================
Create a Compound Index
=======================

.. default-domain:: mongodb

Indexes allow MongoDB to process and fulfill queries quickly by
creating small and efficient representations of the documents in a
:term:`collection`. MongoDB supports indexes that include content on a
single field, as well as :ref:`compound indexes <index-type-compound>`
that include content from multiple fields. Continue reading for
instructions and examples of building a compound index.

Build a Compound Index
----------------------

To create a :ref:`compound index <index-type-compound>` use an
operation that resembles the following prototype:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1, b: 1, c: 1 } )

Example
-------

The following operation will create an index on the
``item``, ``category``, and ``price`` fields of the ``products``
collection:

.. code-block:: javascript

   db.products.ensureIndex( { item: 1, category: 1, price: 1 } )

Additional Considerations
-------------------------

.. include:: /includes/index-tutorials-considerations.rst

.. include:: /includes/tip-index-specification-field-value.rst

.. seealso:: :ref:`index-create-index`, :doc:`/administration/indexes`
   and :doc:`/core/indexes` for more information.
.. index:: index; hashed
.. _index-hashed-index:

=====================
Create a Hashed Index
=====================

.. default-domain:: mongodb

.. versionadded:: 2.4

:ref:`Hashed indexes <index-type-hashed>` compute a hash of the value
of a field in a collection and index the hashed value. These indexes
permit equality queries and may be suitable shard keys for some
collections.


.. include:: /includes/tip-applications-do-not-need-to-compute-hashes.rst

.. see:: :ref:`sharding-hashed-sharding` for more information about hashed
   indexes in sharded clusters, as well as :doc:`/core/indexes` and
   :doc:`/administration/indexes` for more information about indexes.

Procedure
---------

To create a :ref:`hashed index <index-type-hashed>`, specify
``hashed`` as the value of the index key, as in the following
example:

.. example:: Specify a hashed index on ``_id``

   .. code-block:: javascript

      db.collection.ensureIndex( { _id: "hashed" } )

Considerations
--------------

MongoDB supports ``hashed`` indexes of any single field. The hashing
function collapses sub-documents and computes the hash for the entire
value, but does not support multi-key (i.e. arrays) indexes.

You may not create compound indexes that have ``hashed`` index fields.
.. index:: index; sparse
.. _index-sparse-index:

=====================
Create a Sparse Index
=====================

.. default-domain:: mongodb

Sparse indexes are like non-sparse indexes, except that they omit
references to documents that do not include the indexed field. For
fields that are only present in some documents sparse indexes may
provide a significant space savings. See :ref:`index-type-sparse` for
more information about sparse indexes and their use.

.. seealso:: :doc:`/core/indexes` and :doc:`/administration/indexes`
   for more information.

Prototype
---------

To create a :ref:`sparse index <index-type-sparse>` on a field, use an
operation that resembles the following prototype:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1 }, { sparse: true } )

Example
-------

The following operation, creates a sparse index on the ``users``
collection that *only* includes a document in the index if
the ``twitter_name`` field exists in a document.

.. code-block:: javascript

   db.users.ensureIndex( { twitter_name: 1 }, { sparse: true } )

The index excludes all documents that do not include the
``twitter_name`` field.

Considerations
--------------

.. note::

   Sparse indexes can affect the results returned by the query,
   particularly with respect to sorts on fields *not* included in the
   index. See the :ref:`sparse index <index-type-sparse>` section for
   more information.
=====================
Create a Unique Index
=====================

.. default-domain:: mongodb

MongoDB allows you to specify a :ref:`unique constraint
<index-type-unique>` on an index. These constraints prevent
applications from inserting :term:`documents <document>` that have
duplicate values for the inserted fields. Additionally, if you want to
create an index on a collection that has existing data that might have
duplicate values for the indexed field, you may choose to combine unique
enforcement with :ref:`duplicate dropping
<index-creation-duplicate-dropping>`.

.. index:: index; unique
.. _index-unique-index:

Unique Indexes
~~~~~~~~~~~~~~

To create a :ref:`unique index <index-type-unique>`, consider the
following prototype:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1 }, { unique: true } )

For example, you may want to create a unique index on the ``"tax-id":``
of the ``accounts`` collection to prevent storing multiple account
records for the same legal entity:

.. code-block:: javascript

   db.accounts.ensureIndex( { "tax-id": 1 }, { unique: true } )

The :ref:`_id index <index-type-id>` is a unique index. In some
situations you may consider using the ``_id`` field itself for this kind
of data rather than using a unique index on another field.

In many situations you will want to combine the ``unique`` constraint
with the ``sparse`` option. When MongoDB indexes a field, if a
document does not have a value for a field, the index entry for that
item will be ``null``. Since unique indexes cannot have duplicate
values for a field, without the ``sparse`` option, MongoDB will reject
the second document and all subsequent documents without the indexed
field. Consider the following prototype.

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1 }, { unique: true, sparse: true } )

You can also enforce a unique constraint on :ref:`compound indexes
<index-type-compound>`, as in the following prototype:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1, b: 1 }, { unique: true } )

These indexes enforce uniqueness for the *combination* of index keys
and *not* for either key individually.

.. index:: index; drop duplicates
.. index:: index; duplicates
.. _index-drop-duplicates:

Drop Duplicates
~~~~~~~~~~~~~~~

To force the creation of a :ref:`unique index <index-type-unique>`
index on a collection with duplicate values in the field you are
indexing you can use the ``dropDups`` option. This will force MongoDB
to create a *unique* index by deleting documents with duplicate values
when building the index. Consider the following prototype invocation
of :method:`~db.collection.ensureIndex()`:

.. code-block:: javascript

   db.collection.ensureIndex( { a: 1 }, { unique: true, dropDups: true } )

See the full documentation of :ref:`duplicate dropping
<index-creation-duplicate-dropping>` for more information.

.. warning::

   Specifying ``{ dropDups: true }`` may delete data from your
   database. Use with extreme caution.

Refer to the :method:`~db.collection.ensureIndex()`
documentation for additional index creation options.
=============================
Create a Vulnerability Report
=============================

.. default-domain:: mongodb

If you believe you have discovered a vulnerability in MongoDB or have
experienced a security incident related to MongoDB, please report the
issue to aid in its resolution.

To report an issue, we strongly suggest filing a ticket in the
:issue:`SECURITY <SECURITY>` project in JIRA.  MongoDB, Inc
responds to vulnerability notifications within 48 hours.

Create the Report in JIRA
-------------------------

Submit a ticket in the :issue:`Security <SECURITY>`
project at: <http://jira.mongodb.org/browse>.
The ticket number will become the reference identification for the
issue for its lifetime. You can use this identifier for tracking
purposes.

Information to Provide
----------------------

All vulnerability reports should contain as much information
as possible so MongoDB's developers can move quickly to resolve the issue.
In particular, please include the following:

- The name of the product.

- *Common Vulnerability* information, if applicable, including:

- CVSS (Common Vulnerability Scoring System) Score.

- CVE (Common Vulnerability and Exposures) Identifier.
- Contact information, including an email address and/or phone number,
  if applicable.


Send the Report via Email
-------------------------

While JIRA is the preferred reporting method, you may also report
vulnerabilities via email to `security@mongodb.com
<security@mongodb.com>`_.

You may encrypt email using MongoDB's public key at
`http://docs.mongodb.org/10gen-security-gpg-key.asc <http://docs.mongodb.org/10gen-security-gpg-key.asc>`_.

MongoDB, Inc. responds to vulnerability reports sent via
email with a response email that contains a reference number for a JIRA ticket
posted to the :issue:`SECURITY` project.

Evaluation of a Vulnerability Report
------------------------------------

MongoDB, Inc. validates all submitted vulnerabilities and uses Jira
to track all communications regarding a vulnerability,
including requests for clarification or additional information. If
needed, MongoDB representatives set up a conference call to exchange
information regarding the vulnerability.

Disclosure
----------

MongoDB, Inc. requests that you do *not* publicly disclose any information
regarding the vulnerability or exploit the issue until it has had the
opportunity to analyze the vulnerability, to respond to the notification,
and to notify key users, customers, and partners.

The amount of time required to validate a reported vulnerability
depends on the complexity and severity of the issue. MongoDB, Inc. takes all
required vulnerabilities very seriously and will always ensure that
there is a clear and open channel of communication with the reporter.

After validating an issue, MongoDB, Inc. coordinates public disclosure of
the issue with the reporter in a mutually agreed timeframe and
format. If required or requested, the reporter of a vulnerability will
receive credit in the published security bulletin.
==========================================
Create an Auto-Incrementing Sequence Field
==========================================

.. default-domain:: mongodb

Synopsis
--------

MongoDB reserves the ``_id`` field in the top level of all documents
as a primary key. ``_id`` must be unique, and always has an index with
a :ref:`unique constraint <index-type-unique>`. However, except for
the unique constraint you can use any value for the ``_id`` field in
your collections. This tutorial describes two methods for creating an
incrementing sequence number for the ``_id`` field using the
following:

- :ref:`auto-increment-counters-collection`

- :ref:`auto-increment-optimistic-loop`

Considerations
--------------

Generally in MongoDB, you would not use an auto-increment pattern
for the ``_id`` field, or any field, because it does not scale for
databases with large numbers of documents. Typically the default
value :term:`ObjectId <objectid>` is more ideal for the ``_id``.

Procedures
----------

.. _auto-increment-counters-collection:

Use Counters Collection
~~~~~~~~~~~~~~~~~~~~~~~

Counter Collection Implementation
`````````````````````````````````
U
se a separate ``counters`` collection to track the *last* number sequence
used. The ``_id`` field contains the sequence name and the ``seq`` field
contains the last value of the sequence.

1. Insert into the ``counters`` collection, the initial value for the ``userid``:

   .. code-block:: javascript

      db.counters.insert(
         {
            _id: "userid",
            seq: 0
         }
      )

2. Create a ``getNextSequence`` function that accepts a ``name`` of
   the sequence. The function uses the
   :method:`~db.collection.findAndModify()` method to atomically
   increment the ``seq`` value and return this new value:

   .. code-block:: javascript

      function getNextSequence(name) {
         var ret = db.counters.findAndModify(
                {
                  query: { _id: name },
                  update: { $inc: { seq: 1 } },
                  new: true
                }
         );

         return ret.seq;
      }

3. Use this ``getNextSequence()`` function during
   :method:`~db.collection.insert()`.

   .. code-block:: javascript

      db.users.insert(
         {
           _id: getNextSequence("userid"),
           name: "Sarah C."
         }
      )

      db.users.insert(
         {
           _id: getNextSequence("userid"),
           name: "Bob D."
         }
      )

   You can verify the results with :method:`~db.collection.find()`:

   .. code-block:: javascript

      db.users.find()

   The ``_id`` fields contain incrementing sequence values:

   .. code-block:: javascript

      {
        _id : 1,
        name : "Sarah C."
      }
      {
        _id : 2,
        name : "Bob D."
      }

``findAndModify`` Behavior
``````````````````````````

When :method:`~db.collection.findAndModify()` includes the ``upsert:
true`` option **and** the query field(s) is not uniquely indexed, the
method could insert a document multiple times in certain
circumstances. For instance, if multiple clients each invoke the
method with the same query condition and these methods complete the
find phase before any of methods perform the modify phase, these
methods could insert the same document.

In the ``counters`` collection example, the query field is the
``_id`` field, which always has a unique index. Consider that the
:method:`~db.collection.findAndModify()` includes the ``upsert:
true`` option, as in the following modified example:

.. code-block:: javascript
   :emphasize-lines: 7

   function getNextSequence(name) {
      var ret = db.counters.findAndModify(
             {
               query: { _id: name },
               update: { $inc: { seq: 1 } },
               new: true,
               upsert: true
             }
      );

      return ret.seq;
   }

If multiple clients were to invoke the ``getNextSequence()`` method
with the same ``name`` parameter, then the methods would observe one
of the following behaviors:

- Exactly one :method:`~db.collection.findAndModify()` would
  successfully insert a new document.

- Zero or more :method:`~db.collection.findAndModify()` methods
  would update the newly inserted document.

- Zero or more :method:`~db.collection.findAndModify()` methods
  would fail when they attempted to insert a duplicate.

If the method fails due to a unique index constraint violation,
retry the method. Absent a delete of the document, the retry
should not fail.

.. _auto-increment-optimistic-loop:

Optimistic Loop
~~~~~~~~~~~~~~~

In this pattern, an *Optimistic Loop* calculates the incremented
``_id`` value and attempts to insert a document with the calculated
``_id`` value. If the insert is successful, the loop ends. Otherwise,
the loop will iterate through possible ``_id`` values until the insert
is successful.

#. Create a function named ``insertDocument`` that performs the
   "insert if not present" loop. The function wraps the
   :method:`~db.collection.insert()` method and takes a
   ``doc`` and a ``targetCollection`` arguments.

   .. code-block:: javascript

      function insertDocument(doc, targetCollection) {

          while (1) {

              var cursor = targetCollection.find( {}, { _id: 1 } ).sort( { _id: -1 } ).limit(1);

              var seq = cursor.hasNext() ? cursor.next()._id + 1 : 1;

              doc._id = seq;

              targetCollection.insert(doc);

              var err = db.getLastErrorObj();

              if( err && err.code ) {
                  if( err.code == 11000 /* dup key */ )
                      continue;
                  else
                      print( "unexpected error inserting data: " + tojson( err ) );
              }

              break;
          }
      }

   The ``while (1)`` loop performs the following actions:

   - Queries the ``targetCollection`` for the document with the
     maximum ``_id`` value.

   - Determines the next sequence value for ``_id`` by:

     - adding ``1`` to the returned ``_id`` value if the returned
       cursor points to a document.

     - otherwise: it sets the next sequence value to ``1`` if the
       returned cursor points to no document.

   - For the ``doc`` to insert, set its ``_id`` field to the
     calculated sequence value ``seq``.

   - Insert the ``doc`` into the ``targetCollection``.

   - If the insert operation errors with duplicate key, repeat the
     loop.  Otherwise, if the insert operation encounters some
     other error or if the operation succeeds, break out of the loop.

#. Use the ``insertDocument()`` function to perform an insert:

   .. code-block:: javascript

      var myCollection = db.users2;

      insertDocument(
         {
           name: "Grace H."
         },
         myCollection
      );

      insertDocument(
         {
           name: "Ted R."
         },
         myCollection
      )

   You can verify the results with :method:`~db.collection.find()`:

   .. code-block:: javascript

      db.users2.find()

   The ``_id`` fields contain incrementing sequence values:

   .. code-block:: javascript

      {
        _id: 1,
        name: "Grace H."
      }
      {
        _id : 2,
        "name" : "Ted R."
      }

The ``while`` loop may iterate many times in collections with larger
insert volumes.
.. index:: index; create
.. _index-create-index:

===============
Create an Index
===============

.. default-domain:: mongodb

Indexes allow MongoDB to process and fulfill queries quickly by
creating small and efficient representations of the documents in a
:term:`collection`. MongoDB creates an index on the ``_id`` field of
every collection by default, but allows users to create indexes for
any collection using on any field in a :term:`document`.

This tutorial describes how to create an index on a single
field. MongoDB also supports :ref:`compound indexes
<index-type-compound>`, which are indexes on multiple fields. See
:ref:`index-create-compound-index` for instructions on building
compound indexes.

Create an Index on a Single Field
---------------------------------

To create an index, use :method:`~db.collection.ensureIndex()` or a similar
:api:`method from your driver <>`. For example
the following creates an index on the ``phone-number`` field
of the ``people`` collection:

.. code-block:: javascript

   db.people.ensureIndex( { "phone-number": 1 } )

:method:`~db.collection.ensureIndex()` only creates an
index if an index of the same specification does not already exist.

All indexes support and optimize the performance for queries that select
on this field. For queries that cannot use an index, MongoDB must scan
all documents in a collection for documents that match the query.

.. include:: /includes/tip-index-specification-field-value.rst

Examples
--------

If you create an index on the ``user_id`` field in the ``records``,
this index is, the index will support the following query:

.. code-block:: javascript

   db.records.find( { user_id: 2 } )

However, the following query, on the ``profile_url`` field is not
supported by this index:

.. code-block:: javascript

   db.records.find( { profile_url: 2 } )

Additional Considerations
-------------------------

.. include:: /includes/index-tutorials-considerations.rst

.. seealso:: :ref:`index-create-compound-index`,
   :doc:`/administration/indexes` and :doc:`/core/indexes` for more
   information.
==================================
Create Chunks in a Sharded Cluster
==================================

.. default-domain:: mongodb

Pre-splitting the chunk ranges in an empty sharded collection allows
clients to insert data into an already partitioned collection. In most
situations a :term:`sharded cluster` will create and distribute chunks
automatically without user intervention. However, in a limited number
of cases, MongoDB cannot create enough chunks or distribute
data fast enough to support required throughput. For example:

- If you want to partition an existing data collection that resides on a
  single shard.

- If you want to ingest a large volume of data into a cluster that isn't
  balanced, or where the ingestion of data will lead to data imbalance.
  For example, monotonically increasing or decreasing shard keys insert
  all data into a single chunk.

These operations are resource intensive for several reasons:

- Chunk migration requires copying all the data in the chunk from one shard to
  another.

- MongoDB can migrate only a single chunk at a time.

- MongoDB creates splits only after an insert operation.

.. warning::

   Only pre-split an empty collection. If a collection already has data,
   MongoDB automatically splits the collection's data when you enable
   sharding for the collection. Subsequent attempts to manually create
   splits can lead to unpredictable chunk ranges and sizes as well as
   inefficient or ineffective balancing behavior.

To create chunks manually, use the following procedure:

#. Split empty chunks in your collection by manually performing
   the :dbcommand:`split` command on chunks.

   .. example::

      To create chunks for documents in the ``myapp.users``
      collection using the ``email`` field as the :term:`shard key`,
      use the following operation in the :program:`mongo` shell:

        .. code-block:: javascript

           for ( var x=97; x<97+26; x++ ){
             for( var y=97; y<97+26; y+=6 ) {
               var prefix = String.fromCharCode(x) + String.fromCharCode(y);
               db.runCommand( { split : "myapp.users" , middle : { email : prefix } } );
             }
           }

      This assumes a collection size of 100 million documents.

   For information on the balancer and automatic distribution of
   chunks across shards, see :ref:`sharding-balancing-internals`
   and :ref:`sharding-chunk-migration`. For
   information on manually migrating chunks, see
   :doc:`/tutorial/migrate-chunks-in-sharded-cluster`.
======================================
Create Indexes to Support Your Queries
======================================

.. default-domain:: mongodb

An index supports a query when the index contains all the fields scanned
by the query. The query scans the index and not the collection. Creating indexes
that support queries results in greatly increased query performance.

This document describes strategies for creating indexes that support queries.

Create a Single-Key Index if All Queries Use the Same, Single Key
-----------------------------------------------------------------

If you only ever query on a single key in a given collection, then you need
to create just one single-key index for that collection. For example, you
might create an index on ``category`` in the ``product`` collection:

.. code-block:: javascript

   db.products.ensureIndex( { "category": 1 } )

.. _compound-key-indexes:

Create Compound Indexes to Support Several Different Queries
------------------------------------------------------------

If you sometimes query on only one key and at other times query on that
key combined with a second key, then creating a compound index is more
efficient than creating a single-key index. MongoDB will use the
compound index for both queries. For example, you might create an index
on both ``category`` and ``item``.

.. code-block:: javascript

   db.products.ensureIndex( { "category": 1, "item": 1 } )

This allows you both options. You can query on just ``category``, and
you also can query on ``category`` combined with ``item``.
A single :ref:`compound index <index-type-compound>` on multiple fields
can support all the queries that search a "prefix" subset of those fields.

.. example::

   The following index on a collection:

   .. code-block:: javascript

      { x: 1, y: 1, z: 1 }

   Can support queries that the following indexes support:

   .. code-block:: javascript

      { x: 1 }
      { x: 1, y: 1 }

   There are some situations where the prefix indexes may offer better
   query performance: for example if ``z`` is a large array.

   The ``{ x: 1, y: 1, z: 1 }`` index can also support many of the same
   queries as the following index:

   .. code-block:: javascript

      { x: 1, z: 1 }

   Also, ``{ x: 1, z: 1 }`` has an additional use. Given the following
   query:

   .. code-block:: javascript

      db.collection.find( { x: 5 } ).sort( { z: 1} )

   The ``{ x: 1, z: 1 }`` index supports both the query and the sort
   operation, while the ``{ x: 1, y: 1, z: 1 }`` index only supports
   the query. For more information on sorting, see
   :ref:`sorting-with-indexes`.

.. include:: /includes/fact-index-intersection-vs-compound-indexes.rst

.. _covered-queries:
.. _indexes-covered-queries:

Create Indexes that Support Covered Queries
-------------------------------------------

A covered query is a query in which:

- all the fields in the :ref:`query <read-operations-query-document>`
  are part of an index, **and**

- all the fields returned in the results are in the same index.

Because the index "covers" the query, MongoDB can both match the
:ref:`query conditions <read-operations-query-document>` **and** return
the results using only the index; MongoDB does not need to look at the
documents, only the index, to fulfill the query. An index can also
cover an :ref:`aggregation pipeline operation
<aggregation-pipeline-operators-and-performance>` on unsharded
collections.

Querying *only* the index can be much faster than querying documents
outside of the index. Index keys are typically smaller than the
documents they catalog, and indexes are typically available in RAM or
located sequentially on disk.

MongoDB automatically uses an index that covers a query when possible.
To ensure that an index can *cover* a query, create an index that
includes all the fields listed in the :ref:`query document
<read-operations-query-document>` and in the query result. You can
specify the fields to return in the query results with a
:ref:`projection <projection>` document. By default, MongoDB includes
the ``_id`` field in the query result. So, if the index does **not**
include the ``_id`` field, then you must exclude the ``_id`` field
(i.e. ``_id: 0``) from the query results.

.. example::

   Given collection ``users`` with an index on the fields ``user`` and
   ``status``, as created by the following option:

   .. code-block:: javascript

      db.users.ensureIndex( { status: 1, user: 1 } )

   Then, this index will cover the following query which selects on
   the ``status`` field and returns only the ``user`` field:

   .. code-block:: javascript

      db.users.find( { status: "A" }, { user: 1, _id: 0 } )

   In the operation, the projection document explicitly specifies
   ``_id: 0`` to exclude the ``_id`` field from the result since the
   index is only on the ``status`` and the ``user`` fields.

   If the projection document does not specify the exclusion of the
   ``_id`` field, the query returns the ``_id`` field. The following
   query is **not** covered by the index on the ``status`` and the
   ``user`` fields because with the projection document ``{ user: 1
   }``, the query returns both the ``user`` field and the ``_id`` field:

   .. code-block:: javascript

      db.users.find( { status: "A" }, { user: 1 } )

An index **cannot** cover a query if:

- any of the indexed fields in any of the documents in the collection
  includes an array. If an indexed field is an array, the index becomes
  a :ref:`multi-key index <index-type-multikey>` index and cannot
  support a covered query.

- any of the indexed fields are fields in subdocuments. To index fields
  in subdocuments, use :term:`dot notation`. For example, consider
  a collection ``users`` with documents of the following form:

  .. code-block:: javascript

     { _id: 1, user: { login: "tester" } }

  The collection has the following indexes:

  .. code-block:: none

     { user: 1 }

     { "user.login": 1 }

  The ``{ user: 1 }`` index covers the following query:

  .. code-block:: none

     db.users.find( { user: { login: "tester" } }, { user: 1, _id: 0 } )

  However, the ``{ "user.login": 1 }`` index does **not** cover the
  following query:

  .. code-block:: none

     db.users.find( { "user.login": "tester" }, { "user.login": 1, _id: 0 } )

  The query, however, does use the ``{ "user.login": 1 }`` index to
  find matching documents.

To determine whether a query is a covered query, use the
:method:`~cursor.explain()` method. If the :method:`~cursor.explain()`
output displays ``true`` for the :data:`~explain.indexOnly` field, the
query is covered by an index, and MongoDB queries only that index to
match the query **and** return the results.

For more information see :ref:`indexes-measuring-use`.
.. _index-selectivity:

======================================
Create Queries that Ensure Selectivity
======================================

.. default-domain:: mongodb

Selectivity is the ability of a query to narrow results using the index.
Effective indexes are more selective and allow MongoDB to use the index
for a larger portion of the work associated with fulfilling the query.

To ensure selectivity,
write queries that limit the number of possible documents with the
indexed field. Write queries that are appropriately selective relative
to your indexed data.

.. example::

   Suppose you have a field called ``status`` where the possible values
   are ``new`` and ``processed``. If you add an index on ``status``
   you've created a low-selectivity index. The index will
   be of little help in locating records.

   A better strategy, depending on your queries, would be to create a
   :ref:`compound index <index-type-compound>` that includes the
   low-selectivity field and another field. For example, you could
   create a compound index on ``status`` and ``created_at.``

   Another option, again depending on your use case, might be to use
   separate collections, one for each status.

.. example::

   Consider an index ``{ a : 1 }`` (i.e. an index on the key ``a``
   sorted in ascending order) on a collection where ``a`` has three
   values evenly distributed across the collection:

   .. code-block:: javascript

      { _id: ObjectId(), a: 1, b: "ab" }
      { _id: ObjectId(), a: 1, b: "cd" }
      { _id: ObjectId(), a: 1, b: "ef" }
      { _id: ObjectId(), a: 2, b: "jk" }
      { _id: ObjectId(), a: 2, b: "lm" }
      { _id: ObjectId(), a: 2, b: "no" }
      { _id: ObjectId(), a: 3, b: "pq" }
      { _id: ObjectId(), a: 3, b: "rs" }
      { _id: ObjectId(), a: 3, b: "tv" }

   If you query for ``{ a: 2, b: "no" }`` MongoDB must scan 3
   :term:`documents <document>` in the collection to return the one
   matching result. Similarly, a query for ``{ a: { $gt: 1}, b: "tv" }``
   must scan 6 documents, also to return one result.

   Consider the same index on a collection where ``a`` has *nine* values
   evenly distributed across the collection:

   .. code-block:: javascript

      { _id: ObjectId(), a: 1, b: "ab" }
      { _id: ObjectId(), a: 2, b: "cd" }
      { _id: ObjectId(), a: 3, b: "ef" }
      { _id: ObjectId(), a: 4, b: "jk" }
      { _id: ObjectId(), a: 5, b: "lm" }
      { _id: ObjectId(), a: 6, b: "no" }
      { _id: ObjectId(), a: 7, b: "pq" }
      { _id: ObjectId(), a: 8, b: "rs" }
      { _id: ObjectId(), a: 9, b: "tv" }

   If you query for ``{ a: 2, b: "cd" }``, MongoDB must scan only one
   document to fulfill the query. The index and query are more selective
   because the values of ``a`` are evenly distributed *and* the query
   can select a specific document using the index.

   However, although the index on ``a`` is more selective, a query such
   as ``{ a: { $gt: 5 }, b: "tv" }`` would still need to scan 4
   documents.

   .. todo:: is there an answer to that last "However" paragraph?

If overall selectivity is low, and if MongoDB must read a number of
documents to return results, then some queries may perform faster
without indexes. To determine performance, see
:ref:`indexes-measuring-use`.
======================
Create Tailable Cursor
======================

.. default-domain:: mongodb

Overview
--------

By default, MongoDB will automatically close a cursor when the client
has exhausted all results in the cursor. However, for :doc:`capped
collections </core/capped-collections>` you may use a *Tailable
Cursor* that remains open after the client exhausts the results in the
initial cursor. Tailable cursors are conceptually equivalent to the
``tail`` Unix command with the ``-f`` option (i.e. with "follow"
mode.) After clients insert new additional documents into a capped
collection, the tailable cursor will continue to retrieve
documents.

Use tailable cursors on capped collections that have high write
volumes where indexes aren't practical. For instance,
MongoDB :doc:`replication </replication>` uses tailable cursors to
tail the primary's :term:`oplog`.

.. note::

   If your query is on an indexed field, do not use tailable cursors,
   but instead, use a regular cursor. Keep track of the last value of
   the indexed field returned by the query. To retrieve the newly
   added documents, query the collection again using the last value of
   the indexed field in the query criteria, as in the following
   example:

   .. code-block:: javascript

      db.<collection>.find( { indexedField: { $gt: <lastvalue> } } )

Consider the following behaviors related to tailable cursors:

- Tailable cursors do not use indexes and return documents in
  :term:`natural order`.

- Because tailable cursors do not use indexes, the initial scan for the
  query may be expensive; but, after initially exhausting the cursor,
  subsequent retrievals of the newly added documents are inexpensive.

- Tailable cursors may become *dead*, or invalid, if either:

  - the query returns no match.

  - the cursor returns the document at the "end" of the collection and
    then the application deletes those document.

  A *dead* cursor has an id of ``0``.

See your :doc:`driver documentation </applications/drivers>` for the
driver-specific method to specify the tailable cursor. For more
information on the details of specifying a tailable cursor, see
:meta-driver:`MongoDB wire protocol </legacy/mongodb-wire-protocol>`
documentation.

C++ Example
-----------

The ``tail`` function uses a tailable cursor to output the results from
a query to a capped collection:

- The function handles the case of the dead cursor by having the query
  be inside a loop.

- To periodically check for new data, the ``cursor->more()`` statement
  is also inside a loop.

.. code-block:: cpp

   #include "client/dbclient.h"

   using namespace mongo;

   /*
    * Example of a tailable cursor.
    * The function "tails" the capped collection (ns) and output elements as they are added.
    * The function also handles the possibility of a dead cursor by tracking the field 'insertDate'.
    * New documents are added with increasing values of 'insertDate'.
    */

   void tail(DBClientBase& conn, const char *ns) {

       BSONElement lastValue = minKey.firstElement();

       Query query = Query().hint( BSON( "$natural" << 1 ) );

       while ( 1 ) {
           auto_ptr<DBClientCursor> c =
               conn.query(ns, query, 0, 0, 0,
                          QueryOption_CursorTailable | QueryOption_AwaitData );

           while ( 1 ) {
               if ( !c->more() ) {

                   if ( c->isDead() ) {
                       break;
                   }

                   continue;
               }

               BSONObj o = c->next();
               lastValue = o["insertDate"];
               cout << o.toString() << endl;
           }

           query = QUERY( "insertDate" << GT << lastValue ).hint( BSON( "$natural" << 1 ) );
       }
   }

The ``tail`` function performs the following actions:

- Initialize the ``lastValue`` variable, which tracks the last
  accessed value. The function will use the ``lastValue`` if the
  cursor becomes *invalid* and ``tail`` needs to restart the
  query. Use :method:`~cursor.hint()` to ensure that the query uses
  the :operator:`$natural` order.

- In an outer ``while(1)`` loop,

  - Query the capped collection and return a tailable cursor that
    blocks for several seconds waiting for new documents

    .. code-block:: cpp

       auto_ptr<DBClientCursor> c =
            conn.query(ns, query, 0, 0, 0,
                       QueryOption_CursorTailable | QueryOption_AwaitData );

    - Specify the capped collection using ``ns`` as an argument
      to the function.

    - Set the ``QueryOption_CursorTailable`` option to create a
      tailable cursor.

    - Set the ``QueryOption_AwaitData`` option so that the returned
      cursor blocks for a few seconds to wait for data.

  - In an inner ``while (1)`` loop, read the documents from the cursor:

    - If the cursor has no more documents and is not invalid, loop the
      inner ``while`` loop to recheck for more documents.

    - If the cursor has no more documents and is dead, break the inner
      ``while`` loop.

    - If the cursor has documents:

      - output the document,

      - update the ``lastValue`` value,

      - and loop the inner ``while (1)`` loop to recheck for more
        documents.

  - If the logic breaks out of the inner ``while (1)`` loop and the
    cursor is invalid:

    - Use the ``lastValue`` value to create a new query condition that
      matches documents added after the ``lastValue``. Explicitly
      ensure ``$natural`` order with the ``hint()`` method:

      .. code-block:: cpp

         query = QUERY( "insertDate" << GT << lastValue ).hint( BSON( "$natural" << 1 ) );

    - Loop through the outer ``while (1)`` loop to re-query with the new query
      condition and repeat.

.. seealso:: `Detailed blog post on tailable cursor <http://shtylman.com/post/the-tail-of-mongodb>`_
=======================
Create a ``text`` Index
=======================

.. default-domain:: mongodb

You can create a ``text`` index on the field or fields whose value is a
string or an array of string elements. When creating a ``text`` index
on multiple fields, you can specify the individual fields or you can
wildcard specifier (``$**``).

Index Specific Fields
---------------------

The following example creates a ``text`` index on the fields
``subject`` and ``content``:

.. code-block:: javascript

   db.collection.ensureIndex(
                              {
                                subject: "text",
                                content: "text"
                              }
                            )

This ``text`` index catalogs all string data in the ``subject`` field
and the ``content`` field, where the field value is either a string or
an array of string elements.

Index All Fields
----------------

To allow for text search on all fields with string content, use the
wildcard specifier (``$**``) to index all fields that contain string
content.

The following example indexes any string value in the data of every
field of every document in ``collection`` and names the index
``TextIndex``:

.. code-block:: javascript

   db.collection.ensureIndex(
                              { "$**": "text" },
                              { name: "TextIndex" }
                            )
=============
Create a Role
=============

.. default-domain:: mongodb

Overview
--------

Roles grant users access to MongoDB resources. By default, MongoDB
provides a number of
:doc:`built-in roles </reference/built-in-roles>`
that administrators may use to control
access to a MongoDB system. However, if these roles cannot describe
the desired privilege set of a particular user type in a deployment,
you can define a new, customized role.

A role's privileges apply to the database where the role is created. The
role can inherit privileges from other roles in its database. A role
created on the ``admin`` database can include privileges that apply to all
databases or to the :ref:`cluster <resource-cluster>` and can inherit
privileges from roles in other databases.

The combination of the database name and the role name
uniquely defines a role in MongoDB.

.. _define-roles-prereq:

Prerequisites
-------------

.. include:: /includes/access-create-role.rst

.. include:: /includes/access-roles-info.rst

Procedure
---------

.. include:: /includes/steps/define-roles.rst
======================================================
Deploy Three Config Servers for Production Deployments
======================================================

.. default-domain:: mongodb

This procedure converts a test deployment with only one
:ref:`config server <sharding-config-server>` to a production deployment
with three config servers.

.. tip::

   .. include:: /includes/fact-use-cnames-for-config-servers.rst

For redundancy, all production :doc:`sharded clusters
</core/sharding-introduction>` should deploy three config servers on
three different machines. Use a single config server only for testing
deployments, never for production deployments. When you shift to
production, upgrade immediately to three config servers.

To convert a test deployment with one config server to a production
deployment with three config servers:

#. Shut down all existing MongoDB processes in the cluster. This
   includes:

   - all :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - all :program:`mongos` instances in your cluster.

#. Copy the entire :setting:`~storage.dbPath` file system tree from the
   existing config server to the two machines that will provide the
   additional config servers. These commands, issued on the system
   with the existing :ref:`config-database`, ``mongo-config0.example.net`` may
   resemble the following:

   .. code-block:: sh

      rsync -az /data/configdb mongo-config1.example.net:/data/configdb
      rsync -az /data/configdb mongo-config2.example.net:/data/configdb

#. Start all three config servers, using the same invocation that you
   used for the single config server.

   .. code-block:: sh

      mongod --configsvr

#. Restart all shard :program:`mongod` and :program:`mongos` processes.
=============================================
Deploy a Geographically Redundant Replica Set
=============================================

.. default-domain:: mongodb

Overview
--------

This tutorial outlines the process for deploying a :term:`replica set` with members
in multiple locations. The tutorial addresses three-member sets,
four-member sets, and sets with more than four members.

For appropriate background, see :doc:`/replication` and
:doc:`/core/replica-set-architectures`. For related
tutorials, see :doc:`/tutorial/deploy-replica-set` and
:doc:`/tutorial/expand-replica-set`.

Considerations
--------------

While :term:`replica sets <replica set>` provide basic protection
against single-instance failure, replica sets whose members are all
located in a single facility are susceptible to errors in that
facility. Power outages, network interruptions, and natural disasters
are all issues that can affect replica sets whose members are colocated.
To protect against these classes of failures, deploy a
replica set with one or more members in a geographically distinct
facility or data center to provide redundancy.

Prerequisites
-------------

In general, the requirements for any geographically redundant replica
set are as follows:

- Ensure that a majority of the :ref:`voting members
  <replica-set-non-voting-members>` are within a primary facility,
  "Site A". This includes :doc:`priority 0 members
  </core/replica-set-priority-0-member>` and :doc:`arbiters
  </core/replica-set-arbiter>`. Deploy other members in
  secondary facilities, "Site B", "Site C", etc., to provide additional copies
  of the data. See :ref:`determine-geographic-distribution` for more information
  on the voting requirements for geographically redundant replica sets.

- If you deploy a replica set with an even number of members, deploy
  an :doc:`arbiter </core/replica-set-arbiter>` on Site A. The arbiter must
  be on site A to keep the majority there.

For instance, for a three-member replica set you need two instances in a
Site A, and one member in a secondary
facility, Site B. Site A should be the same facility or
very close to your primary application infrastructure
(i.e. application servers, caching layer, users, etc.)

A four-member replica set should have at least two members in Site A,
with the remaining members in one or more secondary sites, as well as a
single :term:`arbiter` in Site A.

For all configurations in this tutorial, deploy each replica set member
on a separate system. Although you may deploy more than one replica set member on a
single system, doing so reduces the redundancy and capacity
of the replica set. Such deployments are typically for testing
purposes and beyond the scope of this tutorial.

This tutorial assumes you have installed MongoDB on each system that
will be part of your replica set. If you have not already installed
MongoDB, see the :ref:`installation tutorials <tutorial-installation>`.

Procedures
----------

General Considerations
~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/considerations-deploying-replica-set.rst

.. _replica-set-deploy-distributed-three-member:

Deploy a Geographically Redundant Three-Member Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /images/replica-set-three-members-geographically-distributed.rst

.. include:: /includes/steps/deploy-geographically-distributed-replica-set-3member.rst

.. _replica-set-deploy-distributed-four-member:

Deploy a Geographically Redundant Four-Member Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A geographically redundant four-member deployment has two additional
considerations:

- One host (e.g. ``mongodb3.example.net``) must be an :term:`arbiter`.
  This host can run on a system that is also used for an application server
  or on the same machine as another MongoDB process.

- You must decide how to distribute your systems. There are three
  possible architectures for the four-member replica set:

  - Three members in Site A, one :ref:`priority 0 member <replica-set-secondary-only-members>`
    in Site B, and an arbiter in Site A.

  - Two members in Site A, two :ref:`priority 0 members
    <replica-set-secondary-only-members>` in Site B, and an
    arbiter in Site A.

  - Two members in Site A, one priority 0 member in Site B, one
    priority 0 member in Site C, and an arbiter in site A.

  In most cases, the first architecture is preferable because it is the
  least complex.

To deploy a geographically redundant four-member set:
`````````````````````````````````````````````````````

.. include:: /includes/steps/deploy-geographically-distributed-replica-set-4member.rst

Deploy a Geographically Redundant Set with More than Four Members
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above procedures detail the steps necessary for deploying a
geographically redundant replica set. Larger replica set deployments
follow the same steps, but have additional considerations:

- Never deploy more than seven voting members.

- If you have an even number of members, use :ref:`the procedure for a
  four-member set <replica-set-deploy-distributed-four-member>`).
  Ensure that a single facility, "Site A", always has a majority of
  the members by deploying the :term:`arbiter` in that site. For
  example, if a set has six members, deploy at least three voting
  members in addition to the arbiter in Site A, and the remaining
  members in alternate sites.

- If you have an odd number of members, use :ref:`the procedure for a
  three-member set <replica-set-deploy-distributed-three-member>`.
  Ensure that a single facility, "Site A" always has a majority of the
  members of the set. For example, if a set has five members, deploy
  three members within Site A and two members in other facilities.

- If you have a majority of the members of the set *outside* of Site A
  and the network partitions to prevent communication between sites,
  the current primary in Site A will step down, even if none of the
  members outside of Site A are eligible to become primary.
================================================
Deploy a Replica Set for Testing and Development
================================================

.. default-domain:: mongodb

This procedure describes deploying a replica set in a development or
test environment. For a production deployment, refer to the
:doc:`/tutorial/deploy-replica-set` tutorial.

.. include:: /includes/introduction-deploy-replica-set.rst

Requirements
------------

For test and development systems, you can run your :program:`mongod`
instances on a local system, or within a virtual instance.

Before you can deploy a replica set, you must install MongoDB on
each system that will be part of your :term:`replica set`.
If you have not already installed MongoDB, see the :ref:`installation tutorials <tutorial-installation>`.

Before creating your replica set, you should verify that your network
configuration allows all possible connections between each member. For
a successful replica set deployment, every member must be able to
connect to every other member. For instructions on how to check
your connection, see :ref:`replica-set-troubleshooting-check-connection`.

Considerations
--------------

Replica Set Naming
~~~~~~~~~~~~~~~~~~

.. important:: These instructions should only be used for test or
   development deployments.

The examples in this procedure create a new replica set named ``rs0``.

.. include:: /includes/fact-unique-replica-set-names.rst

You will begin by starting three :program:`mongod` instances as
members of a replica set named ``rs0``.


Procedure
---------

1. Create the necessary data directories for each member by issuing a
   command similar to the following:

   .. code-block:: sh

      mkdir -p /srv/mongodb/rs0-0 /srv/mongodb/rs0-1 /srv/mongodb/rs0-2

   This will create directories called "rs0-0", "rs0-1", and "rs0-2", which
   will contain the instances' database files.

#. Start your :program:`mongod` instances in their own shell windows by issuing the following
   commands:

   First member:

   .. code-block:: sh

      mongod --port 27017 --dbpath /srv/mongodb/rs0-0 --replSet rs0 --smallfiles --oplogSize 128

   Second member:

   .. code-block:: sh

      mongod --port 27018 --dbpath /srv/mongodb/rs0-1 --replSet rs0 --smallfiles --oplogSize 128

   Third member:

   .. code-block:: sh

      mongod --port 27019 --dbpath /srv/mongodb/rs0-2 --replSet rs0 --smallfiles --oplogSize 128

   This starts each instance as a member of a replica set named
   ``rs0``, each running on a distinct port, and specifies the path to
   your data directory with the :option:`--dbpath <dbpath>` setting.
   If you are already using the suggested ports, select different ports.

   The :option:`--smallfiles <smallfiles>` and
   :option:`--oplogSize <oplogSize>` settings reduce the disk
   space that each :program:`mongod` instance uses. This is ideal for testing and
   development deployments as it prevents overloading your machine.
   For more information on these and other configuration
   options, see :doc:`/reference/configuration-options`.

#. Connect to one of your :program:`mongod` instances through the
   :program:`mongo` shell. You will need to indicate which instance by
   specifying its port number. For the sake of simplicity and clarity,
   you may want to choose the first one, as in the following command;

   .. code-block:: sh

      mongo --port 27017

#. In the :program:`mongo` shell, use :method:`rs.initiate()` to
   initiate the replica set. You can create a replica set
   configuration object in the :program:`mongo` shell environment, as
   in the following example:

   .. code-block:: javascript

      rsconf = {
                 _id: "rs0",
                 members: [
                            {
                             _id: 0,
                             host: "<hostname>:27017"
                            }
                          ]
               }

   replacing ``<hostname>`` with your system's hostname,
   and then pass the ``rsconf`` file to :method:`rs.initiate()` as
   follows:

   .. code-block:: javascript

      rs.initiate( rsconf )

#. Display the current :doc:`replica configuration </reference/replica-configuration>`
   by issuing the following command:

   .. code-block:: javascript

      rs.conf()

   The replica set configuration object resembles the following

   .. code-block:: javascript

      {
         "_id" : "rs0",
         "version" : 4,
         "members" : [
            {
               "_id" : 1,
               "host" : "localhost:27017"
            }
         ]
      }

#. In the :program:`mongo` shell connected to the :term:`primary`, add
   the second and third :program:`mongod` instances to the replica set
   using the :method:`rs.add()` method. Replace ``<hostname>`` with
   your system's hostname in the following examples:

   .. code-block:: javascript

      rs.add("<hostname>:27018")
      rs.add("<hostname>:27019")

   When complete, you should have a fully functional replica set.
   The new replica set will elect a :term:`primary`.

Check the status of your replica set at any time with the
:method:`rs.status()` operation.

.. seealso:: The documentation of the following shell functions for
   more information:

   - :method:`rs.initiate()`
   - :method:`rs.conf()`
   - :method:`rs.reconfig()`
   - :method:`rs.add()`

   You may also consider the `simple setup script
   <https://github.com/mongodb/mongo-snippets/blob/master/replication/simple-setup.py>`_
   as an example of a basic automatically-configured replica set.

   Refer to :doc:`Replica Set Read and Write Semantics </applications/replication>`
   for a detailed explanation of read and write semantics in MongoDB.
=================================================================
Deploy Replica Set and Configure Authentication and Authorization
=================================================================

.. default-domain:: mongodb

Overview
--------

With :doc:`authentication </core/authentication>` enabled, MongoDB
forces all clients to identify themselves before granting access to
the server. :doc:`Authorization </core/authorization>`, in turn,
allows administrators to define and limit the resources and operations
that a user can access. Using authentication and authorization is a
key part of a complete security strategy.

All MongoDB deployments support authentication. By default, MongoDB
does not require authorization checking. You can enforce authorization
checking when deploying MongoDB, or on an existing deploying; however,
you cannot enable authorization checking on a running deployment
without downtime.

This tutorial provides a procedure for creating a MongoDB :doc:`replica
set </core/replication-introduction>` that uses the challenge-response
authentication mechanism. The tutorial includes creation of a minimal
authorization system to support basic operations.

Considerations
--------------

Authentication
~~~~~~~~~~~~~~

In this procedure, you will configure MongoDB using the default
challenge-response authentication mechanism, using the
:setting:`keyFile` to supply the password for :ref:`inter-process
authentication <inter-process-auth-key-file>`. The content of the key
file is the shared secret used for all internal authentication.

All deployments that enforce authorization checking should have one
*user administrator* user that can create new users and modify
existing users. During this procedure you will create a user
administrator that you will use to administer this
deployment.

.. include:: /includes/considerations-deploying-replica-set.rst

Procedure
---------

This procedure deploys a replica set in which all members use the same key
file.

.. include:: /includes/steps/deploy-replica-set-with-auth.rst
====================
Deploy a Replica Set
====================

.. default-domain:: mongodb

.. include:: /includes/introduction-deploy-replica-set.rst

Requirements
------------

For production deployments, you should maintain as much separation between
members as possible by hosting the :program:`mongod`
instances on separate machines. When using virtual machines for
production deployments, you should place each :program:`mongod`
instance on a separate host server serviced by redundant power circuits
and redundant network paths.

Before you can deploy a replica set, you must install MongoDB on
each system that will be part of your :term:`replica set`.
If you have not already installed MongoDB, see the :ref:`installation tutorials <tutorial-installation>`.

Before creating your replica set, you should verify that your network
configuration allows all possible connections between each member. For
a successful replica set deployment, every member must be able to
connect to every other member. For instructions on how to check
your connection, see :ref:`replica-set-troubleshooting-check-connection`.

.. _considerations-when-deploying-rs:

Considerations When Deploying a Replica Set
-------------------------------------------

.. include:: /includes/considerations-deploying-replica-set.rst

Procedure
---------

.. include:: /includes/steps/deploy-replica-set.rst
.. _sharding-procedure-setup:

========================
Deploy a Sharded Cluster
========================

.. default-domain:: mongodb

Use the following sequence of tasks to deploy a sharded cluster:

.. include:: /includes/warning-sharding-hostnames.rst

.. _sharding-setup-start-cfgsrvr:

Start the Config Server Database Instances
------------------------------------------

The config server processes are :program:`mongod` instances that store
the cluster's metadata. You designate a :program:`mongod` as a config
server using the :option:`--configsvr <mongod --configsvr>` option. Each
config server stores a complete copy of the cluster's metadata.

In production deployments, you must deploy exactly three config server
instances, each running on different servers to assure good uptime and
data safety. In test environments, you can run all three instances on a
single server.

.. important:: All members of a sharded cluster must be able to
   connect to *all* other members of a sharded cluster, including all
   shards and all config servers. Ensure that the network and
   security systems including all interfaces and firewalls, allow
   these connections.


1. Create data directories for each of the three config server
   instances. By default, a config server stores its data files in the
   `/data/configdb` directory. You can choose a different location. To
   create a data directory, issue a command similar to the following:

   .. code-block:: sh

      mkdir /data/configdb

#. Start the three config server instances. Start each by issuing a
   command using the following syntax:

   .. code-block:: sh

      mongod --configsvr --dbpath <path> --port <port>

   The default port for config servers is ``27019``. You can specify a
   different port. The following example starts a config server using
   the default port and default data directory:

   .. code-block:: sh

      mongod --configsvr --dbpath /data/configdb --port 27019

   For additional command options, see :doc:`/reference/program/mongod` or
   :doc:`/reference/configuration-options`.

   .. include:: /includes/note-config-server-startup.rst

.. _sharding-setup-start-mongos:

Start the ``mongos`` Instances
------------------------------

The :program:`mongos` instances are lightweight and do not require data
directories. You can run a :program:`mongos` instance on a system that
runs other cluster components, such as on an application server or a
server running a :program:`mongod` process. By default, a
:program:`mongos` instance runs on port ``27017``.

When you start the :program:`mongos` instance, specify the hostnames of
the three config servers, either in the configuration file or as command
line parameters.

.. include:: /includes/tip-hostnames.rst

To start a :program:`mongos` instance, issue a command using the following syntax:

.. code-block:: sh

   mongos --configdb <config server hostnames>

For example, to start a :program:`mongos` that connects to config server
instance running on the following hosts and on the default ports:

- ``cfg0.example.net``
- ``cfg1.example.net``
- ``cfg2.example.net``

You would issue the following command:

.. code-block:: sh

   mongos --configdb cfg0.example.net:27019,cfg1.example.net:27019,cfg2.example.net:27019

Each :program:`mongos` in a sharded cluster must use the same
:setting:`~sharding.configDB` string, with identical host names listed in
identical order.

If you start a :program:`mongos` instance with a string that *does
not* exactly match the string used by the other :program:`mongos`
instances in the cluster, the :program:`mongos` return a
:ref:`config-database-string-error` error and refuse to start.

.. _sharding-setup-add-shards:

Add Shards to the Cluster
-------------------------

A :term:`shard` can be a standalone :program:`mongod` or a
:term:`replica set`. In a production environment, each shard
should be a replica set.

1. From a :program:`mongo` shell, connect to the :program:`mongos`
   instance. Issue a command using the following syntax:

   .. code-block:: sh

      mongo --host <hostname of machine running mongos> --port <port mongos listens on>

   For example, if a :program:`mongos` is accessible at
   ``mongos0.example.net`` on port ``27017``, issue the following
   command:

   .. code-block:: sh

      mongo --host mongos0.example.net --port 27017

#. Add each shard to the cluster using the :method:`sh.addShard()`
   method, as shown in the examples below. Issue :method:`sh.addShard()`
   separately for each shard. If the shard is a replica set, specify the
   name of the replica set and specify a member of the set. In
   production deployments, all shards should be replica sets.

   .. optional:: You can instead use the :dbcommand:`addShard` database
      command, which lets you specify a name and maximum size for the
      shard. If you do not specify these, MongoDB automatically assigns
      a name and maximum size. To use the database command, see
      :dbcommand:`addShard`.

   The following are examples of adding a shard with
   :method:`sh.addShard()`:

   - To add a shard for a replica set named ``rs1`` with a member
     running on port ``27017`` on ``mongodb0.example.net``, issue the
     following command:

     .. code-block:: javascript

        sh.addShard( "rs1/mongodb0.example.net:27017" )

     .. versionchanged:: 2.0.3

     For MongoDB versions prior to 2.0.3, you must specify all members of the replica set. For
     example:

     .. code-block:: javascript

        sh.addShard( "rs1/mongodb0.example.net:27017,mongodb1.example.net:27017,mongodb2.example.net:27017" )

   - To add a shard for a standalone :program:`mongod` on port ``27017``
     of ``mongodb0.example.net``, issue the following command:

      .. code-block:: javascript

         sh.addShard( "mongodb0.example.net:27017" )

   .. note:: It might take some time for :term:`chunks <chunk>` to
      migrate to the new shard.

.. _sharding-setup-enable-sharding:

Enable Sharding for a Database
------------------------------

Before you can shard a collection, you must enable sharding for the
collection's database. Enabling sharding for a database does not
redistribute data but make it possible to shard the collections in that
database.

Once you enable sharding for a database, MongoDB assigns a
:term:`primary shard` for that database where MongoDB stores all data
before sharding begins.

1. From a :program:`mongo` shell, connect to the :program:`mongos`
   instance. Issue a command using the following syntax:

   .. code-block:: sh

      mongo --host <hostname of machine running mongos> --port <port mongos listens on>

#. Issue the :method:`sh.enableSharding()` method, specifying the name
   of the database for which to enable sharding. Use the following syntax:

   .. code-block:: javascript

      sh.enableSharding("<database>")

Optionally, you can enable sharding for a database using the
:dbcommand:`enableSharding` command, which uses the following syntax:

.. code-block:: javascript

   db.runCommand( { enableSharding: <database> } )

.. _sharding-setup-shard-collection:

Enable Sharding for a Collection
--------------------------------

You enable sharding on a per-collection basis.

1. Determine what you will use for the :term:`shard key`. Your selection
   of the shard key affects the efficiency of sharding. See the
   selection considerations listed in the :ref:`sharding-shard-key-selection`.

#. If the collection already contains data you must create an index on
   the :term:`shard key` using :method:`~db.collection.ensureIndex()`.
   If the collection is empty then MongoDB will create the index as part
   of the :method:`sh.shardCollection()` step.

#. Enable sharding for a collection by issuing the
   :method:`sh.shardCollection()` method in the :program:`mongo` shell.
   The method uses the following syntax:

   .. code-block:: javascript

      sh.shardCollection("<database>.<collection>", shard-key-pattern)

   Replace the ``<database>.<collection>`` string with the full
   namespace of your database, which consists of the name of your
   database, a dot (e.g. ``.``), and the full name of the collection.
   The ``shard-key-pattern`` represents your shard key, which you
   specify in the same form as you would an :method:`index
   <db.collection.ensureIndex()>` key pattern.

   .. example:: The following sequence of commands shards four collections:

      .. code-block:: javascript

         sh.shardCollection("records.people", { "zipcode": 1, "name": 1 } )
         sh.shardCollection("people.addresses", { "state": 1, "_id": 1 } )
         sh.shardCollection("assets.chairs", { "type": 1, "_id": 1 } )

         db.alerts.ensureIndex( { _id : "hashed" } )
         sh.shardCollection("events.alerts", { "_id": "hashed" } )

   In order, these operations shard:

   #. The ``people`` collection in the ``records`` database using the
      shard key ``{ "zipcode": 1, "name": 1 }``.

      This shard key distributes documents by the value of the
      ``zipcode`` field. If a number of documents have the same value
      for this field, then that :term:`chunk` will be :ref:`splittable
      <sharding-shard-key-cardinality>` by the values of the ``name``
      field.

   #. The ``addresses`` collection in the ``people`` database using the
      shard key ``{ "state": 1, "_id": 1 }``.

      This shard key distributes documents by the value of the ``state``
      field. If a number of documents have the same value for this
      field, then that :term:`chunk` will be :ref:`splittable
      <sharding-shard-key-cardinality>` by the values of the ``_id``
      field.

   #. The ``chairs`` collection in the ``assets`` database using the shard key
      ``{ "type": 1, "_id": 1 }``.

      This shard key distributes documents by the value of the ``type``
      field. If a number of documents have the same value for this
      field, then that :term:`chunk` will be :ref:`splittable
      <sharding-shard-key-cardinality>` by the values of the ``_id``
      field.

   #. The ``alerts`` collection in the ``events`` database using the shard key
      ``{ "_id": "hashed" }``.

      .. versionadded:: 2.4

      This shard key distributes documents by a hash of the value of
      the ``_id`` field.  MongoDB computes the hash of the ``_id``
      field for the :ref:`hashed index <index-hashed-index>`,
      which should provide an even distribution of documents across a
      cluster.
==========================================
Enable Authentication in a Sharded Cluster
==========================================

.. default-domain:: mongodb

.. versionadded:: 2.0
   Support for authentication with sharded clusters.

Overview
--------

When authentication is enabled on a sharded cluster every client that
accesses the cluster must provide credentials. This includes MongoDB
instances that access each other within the cluster.

To enable authentication on a sharded cluster, you must enable
authentication individually on each component of the cluster. This means
enabling authentication on each :program:`mongos` and each
:program:`mongod`, including each config server, and all members
of a shard's replica set.

Authentication requires an authentication mechanism and, in most cases, a
:setting:`key file <keyFile>`. The content of the key file must be the
same on all cluster members.

Procedure
---------

.. include:: /includes/steps/enable-authentication-in-sharded-cluster.rst

Related Documents
-----------------

- :doc:`/core/authentication`

- :doc:`/security`

- :doc:`/tutorial/configure-x509`
===========================================================
Enable Authentication after Creating the User Administrator
===========================================================

.. default-domain:: mongodb

Overview
--------

Enabling authentication on a MongoDB instance restricts access to the
instance by requiring that users identify themselves when
connecting. In this procedure, you will create the instance's first
user, which must be a user administrator and then enable
authentication. Then, you can authenticate as the user administrator
to create additional users and grant additional access to the instance.

This procedures outlines how enable authentication after creating the
user administrator. The approach requires a restart. To enable
authentication without restarting, see :doc:`/tutorial/enable-authentication`.

Considerations
--------------

This document outlines a procedure for enabling
authentication for MongoDB instance where you create the first user on
an existing MongoDB system that does not require authentication before
restarting the instance and requiring authentication. You can use the
:ref:`localhost exception <localhost-exception>` to gain access to a
system with no users and authentication enabled. See
:doc:`/tutorial/enable-authentication` for the description of that
procedure.

Procedure
---------

.. include:: /includes/steps/create-admin-then-enable-authentication.rst

Next Steps
----------

If you need to disable authentication for any reason, restart the process
without the :setting:`~security.authentication` or :setting:`~security.keyFile` option.
============================
Enable Client Authentication
============================

.. default-domain:: mongodb

Overview
--------

Enabling authentication on a MongoDB instance restricts access to the
instance by requiring that users identify themselves when connecting. In
this procedure, you enable authentication and then create the instance's
first user, which must be a user administrator. The user administrator
grants further access to the instance by creating additional users.

When you enable authentication prior to creating a user, you can access
the instance only through the :ref:`localhost exception
<localhost-exception>`, which allows access only through a local client.
MongoDB disables this access after you create the user administrator.

Considerations
--------------

If you create the user administrator before enabling authentication,
MongoDB disables the :ref:`localhost exception <localhost-exception>`.
In that case, you must use the
":doc:`/tutorial/enable-authentication-without-bypass`" procedure to
enable authentication.

Procedure
---------

.. include:: /includes/steps/enable-authentication.rst

Next Steps
----------

If you need to disable authentication for any reason, restart the process
without the :setting:`~security.authentication` or :setting:`~security.keyFile` option.
:orphan:

==================
Enable Text Search
==================

.. default-domain:: mongodb

.. versionadded:: 2.4

.. versionchanged:: 2.6

   MongoDB enables text search feature by default. Earlier versions of
   MongoDB required that you enable the feature manually using the
   :parameter:`textSearchEnabled` option.
===========================================
Enforce Unique Keys for Sharded Collections
===========================================

.. default-domain:: mongodb

Overview
--------

The :method:`unique <db.collection.ensureIndex()>` constraint on
indexes ensures that only one document can have a value for a field in
a :term:`collection`. For :ref:`sharded collections these unique indexes
cannot enforce uniqueness <limit-sharding-unique-indexes>` because
insert and indexing operations are local to each shard.

MongoDB does not support creating new unique indexes in sharded clusters
and will not allow you to shard collections with unique indexes on
fields other than the ``_id`` field.

If you need to ensure that a field is always unique in all
collections in a sharded environment, there are three options:

#. Enforce uniqueness of the :ref:`shard key <sharding-shard-key>`.

   MongoDB *can* enforce uniqueness for the :term:`shard key`. For
   compound shard keys, MongoDB will enforce uniqueness on the
   *entire* key combination, and not for a specific component of the
   shard key.

   You cannot specify a unique constraint on a
   :ref:`hashed index <index-type-hashed>`.

#. Use a secondary collection to enforce uniqueness.

   Create a minimal collection that only contains the unique field and
   a reference to a document in the main collection. If you always
   insert into a secondary collection *before* inserting to the main
   collection, MongoDB will produce an error if you attempt to use a
   duplicate key.

   If you have a small data set, you may not need to shard this
   collection and you can create multiple unique indexes. Otherwise
   you can shard on a single unique key.

#. Use guaranteed unique identifiers.

   Universally unique identifiers (i.e. UUID) like the ``ObjectId`` are
   guaranteed to be unique.

Procedures
----------

Unique Constraints on the Shard Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. _sharding-pattern-unique-procedure-shard-key:

Process
```````

To shard a collection using the ``unique`` constraint, specify the
:dbcommand:`shardCollection` command in the following form:

.. code-block:: javascript

   db.runCommand( { shardCollection : "test.users" , key : { email : 1 } , unique : true } );

Remember that the ``_id`` field index is always unique. By default, MongoDB
inserts an ``ObjectId`` into the ``_id`` field. However,
you can manually insert your own value into the ``_id`` field and
use this as the shard key. To use the
``_id`` field as the shard key, use the following operation:

.. code-block:: javascript

   db.runCommand( { shardCollection : "test.users" } )

Limitations
```````````

- You can only enforce uniqueness on one single field in the collection
  using this method.

- If you use a compound shard key, you can only enforce
  uniqueness on the *combination* of component keys in the shard
  key.

In most cases, the best shard keys are compound keys that include elements
that permit :ref:`write scaling <sharding-shard-key-write-scaling>`
and :ref:`query isolation <sharding-shard-key-query-isolation>`, as
well as :ref:`high cardinality <sharding-shard-key-cardinality>`.
These ideal shard keys are not often the same keys that require
uniqueness and enforcing unique values in these collections requires a
different approach.

Unique Constraints on Arbitrary Fields
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you cannot use a unique field as the shard key or if you need to
enforce uniqueness over multiple fields, you must create another
:term:`collection` to act as a "proxy collection". This collection
must contain both a reference to the original document (i.e. its
``ObjectId``) and the unique key.

If you must shard this "proxy" collection, then shard on the unique
key using the :ref:`above procedure <sharding-pattern-unique-procedure-shard-key>`;
otherwise, you can simply create multiple unique indexes on the
collection.

Process
```````

Consider the following for the "proxy collection:"

.. code-block:: javascript

   {
     "_id" : ObjectId("...")
     "email" ": "..."
   }

The ``_id`` field holds the ``ObjectId`` of the :term:`document`
it reflects, and the ``email`` field is the field on which you want to
ensure uniqueness.

To shard this collection, use the following operation
using the ``email`` field as the :term:`shard key`:

.. code-block:: javascript

   db.runCommand( { shardCollection : "records.proxy" ,
                    key : { email : 1 } ,
                    unique : true } );

If you do not need to shard the proxy collection, use the following
command to create a unique index on the ``email`` field:

.. code-block:: javascript

   db.proxy.ensureIndex( { "email" : 1 }, { unique : true } )

You may create multiple unique indexes on this collection if you do
not plan to shard the ``proxy`` collection.

To insert documents, use the following procedure in the
:ref:`JavaScript shell <mongo>`:

.. code-block:: javascript

   db = db.getSiblingDB('records');

   var primary_id = ObjectId();

   db.proxy.insert({
      "_id" : primary_id
      "email" : "example@example.net"
   })

   // if: the above operation returns successfully,
   // then continue:

   db.information.insert({
      "_id" : primary_id
      "email": "example@example.net"
      // additional information...
   })

You must insert a document into the ``proxy`` collection first. If
this operation succeeds, the ``email`` field is unique, and you may
continue by inserting the actual document into the ``information``
collection.

.. see:: The full documentation of: :method:`~db.collection.ensureIndex()`
   and :dbcommand:`shardCollection`.

Considerations
``````````````

- Your application must catch errors when inserting documents into the
  "proxy" collection and must enforce consistency between the two
  collections.

- If the proxy collection requires sharding, you must shard on the
  single field on which you want to enforce uniqueness.

- To enforce uniqueness on more than one field using sharded proxy
  collections, you must have *one* proxy collection for *every* field
  for which to enforce uniqueness. If you create multiple unique
  indexes on a single proxy collection, you will *not* be able to
  shard proxy collections.

Use Guaranteed Unique Identifier
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The best way to ensure a field has unique values is to generate
universally unique identifiers (UUID,) such as MongoDB's '``ObjectId``
values.

This approach is particularly useful for the``_id`` field, which
*must* be unique: for collections where you are *not* sharding by the
``_id`` field the application is responsible for ensuring that the
``_id`` field is unique.
.. _indexes-ensure-indexes-fit-ram:

=========================
Ensure Indexes Fit in RAM
=========================

.. default-domain:: mongodb

For the fastest processing, ensure that your indexes fit entirely in RAM so
that the system can avoid reading the index from disk.

To check the size of your indexes, use the
:method:`db.collection.totalIndexSize()` helper, which returns data in
bytes:

.. code-block:: javascript

   > db.collection.totalIndexSize()
   4294976499

The above example shows an index size of almost 4.3 gigabytes. To ensure
this index fits in RAM, you must not only have more than that much RAM
available but also must have RAM available for the rest of the
:term:`working set`. Also remember:

If you have and use multiple collections, you must consider the size
of all indexes on all collections. The indexes and the working set must be able to
fit in memory at the same time.

There are some limited cases where indexes do not need
to fit in memory. See :ref:`indexing-right-handed`.

.. seealso:: :dbcommand:`collStats` and :method:`db.collection.stats()`

.. _indexing-right-handed:

Indexes that Hold Only Recent Values in RAM
-------------------------------------------

Indexes do not have to fit *entirely* into RAM in all cases. If the
value of the indexed field increments with every insert, and most queries
select recently added documents; then MongoDB only needs to keep the
parts of the index that hold the most recent or "right-most" values in
RAM. This allows for efficient index use for read and write
operations and minimize the amount of RAM required to support the
index.
==========================================
Evaluate Performance of Current Operations
==========================================

.. default-domain:: mongodb

The following sections describe techniques for evaluating operational
performance.

Use the Database Profiler to Evaluate Operations Against the Database
---------------------------------------------------------------------

.. todo Add link below: :doc:`database profiler </tutorial/manage-the-database-profiler>`

MongoDB provides a database profiler that shows performance
characteristics of each operation against the database. Use the profiler
to locate any queries or write operations that are running slow. You can
use this information, for example, to determine what indexes to create.

.. todo Add below: , see :doc:`/tutorial/manage-the-database-profiler` and ...

For more information, see :ref:`database-profiling`.

Use ``db.currentOp()`` to Evaluate ``mongod`` Operations
--------------------------------------------------------

The :method:`db.currentOp()` method reports on current operations
running on a :program:`mongod` instance.

Use ``$explain`` to Evaluate Query Performance
----------------------------------------------

The :method:`~cursor.explain()` method returns statistics
on a query, and reports the index MongoDB selected to fulfill the
query, as well as information about the internal operation of the
query.

.. example:: To use :method:`~cursor.explain()` on a query
   for documents matching the expression ``{ a: 1 }``, in the
   collection named ``records``, use an operation that resembles the
   following in the :program:`mongo` shell:

   .. code-block:: javascript

      db.records.find( { a: 1 } ).explain()

.. todo Link to Kay's new explain doc
============================
Add Members to a Replica Set
============================

.. default-domain:: mongodb

Overview
--------

This tutorial explains how to add an additional member to an existing
:term:`replica set`. For background on replication deployment patterns,
see the :doc:`/core/replica-set-architectures` document.

Maximum Voting Members
~~~~~~~~~~~~~~~~~~~~~~

A replica set can have a maximum of seven :ref:`voting members
<replica-set-election-internals>`. To add a member to a replica set
that already has seven votes, you must either add the member as a
:ref:`non-voting member <replica-set-non-voting-members>` or remove a
vote from an :data:`existing member
<local.system.replset.members[n].votes>`.

Control Scripts
~~~~~~~~~~~~~~~

In production deployments you can configure a :term:`control script`
to manage member processes.

Existing Members
~~~~~~~~~~~~~~~~

You can use these procedures to add new members to an existing
set. You can also use the same procedure to "re-add" a removed
member. If the removed member's data is still relatively recent, it
can recover and catch up easily.

Data Files
~~~~~~~~~~

If you have a backup or snapshot of an existing member, you can move
the data files (e.g.  the :setting:`~storage.dbPath` directory) to a new system
and use them to quickly initiate a new member. The files must be:

- A valid copy of the data files from a member of the same replica
  set. See :doc:`/tutorial/backup-with-filesystem-snapshots`
  document for more information.

  .. important:: Always use filesystem snapshots to create a copy of a
     member of the existing replica set. **Do not** use
     :program:`mongodump` and :program:`mongorestore` to seed a new
     replica set member.

- More recent than the oldest operation in the :term:`primary's
  <primary>` :term:`oplog`. The new member must be able to become
  current by applying operations from the primary's oplog.

Requirements
------------

#. An active replica set.

#. A new MongoDB system capable of supporting your data set, accessible by
   the active replica set through the network.

Otherwise, use the MongoDB :ref:`installation tutorial
<tutorials-installation>` and the :doc:`/tutorial/deploy-replica-set`
tutorials.

Procedures
----------

Prepare the Data Directory
~~~~~~~~~~~~~~~~~~~~~~~~~~

Before adding a new member to an existing :term:`replica set`, prepare
the new member's :term:`data directory <dbpath>` using one of the
following strategies:

- Make sure the new member's data directory *does not* contain data. The
  new member will copy the data from an existing member.

  If the new member is in a :term:`recovering` state, it must exit and
  become a :term:`secondary` before MongoDB
  can copy all data as part of the replication process. This process
  takes time but does not require administrator intervention.

- Manually copy the data directory from an existing member. The new
  member becomes a secondary member and will catch up to the current
  state of the replica set. Copying the data over may shorten the
  amount of time for the new member to become current.

  Ensure that you can copy the data directory to the new member and
  begin replication within the :ref:`window allowed by the oplog
  <replica-set-oplog-sizing>`. Otherwise, the new instance will have
  to perform an initial sync, which completely resynchronizes the
  data, as described in :doc:`/tutorial/resync-replica-set-member`.

  Use :method:`rs.printReplicationInfo()` to check the current state
  of replica set members with regards to the oplog.

For background on replication deployment patterns, see the
:doc:`/core/replica-set-architectures` document.

.. _replica-set-add-member:

Add a Member to an Existing Replica Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Start the new :program:`mongod` instance. Specify the data directory
   and the replica set name. The following example specifies the
   ``/srv/mongodb/db0`` data directory and the ``rs0`` replica set:

   .. code-block:: sh

      mongod --dbpath /srv/mongodb/db0 --replSet rs0

   Take note of the host name and port information for the new
   :program:`mongod` instance.

   For more information on configuration options, see the
   :program:`mongod` manual page.

   .. optional::

      You can specify the data directory and replica set in the
      ``mongo.conf`` :doc:`configuration file
      </reference/configuration-options>`, and start the
      :program:`mongod` with the following command:

      .. code-block:: sh

         mongod --config /etc/mongodb.conf

#. Connect to the replica set's primary.

   You can only add members while connected to the primary. If you do
   not know which member is the primary, log into any member of the
   replica set and issue the :method:`db.isMaster()` command.

#. Use :method:`rs.add()` to add the new member to the replica set. For
   example, to add a member at host ``mongodb3.example.net``, issue the
   following command:

   .. code-block:: javascript

      rs.add("mongodb3.example.net")

   You can include the port number, depending on your setup:

   .. code-block:: javascript

      rs.add("mongodb3.example.net:27017")

#. Verify that the member is now part of the replica set. Call the
   :method:`rs.conf()` method, which displays the :doc:`replica set
   configuration </reference/replica-configuration>`:

   .. code-block:: javascript

      rs.conf()

   To view replica set status, issue the :method:`rs.status()` method.
   For a description of the status fields, see
   :doc:`/reference/command/replSetGetStatus`.

.. _replica-set-add-member-alternate-procedure:

Configure and Add a Member
~~~~~~~~~~~~~~~~~~~~~~~~~~

You can add a member to a replica set by passing to the
:method:`rs.add()` method a :data:`~local.system.replset.members`
document. The document must be in the form of a
:data:`local.system.replset.members` document. These documents define
a replica set member in the same form as the :ref:`replica set
configuration document <replica-set-configuration-document>`.

.. important:: Specify a value for the ``_id`` field of the
   :data:`~local.system.replset.members` document.  MongoDB does not
   automatically populate the ``_id`` field in this case. Finally, the
   :data:`~local.system.replset.members` document must declare the
   ``host`` value. All other fields are optional.

.. example::

   To add a member with the following configuration:

   - an ``_id`` of ``1``.

   - a :data:`hostname and port number
     <local.system.replset.members[n].host>` of
     ``mongodb3.example.net:27017``.

   - a :data:`priority <local.system.replset.members[n].priority>` value
     within the replica set of ``0``.

   - a configuration as :data:`hidden
     <local.system.replset.members[n].hidden>`,

   Issue the following:

   .. code-block:: javascript

      rs.add({_id: 1, host: "mongodb3.example.net:27017", priority: 0, hidden: true})
.. _ttl-collections:

===========================================
Expire Data from Collections by Setting TTL
===========================================

.. default-domain:: mongodb

.. versionadded:: 2.2

This document provides an introduction to MongoDB's "*time to live*"
or ":term:`TTL`" collection feature. TTL collections make it possible
to store data in MongoDB and have the :program:`mongod` automatically
remove data after a specified number of seconds or at a specific
clock time.

Data expiration is useful for some classes of information, including
machine generated event data, logs, and session information that only
need to persist for a limited period of time.

A special index type supports the implementation of TTL collections.
TTL relies on a background thread in :program:`mongod` that reads the
date-typed values in the index and removes expired :term:`documents
<document>` from the collection.

Considerations
--------------

- The ``_id`` field does not support TTL indexes.

- You cannot create a TTL index on a field that already has an index.

- A document will not expire if the indexed field does not exist.

- A document will not expire if the indexed field is not a date
  :term:`BSON type <BSON types>` or an array of date :term:`BSON types
  <BSON types>`.

- The TTL index may not be compound (may not have multiple fields).

- If the TTL field holds an array, and there are multiple date-typed
  data in the index, the document will expire when the *lowest* (i.e.
  earliest) date matches the expiration threshold.

- You cannot create a TTL index on a capped collection, because
  MongoDB cannot remove documents from a capped collection.

- You cannot use :method:`~db.collection.ensureIndex()` to change the
  value of ``expireAfterSeconds``. Instead use the
  :dbcommand:`collMod` database command in conjunction with the
  :collflag:`index` collection flag.

- When you build a TTL index in the :ref:`background
  <index-creation-background>`, the TTL thread can begin deleting
  documents while the index is building. If you build a TTL index in
  the foreground, MongoDB begins removing expired documents as soon
  as the index finishes building.

When the TTL thread is active, you will see :doc:`delete
</core/write-operations>` operations in the output of
:method:`db.currentOp()` or in the data collected by the
:ref:`database profiler <database-profiler>`.

When using TTL indexes on :term:`replica sets <replica set>`, the
TTL background thread *only* deletes documents on :term:`primary`
members. However, the TTL background thread *does* run on secondaries.
:term:`Secondary` members replicate deletion operations
from the primary.

.. include:: /includes/fact-ttl-collection-background-timing.rst

All collections with an index using the
``expireAfterSeconds`` option have :collflag:`usePowerOf2Sizes`
enabled. Users cannot modify this setting. As a result of enabling
:collflag:`usePowerOf2Sizes`, MongoDB must allocate more disk space
relative to data size. This approach helps mitigate the possibility
of storage fragmentation caused by frequent delete operations and
leads to more predictable storage use patterns.

Procedures
----------

To enable TTL for a collection, use the
:method:`~db.collection.ensureIndex()` method to create a TTL index,
as shown in the examples below.

With the exception of the background thread, a TTL index supports
queries in the same way normal indexes do. You can use TTL indexes to
expire documents in one of two ways, either:

- remove documents a certain number of seconds after creation. The
  index will support queries for the creation time of the
  documents. Alternately,

- specify an explicit expiration time. The index will support queries
  for the expiration-time of the document.

Expire Documents after a Certain Number of Seconds
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To expire data after a certain number of seconds, create a TTL index on
a field that holds values of BSON date type or an array of BSON
date-typed objects *and* specify a positive non-zero value in the
``expireAfterSeconds`` field. A document will expire when the number of
seconds in the ``expireAfterSeconds`` field has passed since the time
specified in its indexed field. [#field-is-array-of-dates]_

For example, the following operation creates an index on the
``log_events`` collection's ``createdAt`` field and specifies the
``expireAfterSeconds`` value of ``3600`` to set the expiration time to
be one hour after the time specified by ``createdAt``.

.. code-block:: javascript

   db.log_events.ensureIndex( { "createdAt": 1 }, { expireAfterSeconds: 3600 } )

When adding documents to the ``log_events`` collection, set the
``createdAt`` field to the current time:

.. code-block:: javascript

   db.log_events.insert( {
      "createdAt": new Date(),
      "logEvent": 2,
      "logMessage": "Success!"
   } )

MongoDB will automatically delete documents from the ``log_events``
collection when the document's ``createdAt`` value
[#field-is-array-of-dates]_ is older than the number of seconds
specified in ``expireAfterSeconds``.

.. [#field-is-array-of-dates] If the field contains an array of BSON
   date-typed objects, data expires if at least one of BSON date-typed
   object is older than the number of seconds specified in
   ``expireAfterSeconds``.

.. seealso:: :update:`$currentDate` operator

Expire Documents at a Certain Clock Time
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To expire documents at a certain clock time, begin by creating a TTL
index on a field that holds values of BSON date type or an array of
BSON date-typed objects *and* specify an ``expireAfterSeconds`` value
of ``0``. For each document in the collection, set the indexed date
field to a value corresponding to the time the document should expire.
If the indexed date field contains a date in the past, MongoDB
considers the document expired.

For example, the following operation creates an index on the
``app.events`` collection's ``expireAt`` field and specifies the
``expireAfterSeconds`` value of ``0``:

.. code-block:: javascript

   db.app.events.ensureIndex( { "expireAt": 1 }, { expireAfterSeconds: 0 } )

For each document, set the value of ``expireAt`` to correspond to the
time the document should expire. For instance, the following
:method:`~db.collection.insert()` operation adds a document that should
expire at ``July 22, 2013 14:00:00``.

.. code-block:: javascript

   db.app.events.insert( {
      "expireAt": new Date('July 22, 2013 14:00:00'),
      "logEvent": 2,
      "logMessage": "Success!"
   } )

MongoDB will automatically delete documents from the ``app.events``
collection when the documents' ``expireAt`` value is older than the
number of seconds specified in ``expireAfterSeconds``, i.e. ``0``
seconds older in this case. As such, the data expires at the specified
``expireAt`` value.
================================
Force a Member to Become Primary
================================

.. default-domain:: mongodb

Synopsis
--------

You can force a :term:`replica set` member to become :term:`primary`
by giving it a higher
:data:`~local.system.replset.members[n].priority` value than any other
member in the set.

Optionally, you also can force a member never to become primary by
setting its :data:`~local.system.replset.members[n].priority` value to
``0``, which means the member can never seek :ref:`election
<replica-set-elections>` as primary. For more information, see
:ref:`replica-set-secondary-only-members`.

Procedures
----------

.. _replica-set-force-member-to-be-primary-via-priority-setting:

Force a Member to be Primary by Setting its Priority High
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.0

For more information on priorities, see
:data:`~local.system.replset.members[n].priority`.

This procedure assumes your current :term:`primary` is
``m1.example.net`` and that you'd like to instead make ``m3.example.net`` primary.
The procedure also assumes you have a three-member :term:`replica set` with the
configuration below. For more information on configurations, see :ref:`Replica Set
Configuration Use <replica-set-reconfiguration-usage>`.

This procedure assumes this configuration:

.. code-block:: javascript

   {
       "_id" : "rs",
       "version" : 7,
       "members" : [
           {
               "_id" : 0,
               "host" : "m1.example.net:27017"
           },
           {
               "_id" : 1,
               "host" : "m2.example.net:27017"
           },
           {
               "_id" : 2,
               "host" : "m3.example.net:27017"
           }
       ]
   }

1. In the :program:`mongo` shell, use the following sequence of operations
   to make ``m3.example.net`` the primary:

   .. code-block:: javascript

      cfg = rs.conf()
      cfg.members[0].priority = 0.5
      cfg.members[1].priority = 0.5
      cfg.members[2].priority = 1
      rs.reconfig(cfg)

   This sets ``m3.example.net`` to have a higher
   :data:`local.system.replset.members[n].priority` value than the other :program:`mongod`
   instances.

   The following sequence of events occur:

   - ``m3.example.net`` and ``m2.example.net`` sync with
     ``m1.example.net`` (typically within 10 seconds).

   - ``m1.example.net`` sees that it no longer has highest priority and,
     in most cases, steps down. ``m1.example.net`` *does not* step down
     if ``m3.example.net``'s sync is far behind. In that case,
     ``m1.example.net`` waits until ``m3.example.net`` is within 10
     seconds of its optime and then steps down. This minimizes the
     amount of time with no primary following failover.

   - The step down forces on election in which ``m3.example.net``
     becomes primary based on its :data:`priority
     <local.system.replset.members[n].priority>` setting.

#. Optionally, if ``m3.example.net`` is more than 10 seconds behind
   ``m1.example.net``'s optime, and if you don't need to have a primary
   designated within 10 seconds, you can force ``m1.example.net`` to
   step down by running:

   .. code-block:: javascript

      db.adminCommand({replSetStepDown: 86400, force: 1})

   This prevents ``m1.example.net`` from being primary for 86,400
   seconds (24 hours), even if there is no other member that can become primary.
   When ``m3.example.net`` catches up with ``m1.example.net`` it will
   become primary.

   If you later want to make ``m1.example.net``
   primary again while it waits for ``m3.example.net`` to catch up,
   issue the following command to make ``m1.example.net`` seek election
   again:

   .. code-block:: javascript

      rs.freeze()

   The :method:`rs.freeze()` provides a wrapper around the
   :dbcommand:`replSetFreeze` database command.

.. _replica-set-force-member-to-be-primary-via-dbcommands:

Force a Member to be Primary Using Database Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 1.8

Consider a :term:`replica set` with the following members:

- ``mdb0.example.net`` - the  current :term:`primary`.
- ``mdb1.example.net`` - a :term:`secondary`.
- ``mdb2.example.net`` - a secondary .

To force a member to become primary use the following procedure:

1. In a :program:`mongo` shell, run :method:`rs.status()` to ensure your replica
   set is running as expected.

#. In a :program:`mongo` shell connected to the :program:`mongod`
   instance running on ``mdb2.example.net``, freeze
   ``mdb2.example.net`` so that it does not attempt to become primary
   for 120 seconds.

   .. code-block:: javascript

      rs.freeze(120)

#. In a :program:`mongo` shell connected the :program:`mongod` running
   on ``mdb0.example.net``, step down this instance that the
   :program:`mongod` is not eligible to become primary for 120
   seconds:

   .. code-block:: javascript

      rs.stepDown(120)

   ``mdb1.example.net`` becomes primary.

   .. note:: During the transition, there is a short window where
      the set does not have a primary.

For more information, consider the :method:`rs.freeze()` and
:method:`rs.stepDown()` methods that wrap the
:dbcommand:`replSetFreeze` and :dbcommand:`replSetStepDown` commands.
.. _generate-key-file:

===================
Generate a Key File
===================

.. default-domain:: mongodb

Overview
--------

This section describes how to generate a key file to store
authentication information. After generating a key file, specify the key
file using the :setting:`~security.keyFile` option when starting a
:program:`mongod` or :program:`mongos` instance.

A key's length must be between 6 and 1024 characters and may only contain
characters in the base64 set. The key file must not have group or world
permissions on UNIX systems. Key file permissions are not checked on
Windows systems.

MongoDB strips whitespace characters (e.g. ``x0d``,
``x09``, and ``x20``) for cross-platform convenience. As a result,
the following operations produce identical keys:

.. code-block:: sh

   echo -e "my secret key" > key1
   echo -e "my secret key\n" > key2
   echo -e "my    secret    key" > key3
   echo -e "my\r\nsecret\r\nkey\r\n" > key4

Procedure
---------

.. include:: /includes/steps/generate-key-file.rst
==================
Generate Test Data
==================

.. default-domain:: mongodb

This tutorial describes how to quickly generate test data as you need
to test basic MongoDB operations.

Insert Multiple Documents Using a For Loop
------------------------------------------

You can add documents to a new or existing collection by using a JavaScript
``for`` loop run from the :program:`mongo` shell.

1. From the :program:`mongo` shell, insert new documents into the
   ``testData`` collection using the following ``for`` loop. If the
   ``testData`` collection does not exist, MongoDB creates the
   collection implicitly.

   .. code-block:: javascript

      for (var i = 1; i <= 25; i++) db.testData.insert( { x : i } )

#. Use find() to query the collection:

   .. code-block:: javascript

      db.testData.find()

   The :program:`mongo` shell displays the first 20 documents in the
   collection. Your :doc:`ObjectId </reference/object-id>` values will be
   different:

   .. code-block:: javascript

      { "_id" : ObjectId("51a7dc7b2cacf40b79990be6"), "x" : 1 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be7"), "x" : 2 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be8"), "x" : 3 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be9"), "x" : 4 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bea"), "x" : 5 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990beb"), "x" : 6 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bec"), "x" : 7 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bed"), "x" : 8 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bee"), "x" : 9 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bef"), "x" : 10 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf0"), "x" : 11 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf1"), "x" : 12 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf2"), "x" : 13 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf3"), "x" : 14 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf4"), "x" : 15 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf5"), "x" : 16 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf6"), "x" : 17 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf7"), "x" : 18 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf8"), "x" : 19 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf9"), "x" : 20 }

.. _getting-started-cursor-exhaustion:

#. The :method:`~db.collection.find()` returns a cursor. To iterate
   the cursor and return more documents use the ``it`` operation in
   the :program:`mongo` shell. The :program:`mongo` shell will exhaust
   the cursor, and return the following documents:

   .. code-block:: javascript

      { "_id" : ObjectId("51a7dce92cacf40b79990bfc"), "x" : 21 }
      { "_id" : ObjectId("51a7dce92cacf40b79990bfd"), "x" : 22 }
      { "_id" : ObjectId("51a7dce92cacf40b79990bfe"), "x" : 23 }
      { "_id" : ObjectId("51a7dce92cacf40b79990bff"), "x" : 24 }
      { "_id" : ObjectId("51a7dce92cacf40b79990c00"), "x" : 25 }

Insert Multiple Documents with a ``mongo`` Shell Function
---------------------------------------------------------

You can create a JavaScript function in your shell session to generate
the above data. The ``insertData()`` JavaScript function, shown here,
creates new data for use in testing or training by either creating a
new collection or appending data to an existing collection:

.. code-block:: javascript

   function insertData(dbName, colName, num) {

     var col = db.getSiblingDB(dbName).getCollection(colName);

     for (i = 0; i < num; i++) {
       col.insert({x:i});
     }

     print(col.count());

   }

The ``insertData()`` function takes three parameters: a database, a new
or existing collection, and the number of documents to create.
The function creates documents with an ``x`` field that is set to an
incremented integer, as in the following example documents:

.. code-block:: javascript

   { "_id" : ObjectId("51a4da9b292904caffcff6eb"), "x" : 0 }
   { "_id" : ObjectId("51a4da9b292904caffcff6ec"), "x" : 1 }
   { "_id" : ObjectId("51a4da9b292904caffcff6ed"), "x" : 2 }

Store the function in your :ref:`.mongorc.js <mongo-mongorc-file>` file.
The :program:`mongo` shell loads the function for you every time you
start a session.

.. example:: Specify database name, collection name, and the number of
   documents to insert as arguments to ``insertData()``.

   .. code-block:: javascript

      insertData("test", "testData", 400)

   This operation inserts 400 documents into the ``testData`` collection
   in the ``test`` database. If the collection and database do not
   exist, MongoDB creates them implicitly before inserting documents.
========================================
Getting Started with the ``mongo`` Shell
========================================

.. default-domain:: mongodb

This document provides a basic introduction to using the
:program:`mongo` shell. See :doc:`/installation` for instructions on
installing MongoDB for your system.

Start the ``mongo`` Shell
-------------------------

To start the :program:`mongo` shell and connect to your :doc:`MongoDB
</reference/program/mongod>` instance running on **localhost** with **default port**:

#. Go to your ``<mongodb installation dir>``:

   .. code-block:: sh

      cd <mongodb installation dir>

#. Type ``./bin/mongo`` to start :program:`mongo`:

   .. code-block:: sh

      ./bin/mongo

   If you have added the ``<mongodb installation dir>/bin`` to the
   ``PATH`` environment variable, you can just type ``mongo`` instead
   of ``./bin/mongo``.

#. To display the database you are using, type ``db``:

   .. code-block:: sh

      db

   The operation should return ``test``, which is the default database.
   To switch databases, issue the ``use <db>`` helper, as in the
   following example:

   .. code-block:: javascript

      use <database>

   To list the available databases, use the helper ``show dbs``. See
   also :ref:`mongo-shell-getSiblingDB` to access a different database
   from the current database without switching your current database
   context (i.e. ``db.``.)

To start the :program:`mongo` shell with other options, see
:ref:`examples of starting up mongo <mongo-usage-examples>` and
:doc:`mongo reference </reference/program/mongo>` which provides details on the
available options.

.. note::

   When starting, :program:`mongo` checks the user's :envvar:`HOME`
   directory for a JavaScript file named :ref:`.mongorc.js
   <mongo-mongorc-file>`. If found, :program:`mongo` interprets the
   content of :file:`.mongorc.js` before displaying the prompt for the
   first time. If you use the shell to evaluate a JavaScript file or
   expression, either by using the :option:`--eval <mongo --eval>` option on the
   command line or by specifying :ref:`a .js file to mongo
   <mongo-shell-file>`, :program:`mongo` will read the ``.mongorc.js``
   file *after* the JavaScript has finished processing.

.. _mongo-shell-executing-queries:

Executing Queries
-----------------

From the :program:`mongo` shell, you can use the :doc:`shell methods
</reference/method>` to run queries, as in the following example:

.. code-block:: javascript

   db.<collection>.find()

- The ``db`` refers to the current database.

- The ``<collection>`` is the name of the collection to query. See
  :ref:`mongo-shell-help-collection` to list the available collections.

  If the :program:`mongo` shell does not accept the name of the
  collection, for instance if the name contains a space, hyphen, or
  starts with a number, you can use an alternate syntax to refer to
  the collection, as in the following:

  .. code-block:: javascript

     db["3test"].find()

     db.getCollection("3test").find()

- The :method:`~db.collection.find()` method is the JavaScript
  method to retrieve documents from ``<collection>``. The
  :method:`~db.collection.find()` method returns a
  :term:`cursor` to the results; however, in the :program:`mongo`
  shell, if the returned cursor is not assigned to a variable using the
  ``var`` keyword, then the cursor is automatically iterated up to 20
  times to print up to the first 20 documents that match the query. The
  :program:`mongo` shell will prompt ``Type it`` to iterate another 20
  times.

  You can set the ``DBQuery.shellBatchSize`` attribute to change the
  number of iteration from the default value ``20``, as in the
  following example which sets it to ``10``:

  .. code-block:: javascript

     DBQuery.shellBatchSize = 10;

  For more information and examples on cursor handling in the
  :program:`mongo` shell, see :doc:`/core/cursors`.

  See also :ref:`mongo-shell-help-cursor` for list of
  cursor help in the :program:`mongo` shell.

For more documentation of basic MongoDB operations in the
:program:`mongo` shell, see:

- :doc:`/tutorial/getting-started`
- :doc:`/reference/mongo-shell`
- :doc:`/core/read-operations`
- :doc:`/core/write-operations`
- :doc:`/administration/indexes`

.. _mongo-shell-print:

Print
-----

The :program:`mongo` shell automatically prints the results of the
:method:`~db.collection.find()` method if the returned cursor
is not assigned to a variable using the ``var`` keyword. To format the
result, you can add the ``.pretty()`` to the operation, as in the
following:

.. code-block:: javascript

   db.<collection>.find().pretty()

In addition, you can use the following explicit print methods in the
:program:`mongo` shell:

- ``print()`` to print without formatting

- ``print(tojson(<obj>))`` to print with :term:`JSON` formatting and
  equivalent to ``printjson()``

- ``printjson()`` to print with :term:`JSON` formatting and equivalent
  to ``print(tojson(<obj>))``

Evaluate a JavaScript File
--------------------------

.. include:: /includes/fact-execute-javascript-from-shell.rst

Use a Custom Prompt
-------------------

You may modify the content of the prompt by creating the variable
``prompt`` in the shell. The prompt variable can hold strings as well
as any arbitrary JavaScript. If ``prompt`` holds a function that returns a
string, :program:`mongo` can display dynamic information in each
prompt. Consider the following examples:

.. example::

   Create a prompt with the number of operations issued in the current
   session, define the following variables:

   .. code-block:: javascript

      cmdCount = 1;
      prompt = function() {
                   return (cmdCount++) + "> ";
               }

   The prompt would then resemble the following:

   .. code-block:: javascript

      1> db.collection.find()
      2> show collections
      3>

.. example::

   To create a :program:`mongo` shell prompt in the form of
   ``<database>@<hostname>$`` define the following variables:

   .. code-block:: javascript

       host = db.serverStatus().host;

       prompt = function() {
                    return db+"@"+host+"$ ";
                }

   The prompt would then resemble the following:

   .. code-block:: javascript

      <database>@<hostname>$ use records
      switched to db records
      records@<hostname>$

.. example::

   To create a :program:`mongo` shell prompt that contains the system
   up time *and* the number of documents in the current database,
   define the following prompt variable:

   .. code-block:: javascript

      prompt = function() {
                   return "Uptime:"+db.serverStatus().uptime+" Documents:"+db.stats().objects+" > ";
               }

   The prompt would then resemble the following:

   .. code-block:: javascript

       Uptime:5897 Documents:6 > db.people.save({name : "James"});
       Uptime:5948 Documents:7 >

Use an External Editor in the ``mongo`` Shell
---------------------------------------------

.. versionadded:: 2.2

In the :program:`mongo` shell you can use the ``edit`` operation to
edit a function or variable in an external editor. The ``edit``
operation uses the value of your environments ``EDITOR`` variable.

At your system prompt you can define the ``EDITOR`` variable and start
:program:`mongo` with the following two operations:

.. code-block:: bash

   export EDITOR=vim
   mongo

Then, consider the following example shell session:

.. code-block:: javascript

   MongoDB shell version: 2.2.0
   > function f() {}
   > edit f
   > f
   function f() {
       print("this really works");
   }
   > f()
   this really works
   > o = {}
   { }
   > edit o
   > o
   { "soDoes" : "this" }
   >

.. note::

   As :program:`mongo` shell interprets code edited in an external
   editor, it may modify code in functions, depending on the
   JavaScript compiler. For :program:`mongo` may convert ``1+1`` to
   ``2`` or remove comments. The actual changes affect only the
   appearance of the code and will vary based on the version of
   JavaScript used but will not affect the semantics of the code.

.. _mongo-shell-exit:

Exit the Shell
--------------

To exit the shell, type ``quit()`` or use the ``<Ctrl-c>`` shortcut.
============================
Getting Started with MongoDB
============================

.. default-domain:: mongodb

This tutorial provides an introduction to basic database operations
using the :program:`mongo` shell. :program:`mongo` is a part of the
standard MongoDB distribution and provides a full JavaScript
environment with complete access to the JavaScript language
and all standard functions as well as a full database interface for
MongoDB. See the :api:`mongo JavaScript API <js>` documentation and
the :program:`mongo` shell :doc:`JavaScript Method Reference </reference/method>`.

The tutorial assumes that you're running MongoDB on a Linux or OS X
operating system and that you have a running database server; MongoDB
does support Windows and provides a Windows distribution with
identical operation. For instructions on installing MongoDB and
starting the database server, see the appropriate :doc:`installation
</installation>` document.

Connect to a Database
---------------------

In this section, you connect to the database server, which runs as
:program:`mongod`, and begin using the :program:`mongo` shell to
select a logical database within the database instance and access the
help text in the :program:`mongo` shell.

Connect to a :program:`mongod`
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

From a system prompt, start :program:`mongo` by issuing the
:program:`mongo` command, as follows:

.. code-block:: sh

   mongo

By default, :program:`mongo` looks for a database server listening on
port ``27017`` on the ``localhost`` interface. To connect to a server
on a different port or interface, use the
:option:`--port <mongo --port>` and :option:`--host <mongo --host>`
options.

Select a Database
~~~~~~~~~~~~~~~~~

After starting the :program:`mongo` shell your session will use the
``test`` database by default. At any time, issue the following operation
at the :program:`mongo` to report the name of the current database:

.. code-block:: javascript

   db

1. From the :program:`mongo` shell, display the list of
   databases, with the following operation:

   .. code-block:: javascript

      show dbs

#. Switch to a new database named ``mydb``, with the following
   operation:

   .. code-block:: javascript

      use mydb

#. Confirm that your session has the ``mydb`` database as context, by
   checking the value of the ``db`` object, which returns the name
   of the current database, as follows:

   .. code-block:: javascript

      db

   At this point, if you issue the ``show dbs`` operation again, it will
   not include the ``mydb`` database. MongoDB will not permanently
   create a database until you insert data into that database. The
   :ref:`getting-started-create-documents` section describes the process
   for inserting data.

   .. versionadded:: 2.4
      ``show databases`` also returns a list of databases.

Display ``mongo`` Help
~~~~~~~~~~~~~~~~~~~~~~

At any point, you can access help for the :program:`mongo` shell using
the following operation:

.. code-block:: javascript

   help

Furthermore, you can append the ``.help()`` method to some JavaScript
methods, any cursor object, as well as the ``db`` and
``db.collection`` objects to return additional help information.

.. _getting-started-create-documents:

Create a Collection and Insert Documents
----------------------------------------

In this section, you insert documents into a new :term:`collection`
named ``testData`` within the new :term:`database` named
``mydb``.

MongoDB will create a collection implicitly upon its first use. You do
not need to create a collection before inserting data. Furthermore,
because MongoDB uses :ref:`dynamic schemas <faq-schema-free>`, you also
need not specify the structure of your documents before inserting them
into the collection.

1. From the :program:`mongo` shell, confirm you are in the ``mydb``
   database by issuing the following:

   .. code-block:: javascript

      db

#. If :program:`mongo` does not return ``mydb`` for the previous
   operation, set the context to the ``mydb`` database, with the
   following operation:

   .. code-block:: javascript

      use mydb

#. Create two documents named ``j`` and ``k`` by using the following
   sequence of JavaScript operations:

   .. code-block:: javascript

      j = { name : "mongo" }
      k = { x : 3 }

#. Insert the ``j`` and ``k`` documents into the ``testData``
   collection with the following sequence of operations:

   .. code-block:: javascript

      db.testData.insert( j )
      db.testData.insert( k )

   When you insert the first document, the :program:`mongod` will
   create both the ``mydb`` database and the ``testData`` collection.

#. Confirm that the ``testData`` collection exists. Issue
   the following operation:

   .. code-block:: javascript

      show collections

   The :program:`mongo` shell will return the list of the collections
   in the current (i.e. ``mydb``) database. At this point, the only
   collection is ``testData``. All :program:`mongod` databases also have
   a :data:`system.indexes <<database>.system.indexes>` collection.

#. Confirm that the documents exist in the ``testData`` collection by
   issuing a query on the collection using the
   :method:`~db.collection.find()` method:

   .. code-block:: javascript

      db.testData.find()

   This operation returns the following results. The :doc:`ObjectId
   </reference/object-id>` values will be unique:

   .. code-block:: javascript

      { "_id" : ObjectId("4c2209f9f3924d31102bd84a"), "name" : "mongo" }
      { "_id" : ObjectId("4c2209fef3924d31102bd84b"), "x" : 3 }

   All MongoDB documents must have an ``_id`` field with a unique
   value.  These operations do not explicitly specify a value for the
   ``_id`` field, so :program:`mongo` creates a unique :doc:`ObjectId
   </reference/object-id>` value for the field before inserting it into the
   collection.

Insert Documents using a For Loop or a JavaScript Function
----------------------------------------------------------

To perform the remaining procedures in this tutorial, first add more
documents to your database using one or both of the procedures described
in :doc:`/tutorial/generate-test-data`.

Working with the Cursor
-----------------------

When you query a :term:`collection`, MongoDB returns a "cursor" object
that contains the results of the query. The :program:`mongo` shell then
iterates over the cursor to display the results. Rather than returning
all results at once, the shell iterates over the cursor 20 times to
display the first 20 results and then waits for a request to iterate
over the remaining results. In the shell, use enter ``it`` to iterate
over the next set of results.

The procedures in this section show other ways to work with a cursor.
For comprehensive documentation on cursors, see
:ref:`crud-read-cursor`.

Iterate over the Cursor with a Loop
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before using this procedure, make sure to add at least 25 documents to a
collection using one of the procedures in
:doc:`/tutorial/generate-test-data`. You can name your database and
collections anything you choose, but this procedure will assume the
database named ``test`` and a collection named ``testData``.

1. In the MongoDB JavaScript shell, query the ``testData`` collection
   and assign the resulting cursor object to the ``c`` variable:

   .. code-block:: javascript

      var c = db.testData.find()

#. Print the full result set by using a ``while`` loop to iterate over
   the ``c`` variable:

   .. code-block:: javascript

      while ( c.hasNext() ) printjson( c.next() )

   The ``hasNext()`` function returns true if the cursor has documents.
   The ``next()`` method returns the next document. The
   ``printjson()`` method renders the document in a JSON-like format.

   The operation displays 20 documents. For example, if the documents
   have a single field named ``x``, the operation displays the field as
   well as each document's ``ObjectId``:

   .. code-block:: javascript

      { "_id" : ObjectId("51a7dc7b2cacf40b79990be6"), "x" : 1 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be7"), "x" : 2 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be8"), "x" : 3 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990be9"), "x" : 4 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bea"), "x" : 5 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990beb"), "x" : 6 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bec"), "x" : 7 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bed"), "x" : 8 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bee"), "x" : 9 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bef"), "x" : 10 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf0"), "x" : 11 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf1"), "x" : 12 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf2"), "x" : 13 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf3"), "x" : 14 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf4"), "x" : 15 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf5"), "x" : 16 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf6"), "x" : 17 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf7"), "x" : 18 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf8"), "x" : 19 }
      { "_id" : ObjectId("51a7dc7b2cacf40b79990bf9"), "x" : 20 }

Use Array Operations with the Cursor
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following procedure lets you manipulate a cursor object as if it
were an array:

1. In the :program:`mongo` shell, query the ``testData`` collection
   and assign the resulting cursor object to the ``c`` variable:

   .. code-block:: javascript

      var c = db.testData.find()

#. To find the document at the array index ``4``, use the following
   operation:

   .. code-block:: javascript

      printjson( c [ 4 ] )

   MongoDB returns the following:

   .. code-block:: javascript

      { "_id" : ObjectId("51a7dc7b2cacf40b79990bea"), "x" : 5 }

   When you access documents in a cursor using the array index
   notation, :program:`mongo` first calls the ``cursor.toArray()``
   method and loads into RAM all documents returned by the cursor. The
   index is then applied to the resulting array. This operation
   iterates the cursor completely and exhausts the cursor.

   For very large result sets, :program:`mongo` may run out of
   available memory.

For more information on the cursor, see :ref:`crud-read-cursor`.

Query for Specific Documents
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB has a rich query system that allows you to select and filter
the documents in a collection along specific fields and values. See
:ref:`read-operations-query-document` and :doc:`/core/read-operations`
for a full account of queries in MongoDB.

In this procedure, you query for specific documents in the ``testData``
:term:`collection` by passing a "query document" as a parameter to the
:method:`~db.collection.find()` method. A query document
specifies the criteria the query must match to return a document.

In the :program:`mongo` shell, query for all documents where the ``x``
field has a value of ``18`` by passing the ``{ x : 18 }`` query document
as a parameter to the :method:`~db.collection.find()` method:

.. code-block:: javascript

   db.testData.find( { x : 18 } )

MongoDB returns one document that fits this criteria:

.. code-block:: javascript

   { "_id" : ObjectId("51a7dc7b2cacf40b79990bf7"), "x" : 18 }

Return a Single Document from a Collection
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With the :method:`~db.collection.findOne()` method you can return a
single *document* from a MongoDB collection. The :method:`~db.collection.findOne()` method takes the same parameters as
:method:`~db.collection.find()`, but returns a document rather
than a cursor.

To retrieve one document from the ``testData`` collection, issue the
following command:

.. code-block:: javascript

   db.testData.findOne()

For more information on querying for documents, see the
:ref:`read-operations-query-document` and :doc:`/core/read-operations` documentation.

Limit the Number of Documents in the Result Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To increase performance, you can constrain the size of the result by
limiting the amount of data your application must receive over the
network.

To specify the maximum number of documents in the result set, call the
:method:`~cursor.limit()` method on a cursor, as in the
following command:

.. code-block:: javascript

   db.testData.find().limit(3)

MongoDB will return the following result, with different
:doc:`ObjectId </reference/object-id>` values:

.. code-block:: javascript

   { "_id" : ObjectId("51a7dc7b2cacf40b79990be6"), "x" : 1 }
   { "_id" : ObjectId("51a7dc7b2cacf40b79990be7"), "x" : 2 }
   { "_id" : ObjectId("51a7dc7b2cacf40b79990be8"), "x" : 3 }

.. todo Add sections on Update and Remove

Next Steps with MongoDB
-------------------------

For more information on manipulating the documents in a database as
you continue to learn MongoDB, consider the following resources:

- :doc:`/crud`
- :doc:`/reference/sql-comparison`
- :doc:`/applications/drivers`
===============================
Implement Field Level Redaction
===============================

.. default-domain:: mongodb

The :pipeline:`$redact` pipeline operator restricts the contents of the
documents based on information stored in the documents themselves.

.. include:: /images/redact-security-architecture.rst

To store the access criteria data, add a field to the documents and
subdocuments. To allow for multiple combinations of access levels for
the same data, consider setting the access field to an array of arrays.
Each array element contains a required set that allows a user with that
set to access the data.

Then, include the :pipeline:`$redact` stage in the
:method:`db.collection.aggregate()` operation to restrict contents of
the result set based on the access required to view the data.

For more information on the :pipeline:`$redact` pipeline operator,
including its syntax and associated system variables as well as
additional examples, see :pipeline:`$redact`.

.. TODO reformat into steps (so should include rewording)

Procedure
---------

For example, a ``forecasts`` collection contains documents of the
following form where the ``tags`` field determines the access levels
required to view the data:

.. code-block:: javascript

   {
      _id: 1,
      title: "123 Department Report",
      tags: [ [ "G" ], [ "FDW" ] ],
      year: 2014,
      subsections: [
          {
              subtitle: "Section 1: Overview",
              tags: [ [ "SI", "G" ], [ "FDW" ] ],
              content:  "Section 1: This is the content of section 1."
          },
          {
              subtitle: "Section 2: Analysis",
              tags: [ [ "STLW" ] ],
              content: "Section 2: This is the content of section 2."
          },
          {
              subtitle: "Section 3: Budgeting",
              tags: [ [ "TK" ], [ "FDW", "TGE" ] ],
              content: {
                  text: "Section 3: This is the content of section3.",
                  tags: [ [ "HCS"], [ "FDW", "TGE", "BX" ] ]
              }
          }
      ]
   }

For each document, the ``tags`` field contains various access groupings
necessary to view the data. For example, the value ``[ [ "G" ], [
"FDW", "TGE" ] ]`` can specify that a user requires either access level
``["G"]`` or both ``[ "FDW", "TGE" ]`` to view the data.

Consider a user who only has access to view information tagged with
either ``"FDW"`` or ``"TGE"``. To run a query on all documents with
year ``2014`` for this user, include a :pipeline:`$redact` stage as in
the following:

.. code-block:: none

   var userAccess = [ "FDW", "TGE" ];
   db.forecasts.aggregate(
      [
        { $match: { year: 2014 } },
        { $redact:
            {
              $cond: {
                       if: { $anyElementTrue:
                              {
                                $map: {
                                        input: "$tags" ,
                                        as: "fieldTag",
                                        in: { $setIsSubset: [ "$$fieldTag", userAccess ] }
                                      }
                              }
                           },
                        then: "$$DESCEND",
                        else: "$$PRUNE"
                     }
            }
        }
      ]
   )

The aggregation operation returns the following "redacted" document for the user:

.. code-block:: none

   { "_id" : 1,
     "title" : "123 Department Report",
     "tags" : [ [ "G" ], [ "FDW" ] ],
     "year" : 2014,
     "subsections" :
        [
           {
             "subtitle" : "Section 1: Overview",
             "tags" : [ [ "SI", "G" ], [ "FDW" ] ],
             "content" : "Section 1: This is the content of section 1."
           },
          {
            "subtitle" : "Section 3: Budgeting",
            "tags" : [ [ "TK" ], [ "FDW", "TGE" ] ]
          }
        ]
   }

.. seealso:: :expression:`$map`, :expression:`$setIsSubset`,
   :expression:`$anyElementTrue`
================
Insert Documents
================

.. default-domain:: mongodb

In MongoDB, the :method:`db.collection.insert()` method adds new
documents into a collection. In addition, both the
:method:`db.collection.update()` method and the
:method:`db.collection.save()` method can also add new documents
through an operation called an *upsert*. An *upsert* is an operation
that performs either an update of an existing document or an insert of
a new document if the document to modify does not exist.

This tutorial provides examples of insert operations using each of the
three methods in the :program:`mongo` shell.

Insert a Document with ``insert()`` Method
------------------------------------------

The following statement inserts a document with three fields into the
collection ``inventory``:

.. code-block:: javascript

   db.inventory.insert( { _id: 10, type: "misc", item: "card", qty: 15 } )

In the example, the document has a user-specified ``_id`` field value
of ``10``. The value must be unique within the ``inventory`` collection.

For more examples, see :method:`~db.collection.insert()`.

Insert a Document with ``update()`` Method
------------------------------------------

Call the :method:`~db.collection.update()` method with the ``upsert``
flag to create a new document if no document matches the update's
query criteria. [#previous-versions-upsert]_

The following example creates a new document if no document in the
``inventory`` collection contains ``{ type: "books", item : "journal"
}``:

.. code-block:: javascript

   db.inventory.update(
                        { type: "book", item : "journal" },
                        { $set : { qty: 10 } },
                        { upsert : true }
                      )

MongoDB adds the ``_id`` field and assigns as its value a unique
ObjectId. The new document includes the ``item`` and ``type`` fields
from the ``<query>`` criteria and the ``qty`` field from the
``<update>`` parameter.

.. code-block:: javascript

   { "_id" : ObjectId("51e8636953dbe31d5f34a38a"), "item" : "journal", "qty" : 10, "type" : "book" }

For more examples, see :method:`~db.collection.update()`.

.. [#previous-versions-upsert]
   .. include:: /includes/fact-upsert-multi-options.rst

Insert a Document with ``save()`` Method
----------------------------------------

To insert a document with the :method:`~db.collection.save()` method,
pass the method a document that does not contain the ``_id`` field or a
document that contains an ``_id`` field that does not exist in the
collection.

The following example creates a new document in the ``inventory``
collection:

.. code-block:: javascript

   db.inventory.save( { type: "book", item: "notebook", qty: 40 } )

MongoDB adds the ``_id`` field and assigns as its value a unique
ObjectId.

.. code-block:: javascript

   { "_id" : ObjectId("51e866e48737f72b32ae4fbc"), "type" : "book", "item" : "notebook", "qty" : 40 }

For more examples, see :method:`~db.collection.save()`.
==============================================
Install MongoDB Enterprise on Amazon Linux AMI
==============================================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install :term:`MongoDB Enterprise` on Amazon Linux
AMI. MongoDB Enterprise is available on select platforms and contains
support for several features related to security and monitoring.

Prerequisites
-------------

To use MongoDB Enterprise on Amazon Linux AMI, you must install several
prerequisite packages:

- ``net-snmp``
- ``net-snmp-libs``
-  ``openssl``
-  ``net-snmp-utils``
-  ``cyrus-sasl``
- ``cyrus-sasl-lib``
- ``cyrus-sasl-devel``
- ``cyrus-sasl-gssapi``

To install these packages, you can issue the following command:

.. code-block:: sh

   sudo yum install openssl net-snmp net-snmp-libs net-snmp-utils cyrus-sasl cyrus-sasl-lib cyrus-sasl-devel cyrus-sasl-gssapi

Install MongoDB Enterprise
--------------------------

.. note::

   The Enterprise packages include an example SNMP configuration file
   named ``mongod.conf``. This file is not a MongoDB configuration file.

.. include:: /includes/steps/install-mongodb-enterprise-on-amazon.rst

Run MongoDB Enterprise
----------------------

.. include:: /includes/steps/run-mongodb-on-linux.rst
==========================================================
Install MongoDB Enterprise on Red Hat Enterprise or CentOS
==========================================================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB Enterprise on Red Hat Enterprise
Linux or CentOS Linux. The tutorial uses ``.rpm`` packages to install.

.. The following include includes two h2 headers:
.. Packages and Control Scripts

.. include:: /includes/list-mongodb-enterprise-packages.rst

Install MongoDB Enterprise
--------------------------

When you install the packages for MongoDB Enterprise, you choose whether
to install the current release or a previous one. This procedure describes
how to do both.

.. include:: /includes/steps/install-mongodb-enterprise-on-red-hat-or-centos.rst

Run MongoDB Enterprise
----------------------

.. include:: /includes/steps/run-mongodb-on-a-linux-distribution.rst
==================================
Install MongoDB Enterprise on SUSE
==================================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install :term:`MongoDB Enterprise` on SUSE Linux.
MongoDB Enterprise is available on select platforms and contains support
for several features related to security and monitoring.

Prerequisites
-------------

To use MongoDB Enterprise on SUSE Enterprise Linux, you must install
several prerequisite packages:

- ``libopenssl0_9_8``
- ``libsnmp15``
- ``net-snmp``
- ``snmp-mibs``
- ``cyrus-sasl``
- ``cyrus-sasl-devel``
- ``cyrus-sasl-gssapi``

To install these packages, you can issue the following command:

.. code-block:: sh

   sudo zypper install libopenssl0_9_8 net-snmp libsnmp15 snmp-mibs ncyrus-sasl cyrus-sasl-devel cyrus-sasl-gssapi

Install MongoDB Enterprise
--------------------------

.. note::

   The Enterprise packages include an example SNMP configuration file
   named ``mongod.conf``. This file is not a MongoDB configuration file.

.. include:: /includes/steps/install-mongodb-enterprise-on-suse.rst

Run MongoDB Enterprise
----------------------

.. include:: /includes/steps/run-mongodb-on-linux.rst
====================================
Install MongoDB Enterprise on Ubuntu
====================================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB Enterprise on Ubuntu Linux systems.
The tutorial uses ``.deb`` packages to install.

.. include:: /includes/list-mongodb-enterprise-packages.rst

Install MongoDB Enterprise
--------------------------

.. include:: /includes/steps/install-mongodb-enterprise-on-ubuntu.rst

Run MongoDB Enterprise
----------------------

The MongoDB Enterprise instance stores its data files in ``/var/lib/mongo``
and its log files in ``/var/log/mongo``, and runs using the ``mongod``
user account. If you change the user that runs the MongoDB process, you
**must** modify the access control rights to the ``/var/lib/mongo`` and
``/var/log/mongo`` directories.

.. include:: /includes/steps/run-mongodb-on-a-linux-distribution.rst
=====================================
Install MongoDB Enterprise on Windows
=====================================

.. default-domain:: mongodb

.. versionadded:: 2.6

Overview
--------

Use this tutorial to install :term:`MongoDB Enterprise` on Windows
systems. MongoDB Enterprise is available on select platforms and contains
support for several features related to security and monitoring.

Prerequisites
-------------

MongoDB Enterprise Server for Windows requires Windows Server 2008 R2 or
later. The MSI installer includes all other software dependencies.

Install MongoDB Enterprise
--------------------------

.. include:: /includes/steps/install-mongodb-enterprise-on-windows.rst

Run MongoDB Enterprise
----------------------

.. warning::

   Do not make :program:`mongod.exe` visible on public networks without
   running in "Secure Mode" with the :setting:`auth` setting. MongoDB is
   designed to be run in trusted environments, and the database does not
   enable "Secure Mode" by default.

.. include:: /includes/steps/run-mongodb-on-windows.rst

Configure a Windows Service for MongoDB Enterprise
--------------------------------------------------

You can set up the MongoDB server as a :guilabel:`Windows Service` that
starts automatically at boot time.

.. include:: /includes/steps/configure-windows-service-for-mongodb.rst
=========================
Install MongoDB on Debian
=========================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB on Debian systems. The tutorial uses
``.deb`` packages to install. While some Debian distributions include
their own MongoDB packages, the official MongoDB packages are generally
more up to date.

.. note::

   This tutorial applies to both Debian systems and versions of Ubuntu
   Linux prior to 9.10 "Karmic" which do not use Upstart. Other Ubuntu
   users will want to follow the :doc:`/tutorial/install-mongodb-on-ubuntu`
   tutorial.

.. The following include includes two h2 headers:
.. Packages and Control Scripts

.. include:: /includes/list-mongodb-org-packages.rst

Considerations
--------------

You cannot install this package concurrently with the ``mongodb``,
``mongodb-server``, or ``mongodb-clients`` packages that
your release of Debian may include.

Install MongoDB
---------------

The Debian package management tools (i.e. ``dpkg`` and ``apt``) ensure
package consistency and authenticity by requiring that distributors
sign packages with GPG keys.

.. include:: /includes/steps/install-mongodb-on-debian.rst

Run MongoDB
-----------

The MongoDB instance stores its data files in ``/var/lib/mongo``
and its log files in ``/var/log/mongo``, and runs using the ``mongod``
user account. If you change the user that runs the MongoDB process, you
**must** modify the access control rights to the ``/var/lib/mongo`` and
``/var/log/mongo`` directories.

.. include:: /includes/steps/run-mongodb-on-debian.rst
================================
Install MongoDB on Linux Systems
================================

.. default-domain:: mongodb


Overview
--------

Compiled versions of MongoDB for Linux provide a simple option for
installing MongoDB for other Linux systems without supported packages.

Install MongoDB
---------------

MongoDB provides archives for both 64-bit and 32-bit Linux. Follow the
installation procedure appropriate for your system.

Install for 64-bit Linux
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/steps/install-mongodb-on-linux-64.rst

Install for 32-bit Linux
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/steps/install-mongodb-on-linux-32.rst

Run MongoDB
-----------

.. include:: /includes/steps/run-mongodb-on-linux.rst
=======================
Install MongoDB on OS X
=======================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB on on OS X systems.

.. admonition:: Platform Support

   Starting in version 2.4, MongoDB only supports OS X versions 10.6 (Snow
   Leopard) on Intel x86-64 and later.

MongoDB is available through the popular OS X package manager `Homebrew
<http://brew.sh/>`_ or through the `MongoDB Download site
<http://www.mongodb.org/downloads>`_.

Install MongoDB
---------------

You can install MongoDB with `Homebrew <http://brew.sh/>`_ or manually.
This section describes both.

Install MongoDB with Homebrew
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`Homebrew <http://brew.sh/>`_ installs binary packages based on published
"formulae." This section describes how to update ``brew`` to the latest
packages and install MongoDB. Homebrew requires some initial setup and
configuration, which is beyond the scope of this document.

.. include:: /includes/steps/install-mongodb-on-osx-with-homebrew.rst

Install MongoDB Manually
~~~~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/steps/install-mongodb-on-osx-manually.rst

Run MongoDB
-----------

.. include:: /includes/steps/run-mongodb-on-linux.rst
======================================================================
Install MongoDB on Red Hat Enterprise, CentOS, Fedora, or Amazon Linux
======================================================================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB on Red Hat Enterprise Linux, CentOS
Linux, Fedora Linux, or a related system. The tutorial uses ``.rpm``
packages to install. While some of these distributions include their own
MongoDB packages, the official MongoDB packages are generally more up to
date.

.. The following include includes two h2 headers:
.. Packages and Control Scripts

.. include:: /includes/list-mongodb-org-packages.rst

.. warning::

   With the introduction of ``systemd`` in Fedora 15, the control scripts
   included in the packages available in the MongoDB downloads repository
   are not compatible with Fedora systems. A correction is
   forthcoming, see :issue:`SERVER-7285` for more information, and in
   the mean time use your own control scripts *or* install using the
   procedure outlined in :doc:`/tutorial/install-mongodb-on-linux`.

Install MongoDB
---------------

For production deployments, always run MongoDB on 64-bit systems.

.. include:: /includes/steps/install-mongodb-on-red-hat-centos-or-fedora-linux.rst

Run MongoDB
-----------

.. important:: You must configure SELinux to allow MongoDB to start on
   Fedora systems. Administrators have two options:

   - enable access to the relevant ports (e.g. 27017) for SELinux. See
     :ref:`security-port-numbers` for more information on MongoDB's
     :doc:`default ports </reference/default-mongodb-port>`.

   - disable SELinux entirely. This requires a system reboot and may have
     larger implications for your deployment.

The MongoDB instance stores its data files in ``/var/lib/mongo``
and its log files in ``/var/log/mongo``, and runs using the ``mongod``
user account. If you change the user that runs the MongoDB process, you
**must** modify the access control rights to the ``/var/lib/mongo`` and
``/var/log/mongo`` directories.

.. include:: /includes/steps/run-mongodb-on-a-linux-distribution.rst
=========================
Install MongoDB on Ubuntu
=========================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB on Ubuntu Linux systems. The tutorial
uses ``.deb`` packages to install. While Ubuntu includes its own MongoDB
packages, the official MongoDB packages are generally more up-to-date.

.. note::

   If you use an older Ubuntu that does **not** use Upstart (i.e. any
   version before 9.10 "Karmic"), please follow the instructions on the
   :doc:`install-mongodb-on-debian` tutorial.

.. The following include includes two h2 headers:
.. Packages and Control Scripts

.. include:: /includes/list-mongodb-org-packages.rst

You cannot install this package concurrently with the ``mongodb``,
``mongodb-server``, or ``mongodb-clients`` packages provided by Ubuntu.

Install MongoDB
---------------

.. include:: /includes/steps/install-mongodb-on-ubuntu.rst

Run MongoDB
-----------

The MongoDB instance stores its data files in ``/var/lib/mongo``
and its log files in ``/var/log/mongo``, and runs using the ``mongod``
user account. If you change the user that runs the MongoDB process, you
**must** modify the access control rights to the ``/var/lib/mongo`` and
``/var/log/mongo`` directories.

.. include:: /includes/steps/run-mongodb-on-a-linux-distribution.rst
==========================
Install MongoDB on Windows
==========================

.. default-domain:: mongodb

Overview
--------

Use this tutorial to install MongoDB on a Windows systems.

.. admonition:: Platform Support

   Starting in version 2.2, MongoDB does not support Windows XP. Please
   use a more recent version of Windows to use more recent releases of
   MongoDB.

.. important:: If you are running any edition of Windows Server 2008
   R2 or Windows 7, please install `a hotfix to resolve an issue with
   memory mapped files on Windows <http://support.microsoft.com/kb/2731284>`_.

Install MongoDB
---------------

.. include:: /includes/steps/install-mongodb-on-windows.rst

Run MongoDB
-----------

.. warning::

   Do not make :program:`mongod.exe` visible on public networks without
   running in "Secure Mode" with the :setting:`auth` setting. MongoDB is
   designed to be run in trusted environments, and the database does not
   enable "Secure Mode" by default.

.. include:: /includes/steps/run-mongodb-on-windows.rst

.. _tutorial-mongod-as-windows-service:

Configure a Windows Service for MongoDB
---------------------------------------

.. include:: /includes/steps/configure-windows-service-for-mongodb.rst
==============================
Isolate Sequence of Operations
==============================

.. default-domain:: mongodb

Overview
--------

Write operations are atomic on the level of a single document: no
single write operation can atomically affect more than one document or
more than one collection.

When a single write operation modifies multiple documents, the
operation as a whole is not atomic, and other operations may
interleave. The modification of a single document, or record, is always
atomic, even if the write operation modifies multiple sub-documents
*within* the single record.

No other operations are atomic; however, you can *isolate* a
single write operation that affects multiple documents using the
:doc:`isolation operator </reference/operator/update/isolated>`.

This document describes one method of updating documents *only* if the
local copy of the document reflects the current state of the document
in the database. In addition the following methods provide a way to
manage isolated sequences of operations:

- the :method:`~db.collection.findAndModify()`
  provides an isolated query and modify operation.

- :doc:`/tutorial/perform-two-phase-commits`

- Create a :ref:`unique index <index-type-unique>`, to ensure that a
  key doesn't exist when you insert it.

.. _tutorial-atomic-update-if-current:

Update if Current
-----------------

In this pattern, you will:

- query for a document,

- modify the fields in that document

- and update the fields of a document *only if* the fields have not
  changed in the collection since the query.

Consider the following example in JavaScript which attempts to update
the ``qty`` field of a document in the ``products`` collection:

.. code-block:: javascript

   var myCollection = db.products;
   var myDocument = myCollection.findOne( { sku: 'abc123' } );

   if (myDocument) {

      var oldQty = myDocument.qty;

      if (myDocument.qty < 10) {
          myDocument.qty *= 4;
      } else if ( myDocument.qty < 20 ) {
          myDocument.qty *= 3;
      } else {
          myDocument.qty *= 2;
      }

      myCollection.update(
         {
           _id: myDocument._id,
           qty: oldQty
         },
         {
           $set: { qty: myDocument.qty }
         }
      )

      var err = db.getLastErrorObj();

      if ( err && err.code ) {
          print("unexpected error updating document: " + tojson( err ));
      } else if ( err.n == 0 ) {
          print("No update: no matching document for { _id: " + myDocument._id + ", qty: " + oldQty + " }")
      }

   }

Your application may require some modifications of this pattern, such
as:

- Use the entire document as the query in the
  :method:`~db.collection.update()` operation, to generalize the
  operation and guarantee that the original document was not modified,
  rather than ensuring that as single field was not changed.

- Add a version variable to the document that applications increment
  upon each update operation to the documents. Use this version
  variable in the query expression. You must be able to ensure that
  *all* clients that connect to your database obey this constraint.

- Use :update:`$set` in the update expression to modify only your
  fields and prevent overriding other fields.

- Use one of the methods described in :doc:`/tutorial/create-an-auto-incrementing-field`.

.. Maybe incorporate the blurb: "MongoDB does not
   support traditional locking and complex transactions for a number of
   reasons: First, in sharded environments, distributed locks could be
   expensive and slow. MongoDB's goal is to be lightweight and fast. We
   dislike the concept of deadlocks. We want the system to be simple and
   predictable without these sort of surprises. We want MongoDB to work
   well for realtime problems. If an operation may execute which locks
   large amounts of data, it might stop some small light queries for an
   extended period of time."
=======================================
Iterate a Cursor in the ``mongo`` Shell
=======================================

.. default-domain:: mongodb

The :method:`db.collection.find()` method returns a cursor. To access
the documents, you need to iterate the cursor. However, in the
:program:`mongo` shell, if the returned cursor is not assigned to a
variable using the ``var`` keyword, then the cursor is automatically
iterated up to 20 times to print up to the first 20 documents in the
results. The following describes ways to manually iterate the cursor to
access the documents or to use the iterator index.

Manually Iterate the Cursor
---------------------------

In the :program:`mongo` shell, when you assign the cursor returned from
the :method:`find() <db.collection.find()>` method to a variable using
the ``var`` keyword, the cursor does not automatically iterate.

You can call the cursor variable in the shell to iterate up to 20 times
[#set-shell-batch-size]_ and print the matching documents, as in the
following example:

.. code-block:: javascript

   var myCursor = db.inventory.find( { type: 'food' } );

   myCursor

You can also use the cursor method :method:`next() <cursor.next()>` to
access the documents, as in the following example:

.. code-block:: javascript

   var myCursor = db.inventory.find( { type: 'food' } );
   var myDocument = myCursor.hasNext() ? myCursor.next() : null;

   if (myDocument) {
       var myItem = myDocument.item;
       print(tojson(myItem));
   }

As an alternative print operation, consider the ``printjson()`` helper
method to replace ``print(tojson())``:

.. code-block:: javascript

   if (myDocument) {
       var myItem = myDocument.item;
       printjson(myItem);
   }

You can use the cursor method :method:`forEach() <cursor.forEach()>` to
iterate the cursor and access the documents, as in the following
example:

.. code-block:: javascript

   var myCursor =  db.inventory.find( { type: 'food' } );

   myCursor.forEach(printjson);

See :ref:`JavaScript cursor methods <js-query-cursor-methods>` and your
:doc:`driver </applications/drivers>` documentation for more
information on cursor methods.

.. include:: /includes/footnote-set-shell-batch-size.rst

Iterator Index
--------------

In the :program:`mongo` shell, you can use the
:method:`~cursor.toArray()` method to iterate the cursor and return
the documents in an array, as in the following:

.. code-block:: javascript

   var myCursor = db.inventory.find( { type: 'food' } );
   var documentArray = myCursor.toArray();
   var myDocument = documentArray[3];

The :method:`~cursor.toArray()` method loads into RAM all
documents returned by the cursor; the :method:`~cursor.toArray()`
method exhausts the cursor.

Additionally, some :doc:`drivers </applications/drivers>` provide
access to the documents by using an index on the cursor (i.e.
``cursor[index]``). This is a shortcut for first calling the
:method:`~cursor.toArray()` method and then using an index
on the resulting array.

Consider the following example:

.. code-block:: javascript

   var myCursor = db.inventory.find( { type: 'food' } );
   var myDocument = myCursor[3];

The ``myCursor[3]`` is equivalent to the following example:

.. code-block:: javascript

   myCursor.toArray() [3];
====================================================
Limit Number of Elements in an Array after an Update
====================================================

.. default-domain:: mongodb

.. versionadded:: 2.4

Synopsis
--------

Consider an application where users may submit many scores (e.g. for a
test), but the application only needs to track the top three test
scores.

This pattern uses the :update:`$push` operator with the
:update:`$each`, :update:`$sort`, and :update:`$slice`
modifiers to sort and maintain an array of fixed size.

.. important:: The array elements must be documents in order to use the
   :update:`$sort` modifier.

Pattern
-------

Consider the following document in the collection ``students``:

.. code-block:: javascript

   {
     _id: 1,
     scores: [
               { attempt: 1, score: 10 },
               { attempt: 2 , score:8 }
             ]
   }

The following update uses the :update:`$push` operator with:

- the :update:`$each` modifier to append to the array 2 new elements,

- the :update:`$sort` modifier to order the elements by ascending
  (``1``) score, and

- the :update:`$slice` modifier to keep the last ``3`` elements of
  the ordered array.

.. code-block:: javascript

   db.students.update(
                       { _id: 1 },
                       { $push: { scores: { $each : [
                                                      { attempt: 3, score: 7 },
                                                      { attempt: 4, score: 4 }
                                                    ],
                                            $sort: { score: 1 },
                                            $slice: -3
                                          }
                                 }
                       }
                     )

.. note::

   When using the :update:`$sort` modifier on the array element,
   access the field in the subdocument element directly instead of
   using the :term:`dot notation` on the array field.

After the operation, the document contains the only the top 3 scores in
the ``scores`` array:

.. code-block:: javascript

   {
      "_id" : 1,
      "scores" : [
                   { "attempt" : 3, "score" : 7 },
                   { "attempt" : 2, "score" : 8 },
                   { "attempt" : 1, "score" : 10 }
                 ]
   }

.. seealso::

   - :update:`$push` operator,

   - :update:`$each` modifier,

   - :update:`$sort` modifier, and

   - :update:`$slice` modifier.
===================================
Limit the Number of Entries Scanned
===================================

.. default-domain:: mongodb

This tutorial describes how to create indexes to limit the number of
index entries scanned for queries that includes a :query:`$text`
expression and equality conditions.

A collection ``inventory`` contains the following documents:

.. code-block:: javascript

   { _id: 1, dept: "tech", description: "lime green computer" }
   { _id: 2, dept: "tech", description: "wireless red mouse" }
   { _id: 3, dept: "kitchen", description: "green placemat" }
   { _id: 4, dept: "kitchen", description: "red peeler" }
   { _id: 5, dept: "food", description: "green apple" }
   { _id: 6, dept: "food", description: "red potato" }

Consider the common use case that performs text searches by
*individual* departments, such as:

.. code-block:: javascript

   db.inventory.find( { dept: "kitchen", $text: { $search: "green" } } )

To limit the text search to scan only those documents within a specific
``dept``, create a compound index that *first* specifies an
ascending/descending index key on the field ``dept`` and then a
``text`` index key on the field ``description``:

.. code-block:: javascript

   db.inventory.ensureIndex(
      {
        dept: 1,
        description: "text"
      }
   )

Then, the text search [#text-command]_ within a particular department
will limit the scan of indexed documents. For example, the following
query scans only those documents with ``dept`` equal to ``kitchen`` or
``food``:

.. code-block:: javascript

   db.inventory.find( { dept: "kitchen", $text: { $search: "green" } } )

.. include:: /includes/fact-compound-index-with-text-restrictions.rst

.. [#text-command] If using the deprecated :dbcommand:`text` command, the
   :dbcommand:`text` command **must** include the ``filter`` option
   that specifies an **equality** condition for the prefix fields.

.. seealso:: :doc:`/core/index-text`
============================
Return a List of All Indexes
============================

.. default-domain:: mongodb

When performing maintenance you may want to check which indexes exist
on a collection. Every index on a collection has a corresponding
:term:`document` in the :data:`system.indexes
<<database>.system.indexes>` collection, and you can use standard
queries (i.e. :method:`~db.collection.find()`) to list the indexes, or
in the :program:`mongo` shell, the
:method:`~db.collection.getIndexes()` method to return a list of the
indexes on a collection, as in the following examples.

.. seealso:: :doc:`/core/indexes` and :doc:`/administration/indexes`
   for more information about indexes in MongoDB and common index
   management operations.

.. index:: index; list indexes
.. _index-list-indexes-for-collection:

List all Indexes on a Collection
--------------------------------

To return a list of all indexes on a collection, use the
:method:`db.collection.getIndexes()` method or a similar
:api:`method for your driver <>`.

For example, to view all indexes on the ``people`` collection:

.. code-block:: javascript

   db.people.getIndexes()

.. index:: index; list indexes
.. _index-list-indexes-for-database:

List all Indexes for a Database
-------------------------------

To return a list of all indexes on all collections in a database, use
the following operation in the :program:`mongo` shell:

.. code-block:: javascript

   db.system.indexes.find()

See :data:`system.indexes <<database>.system.indexes>` for more
information about these documents.
==========================
Manage Chained Replication
==========================

.. default-domain:: mongodb

Starting in version 2.0, MongoDB supports chained replication. A
chained replication occurs when a :term:`secondary` member replicates
from another secondary member instead of from the :term:`primary`. This
might be the case, for example, if a secondary selects its replication
target based on ping time and if the closest member is another
secondary.

Chained replication can reduce load on the primary. But chained
replication can also result in increased replication lag, depending on
the topology of the network.

.. versionadded:: 2.2.2

You can use the :data:`~local.system.replset.settings.chainingAllowed`
setting in :doc:`/reference/replica-configuration` to disable chained
replication for situations where chained replication is causing lag.

MongoDB enables chained replication by default. This procedure
describes how to disable it and how to re-enable it.

.. note::

   If chained replication is disabled, you still can use
   :dbcommand:`replSetSyncFrom` to specify that a secondary replicates
   from another secondary. But that configuration will last only until the
   secondary recalculates which member to sync from.

Disable Chained Replication
---------------------------

To disable chained replication, set the
:data:`~local.system.replset.settings.chainingAllowed`
field in :doc:`/reference/replica-configuration` to ``false``.

You can use the following sequence of commands to set
:data:`~local.system.replset.settings.chainingAllowed` to
``false``:

1. Copy the configuration settings into the ``cfg`` object:

   .. code-block:: javascript

      cfg = rs.config()

#. Take note of whether the current configuration settings contain the
   ``settings`` sub-document. If they do, skip this step.

   .. warning:: To avoid data loss, skip this step if the configuration
      settings contain the ``settings`` sub-document.

   If the current configuration settings **do not** contain the
   ``settings`` sub-document, create the sub-document by issuing the
   following command:

   .. code-block:: javascript

      cfg.settings = { }

#. Issue the following sequence of commands to set
   :data:`~local.system.replset.settings.chainingAllowed` to
   ``false``:

   .. code-block:: javascript

      cfg.settings.chainingAllowed = false
      rs.reconfig(cfg)

Re-enable Chained Replication
-----------------------------

To re-enable chained replication, set
:data:`~local.system.replset.settings.chainingAllowed` to ``true``.
You can use the following sequence of commands:

.. code-block:: javascript

   cfg = rs.config()
   cfg.settings.chainingAllowed = true
   rs.reconfig(cfg)
.. index:: index; monitor index building
.. _index-monitor-index-building:
.. _indexes-admin-stop-in-progress-build:

=================================
Manage In-Progress Index Creation
=================================

.. default-domain:: mongodb

To see the status of the indexing processes, you can use the
:method:`db.currentOp()` method in the :program:`mongo` shell. The value
of the ``query`` field and the ``msg`` field will indicate if the
operation is an index build. The ``msg`` field also indicates the
percent of the build that is complete.

To terminate an ongoing index build, use the
:method:`db.killOp()` method in the :program:`mongo` shell.

For more information see :method:`db.currentOp()`.

.. versionchanged:: 2.4
   Before MongoDB 2.4, you could *only* terminate *background* index
   builds. After 2.4, you can terminate any index build, including
   foreground index builds.
=================
Manage Journaling
=================

.. default-domain:: mongodb

MongoDB uses *write ahead logging* to an on-disk :term:`journal` to
guarantee :doc:`write operation </core/write-operations>` durability
and to provide crash resiliency. Before applying a change to the data
files, MongoDB writes the change operation to the journal. If MongoDB
should terminate or encounter an error before it can write the changes
from the journal to the data files, MongoDB can re-apply the write
operation and maintain a consistent state.

*Without* a journal, if :program:`mongod` exits unexpectedly, you must
assume your data is in an inconsistent state, and you must run either
:doc:`repair </tutorial/recover-data-following-unexpected-shutdown>`
or, preferably, :doc:`resync </tutorial/resync-replica-set-member>`
from a clean member of the replica set.

With journaling enabled, if :program:`mongod` stops unexpectedly,
the program can recover everything written to the journal, and the
data remains in a consistent state. By default, the greatest extent of lost
writes, i.e., those not made to the journal, are those made in the last
100 milliseconds. See :setting:`~storage.journal.commitIntervalMs` for more
information on the default.

With journaling, if you want a data set to reside entirely in RAM, you
need enough RAM to hold the data set plus the "write working set." The
"write working set" is the amount of unique data you expect to see
written between re-mappings of the private view. For information on
views, see :ref:`journaling-storage-views`.

.. important::

   .. versionchanged:: 2.0
      For 64-bit builds of :program:`mongod`, journaling is enabled by
      default.  For other platforms, see :setting:`storage.journal.enabled`.

Procedures
----------

Enable Journaling
~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.0
   For 64-bit builds of :program:`mongod`, journaling is enabled by default.

To enable journaling, start :program:`mongod` with the
:option:`--journal <mongod --journal>` command line option.

If no journal files exist, when :program:`mongod` starts, it must
preallocate new journal files. During this operation, the
:program:`mongod` is not listening for connections until preallocation
completes: for some systems this may take a several minutes. During
this period your applications and the :program:`mongo` shell are not
available.

Disable Journaling
~~~~~~~~~~~~~~~~~~

.. warning::

   Do not disable journaling on production systems. If your
   :program:`mongod` instance stops without shutting down cleanly
   unexpectedly for any reason, (e.g. power failure) and you are
   not running with journaling, then you must recover from an
   unaffected :term:`replica set` member or backup, as described in
   :doc:`repair  </tutorial/recover-data-following-unexpected-shutdown>`.

To disable journaling, start :program:`mongod` with the
:option:`--nojournal <mongod --nojournal>` command line option.

Get Commit Acknowledgment
~~~~~~~~~~~~~~~~~~~~~~~~~

You can get commit acknowledgment with the
:dbcommand:`getLastError` command and the ``j`` option. For details, see
:ref:`write-concern-operation`.

.. _journaling-avoid-preallocation-lag:

Avoid Preallocation Lag
~~~~~~~~~~~~~~~~~~~~~~~

To avoid :ref:`preallocation lag <journaling-journal-files>`, you can
preallocate files in the journal directory by copying them from another
instance of :program:`mongod`.

Preallocated files do not contain data. It is safe to later remove them.
But if you restart :program:`mongod` with journaling, :program:`mongod`
will create them again.

.. example:: The following sequence preallocates journal files for an
   instance of :program:`mongod` running on port ``27017`` with a database
   path of ``/data/db``.

   For demonstration purposes, the sequence starts by creating a set of
   journal files in the usual way.

   1. Create a temporary directory into which to create a set of journal
      files:

      .. code-block:: sh

         mkdir ~/tmpDbpath

   #. Create a set of journal files by staring a :program:`mongod`
      instance that uses the temporary directory:

      .. code-block:: sh

         mongod --port 10000 --dbpath ~/tmpDbpath --journal

   #. When you see the following log output, indicating
      :program:`mongod` has the files, press CONTROL+C to stop the
      :program:`mongod` instance:

      .. code-block:: sh

         [initandlisten] waiting for connections on port 10000

   #. Preallocate journal files for the new instance of
      :program:`mongod` by moving the journal files from the data directory
      of the existing instance to the data directory of the new instance:

      .. code-block:: sh

         mv ~/tmpDbpath/journal /data/db/

   #. Start the new :program:`mongod` instance:

      .. code-block:: sh

         mongod --port 27017 --dbpath /data/db --journal

Monitor Journal Status
~~~~~~~~~~~~~~~~~~~~~~

Use the following commands and methods to monitor journal status:

- :dbcommand:`serverStatus`

  The :dbcommand:`serverStatus` command returns database status
  information that is useful for assessing performance.

- :dbcommand:`journalLatencyTest`

  Use :dbcommand:`journalLatencyTest` to measure how long it takes on
  your volume to write to the disk in an append-only fashion. You can
  run this command on an idle system to get a baseline sync time for
  journaling. You can also run this command on a busy system to see the
  sync time on a busy system, which may be higher if the journal
  directory is on the same volume as the data files.

  The :dbcommand:`journalLatencyTest` command also provides a way to
  check if your disk drive is buffering writes in its local cache. If
  the number is very low (i.e., less than 2 milliseconds) and the drive
  is non-SSD, the drive is probably buffering writes. In that case,
  enable cache write-through for the device in your operating system,
  unless you have a disk controller card with battery backed RAM.

.. _journaling-journal-commit-interval:

Change the Group Commit Interval
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. versionchanged:: 2.0

You can set the group commit interval using the
:option:`--journalCommitInterval <mongod --journalCommitInterval>`
command line option. The allowed range is ``2`` to ``300`` milliseconds.

Lower values increase the durability of the journal at the expense of
disk performance.

Recover Data After Unexpected Shutdown
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On a restart after a crash, MongoDB replays all journal files in the
journal directory before the server becomes available. If MongoDB must
replay journal files, :program:`mongod` notes these events in the log
output.

There is no reason to run :dbcommand:`repairDatabase` in these
situations.
===========================
Manage ``mongod`` Processes
===========================

.. default-domain:: mongodb

MongoDB runs as a standard program. You can start MongoDB from a command
line by issuing the :program:`mongod` command and specifying options.
For a list of options, see :doc:`/reference/program/mongod`. MongoDB can also run
as a Windows service. For details, see
:ref:`tutorial-mongod-as-windows-service`. To install MongoDB, see
:doc:`/installation`.

The following examples assume the directory containing the
:program:`mongod` process is in your system paths. The
:program:`mongod` process is the primary database process that runs on
an individual server. :program:`mongos` provides a coherent MongoDB
interface equivalent to a :program:`mongod` from the perspective of a
client. The :program:`mongo` binary provides the administrative
shell.

This document  page discusses the :program:`mongod` process; however,
some portions of this document may be applicable to :program:`mongos`
instances.

.. seealso:: :doc:`/administration/configuration`,
   :doc:`/reference/program/mongod`, :doc:`/reference/program/mongos`, and
   :doc:`/reference/configuration-options`.

Start ``mongod`` Processes
--------------------------

By default, MongoDB stores data in the ``/data/db`` directory. On
Windows, MongoDB stores data in ``C:\data\db``. On all platforms,
MongoDB listens for connections from clients on port ``27017``.

To start MongoDB using all defaults, issue the following command at
the system shell:

.. code-block:: sh

   mongod

Specify a Data Directory
~~~~~~~~~~~~~~~~~~~~~~~~

If you want :program:`mongod` to store data files at a path *other
than* ``/data/db`` you can specify a :setting:`~storage.dbPath`. The
:setting:`~storage.dbPath` must exist before you start :program:`mongod`. If it
does not exist, create the directory and the permissions so that
:program:`mongod` can read and write data to this path. For more
information on permissions, see the :ref:`security operations
documentation <security-operations>`.

To specify a :setting:`~storage.dbPath` for :program:`mongod` to use as a data
directory, use the :option:`--dbpath <mongod --dbpath>` option. The
following invocation will start a :program:`mongod` instance and store
data in the ``/srv/mongodb`` path

.. code-block:: sh

   mongod --dbpath /srv/mongodb/

Specify a TCP Port
~~~~~~~~~~~~~~~~~~

Only a single process can listen for connections on a network
interface at a time. If you run multiple :program:`mongod` processes
on a single machine, or have other processes that must use this port,
you must assign each a different port to listen on for client
connections.

To specify a port to :program:`mongod`, use the :option:`--port
<mongod --port>` option on the command line. The following command
starts :program:`mongod` listening on port ``12345``:

.. code-block:: sh

   mongod --port 12345

Use the default port number when possible, to avoid confusion.

Start ``mongod`` as a Daemon
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To run a :program:`mongod` process as a daemon (i.e. :setting:`fork`),
*and* write its output to a log file, use the :option:`--fork
<mongod --fork>` and :option:`--logpath <mongod --logpath>`
options. You must create the log directory; however, :program:`mongod`
will create the log file if it does not exist.

The following command starts :program:`mongod` as a daemon and records log
output to ``/var/log/mongodb.log``.

.. code-block:: sh

   mongod --fork --logpath /var/log/mongodb.log

Additional Configuration Options
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For an overview of common configurations and common configuration deployments.
configurations for common use cases, see
:doc:`/administration/configuration`.

.. _terminate-mongod-processes:

Stop ``mongod`` Processes
-------------------------

In a clean shutdown a :program:`mongod` completes all pending
operations, flushes all data to data files, and closes all data
files. Other shutdowns are *unclean* and can compromise the validity the
data files.

.. COMMENT add the following to the last sentence when the paragraph
   when maintain-valid-data-files is published.

   and may lead to corruption. See
   :doc:`/tutorial/maintain-valid-data-files` for more information.

To ensure a clean shutdown, always shutdown :program:`mongod`
instances using one of the following methods:

Use ``shutdownServer()``
~~~~~~~~~~~~~~~~~~~~~~~~

Shut down the :program:`mongod` from the :program:`mongo` shell using
the :method:`db.shutdownServer()` method as follows:

.. code-block:: javascript

   use admin
   db.shutdownServer()

Calling the same method from a control script accomplishes the same result.

For systems with :setting:`~security.authentication` enabled, users may only issue
:method:`db.shutdownServer()` when authenticated to the ``admin``
database or via the localhost interface on systems without
authentication enabled.

Use ``--shutdown``
~~~~~~~~~~~~~~~~~~

From the Linux command line, shut down the :program:`mongod` using the
:option:`--shutdown <mongod --shutdown>` option in the following command:

.. code-block:: sh

   mongod --shutdown

Use ``CTRL-C``
~~~~~~~~~~~~~~

When running the :program:`mongod` instance in interactive mode
(i.e. without :option:`--fork <mongod --fork>`), issue ``Control-C``
to perform a clean shutdown.

Use ``kill``
~~~~~~~~~~~~

From the Linux command line, shut down a specific :program:`mongod` instance
using the following command:

.. code-block:: none

   kill <mongod process ID>

.. warning::

   Never use ``kill -9`` (i.e. ``SIGKILL``) to terminate a mongod instance.

Stop a Replica Set
------------------

Procedure
~~~~~~~~~

If the :program:`mongod` is the :term:`primary` in a :term:`replica
set`, the shutdown process for these :program:`mongod` instances has
the following steps:

#. Check how up-to-date the :term:`secondaries <secondary>` are.

#. If no secondary is within 10 seconds of the primary,
   :program:`mongod` will return a message that it will not shut down.
   You can pass the :dbcommand:`shutdown` command a ``timeoutSecs``
   argument to wait for a secondary to catch up.

#. If there is a secondary within 10 seconds of the primary, the primary
   will step down and wait for the secondary to catch up.

#. After 60 seconds or once the secondary has caught up, the primary
   will shut down.

Force Replica Set Shutdown
~~~~~~~~~~~~~~~~~~~~~~~~~~

If there is no up-to-date secondary and you want the primary to shut
down, issue the :dbcommand:`shutdown` command with the ``force``
argument, as in the following :program:`mongo` shell operation:

.. code-block:: javascript

   db.adminCommand({shutdown : 1, force : true})

To keep checking the secondaries for a specified number of seconds if
none are immediately up-to-date, issue :dbcommand:`shutdown` with the
``timeoutSecs`` argument. MongoDB will keep checking the secondaries for
the specified number of seconds if none are immediately up-to-date. If
any of the secondaries catch up within the allotted time, the primary
will shut down. If no secondaries catch up, it will not shut down.

The following command issues :dbcommand:`shutdown` with ``timeoutSecs``
set to ``5``:

.. code-block:: javascript

   db.adminCommand({shutdown : 1, timeoutSecs : 5})

Alternately you can use the ``timeoutSecs`` argument with the
:method:`db.shutdownServer()` method:

.. code-block:: javascript

   db.shutdownServer({timeoutSecs : 5})
.. index:: balancing; operations

===============================
Manage Sharded Cluster Balancer
===============================

.. default-domain:: mongodb

This page describes common administrative procedures related
to balancing. For an introduction to balancing, see
:ref:`sharding-balancing`. For lower level information on balancing, see
:ref:`sharding-balancing-internals`.

.. seealso:: :doc:`/tutorial/configure-sharded-cluster-balancer`

Check the Balancer State
------------------------

The following command checks if the balancer is enabled (i.e. that the
balancer is allowed to run). The command does not check if the balancer
is active (i.e. if it is actively balancing chunks).

To see if the balancer is enabled in your :term:`cluster <sharded
cluster>`, issue the following command, which returns a boolean:

.. code-block:: javascript

   sh.getBalancerState()

.. _sharding-balancing-check-lock:

Check the Balancer Lock
-----------------------

To see if the balancer process is active in your :term:`cluster
<sharded cluster>`, do the following:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use the following query to return the balancer lock:

   .. code-block:: javascript

      db.locks.find( { _id : "balancer" } ).pretty()

When this command returns, you will see output like the following:

.. code-block:: javascript

   {   "_id" : "balancer",
   "process" : "mongos0.example.net:1292810611:1804289383",
     "state" : 2,
        "ts" : ObjectId("4d0f872630c42d1978be8a2e"),
      "when" : "Mon Dec 20 2010 11:41:10 GMT-0500 (EST)",
       "who" : "mongos0.example.net:1292810611:1804289383:Balancer:846930886",
       "why" : "doing balance round" }

This output confirms that:

- The balancer originates from the :program:`mongos` running on the
  system with the hostname ``mongos0.example.net``.

- The value in the ``state`` field indicates that a :program:`mongos`
  has the lock. For version 2.0 and later, the value of an active lock
  is ``2``; for earlier versions the value is ``1``.

.. _sharding-schedule-balancing-window:

Schedule the Balancing Window
-----------------------------

In some situations, particularly when your data set grows slowly and a
migration can impact performance, it's useful to be able to ensure
that the balancer is active only at certain times.  Use the following
procedure to specify a window during which the :term:`balancer` will
be able to migrate chunks:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Use an operation modeled on the following example :method:`update()
   <db.collection.update()>` operation to modify the balancer's
   window:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } }, true )

   Replace ``<start-time>`` and ``<end-time>`` with time values using
   two digit hour and minute values (e.g ``HH:MM``) that describe the
   beginning and end boundaries of the balancing window.
   These times will be evaluated relative to the time zone of each individual
   :program:`mongos` instance in the sharded cluster.
   If your :program:`mongos` instances are physically located in different
   time zones, use a common time zone (e.g. GMT) to ensure that the
   balancer window is interpreted correctly.

   For instance, running the following
   will force the balancer to run between 11PM and 6AM local time only:

   .. code-block:: javascript

      db.settings.update({ _id : "balancer" }, { $set : { activeWindow : { start : "23:00", stop : "6:00" } } }, true )

.. note::

   The balancer window must be sufficient to *complete* the migration
   of all data inserted during the day.

   As data insert rates can change based on activity and usage
   patterns, it is important to ensure that the balancing window you
   select will be sufficient to support the needs of your deployment.

.. _sharding-balancing-remove-window:

Remove a Balancing Window Schedule
----------------------------------

If you have :ref:`set the balancing window
<sharding-schedule-balancing-window>` and wish to remove the schedule
so that the balancer is always running, issue the following sequence
of operations:

.. code-block:: javascript

   use config
   db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true } })

.. _sharding-balancing-disable-temporally:
.. _sharding-balancing-disable-temporarily:

Disable the Balancer
--------------------

By default the balancer may run at any time and only moves chunks as
needed. To disable the balancer for a short period of time and prevent
all migration, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following operation to disable the balancer:

   .. code-block:: javascript

      sh.setBalancerState(false)

   If a migration is in progress, the system will complete the
   in-progress migration before stopping.

#. To verify that the balancer has stopped, issue the following command,
   which returns ``false`` if the balancer is stopped:

   .. code-block:: javascript

      sh.getBalancerState()

   Optionally, to verify no migrations are in progress after disabling,
   issue the following operation in the :program:`mongo` shell:

   .. code-block:: javascript

      use config
      while( sh.isBalancerRunning() ) {
                print("waiting...");
                sleep(1000);
      }

.. note::

   To disable the balancer from a driver that does not have the
   :method:`sh.startBalancer()` helper, issue the following command from
   the ``config`` database:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: true } } , true )

.. _sharding-balancing-re-enable:
.. _sharding-balancing-enable:

Enable the Balancer
-------------------

Use this procedure if you have disabled the balancer and are ready to
re-enable it:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue one of the following operations to enable the balancer:

   From the :program:`mongo` shell, issue:

   .. code-block:: javascript

      sh.setBalancerState(true)

   From a driver that does not have the :method:`sh.startBalancer()` helper,
   issue the following from the ``config`` database:

   .. code-block:: javascript

      db.settings.update( { _id: "balancer" }, { $set : { stopped: false } } , true )

Disable Balancing During Backups
--------------------------------

If MongoDB migrates a :term:`chunk` during a :doc:`backup
</core/backups>`, you can end with an inconsistent snapshot
of your :term:`sharded cluster`. Never run a backup while the balancer is
active. To ensure that the balancer is inactive during your backup
operation:

- Set the :ref:`balancing window <sharding-schedule-balancing-window>`
  so that the balancer is inactive during the backup. Ensure that the
  backup can complete while you have the balancer disabled.

- :ref:`manually disable the balancer <sharding-balancing-disable-temporarily>`
  for the duration of the backup procedure.

If you turn the balancer off while it is in the middle of a balancing round,
the shut down is not instantaneous. The balancer completes the chunk
move in-progress and then ceases all further balancing rounds.

Before starting a backup operation, confirm that the balancer is not
active. You can use the following command to determine if the balancer
is active:

.. code-block:: javascript

   !sh.getBalancerState() && !sh.isBalancerRunning()

When the backup procedure is complete you can reactivate
the balancer process.
.. _database-profiler:

==========================================
Analyze Performance of Database Operations
==========================================

.. default-domain:: mongodb

The database profiler collects fine grained data about MongoDB write
operations, cursors, database commands on a running :program:`mongod`
instance. You can enable profiling on a per-database or per-instance
basis. The :ref:`profiling level <database-profiling-level>` is also
configurable when enabling profiling.

The database profiler writes all the data it collects to the
:data:`system.profile <<database>.system.profile>` collection, which
is a :doc:`capped collection </core/capped-collections>`.  See
:doc:`/reference/database-profiler` for overview of the data in the
:data:`system.profile <<database>.system.profile>` documents created
by the profiler.

This document outlines a number of key administration options for the
database profiler. For additional related information, consider the
following resources:

- :doc:`/reference/database-profiler`
- :doc:`Profile Command </reference/command/profile>`
- :doc:`/reference/method/db.currentOp`

.. _database-profiling-levels:
.. _database-profiling-level:

Profiling Levels
----------------

The following profiling levels are available:

- ``0`` - the profiler is off, does not collect any data. :program:`mongod`
  always writes operations longer than the :setting:`~operationProfiling.slowOpThresholdMs` threshold
  to its log.

- ``1`` - collects profiling data for slow operations only. By default
  slow operations are those slower than 100 milliseconds.

  You can modify the threshold for "slow" operations with the
  :setting:`~operationProfiling.slowOpThresholdMs` runtime option or the :dbcommand:`setParameter`
  command. See the :ref:`database-profiling-specify-slowms-threshold`
  section for more information.

- ``2`` - collects profiling data for all database operations.

.. todo these objects should indexed.

.. _database-profiling-enable-profiling:

Enable Database Profiling and Set the Profiling Level
-----------------------------------------------------

You can enable database profiling from the :program:`mongo` shell or
through a driver using the :dbcommand:`profile` command. This section
will describe how to do so from the :program:`mongo` shell. See your :doc:`driver documentation
</applications/drivers>` if you want to control the profiler from
within your application.

When you enable profiling, you also set the :ref:`profiling level
<database-profiling-levels>`. The profiler records data in the
:data:`system.profile <<database>.system.profile>`
collection. MongoDB creates the :data:`system.profile
<<database>.system.profile>` collection in a database after you
enable profiling for that database.

To enable profiling and set the profiling level, use the
:method:`db.setProfilingLevel()` helper in the :program:`mongo` shell,
passing the profiling level as a parameter. For example, to enable profiling
for all database operations, consider the following operation in the
:program:`mongo` shell:

.. code-block:: javascript

   db.setProfilingLevel(2)

The shell returns a document showing the *previous* level of profiling.
The ``"ok" : 1`` key-value pair indicates the operation succeeded:

.. code-block:: javascript

   { "was" : 0, "slowms" : 100, "ok" : 1 }

To verify the new setting, see the
:ref:`database-profiling-view-status` section.

.. _database-profiling-specify-slowms-threshold:

Specify the Threshold for Slow Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The threshold for slow operations applies to the entire
:program:`mongod` instance. When you change the threshold, you change it
for all databases on the instance.

.. important:: Changing the slow operation threshold for the database
   profiler also affects the profiling subsystem's slow operation
   threshold for the entire :program:`mongod` instance. Always set the
   threshold to the highest useful value.

By default the slow operation threshold is 100 milliseconds. Databases with a profiling level
of ``1`` will log operations slower than 100 milliseconds.

To change the threshold, pass two parameters to the
:method:`db.setProfilingLevel()` helper in the :program:`mongo` shell. The first parameter sets the
profiling level for the current database, and the second sets the default
slow operation threshold *for the entire* :program:`mongod`
*instance*.

For example, the following command sets the profiling level for the current
database to ``0``, which disables profiling, and sets the
slow-operation threshold for the :program:`mongod` instance to 20
milliseconds. Any database on the instance with a profiling level of ``1``
will use this threshold:

.. code-block:: javascript

   db.setProfilingLevel(0,20)

.. _database-profiling-view-status:
.. _database-profiling-check-level:

Check Profiling Level
~~~~~~~~~~~~~~~~~~~~~

To view the :ref:`profiling level <database-profiling-levels>`, issue
the following from the :program:`mongo` shell:

.. code-block:: javascript

   db.getProfilingStatus()

The shell returns a document similar to the following:

.. code-block:: javascript

   { "was" : 0, "slowms" : 100 }

The ``was`` field indicates the current level of profiling.

The ``slowms`` field indicates how long an operation must exist in
milliseconds for an operation to pass the "slow" threshold. MongoDB
will log operations that take longer than the threshold if the
profiling level is ``1``. This document returns the profiling level in
the ``was`` field. For an explanation of profiling levels, see
:ref:`database-profiling-levels`.

To return only the profiling level, use the :method:`db.getProfilingLevel()`
helper in the :program:`mongo` as in the following:

.. code-block:: javascript

   db.getProfilingLevel()

Disable Profiling
~~~~~~~~~~~~~~~~~

To disable profiling, use the following helper in the :program:`mongo`
shell:

.. code-block:: javascript

   db.setProfilingLevel(0)

Enable Profiling for an Entire ``mongod`` Instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For development purposes in testing environments, you can enable
database profiling for an entire :program:`mongod` instance. The
profiling level applies to all databases provided by the
:program:`mongod` instance.

To enable profiling for a :program:`mongod` instance, pass the following
parameters to :program:`mongod` at startup or within the
:doc:`configuration file </reference/configuration-options>`:

.. code-block:: sh

   mongod --profile=1 --slowms=15

This sets the profiling level to ``1``, which collects profiling data
for slow operations only, and defines slow operations as those that
last longer than ``15`` milliseconds.

.. seealso:: :setting:`profile` and :setting:`~operationProfiling.slowOpThresholdMs`.

Database Profiling and Sharding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You *cannot* enable profiling on a :program:`mongos` instance. To enable
profiling in a shard cluster, you must enable profiling for each
:program:`mongod` instance in the cluster.

View Profiler Data
------------------

The database profiler logs information about database operations in the
:data:`system.profile <<database>.system.profile>` collection.

To view profiling information, query the :data:`system.profile <<database>.system.profile>` collection. To
view example queries, see :ref:`database-profiling-example-queries`

For an explanation of the output data, see
:doc:`/reference/database-profiler`.

Example Profiler Data Queries
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This section displays example queries to the :data:`system.profile <<database>.system.profile>`
collection. For an explanation of the query output, see
:doc:`/reference/database-profiler`.

To return the most recent 10 log entries in the :data:`system.profile <<database>.system.profile>`
collection, run a query similar to the following:

.. code-block:: javascript

   db.system.profile.find().limit(10).sort( { ts : -1 } ).pretty()

To return all operations except command operations (:term:`$cmd`), run a query
similar to the following:

.. code-block:: javascript

   db.system.profile.find( { op: { $ne : 'command' } } ).pretty()

To return operations for a particular collection, run a query similar to
the following. This example returns operations in the ``mydb`` database's
``test`` collection:

.. code-block:: javascript

   db.system.profile.find( { ns : 'mydb.test' } ).pretty()

To return operations slower than ``5`` milliseconds, run a query
similar to the following:

.. code-block:: javascript

   db.system.profile.find( { millis : { $gt : 5 } } ).pretty()

To return information from a certain time range, run a query similar to the following:

.. code-block:: javascript

   db.system.profile.find(
                          {
                           ts : {
                                 $gt : new ISODate("2012-12-09T03:00:00Z") ,
                                 $lt : new ISODate("2012-12-09T03:40:00Z")
                                }
                          }
                         ).pretty()

The following example looks at the time range, suppresses the ``user`` field
from the output to make it easier to read, and sorts the results by how
long each operation took to run:

.. code-block:: javascript

   db.system.profile.find(
                          {
                            ts : {
                                  $gt : new ISODate("2011-07-12T03:00:00Z") ,
                                  $lt : new ISODate("2011-07-12T03:40:00Z")
                                 }
                          },
                          { user : 0 }
                         ).sort( { millis : -1 } )

Show the Five Most Recent Events
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On a database that has profiling enabled, the ``show profile`` helper
in the :program:`mongo` shell displays the 5 most recent operations
that took at least 1 millisecond to execute. Issue ``show profile``
from the :program:`mongo` shell, as follows:

.. code-block:: javascript

   show profile

.. _database-profiling-example-queries:

Profiler Overhead
-----------------

When enabled, profiling has a minor effect on performance. The
:data:`system.profile <<database>.system.profile>` collection is a
:term:`capped collection` with a default size of 1 megabyte. A
collection of this size can typically store several thousand profile
documents, but some application may use more or less profiling data per
operation.

To change the size of the :data:`system.profile
<<database>.system.profile>` collection, you must:

1. Disable profiling.

#. Drop the :data:`system.profile <<database>.system.profile>` collection.

#. Create a new :data:`system.profile <<database>.system.profile>` collection.

#. Re-enable profiling.

For example, to create a new :data:`system.profile
<<database>.system.profile>` collections that's ``4000000`` bytes, use
the following sequence of operations in the :program:`mongo` shell:

.. code-block:: javascript

   db.setProfilingLevel(0)

   db.system.profile.drop()

   db.createCollection( "system.profile", { capped: true, size:4000000 } )

   db.setProfilingLevel(1)

Change Size of ``system.profile`` Collection
--------------------------------------------

To change the size of the :data:`system.profile
<<database>.system.profile>` collection on a :term:`secondary`, you must
stop the secondary, run it as a standalone, and then perform the steps
above. When done, restart the standalone as a member of the replica set.
For more information, see :doc:`/tutorial/perform-maintence-on-replica-set-members`.
===================
Map-Reduce Examples
===================

.. default-domain:: mongodb

In the :program:`mongo` shell, the :method:`db.collection.mapReduce()`
method is a wrapper around the :dbcommand:`mapReduce` command. The
following examples use the :method:`db.collection.mapReduce()` method:

.. include:: /includes/examples-map-reduce.rst
   :start-after: map-reduce-document-prototype-begin
.. index:: index; measure use
.. _index-measure-index-use:
.. _indexes-measuring-use:

=================
Measure Index Use
=================

.. default-domain:: mongodb

Synopsis
--------

Query performance is a good general indicator of index use;
however, for more precise insight into index use, MongoDB provides a
number of tools that allow you to study query operations and observe
index use for your database.

.. seealso:: :doc:`/core/indexes` and
   :doc:`/administration/indexes` for more information.

Operations
----------

Return Query Plan with ``explain()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Append the :method:`~cursor.explain()` method to any cursor
(e.g. query) to return a document with statistics about the query
process, including the index used, the number of documents scanned,
and the time the query takes to process in milliseconds.

Control Index Use with ``hint()``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Append the :method:`~cursor.hint()` to any cursor (e.g.
query) with the index as the argument to *force* MongoDB
to use a specific index to fulfill the query. Consider the following
example:

.. code-block:: javascript

   db.people.find( { name: "John Doe", zipcode: { $gt: "63000" } } } ).hint( { zipcode: 1 } )

You can use :method:`~cursor.hint()` and :method:`~cursor.explain()` in conjunction with each other to compare the
effectiveness of a specific index. Specify the ``$natural`` operator
to the :method:`~cursor.hint()` method to prevent MongoDB from
using *any* index:

.. code-block:: javascript

   db.people.find( { name: "John Doe", zipcode: { $gt: "63000" } } } ).hint( { $natural: 1 } )

Instance Index Use Reporting
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB provides a number of metrics of index use and operation that
you may want to consider when analyzing index use for your database:

- In the output of :dbcommand:`serverStatus`:

  - :data:`~serverStatus.indexCounters`

  - :data:`~serverStatus.metrics.queryExecutor.scanned`

  - :data:`~serverStatus.metrics.operation.scanAndOrder`

- In the output of :dbcommand:`collStats`:

  - :data:`~collStats.totalIndexSize`

  - :data:`~collStats.indexSizes`

- In the output of :dbcommand:`dbStats`:

  - :data:`dbStats.indexes`

  - :data:`dbStats.indexSize`
=================================
Merge Chunks in a Sharded Cluster
=================================

.. default-domain:: mongodb

Overview
--------

The :dbcommand:`mergeChunks` command allows you to collapse empty chunks
into neighboring chunks on the same shard. A :term:`chunk` is empty if
it has no documents associated with its shard key range.

.. important::

   Empty :term:`chunks <chunk>` can make the :term:`balancer` assess
   the cluster as properly balanced when it is not.

Empty chunks can occur under various circumstances, including:

- If a :doc:`pre-split </tutorial/create-chunks-in-sharded-cluster>`
  creates too many chunks, the distribution of data to chunks may be
  uneven.

- If you delete many documents from a sharded collection, some chunks
  may no longer contain data.

This tutorial explains how to identify chunks available to merge, and
how to merge those chunks with neighboring chunks.

Procedure
---------

.. note::

   Examples in this procedure use a ``users`` :term:`collection` in the
   ``test`` :term:`database`, using the ``username`` filed as a
   :term:`shard key`.

Identify Chunk Ranges
~~~~~~~~~~~~~~~~~~~~~

In the :program:`mongo` shell, identify the :term:`chunk`
ranges with the following operation:

.. code-block:: javascript

   sh.status()

The output of the :method:`sh.status()` will resemble the following:

.. code-block:: none

   --- Sharding Status ---
   sharding version: {
        "_id" : 1,
        "version" : 4,
        "minCompatibleVersion" : 4,
        "currentVersion" : 5,
        "clusterId" : ObjectId("5260032c901f6712dcd8f400")
   }
   shards:
        {  "_id" : "shard0000",  "host" : "localhost:30000" }
        {  "_id" : "shard0001",  "host" : "localhost:30001" }
     databases:
        {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
        {  "_id" : "test",  "partitioned" : true,  "primary" : "shard0001" }
                test.users
                        shard key: { "username" : 1 }
                        chunks:
                                shard0000       7
                                shard0001       7
                        { "username" : { "$minKey" : 1 } } -->> { "username" : "user16643" } on : shard0000 Timestamp(2, 0)
                        { "username" : "user16643" } -->> { "username" : "user2329" } on : shard0000 Timestamp(3, 0)
                        { "username" : "user2329" } -->> { "username" : "user29937" } on : shard0000 Timestamp(4, 0)
                        { "username" : "user29937" } -->> { "username" : "user36583" } on : shard0000 Timestamp(5, 0)
                        { "username" : "user36583" } -->> { "username" : "user43229" } on : shard0000 Timestamp(6, 0)
                        { "username" : "user43229" } -->> { "username" : "user49877" } on : shard0000 Timestamp(7, 0)
                        { "username" : "user49877" } -->> { "username" : "user56522" } on : shard0000 Timestamp(8, 0)
                        { "username" : "user56522" } -->> { "username" : "user63169" } on : shard0001 Timestamp(8, 1)
                        { "username" : "user63169" } -->> { "username" : "user69816" } on : shard0001 Timestamp(1, 8)
                        { "username" : "user69816" } -->> { "username" : "user76462" } on : shard0001 Timestamp(1, 9)
                        { "username" : "user76462" } -->> { "username" : "user83108" } on : shard0001 Timestamp(1, 10)
                        { "username" : "user83108" } -->> { "username" : "user89756" } on : shard0001 Timestamp(1, 11)
                        { "username" : "user89756" } -->> { "username" : "user96401" } on : shard0001 Timestamp(1, 12)
                        { "username" : "user96401" } -->> { "username" : { "$maxKey" : 1 } } on : shard0001 Timestamp(1, 13)

The chunk ranges appear after the chunk counts for each sharded
collection, as in the following excerpts:

**Chunk counts:**

   .. code-block:: none

      chunks:
              shard0000       7
              shard0001       7

**Chunk range:**

   .. code-block:: none

      { "username" : "user36583" } -->> { "username" : "user43229" } on : shard0000 Timestamp(6, 0)

Verify a Chunk is Empty
~~~~~~~~~~~~~~~~~~~~~~~

The :dbcommand:`mergeChunks` command requires at least one empty input
chunk. In the :program:`mongo` shell, check the amount of data in a
chunk using an operation that resembles:

.. code-block:: javascript

   db.runCommand({
      "dataSize": "test.users",
      "keyPattern": { username: 1 },
      "min": { "username": "user36583" },
      "max": { "username": "user43229" }
   })

If the input chunk to :dbcommand:`dataSize` is empty,
:dbcommand:`dataSize` produces output similar to:

.. code-block:: javascript

   { "size" : 0, "numObjects" : 0, "millis" : 0, "ok" : 1 }

Merge Chunks
~~~~~~~~~~~~

Merge two contiguous :term:`chunks <chunk>` on the same :term:`shard`,
where at least one of the contains no data, with an operation that
resembles the following:

.. code-block:: javascript

   db.runCommand( { mergeChunks: "test.users",
                    bounds: [ { "username": "user68982" },
                              { "username": "user95197" } ]
                } )

On success, :dbcommand:`mergeChunks` produces the following output:

.. code-block:: javascript

   { "ok" : 1 }

On any failure condition, :dbcommand:`mergeChunks` returns a document
where the value of the ``ok`` field is ``0``.

View Merged Chunks Ranges
~~~~~~~~~~~~~~~~~~~~~~~~~

After merging all empty chunks, confirm the new chunk, as follows:

.. code-block:: javascript

   sh.status()

The output of :method:`sh.status()` should resemble:

.. code-block:: none

   --- Sharding Status ---
   sharding version: {
        "_id" : 1,
        "version" : 4,
        "minCompatibleVersion" : 4,
        "currentVersion" : 5,
        "clusterId" : ObjectId("5260032c901f6712dcd8f400")
   }
   shards:
        {  "_id" : "shard0000",  "host" : "localhost:30000" }
        {  "_id" : "shard0001",  "host" : "localhost:30001" }
     databases:
        {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
        {  "_id" : "test",  "partitioned" : true,  "primary" : "shard0001" }
                test.users
                        shard key: { "username" : 1 }
                        chunks:
                                shard0000       2
                                shard0001       2
                        { "username" : { "$minKey" : 1 } } -->> { "username" : "user16643" } on : shard0000 Timestamp(2, 0)
                        { "username" : "user16643" } -->> { "username" : "user56522" } on : shard0000 Timestamp(3, 0)
                        { "username" : "user56522" } -->> { "username" : "user96401" } on : shard0001 Timestamp(8, 1)
                        { "username" : "user96401" } -->> { "username" : { "$maxKey" : 1 } } on : shard0001 Timestamp(1, 13)


.. .. include /includes/steps/merge-chunks-in-sharded-cluster.rst
===================================
Migrate Chunks in a Sharded Cluster
===================================

.. default-domain:: mongodb

In most circumstances, you should let the automatic :term:`balancer`
migrate :term:`chunks <chunk>` between :term:`shards <shard>`. However,
you may want to migrate chunks manually in a few cases:

- When :term:`pre-splitting` an empty collection, migrate chunks
  manually to distribute them evenly across the shards. Use
  pre-splitting in limited situations to support bulk data ingestion.

- If the balancer in an active cluster cannot distribute chunks within
  the :ref:`balancing window <sharding-schedule-balancing-window>`, then
  you will have to migrate chunks manually.

To manually migrate chunks, use the :dbcommand:`moveChunk` command.
For more information on how the automatic balancer moves chunks
between shards, see :ref:`sharding-balancing-internals` and
:ref:`sharding-chunk-migration`.

.. example:: Migrate a single chunk

   The following example assumes that the field ``username`` is the
   :term:`shard key` for a collection named ``users`` in the ``myapp``
   database, and that the value ``smith`` exists within the :term:`chunk`
   to migrate. Migrate the chunk using the following command in the
   :program:`mongo` shell.

   .. code-block:: javascript

      db.adminCommand( { moveChunk : "myapp.users",
                         find : {username : "smith"},
                         to : "mongodb-shard3.example.net" } )

   This command moves the chunk that includes the shard key value "smith" to the
   :term:`shard` named ``mongodb-shard3.example.net``. The command will
   block until the migration is complete.

   .. tip::

      To return a list of shards, use the :dbcommand:`listShards`
      command.

.. example:: Evenly migrate chunks

   To evenly migrate chunks for the ``myapp.users`` collection,
   put each prefix chunk on the next shard from the other and run
   the following commands in the mongo shell:

   .. code-block:: javascript

      var shServer = [ "sh0.example.net", "sh1.example.net", "sh2.example.net", "sh3.example.net", "sh4.example.net" ];
      for ( var x=97; x<97+26; x++ ){
        for( var y=97; y<97+26; y+=6 ) {
          var prefix = String.fromCharCode(x) + String.fromCharCode(y);
          db.adminCommand({moveChunk : "myapp.users", find : {email : prefix}, to : shServer[(y-97)/6]})
        }
      }

See :doc:`/tutorial/create-chunks-in-sharded-cluster` for an introduction
to pre-splitting.

.. versionadded:: 2.2
   The :dbcommand:`moveChunk` command has the: ``_secondaryThrottle``
   parameter. When set to ``true``, MongoDB ensures that changes to
   shards as part of chunk migrations replicate to :term:`secondaries
   <secondary>` throughout the migration operation. For more
   information, see :ref:`sharded-cluster-config-secondary-throttle`.

.. versionchanged:: 2.4
   In 2.4, ``_secondaryThrottle`` is ``true`` by default.

.. warning::

   The :dbcommand:`moveChunk` command may produce the following error
   message:

   .. code-block:: none

      The collection's metadata lock is already taken.

   This occurs when clients have too many open :term:`cursors
   <cursor>` that access the migrating chunk. You may either
   wait until the cursors complete their operations or close the
   cursors manually.

   .. todo:: insert link to killing a cursor.
===============================================
Migrate Config Servers with Different Hostnames
===============================================

.. default-domain:: mongodb

This procedure migrates a :ref:`config server <sharding-config-server>`
in a :doc:`sharded cluster </core/sharding>`
to a new server that uses a different hostname. Use this procedure only
if the config server *will not* be accessible via the same hostname.

Changing a :ref:`config server's <sharding-config-server>` hostname
**requires downtime** and requires restarting every process in the
sharded cluster.
If possible, avoid changing the hostname so that you can instead use the
procedure to :doc:`migrate a config server and use the same hostname
<migrate-config-servers-with-same-hostname>`.

To migrate all the config servers in a cluster, perform this procedure
for each config server separately and migrate the config servers in
reverse order from how they are listed in the :program:`mongos`
instances' :setting:`~sharding.configDB` string. Start with the last config server
listed in the :setting:`~sharding.configDB` string.

1. Disable the cluster balancer process temporarily. See
   :ref:`sharding-balancing-disable-temporarily` for more information.

#. Shut down the config server.

   This renders all config data for the sharded cluster "read only."

#. Copy the contents of :setting:`~storage.dbPath` from the old config server to
   the new config server.

   .. example::

      To copy the contents of :setting:`~storage.dbPath` to a machine
      named ``mongodb.config2.example.net``,  use a command that
      resembles the following:

      .. code-block:: sh

         rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Start the config server instance on the new system. The default
   invocation is:

   .. code-block:: sh

      mongod --configsvr

#. Shut down all existing MongoDB processes. This includes:

   - the :program:`mongod` instances or :term:`replica sets <replica set>`
     that provide your shards.

   - the :program:`mongod` instances that provide your existing
     :ref:`config databases <config-database>`.

   - the :program:`mongos` instances.

#. Restart all :program:`mongod` processes that provide the shard
   servers.

#. Update the :setting:`~sharding.configDB` setting for each :program:`mongos`
   instances.

#. Restart the :program:`mongos` instances.

#. Re-enable the balancer to allow the cluster to resume normal
   balancing operations. See the
   :ref:`sharding-balancing-disable-temporarily` section for more
   information on managing the balancer process.
=============================================
Migrate Config Servers with the Same Hostname
=============================================

.. default-domain:: mongodb

This procedure migrates a :ref:`config server <sharding-config-server>`
in a :doc:`sharded cluster </core/sharding>`
to a new system that uses *the same* hostname.

To migrate all the config servers in a cluster, perform this procedure
for each config server separately and migrate the config servers in
reverse order from how they are listed in the :program:`mongos`
instances' :setting:`~sharding.configDB` string. Start with the last config server
listed in the :setting:`~sharding.configDB` string.

.. start-migrate-config-server-with-same-hostname

#. Shut down the config server.

   This renders all config data for the sharded cluster "read only."

#. Change the DNS entry that points to the system that provided the old
   config server, so that the *same* hostname points to the new
   system.
   How you do this depends on how you organize your DNS and
   hostname resolution services.

#. Copy the contents of :setting:`~storage.dbPath` from the old config server to
   the new config server.

   For example, to copy the contents of :setting:`~storage.dbPath` to a machine
   named ``mongodb.config2.example.net``, you might issue a command
   similar to the following:

   .. code-block:: sh

      rsync -az /data/configdb/ mongodb.config2.example.net:/data/configdb

#. Start the config server instance on the new system. The default
   invocation is:

   .. code-block:: sh

      mongod --configsvr

.. end-migrate-config-server-with-same-hostname

When you start the third config server, your cluster will become
writable and it will be able to create new splits and migrate chunks
as needed.
===============================================
Migrate a Sharded Cluster to Different Hardware
===============================================

.. default-domain:: mongodb

This procedure moves the components of the :term:`sharded cluster` to a
new hardware system without downtime for reads and writes.

.. important:: While the migration is in progress, do not attempt to change to the
   :doc:`cluster metadata </core/sharded-cluster-metadata>`. Do not use
   any operation that modifies the cluster metadata *in any way*. For
   example, do not create or drop databases, create or drop collections,
   or use any sharding commands.

If your cluster includes a shard backed by a :term:`standalone`
:program:`mongod` instance, consider :doc:`converting the standalone
to a replica set </tutorial/convert-standalone-to-replica-set>` to
simplify migration and to let you keep the cluster online during
future maintenance. Migrating a shard as standalone is a multi-step
process that may require downtime.

To migrate a cluster to new hardware, perform the following tasks.

.. _migrate-to-new-hardware-disable-balancer:

Disable the Balancer
--------------------

Disable the balancer to stop :doc:`chunk migration
</core/sharding-chunk-migration>` and do not perform any metadata
write operations until the process finishes. If a migration is in
progress, the balancer will complete the in-progress migration before
stopping.

To disable the balancer, connect to one of the cluster's
:program:`mongos` instances and issue the following method:

.. code-block:: javascript

   sh.stopBalancer()

To check the balancer state, issue the :method:`sh.getBalancerState()`
method.

For more information, see :ref:`sharding-balancing-disable-temporarily`.

.. _migrate-to-new-hardware-config-servers:

Migrate Each Config Server Separately
-------------------------------------

Migrate each :ref:`config server <sharding-config-server>` by starting
with the *last* config server listed in the :setting:`~sharding.configDB` string.
Proceed in reverse order of the :setting:`~sharding.configDB` string. Migrate and
restart a config server before proceeding to the next.
Do not rename a config server during this process.


.. note::

   .. include:: /includes/fact-rename-config-servers-requires-cluster-restart.rst

   See
   :doc:`/tutorial/migrate-config-servers-with-different-hostnames`
   for more information.

.. important:: Start with the *last* config server listed in :setting:`~sharding.configDB`.

.. include:: /tutorial/migrate-config-servers-with-same-hostname.txt
   :start-after: start-migrate-config-server-with-same-hostname
   :end-before: end-migrate-config-server-with-same-hostname

.. _migrate-to-new-hardware-restart-mongos:

Restart the ``mongos`` Instances
--------------------------------

If the :setting:`~sharding.configDB` string will change as part of the
migration, you must shut down *all* :program:`mongos` instances before
changing the :setting:`~sharding.configDB` string. This avoids errors in the
sharded cluster over :setting:`~sharding.configDB` string conflicts.

If the :setting:`~sharding.configDB` string will remain the same, you can migrate
the :program:`mongos` instances sequentially or all at once.

1. Shut down the :program:`mongos` instances using the
   :dbcommand:`shutdown` command. If the :setting:`~sharding.configDB` string is
   changing, shut down *all* :program:`mongos` instances.

#. If the hostname has changed for any of the config servers, update the
   :setting:`~sharding.configDB` string for each :program:`mongos` instance. The
   :program:`mongos` instances must all use the same :setting:`~sharding.configDB`
   string. The strings must list identical host names in identical order.

   .. include:: /includes/tip-hostnames.rst

#. Restart the :program:`mongos` instances being sure to use the
   updated :setting:`~sharding.configDB` string if hostnames have changed.

For more information, see :ref:`sharding-setup-start-mongos`.

.. _migrate-to-new-hardware-shards:

Migrate the Shards
------------------

Migrate the shards one at a time. For each shard, follow the appropriate
procedure in this section.

.. _migrate-replica-set-shard:

Migrate a Replica Set Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To migrate a sharded cluster, migrate each member separately. First
migrate the non-primary members, and then migrate the :term:`primary`
last.

If the replica set has two voting members, add an :doc:`arbiter
</core/replica-set-arbiter>` to the replica set to ensure the set
keeps a majority of its votes available during the migration. You can
remove the arbiter after completing the migration.

.. _migrate-replica-set-shard-member:

Migrate a Member of a Replica Set Shard
````````````````````````````````````````

1. Shut down the :program:`mongod` process. To ensure a
   clean shutdown, use the :dbcommand:`shutdown` command.

#. Move the data directory (i.e., the :setting:`~storage.dbPath`)
   to the new machine.

#. Restart the :program:`mongod` process at the new
   location.

#. Connect to the replica set's current primary.

#. If the hostname of the member has changed, use
   :method:`rs.reconfig()` to update the :doc:`replica set configuration
   document </reference/replica-configuration>` with the new hostname.

   For example, the following sequence of commands updates the
   hostname for the instance at position ``2`` in the ``members``
   array:

   .. code-block:: javascript

      cfg = rs.conf()
      cfg.members[2].host = "pocatello.example.net:27017"
      rs.reconfig(cfg)

   For more information on updating the configuration document, see
   :ref:`replica-set-reconfiguration-usage`.

#. To confirm the new configuration, issue :method:`rs.conf()`.

#. Wait for the member to recover. To check the member's state, issue
   :method:`rs.status()`.

Migrate the Primary in a Replica Set Shard
``````````````````````````````````````````

While migrating the replica set's primary, the set must elect a new
primary. This failover process which renders the replica set
unavailable to perform reads or accept writes for the duration of the
election, which typically completes quickly. If possible, plan the
migration during a maintenance window.

1. Step down the primary to allow the normal :ref:`failover
   <replica-set-failover>` process.  To step down the primary, connect
   to the primary and issue the either the
   :dbcommand:`replSetStepDown` command or the :method:`rs.stepDown()`
   method. The following example shows the :method:`rs.stepDown()`
   method:

   .. code-block:: javascript

      rs.stepDown()

#. Once the primary has stepped down and another member has become
   :replstate:`PRIMARY` state. To migrate the stepped-down primary,
   follow the :ref:`migrate-replica-set-shard-member` procedure

   You can check the output of :method:`rs.status()` to confirm the
   change in status.

Migrate a Standalone Shard
~~~~~~~~~~~~~~~~~~~~~~~~~~

The ideal procedure for migrating a standalone shard is to
:doc:`convert the standalone to a replica set
</tutorial/convert-standalone-to-replica-set>` and then use the
procedure for :ref:`migrating a replica set shard
<migrate-replica-set-shard>`. In production clusters, all shards
should be replica sets, which provides continued availability during
maintenance windows.

Migrating a shard as standalone is a multi-step process during which
part of the shard may be unavailable. If the shard is the
:term:`primary shard` for a database,the process includes the
:dbcommand:`movePrimary` command. While the :dbcommand:`movePrimary`
runs, you should stop modifying data in that database. To migrate the
standalone shard, use the :doc:`/tutorial/remove-shards-from-cluster`
procedure.

.. _migrate-to-new-hardware-enable-balancer:

Re-Enable the Balancer
----------------------

To complete the migration, re-enable the balancer to resume
:doc:`chunk migrations </core/sharding-chunk-migration>`.

Connect to one of the cluster's :program:`mongos` instances and pass
``true`` to the :method:`sh.setBalancerState()` method:

.. code-block:: javascript

   sh.setBalancerState(true)

To check the balancer state, issue the :method:`sh.getBalancerState()`
method.

For more information, see :ref:`sharding-balancing-enable`.
.. _data-modeling-atomic-operation:

================================
Model Data for Atomic Operations
================================

.. default-domain:: mongodb

Pattern
-------

Consider the following example that keeps a library book and its
checkout information. The example illustrates how embedding fields
related to an atomic update within the same document ensures that the
fields are in sync.

Consider the following ``book`` document that stores the number of
available copies for checkout and the current checkout information:

.. code-block:: javascript
   :emphasize-lines: 9

   book = {
             _id: 123456789,
             title: "MongoDB: The Definitive Guide",
             author: [ "Kristina Chodorow", "Mike Dirolf" ],
             published_date: ISODate("2010-09-24"),
             pages: 216,
             language: "English",
             publisher_id: "oreilly",
             available: 3,
             checkout: [ { by: "joe", date: ISODate("2012-10-15") } ]
           }

You can use the :method:`db.collection.findAndModify()` method to
atomically determine if a book is available for checkout and update
with the new checkout information. Embedding the ``available`` field
and the ``checkout`` field within the same document ensures that the
updates to these fields are in sync:

.. code-block:: javascript

   db.books.findAndModify ( {
      query: {
               _id: 123456789,
               available: { $gt: 0 }
             },
      update: {
                $inc: { available: -1 },
                $push: { checkout: { by: "abc", date: new Date() } }
              }
   } )
====================================
Model Data to Support Keyword Search
====================================

.. default-domain:: mongodb

.. note::

   Keyword search is *not* the same as text search or full text
   search, and does not provide stemming or other text-processing
   features. See the :ref:`limit-keyword-indexes` section for more
   information.

   In 2.4, MongoDB provides a text search feature. See
   :doc:`/core/index-text` for more information.

If your application needs to perform queries on the content of a field
that holds text you can perform exact matches on the text or use
:query:`$regex` to use regular expression pattern matches. However,
for many operations on text, these methods do not satisfy application
requirements.

This pattern describes one method for supporting keyword search using
MongoDB to support application search functionality, that uses
keywords stored in an array in the same document as the text
field. Combined with a :ref:`multi-key index <index-type-multikey>`,
this pattern can support application's keyword search operations.

Pattern
-------

To add structures to your document to support keyword-based queries,
create an array field in your documents and add the keywords as
strings in the array. You can then create a :ref:`multi-key index
<index-type-multi-key>` on the array and create queries that select
values from the array.

.. example::

   Given a collection of library volumes that you want to provide
   topic-based search. For each volume, you add the array ``topics``,
   and you add as many keywords as needed for a given volume.

   For the ``Moby-Dick`` volume you might have the following document:

   .. code-block:: javascript

      { title : "Moby-Dick" ,
        author : "Herman Melville" ,
        published : 1851 ,
        ISBN : 0451526996 ,
        topics : [ "whaling" , "allegory" , "revenge" , "American" ,
          "novel" , "nautical" , "voyage" , "Cape Cod" ]
      }

   You then create a multi-key index on the ``topics`` array:

   .. code-block:: javascript

      db.volumes.ensureIndex( { topics: 1 } )

   The multi-key index creates separate index entries for each keyword in
   the ``topics`` array. For example the index contains one entry for
   ``whaling`` and another for ``allegory``.

   You then query based on the keywords. For example:

   .. code-block:: javascript

      db.volumes.findOne( { topics : "voyage" }, { title: 1 } )

.. note:: An array with a large number of elements, such as one with
   several hundreds or thousands of keywords will incur greater
   indexing costs on insertion.

.. _limit-keyword-indexes:

Limitations of Keyword Indexes
------------------------------

MongoDB can support keyword searches using specific data models and
:ref:`multi-key indexes <index-type-multikey>`; however, these keyword
indexes are not sufficient or comparable to full-text products in the
following respects:

- *Stemming*. Keyword queries in MongoDB can not parse keywords for
  root or related words.

- *Synonyms*. Keyword-based search features must provide support for
  synonym or related queries in the application layer.

- *Ranking*. The keyword look ups described in this document do not
  provide a way to weight results.

- *Asynchronous Indexing*. MongoDB builds indexes synchronously, which
  means that the indexes used for keyword indexes are always current
  and can operate in real-time. However, asynchronous bulk indexes
  may be more efficient for some kinds of content and workloads.
.. _data-modeling-example-one-to-many:

=======================================================
Model One-to-Many Relationships with Embedded Documents
=======================================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that uses :ref:`embedded
<data-modeling-embedding>` documents to describe relationships between
connected data.

Pattern
-------

Consider the following example that maps patron and multiple address
relationships. The example illustrates the advantage of embedding over
referencing if you need to view many data entities in context of
another. In this one-to-many relationship between ``patron`` and
``address`` data, the ``patron`` has multiple ``address`` entities.

In the normalized data model, the ``address`` documents contain a
reference to the ``patron`` document.

.. code-block:: javascript

   {
      _id: "joe",
      name: "Joe Bookreader"
   }

   {
      patron_id: "joe",
      street: "123 Fake Street",
      city: "Faketon",
      state: "MA",
      zip: "12345"
   }

   {
      patron_id: "joe",
      street: "1 Some Other Street",
      city: "Boston",
      state: "MA",
      zip: "12345"
   }

If your application frequently retrieves the ``address`` data with the
``name`` information, then your application needs to issue multiple
queries to resolve the references. A more optimal schema would be to
embed the ``address`` data entities in the ``patron`` data, as in the
following document:

.. code-block:: javascript

   {
      _id: "joe",
      name: "Joe Bookreader",
      addresses: [
                   {
                     street: "123 Fake Street",
                     city: "Faketon",
                     state: "MA",
                     zip: "12345"
                   },
                   {
                     street: "1 Some Other Street",
                     city: "Boston",
                     state: "MA",
                     zip: "12345"
                   }
                 ]
    }

With the embedded data model, your application can retrieve the
complete patron information with one query.
.. _data-modeling-example-one-to-one:

======================================================
Model One-to-One Relationships with Embedded Documents
======================================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that uses :ref:`embedded
<data-modeling-embedding>` documents to describe relationships between
connected data.

Pattern
-------

Consider the following example that maps patron and address
relationships. The example illustrates the advantage of embedding over
referencing if you need to view one data entity in context of the
other. In this one-to-one relationship between ``patron`` and
``address`` data, the ``address`` belongs to the ``patron``.

In the normalized data model, the ``address`` document contains a
reference to the ``patron`` document.

.. code-block:: javascript

   {
      _id: "joe",
      name: "Joe Bookreader"
   }

   {
      patron_id: "joe",
      street: "123 Fake Street",
      city: "Faketon",
      state: "MA",
      zip: "12345"
   }

If the ``address`` data is frequently retrieved with the ``name``
information, then with referencing, your application needs to issue
multiple queries to resolve the reference. The better data model would
be to embed the ``address`` data in the ``patron`` data, as in the
following document:

.. code-block:: javascript

   {
      _id: "joe",
      name: "Joe Bookreader",
      address: {
                 street: "123 Fake Street",
                 city: "Faketon",
                 state: "MA",
                 zip: "12345"
               }
   }

With the embedded data model, your application can retrieve the
complete patron information with one query.
.. _data-modeling-publisher-and-books:

========================================================
Model One-to-Many Relationships with Document References
========================================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that uses :ref:`references
<data-modeling-referencing>` between documents to describe
relationships between connected data.

Pattern
-------

Consider the following example that maps publisher and book
relationships. The example illustrates the advantage of referencing
over embedding to avoid repetition of the publisher information.

Embedding the publisher document inside the book document would lead to
**repetition** of the publisher data, as the following documents show:

.. code-block:: javascript
   :emphasize-lines: 7-11,20-24

   {
      title: "MongoDB: The Definitive Guide",
      author: [ "Kristina Chodorow", "Mike Dirolf" ],
      published_date: ISODate("2010-09-24"),
      pages: 216,
      language: "English",
      publisher: {
                 name: "O'Reilly Media",
                 founded: 1980,
                 location: "CA"
               }
   }

   {
      title: "50 Tips and Tricks for MongoDB Developer",
      author: "Kristina Chodorow",
      published_date: ISODate("2011-05-06"),
      pages: 68,
      language: "English",
      publisher: {
                 name: "O'Reilly Media",
                 founded: 1980,
                 location: "CA"
               }
   }

To avoid repetition of the publisher data, use *references* and keep
the publisher information in a separate collection from the book
collection.

When using references, the growth of the relationships determine where
to store the reference. If the number of books per publisher is small
with limited growth, storing the book reference inside the publisher
document may sometimes be useful. Otherwise, if the number of books per
publisher is unbounded, this data model would lead to mutable, growing
arrays, as in the following example:

.. code-block:: javascript
   :emphasize-lines: 5

   {
      name: "O'Reilly Media",
      founded: 1980,
      location: "CA",
      books: [12346789, 234567890, ...]
   }

   {
       _id: 123456789,
       title: "MongoDB: The Definitive Guide",
       author: [ "Kristina Chodorow", "Mike Dirolf" ],
       published_date: ISODate("2010-09-24"),
       pages: 216,
       language: "English"
   }

   {
      _id: 234567890,
      title: "50 Tips and Tricks for MongoDB Developer",
      author: "Kristina Chodorow",
      published_date: ISODate("2011-05-06"),
      pages: 68,
      language: "English"
   }

To avoid mutable, growing arrays, store the publisher reference inside
the book document:

.. code-block:: javascript
   :emphasize-lines: 15, 25

   {
      _id: "oreilly",
      name: "O'Reilly Media",
      founded: 1980,
      location: "CA"
   }

   {
      _id: 123456789,
      title: "MongoDB: The Definitive Guide",
      author: [ "Kristina Chodorow", "Mike Dirolf" ],
      published_date: ISODate("2010-09-24"),
      pages: 216,
      language: "English",
      publisher_id: "oreilly"
   }

   {
      _id: 234567890,
      title: "50 Tips and Tricks for MongoDB Developer",
      author: "Kristina Chodorow",
      published_date: ISODate("2011-05-06"),
      pages: 68,
      language: "English",
      publisher_id: "oreilly"
   }

.. Reworked the Queue slide from the presentation to Atomic Operation
.. TODO later, include a separate queue example for maybe checkout requests,
   and possibly bucket example that is separate from the pre-allocation
   example link above in the Document Growth section
================================================
Model Tree Structures with an Array of Ancestors
================================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that describes a tree-like
structure in MongoDB documents using :ref:`references
<data-modeling-referencing>` to parent nodes and an array that stores
all ancestors.

Pattern
-------

.. start-model-tree-structures-include-here

The *Array of Ancestors* pattern stores each tree node in a document;
in addition to the tree node, document stores in an array the id(s) of
the node's ancestors or path.

Consider the following hierarchy of categories:

.. include:: /images/data-model-tree.rst

The following example models the tree using *Array of Ancestors*. In
addition to the ``ancestors`` field, these documents also store the
reference to the immediate parent category in the ``parent`` field:

.. code-block:: javascript

   db.categories.insert( { _id: "MongoDB", ancestors: [ "Books", "Programming", "Databases" ], parent: "Databases" } )
   db.categories.insert( { _id: "dbm", ancestors: [ "Books", "Programming", "Databases" ], parent: "Databases" } )
   db.categories.insert( { _id: "Databases", ancestors: [ "Books", "Programming" ], parent: "Programming" } )
   db.categories.insert( { _id: "Languages", ancestors: [ "Books", "Programming" ], parent: "Programming" } )
   db.categories.insert( { _id: "Programming", ancestors: [ "Books" ], parent: "Books" } )
   db.categories.insert( { _id: "Books", ancestors: [ ], parent: null } )

- The query to retrieve the ancestors or path of a node is fast and
  straightforward:

  .. code-block:: javascript

     db.categories.findOne( { _id: "MongoDB" } ).ancestors

- You can create an index on the field ``ancestors`` to enable fast
  search by the ancestors nodes:

  .. code-block:: javascript

     db.categories.ensureIndex( { ancestors: 1 } )

- You can query by the field ``ancestors`` to find all its descendants:

  .. code-block:: javascript

     db.categories.find( { ancestors: "Programming" } )

The *Array of Ancestors* pattern provides a fast and efficient solution
to find the descendants and the ancestors of a node by creating an
index on the elements of the ancestors field. This makes *Array of
Ancestors* a good choice for working with subtrees.

The *Array of Ancestors* pattern is slightly slower than the
:doc:`Materialized Paths
</tutorial/model-tree-structures-with-materialized-paths>` pattern but
is more straightforward to use.
===========================================
Model Tree Structures with Child References
===========================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that describes a tree-like structure
in MongoDB documents by storing :ref:`references
<data-modeling-referencing>` in the parent-nodes to children nodes.

Pattern
-------

.. start-model-tree-structures-include-here

The *Child References* pattern stores each tree node in a document; in
addition to the tree node, document stores in an array the id(s) of the
node's children.

Consider the following hierarchy of categories:

.. include:: /images/data-model-tree.rst

The following example models the tree using *Child References*, storing
the reference to the node's children in the field ``children``:

.. code-block:: javascript

   db.categories.insert( { _id: "MongoDB", children: [] } )
   db.categories.insert( { _id: "dbm", children: [] } )
   db.categories.insert( { _id: "Databases", children: [ "MongoDB", "dbm" ] } )
   db.categories.insert( { _id: "Languages", children: [] } )
   db.categories.insert( { _id: "Programming", children: [ "Databases", "Languages" ] } )
   db.categories.insert( { _id: "Books", children: [ "Programming" ] } )

- The query to retrieve the immediate children of a node is fast and
  straightforward:

  .. code-block:: javascript

     db.categories.findOne( { _id: "Databases" } ).children

- You can create an index on the field ``children`` to enable fast
  search by the child nodes:

  .. code-block:: javascript

     db.categories.ensureIndex( { children: 1 } )

- You can query for a node in the ``children`` field to find its parent
  node as well as its siblings:

  .. code-block:: javascript

     db.categories.find( { children: "MongoDB" } )

The *Child References* pattern provides a suitable solution to tree storage
as long as no operations on subtrees are necessary. This pattern may
also provide a suitable solution for storing graphs where a node may
have multiple parents.
=============================================
Model Tree Structures with Materialized Paths
=============================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that describes a tree-like
structure in MongoDB documents by storing full relationship paths
between documents.

Pattern
-------

.. start-model-tree-structures-include-here

The *Materialized Paths* pattern stores each tree node in a document;
in addition to the tree node, document stores as a string the id(s) of
the node's ancestors or path. Although the *Materialized Paths* pattern
requires additional steps of working with strings and regular
expressions, the pattern also provides more flexibility in working with
the path, such as finding nodes by partial paths.


Consider the following hierarchy of categories:

.. include:: /images/data-model-tree.rst

The following example models the tree using *Materialized Paths*,
storing the path in the field ``path``; the path string uses the comma
``,`` as a delimiter:

.. code-block:: javascript

   db.categories.insert( { _id: "Books", path: null } )
   db.categories.insert( { _id: "Programming", path: ",Books," } )
   db.categories.insert( { _id: "Databases", path: ",Books,Programming," } )
   db.categories.insert( { _id: "Languages", path: ",Books,Programming," } )
   db.categories.insert( { _id: "MongoDB", path: ",Books,Programming,Databases," } )
   db.categories.insert( { _id: "dbm", path: ",Books,Programming,Databases," } )

- You can query to retrieve the whole tree, sorting by the field
  ``path``:

  .. code-block:: javascript

     db.categories.find().sort( { path: 1 } )

- You can use regular expressions on the ``path`` field to find the
  descendants of ``Programming``:

  .. code-block:: javascript

     db.categories.find( { path: /,Programming,/ } )

- You can also retrieve the descendants of ``Books`` where the
  ``Books`` is also at the topmost level of the hierarchy:

  .. code-block:: javascript

     db.categories.find( { path: /^,Books,/ } )

- To create an index on the field ``path`` use the following
  invocation:

  .. code-block:: javascript

     db.categories.ensureIndex( { path: 1 } )

  This index may improve performance depending on the query:

  - For queries of the ``Books`` sub-tree (e.g. ``/^,Books,/``) an
    index on the ``path`` field improves the query performance
    significantly.

  - For queries of the ``Programming`` sub-tree
    (e.g. ``/,Programming,/``), or similar queries of sub-tress, where
    the node might be in the middle of the indexed string, the query
    must inspect the entire index.

    For these queries an index *may* provide some performance
    improvement *if* the index is significantly smaller than the
    entire collection.
======================================
Model Tree Structures with Nested Sets
======================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that describes a tree like
structure that optimizes discovering subtrees at the expense of tree
mutability.

Pattern
-------

.. start-model-tree-structures-include-here

The *Nested Sets* pattern identifies each node in the tree as stops in
a round-trip traversal of the tree. The application visits each node
in the tree twice; first during the initial trip, and second during
the return trip. The *Nested Sets* pattern stores each tree node in a
document; in addition to the tree node, document stores the id of
node's parent, the node's initial stop in the ``left`` field, and its
return stop in the ``right`` field.

Consider the following hierarchy of categories:

.. include:: /images/data-model-example-nested-set.rst

The following example models the tree using *Nested Sets*:

.. code-block:: javascript

   db.categories.insert( { _id: "Books", parent: 0, left: 1, right: 12 } )
   db.categories.insert( { _id: "Programming", parent: "Books", left: 2, right: 11 } )
   db.categories.insert( { _id: "Languages", parent: "Programming", left: 3, right: 4 } )
   db.categories.insert( { _id: "Databases", parent: "Programming", left: 5, right: 10 } )
   db.categories.insert( { _id: "MongoDB", parent: "Databases", left: 6, right: 7 } )
   db.categories.insert( { _id: "dbm", parent: "Databases", left: 8, right: 9 } )

You can query to retrieve the descendants of a node:

.. code-block:: javascript

   var databaseCategory = db.categories.findOne( { _id: "Databases" } );
   db.categories.find( { left: { $gt: databaseCategory.left }, right: { $lt: databaseCategory.right } } );

The *Nested Sets* pattern provides a fast and efficient solution for
finding subtrees but is inefficient for modifying the tree structure.
As such, this pattern is best for static trees that do not change.
============================================
Model Tree Structures with Parent References
============================================

.. default-domain:: mongodb

Overview
--------

Data in MongoDB has a *flexible schema*. :term:`Collections
<collection>` do not enforce :term:`document` structure. Decisions
that affect how you model data can affect application performance and
database capacity. See :doc:`/core/data-models` for a full high
level overview of data modeling in MongoDB.

This document describes a data model that describes a tree-like
structure in MongoDB documents by storing
:ref:`references <data-modeling-referencing>` to "parent" nodes in
children nodes.

Pattern
-------

.. start-model-tree-structures-include-here

The *Parent References* pattern stores each tree node in a document; in
addition to the tree node, the document stores the id of the node's
parent.

Consider the following hierarchy of categories:

.. include:: /images/data-model-tree.rst


The following example models the tree using *Parent References*,
storing the reference to the parent category in the field ``parent``:

.. code-block:: javascript

   db.categories.insert( { _id: "MongoDB", parent: "Databases" } )
   db.categories.insert( { _id: "dbm", parent: "Databases" } )
   db.categories.insert( { _id: "Databases", parent: "Programming" } )
   db.categories.insert( { _id: "Languages", parent: "Programming" } )
   db.categories.insert( { _id: "Programming", parent: "Books" } )
   db.categories.insert( { _id: "Books", parent: null } )

- The query to retrieve the parent of a node is fast and
  straightforward:

  .. code-block:: javascript

     db.categories.findOne( { _id: "MongoDB" } ).parent

- You can create an index on the field ``parent`` to enable fast search
  by the parent node:

  .. code-block:: javascript

     db.categories.ensureIndex( { parent: 1 } )

- You can query by the ``parent`` field to find its immediate children
  nodes:

  .. code-block:: javascript

     db.categories.find( { parent: "Databases" } )

The *Parent Links* pattern provides a simple solution to tree storage
but requires multiple queries to retrieve subtrees.
:orphan:

.. _data-modeling-trees:

================================
Model Tree Structures in MongoDB
================================

To model hierarchical or nested data relationships, you can use
references to implement tree-like structures. The following *Tree*
pattern examples model book categories that have hierarchical
relationships.

Model Tree Structures with Child References
-------------------------------------------

(:doc:`link </tutorial/model-tree-structures-with-child-references>`)

.. include:: /tutorial/model-tree-structures-with-child-references.txt
   :start-after: start-model-tree-structures-include-here

Model Tree Structures with Parent References
--------------------------------------------

(:doc:`link </tutorial/model-tree-structures-with-parent-references>`)

.. include:: /tutorial/model-tree-structures-with-parent-references.txt
   :start-after: start-model-tree-structures-include-here

Model Tree Structures with an Array of Ancestors
------------------------------------------------

(:doc:`link </tutorial/model-tree-structures-with-ancestors-array>`)

.. include:: /tutorial/model-tree-structures-with-ancestors-array.txt
   :start-after: start-model-tree-structures-include-here

Model Tree Structures with Materialized Paths
---------------------------------------------

(:doc:`link </tutorial/model-tree-structures-with-materialized-paths>`)

.. include:: /tutorial/model-tree-structures-with-materialized-paths.txt
   :start-after: start-model-tree-structures-include-here

Model Tree Structures with Nested Sets
--------------------------------------

(:doc:`link </tutorial/model-tree-structures-with-nested-sets>`)

.. include:: /tutorial/model-tree-structures-with-nested-sets.txt
   :start-after: start-model-tree-structures-include-here
======================================
Modify Chunk Size in a Sharded Cluster
======================================

.. default-domain:: mongodb

When the first :program:`mongos` connects to a set of :term:`config
servers <config database>`, it initializes the sharded cluster with a
default chunk size of 64 megabytes. This default chunk size works well
for most deployments; however, if you notice that automatic migrations
have more I/O than your hardware can handle, you may want to reduce the
chunk size. For automatic splits and migrations, a small chunk size
leads to more rapid and frequent migrations.

To modify the chunk size, use the following procedure:

#. Connect to any :program:`mongos` in the cluster using the
   :program:`mongo` shell.

#. Issue the following command to switch to the :ref:`config-database`:

   .. code-block:: javascript

      use config

#. Issue the following :method:`~db.collection.save()` operation to
   store the global chunk size configuration value:

   .. code-block:: javascript

      db.settings.save( { _id:"chunksize", value: <sizeInMB> } )

.. note::

   The :setting:`~sharding.chunkSize` and
   :option:`--chunkSize <mongos --chunkSize>`
   options, passed at runtime to the :program:`mongos`,
   **do not** affect the chunk size after you have initialized the
   cluster.

   To avoid confusion, *always* set the chunk size using the above
   procedure instead of the runtime options.

Modifying the chunk size has several limitations:

- Automatic splitting only occurs on insert or update.

- If you lower the chunk size, it may take time for all chunks to split to
  the new size.

- Splits cannot be undone.

- If you increase the chunk size, existing chunks grow only through
  insertion or updates until they reach the new size.
================
Modify Documents
================

.. default-domain:: mongodb

In MongoDB, both :method:`db.collection.update()` and
:method:`db.collection.save()` modify existing documents in a
collection. :method:`db.collection.update()` provides additional
control over the modification. For example, you can modify existing
data or modify a group of documents that match a query with
:method:`db.collection.update()`. Alternately,
:method:`db.collection.save()` replaces an existing document with the
same ``_id`` field.

This document provides examples of the update operations using each of
the two methods in the :program:`mongo` shell.

Modify Multiple Documents with ``update()`` Method
--------------------------------------------------

By default, the :method:`~db.collection.update()` method updates a
single document that matches its selection criteria. Call the method
with the ``multi`` option set to ``true`` to update multiple documents.
[#previous-version]_

The following example finds all documents with ``type`` equal to
``"book"`` and modifies their ``qty`` field by ``-1``. The example uses
:update:`$inc`, which is one of the :ref:`update operators
<update-operators>` available.

.. code-block:: javascript

   db.inventory.update(
      { type : "book" },
      { $inc : { qty : -1 } },
      { multi: true }
   )

For more examples, see :method:`~db.collection.update()`.

.. [#previous-version] This shows the syntax for MongoDB 2.2 and later.
   For syntax for versions prior to 2.2, see :method:`~db.collection.update()`.

.. _crud-update-save:

Modify a Document with ``save()`` Method
----------------------------------------

The :method:`~db.collection.save()` method can replace an existing
document. To replace a document with the
:method:`~db.collection.save()` method, pass the method a document with
an ``_id`` field that matches an existing document.

The following example completely replaces the document with the ``_id``
equal to ``10`` in the ``inventory`` collection:

.. code-block:: javascript

   db.inventory.save(
      {
        _id: 10,
        type: "misc",
        item: "placard"
      }
   )

For further examples, see :method:`~db.collection.save()`.
=================================
Monitor MongoDB Windows with SNMP
=================================

.. default-domain:: mongodb

.. versionadded:: 2.6

.. admonition:: Enterprise Feature

   SNMP is only available in `MongoDB Enterprise
   <http://www.mongodb.com/products/mongodb-enterprise>`_.

.. |mongod-program| replace:: :program:`mongod.exe`

.. |copy-command| replace:: copy mongod.conf.master C:\snmp\etc\config\mongod.conf

Overview
--------

MongoDB Enterprise can report system information into SNMP traps, to
support centralized data collection and aggregation. This procedure
explains the setup and configuration of a |mongod-program| instance
as an SNMP subagent, as well as initializing and testing of SNMP
support with MongoDB Enterprise.

.. seealso:: :doc:`/tutorial/monitor-with-snmp` and
   :doc:`/tutorial/troubleshoot-snmp` for more information.

Considerations
--------------

Only :program:`mongod.exe` instances provide SNMP
support. :program:`mongos.exe` and the other MongoDB binaries do not
support SNMP.

Configuration Files
-------------------

.. include:: /includes/fact-snmp-configuration-files.rst

Procedure
---------

.. include:: /includes/steps/monitor-with-snmp-windows.rst

Optional: Run MongoDB as SNMP Master
------------------------------------

.. include:: /includes/fact-snmp-run-mongodb-as-snmp-master.rst

.. code-block:: powershell

   copy mongod.conf.master C:\snmp\etc\config\mongod.conf

Additionally, start |mongod-program| with the :setting:`snmp-master`
option, as in the following:

.. code-block:: powershell

   mongod.exe --snmp-master
==================================
Monitor MongoDB With SNMP on Linux
==================================

.. default-domain:: mongodb

.. versionadded:: 2.2

.. admonition:: Enterprise Feature

   SNMP is only available in `MongoDB Enterprise
   <http://www.mongodb.com/products/mongodb-enterprise>`_.

.. |mongod-program| replace:: :program:`mongod`

Overview
--------

MongoDB Enterprise can report system information into SNMP traps, to
support centralized data collection and aggregation. This procedure
explains the setup and configuration of a |mongod-program| instance
as an SNMP subagent, as well as initializing and testing of SNMP
support with MongoDB Enterprise.

.. seealso:: :doc:`/tutorial/troubleshoot-snmp` and
   :doc:`/tutorial/monitor-with-snmp-on-windows` for
   complete instructions on using MongoDB with SNMP on Windows
   systems.

Considerations
--------------

Only :program:`mongod` instances provide SNMP
support. :program:`mongos` and the other MongoDB binaries do not
support SNMP.

Configuration Files
-------------------

.. include:: /includes/fact-snmp-configuration-files.rst

Procedure
---------

.. include:: /includes/steps/monitor-with-snmp.rst

Optional: Run MongoDB as SNMP Master
------------------------------------

.. include:: /includes/fact-snmp-run-mongodb-as-snmp-master.rst

.. code-block:: sh

   cp mongod.conf.master /etc/snmp/mongod.conf

Additionally, start |mongod-program| with the :setting:`snmp-master`
option, as in the following:

.. code-block:: sh

   mongod --snmp-master
==========================
Optimize Query Performance
==========================

.. default-domain:: mongodb

Create Indexes to Support Queries
---------------------------------

For commonly issued queries, create :doc:`indexes </indexes>`. If a
query searches multiple fields, create a :ref:`compound index
<index-type-compound>`. Scanning an index is much faster than scanning a
collection. The indexes structures are smaller than the documents
reference, and store references in order.

.. example:: If you have a ``posts`` collection containing blog posts,
   and if you regularly issue a query that sorts on the ``author_name``
   field, then you can optimize the query by creating an index on the
   ``author_name`` field:

   .. code-block:: javascript

      db.posts.ensureIndex( { author_name : 1 } )

Indexes also improve efficiency on queries that routinely sort on a
given field.

.. example:: If you regularly issue a query that sorts on the
   ``timestamp`` field, then you can optimize the query by creating an
   index on the ``timestamp`` field:

   Creating this index:

   .. code-block:: javascript

      db.posts.ensureIndex( { timestamp : 1 } )

   Optimizes this query:

   .. code-block:: javascript

      db.posts.find().sort( { timestamp : -1 } )

Because MongoDB can read indexes in both ascending and descending
order, the direction of a single-key index does not matter.

Indexes support queries, update operations, and some phases of the
:ref:`aggregation pipeline
<aggregation-pipeline-operators-and-performance>`.

.. include:: /includes/fact-bindata-storage-optimization.rst


Limit the Number of Query Results to Reduce Network Demand
----------------------------------------------------------

MongoDB :term:`cursors <cursor>` return results in groups of multiple
documents. If you know the number of results you want, you can reduce
the demand on network resources by issuing the :method:`~cursor.limit()`
method.

This is typically used in conjunction with sort operations. For example,
if you need only 10 results from your query to the ``posts``
collection, you would issue the following command:

.. code-block:: javascript

   db.posts.find().sort( { timestamp : -1 } ).limit(10)

For more information on limiting results, see :method:`~cursor.limit()`

Use Projections to Return Only Necessary Data
---------------------------------------------

When you need only a subset of fields from documents, you can achieve better
performance by returning only the fields you need:

For example, if in your query to the ``posts`` collection, you need only
the ``timestamp``, ``title``, ``author``, and ``abstract`` fields, you
would issue the following command:

.. code-block:: javascript

   db.posts.find( {}, { timestamp : 1 , title : 1 , author : 1 , abstract : 1} ).sort( { timestamp : -1 } )

For more information on using projections, see
:ref:`read-operations-projection`.

Use ``$hint`` to Select a Particular Index
------------------------------------------

In most cases the :ref:`query optimizer
<read-operations-query-optimization>` selects the optimal index for a
specific operation; however, you can force MongoDB to use a specific
index using the :method:`~cursor.hint()` method. Use
:method:`~cursor.hint()` to support performance testing, or on
some queries where you must select a field or field included in
several indexes.

Use the Increment Operator to Perform Operations Server-Side
------------------------------------------------------------

Use MongoDB's :update:`$inc` operator to increment or decrement
values in documents. The operator increments the value of the field on
the server side, as an alternative to selecting a document, making
simple modifications in the client and then writing the entire
document to the server.  The :update:`$inc` operator can also help
avoid race conditions, which would result when two application
instances queried for a document, manually incremented a field, and
saved the entire document back at the same time.
==============================
Perform Incremental Map-Reduce
==============================

.. default-domain:: mongodb

Map-reduce operations can handle complex aggregation tasks. To perform
map-reduce operations, MongoDB provides the :dbcommand:`mapReduce`
command and, in the :program:`mongo` shell, the
:method:`db.collection.mapReduce()` wrapper method.

If the map-reduce data set is constantly growing, you may want to
perform an incremental map-reduce rather than
performing the map-reduce operation over the entire data set each time.

To perform incremental map-reduce:

#. Run a map-reduce job over the current collection and output the
   result to a separate collection.

#. When you have more data to process, run subsequent map-reduce job
   with:

   - the ``query`` parameter that specifies conditions that match
     *only* the new documents.

   - the ``out`` parameter that specifies the ``reduce`` action to
     merge the new results into the existing output collection.

Consider the following example where you schedule a map-reduce
operation on a ``sessions`` collection to run at the end of each day.

Data Setup
----------

The ``sessions`` collection contains documents that log users' sessions
each day, for example:

.. code-block:: javascript

   db.sessions.save( { userid: "a", ts: ISODate('2011-11-03 14:17:00'), length: 95 } );
   db.sessions.save( { userid: "b", ts: ISODate('2011-11-03 14:23:00'), length: 110 } );
   db.sessions.save( { userid: "c", ts: ISODate('2011-11-03 15:02:00'), length: 120 } );
   db.sessions.save( { userid: "d", ts: ISODate('2011-11-03 16:45:00'), length: 45 } );

   db.sessions.save( { userid: "a", ts: ISODate('2011-11-04 11:05:00'), length: 105 } );
   db.sessions.save( { userid: "b", ts: ISODate('2011-11-04 13:14:00'), length: 120 } );
   db.sessions.save( { userid: "c", ts: ISODate('2011-11-04 17:00:00'), length: 130 } );
   db.sessions.save( { userid: "d", ts: ISODate('2011-11-04 15:37:00'), length: 65 } );

Initial Map-Reduce of Current Collection
----------------------------------------

Run the first map-reduce operation as follows:

#. Define the map function that maps the ``userid`` to an
   object that contains the fields ``userid``, ``total_time``, ``count``,
   and ``avg_time``:

   .. code-block:: javascript

      var mapFunction = function() {
                            var key = this.userid;
                            var value = {
                                          userid: this.userid,
                                          total_time: this.length,
                                          count: 1,
                                          avg_time: 0
                                         };

                            emit( key, value );
                        };

#. Define the corresponding reduce function with two arguments
   ``key`` and ``values`` to calculate the total time and the count.
   The ``key`` corresponds to the ``userid``, and the ``values`` is an
   array whose elements corresponds to the individual objects mapped to the
   ``userid`` in the ``mapFunction``.

   .. code-block:: javascript

      var reduceFunction = function(key, values) {

                              var reducedObject = {
                                                    userid: key,
                                                    total_time: 0,
                                                    count:0,
                                                    avg_time:0
                                                  };

                              values.forEach( function(value) {
                                                    reducedObject.total_time += value.total_time;
                                                    reducedObject.count += value.count;
                                              }
                                            );
                              return reducedObject;
                           };

#. Define the finalize function with two arguments ``key`` and
   ``reducedValue``. The function modifies the ``reducedValue`` document
   to add another field ``average`` and returns the modified document.

   .. code-block:: javascript

      var finalizeFunction = function (key, reducedValue) {

                                if (reducedValue.count > 0)
                                    reducedValue.avg_time = reducedValue.total_time / reducedValue.count;

                                return reducedValue;
                             };

#. Perform map-reduce on the ``session`` collection using the
   ``mapFunction``, the ``reduceFunction``, and the
   ``finalizeFunction`` functions. Output the results to a collection
   ``session_stat``. If the ``session_stat`` collection already exists,
   the operation will replace the contents:

   .. code-block:: javascript

      db.sessions.mapReduce( mapFunction,
                             reduceFunction,
                             {
                               out: { reduce: "session_stat" },
                               finalize: finalizeFunction
                             }
                           )

Subsequent Incremental Map-Reduce
---------------------------------

Later, as the ``sessions`` collection grows, you can run additional
map-reduce operations. For example, add new documents to the
``sessions`` collection:

.. code-block:: javascript

   db.sessions.save( { userid: "a", ts: ISODate('2011-11-05 14:17:00'), length: 100 } );
   db.sessions.save( { userid: "b", ts: ISODate('2011-11-05 14:23:00'), length: 115 } );
   db.sessions.save( { userid: "c", ts: ISODate('2011-11-05 15:02:00'), length: 125 } );
   db.sessions.save( { userid: "d", ts: ISODate('2011-11-05 16:45:00'), length: 55 } );

At the end of the day, perform incremental map-reduce on the
``sessions`` collection, but use the ``query`` field to select only the
new documents. Output the results to the collection ``session_stat``,
but ``reduce`` the contents with the results of the incremental
map-reduce:

.. code-block:: javascript

   db.sessions.mapReduce( mapFunction,
                          reduceFunction,
                          {
                            query: { ts: { $gt: ISODate('2011-11-05 00:00:00') } },
                            out: { reduce: "session_stat" },
                            finalize: finalizeFunction
                          }
                        );
==========================================
Perform Maintenance on Replica Set Members
==========================================

.. default-domain:: mongodb

Overview
--------

:term:`Replica sets <replica set>` allow a MongoDB deployment to
remain available during the majority of a maintenance window.

This document outlines the basic procedure for performing maintenance on
each of the members of a replica set. Furthermore, this particular
sequence strives to minimize the amount of time that the
:term:`primary` is unavailable and controlling the impact on the
entire deployment.

Use these steps as the basis for common replica set operations,
particularly for procedures such as :doc:`upgrading to the latest
version of MongoDB </tutorial/upgrade-revision>` and :doc:`changing
the size of the oplog</tutorial/change-oplog-size>`.

Procedure
---------

For each member of a replica set, starting with a secondary member,
perform the following sequence of events, ending with the primary:

- Restart the :program:`mongod` instance as a standalone.

- Perform the task on the standalone instance.

- Restart the :program:`mongod` instance as a member of the replica
  set.

.. include:: /includes/steps/perform-maintenance-task-on-replica-set-members.rst
=========================
Perform Two Phase Commits
=========================

.. default-domain:: mongodb

Synopsis
--------

This document provides a pattern for doing multi-document updates or
"transactions" using a two-phase commit approach for writing data to
multiple documents. Additionally, you can extend this process to
provide a :ref:`rollback <2-phase-commits-rollback>` like
functionality.

Background
----------

Operations on a single :term:`document` are always atomic with MongoDB
databases; however, operations that involve multiple documents, which
are often referred to as "transactions," are not atomic. Since
documents can be fairly complex and contain multiple "nested"
documents, single-document atomicity provides necessary support for
many practical use cases.

Thus, without precautions, success or failure of the database
operation cannot be "all or nothing," and without support for
multi-document transactions it's possible for an operation to succeed
for some operations and fail with others. When executing a transaction
composed of several sequential operations the following issues arise:

- Atomicity: if one operation fails, the previous operation within the
  transaction must "rollback" to the previous state (i.e. the
  "nothing," in "all or nothing.")

- Isolation: operations that run concurrently with the transaction
  operation set must "see" a consistent view of the data throughout
  the transaction process.

- Consistency: if a major failure (i.e. network, hardware) interrupts
  the transaction, the database must be able to recover a consistent
  state.

Despite the power of single-document atomic operations, there are
cases that require multi-document transactions. For these situations,
you can use a two-phase commit, to provide support for these kinds of
multi-document updates.

Because documents can represent both pending data and states, you can
use a two-phase commit to ensure that data is consistent, and that in
the case of an error, the state that preceded the transaction is
:ref:`recoverable <2-phase-commits-rollback>`.

.. note::

   Because only single-document operations are atomic with MongoDB,
   two-phase commits can only offer transaction-*like* semantics. It's
   possible for applications to return intermediate data at
   intermediate points during the two-phase commit or rollback.

Pattern
-------

Overview
~~~~~~~~

The most common example of transaction is to transfer funds from
account A to B in a reliable way, and this pattern uses this operation
as an example. In a relational database system, this operation would
encapsulate subtracting funds from the source (``A``) account and
adding them to the destination (``B``) within a single atomic
transaction. For MongoDB, you can use a two-phase commit in these
situations to achieve a compatible response.

All of the examples in this document use the :program:`mongo` shell to
interact with the database, and assume that you have two collections:
First, a collection named ``accounts`` that will store data about
accounts with one account per document, and a collection named
``transactions`` which will store the transactions themselves.

Begin by creating two accounts named ``A`` and ``B``, with the
following command:

.. code-block:: javascript

   db.accounts.save({name: "A", balance: 1000, pendingTransactions: []})
   db.accounts.save({name: "B", balance: 1000, pendingTransactions: []})

To verify that these operations succeeded, use :method:`~db.collection.find()`:

.. code-block:: javascript

   db.accounts.find()

:program:`mongo` will return two :term:`documents <document>` that
resemble the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc66cb8a04f512696151f"), "name" : "A", "balance" : 1000, "pendingTransactions" : [ ] }
   { "_id" : ObjectId("4d7bc67bb8a04f5126961520"), "name" : "B", "balance" : 1000, "pendingTransactions" : [ ] }

Transaction Description
~~~~~~~~~~~~~~~~~~~~~~~

.. _2-phase-commits-step-1:

Set Transaction State to Initial
````````````````````````````````

Create the ``transaction`` collection by inserting the following
document. The transaction document holds the ``source`` and
``destination``, which refer to the ``name`` fields of the
``accounts`` collection, as well as the ``value`` field that
represents the amount of data change to the ``balance``
field. Finally, the ``state`` field reflects the current state of the
transaction.

.. code-block:: javascript

   db.transactions.save({source: "A", destination: "B", value: 100, state: "initial"})

To verify that these operations succeeded, use :method:`~db.collection.find()`:

.. code-block:: javascript

   db.transactions.find()

This will return a document similar to the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc7a8b8a04f5126961522"), "source" : "A", "destination" : "B", "value" : 100, "state" : "initial" }

.. _2-phase-commits-step-2:

Switch Transaction State to Pending
```````````````````````````````````

Before modifying either records in the ``accounts`` collection, set
the transaction state to ``pending`` from ``initial``.

Set the local variable ``t`` in your shell session, to the transaction
document using :method:`~db.collection.findOne()`:

.. code-block:: javascript

   t = db.transactions.findOne({state: "initial"})

After assigning this variable ``t``, the shell will return the value
of ``t``, you will see the following output:

.. code-block:: javascript

   {
        "_id" : ObjectId("4d7bc7a8b8a04f5126961522"),
        "source" : "A",
        "destination" : "B",
        "value" : 100,
        "state" : "initial"
   }

Use :method:`~db.collection.update()` to change the value of
``state`` to ``pending``:

.. code-block:: javascript

   db.transactions.update({_id: t._id}, {$set: {state: "pending"}})
   db.transactions.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``transactions`` collection, which should resemble the
following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc7a8b8a04f5126961522"), "source" : "A", "destination" : "B", "value" : 100, "state" : "pending" }

.. _2-phase-commits-step-3:

Apply Transaction to Both Accounts
``````````````````````````````````

Continue by applying the transaction to both accounts. The
:method:`~db.collection.update()` query will prevent you from
applying the transaction *if* the transaction is *not* already
marked as pending. Use the following :method:`~db.collection.update()`
operation:

.. code-block:: javascript

   db.accounts.update({name: t.source, pendingTransactions: {$ne: t._id}}, {$inc: {balance: -t.value}, $push: {pendingTransactions: t._id}})
   db.accounts.update({name: t.destination, pendingTransactions: {$ne: t._id}}, {$inc: {balance: t.value}, $push: {pendingTransactions: t._id}})
   db.accounts.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``accounts`` collection, which should now resemble the
following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc97fb8a04f5126961523"), "balance" : 900, "name" : "A", "pendingTransactions" : [ ObjectId("4d7bc7a8b8a04f5126961522") ] }
   { "_id" : ObjectId("4d7bc984b8a04f5126961524"), "balance" : 1100, "name" : "B", "pendingTransactions" : [ ObjectId("4d7bc7a8b8a04f5126961522") ] }

.. _2-phase-commits-step-4:

Set Transaction State to Committed
``````````````````````````````````

Use the following :method:`~db.collection.update()` operation
to set the transaction's state to ``committed``:

.. code-block:: javascript

   db.transactions.update({_id: t._id}, {$set: {state: "committed"}})
   db.transactions.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``transactions`` collection, which should now resemble
the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc7a8b8a04f5126961522"), "destination" : "B", "source" : "A", "state" : "committed", "value" : 100 }

.. _2-phase-commits-step-5:

Remove Pending Transaction
``````````````````````````

Use the following :method:`~db.collection.update()` operation
to set remove the pending transaction from the :term:`documents
<document>` in the ``accounts`` collection:

.. code-block:: javascript

   db.accounts.update({name: t.source}, {$pull: {pendingTransactions: t._id}})
   db.accounts.update({name: t.destination}, {$pull: {pendingTransactions: t._id}})
   db.accounts.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``accounts`` collection, which should now resemble
the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc97fb8a04f5126961523"), "balance" : 900, "name" : "A", "pendingTransactions" : [ ] }
   { "_id" : ObjectId("4d7bc984b8a04f5126961524"), "balance" : 1100, "name" : "B", "pendingTransactions" : [ ] }

.. _2-phase-commits-step-6:

Set Transaction State to Done
`````````````````````````````

Complete the transaction by setting the ``state`` of the transaction
:term:`document` to ``done``:

.. code-block:: javascript

   db.transactions.update({_id: t._id}, {$set: {state: "done"}})
   db.transactions.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``transactions`` collection, which should now resemble
the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc7a8b8a04f5126961522"), "destination" : "B", "source" : "A", "state" : "done", "value" : 100 }

.. _2-phase-commits-recovery:

Recovering from Failure Scenarios
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The most important part of the transaction procedure is not the
prototypical example above, but rather the possibility for recovering
from the various failure scenarios when transactions do not complete
as intended. This section will provide an overview of possible
failures and provide methods to recover from these kinds of events.

There are two classes of failures:

- all failures that occur after the first step (i.e. :ref:`setting
  the transaction set to initial <2-phase-commits-step-1>`) but
  before the third step (i.e. :ref:`applying the transaction to both
  accounts <2-phase-commits-step-3>`.)

  To recover, applications should get a list of transactions in the
  ``pending`` state and resume from the second step
  (i.e. :ref:`switching the transaction state to pending
  <2-phase-commits-step-2>`.)

- all failures that occur after the third step (i.e. :ref:`applying
  the transaction to both accounts <2-phase-commits-step-3>`) but
  before the fifth step (i.e. :ref:`setting the transaction state to
  done <2-phase-commits-step-5>`.)

  To recover, application should get a list of transactions in the
  ``committed`` state and resume from the fourth step
  (i.e. :ref:`remove the pending transaction
  <2-phase-commits-step-5>`.)

Thus, the application will always be able to resume the transaction
and eventually arrive at a consistent state. Run the following
recovery operations every time the application starts to catch any
unfinished transactions. You may also wish run the recovery operation
at regular intervals to ensure that your data remains in a consistent state.

The time required to reach a consistent state depends on how long the
application needs to recover each transaction.

.. _2-phase-commits-rollback:

Rollback
````````

In some cases you may need to "rollback" or undo a transaction when
the application needs to "cancel" the transaction, or because it can
never recover as in cases where one of the accounts doesn't exist, or
stops existing during the transaction.

There are two possible rollback operations:

#. After you :ref:`apply the transaction <2-phase-commits-step-3>`
   (i.e. the third step), you have fully committed the transaction and
   you should not roll back the transaction. Instead, create a new
   transaction and switch the values in the source and destination
   fields.

#. After you :ref:`create the transaction <2-phase-commits-step-1>`
   (i.e. the first step), but before you :ref:`apply the transaction
   <2-phase-commits-step-3>` (i.e the third step), use the following
   process:

.. _2-phase-commits-rollback-step-1:

Set Transaction State to Canceling
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Begin by setting the transaction's state to ``canceling`` using the
following :method:`~db.collection.update()` operation:

.. code-block:: javascript

   db.transactions.update({_id: t._id}, {$set: {state: "canceling"}})

.. _2-phase-commits-rollback-step-2:

Undo the Transaction
^^^^^^^^^^^^^^^^^^^^

Use the following sequence of operations to undo the transaction
operation from both accounts:

.. code-block:: javascript

   db.accounts.update({name: t.source, pendingTransactions: t._id}, {$inc: {balance: t.value}, $pull: {pendingTransactions: t._id}})
   db.accounts.update({name: t.destination, pendingTransactions: t._id}, {$inc: {balance: -t.value}, $pull: {pendingTransactions: t._id}})
   db.accounts.find()

The :method:`~db.collection.find()` operation will return the
contents of the ``accounts`` collection, which should resemble the
following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7bc97fb8a04f5126961523"), "balance" : 1000, "name" : "A", "pendingTransactions" : [ ] }
   { "_id" : ObjectId("4d7bc984b8a04f5126961524"), "balance" : 1000, "name" : "B", "pendingTransactions" : [ ] }

.. _2-phase-commits-rollback-step-3:

Set Transaction State to Canceled
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Finally, use the following :method:`~db.collection.update()` operation
to set the transaction's state to ``canceled``:

.. code-block:: javascript

   db.transactions.update({_id: t._id}, {$set: {state: "canceled"}})

.. _2-phase-commits-concurrency:

Multiple Applications
`````````````````````

Transactions exist, in part, so that several applications can create
and run operations concurrently without causing data inconsistency or
conflicts. As a result, it is crucial that only one 1 application can
handle a given transaction at any point in time.

Consider the following example, with a single transaction
(i.e. ``T1``) and two applications (i.e. ``A1`` and ``A2``). If both
applications begin processing the transaction which is still in the
``initial`` state (i.e. :ref:`step 1 <2-phase-commits-step-1>`), then:

- ``A1`` can apply the entire whole transaction before ``A2`` starts.

- ``A2`` will then apply ``T1`` for the second time, because the
  transaction does not appear as pending in the ``accounts``
  documents.

To handle multiple applications, create a marker in the transaction
document itself to identify the application that is handling the
transaction. Use :method:`~db.collection.findAndModify()`
method to modify the transaction:

.. code-block:: javascript

   t = db.transactions.findAndModify({query: {state: "initial", application: {$exists: 0}},
                                      update: {$set: {state: "pending", application: "A1"}},
                                      new: true})

When you modify and reassign the local shell variable ``t``, the
:program:`mongo` shell will return the ``t`` object, which should
resemble the following:

.. code-block:: javascript

   {
        "_id" : ObjectId("4d7be8af2c10315c0847fc85"),
        "application" : "A1",
        "destination" : "B",
        "source" : "A",
        "state" : "pending",
        "value" : 150
   }

Amend the transaction operations to ensure that only applications
that match the identifier in the value of the ``application`` field
before applying the transaction.

If the application ``A1`` fails during transaction execution, you can
use the :ref:`recovery procedures <2-phase-commits-recovery>`, but
applications should ensure that they "own" the transaction before
applying the transaction. For example to resume pending jobs, use a
query that resembles the following:

.. code-block:: javascript

   db.transactions.find({application: "A1", state: "pending"})

This will (or may) return a document from the ``transactions``
document that resembles the following:

.. code-block:: javascript

   { "_id" : ObjectId("4d7be8af2c10315c0847fc85"), "application" : "A1", "destination" : "B", "source" : "A", "state" : "pending", "value" : 150 }

.. _2-phase-commits-in-production:

Using Two-Phase Commits in Production Applications
--------------------------------------------------

The example transaction above is intentionally simple. For example, it
assumes that:

- it is always possible to roll back operations an account.

- account balances can hold negative values.

Production implementations would likely be more complex. Typically
accounts need information about current balance, pending credits,
pending debits. Then:

- when your application :ref:`switches the transaction state to
  pending <2-phase-commits-step-2>` (i.e. step 2) it would also make
  sure that the account has sufficient funds for the
  transaction. During this update operation, the application would
  also modify the values of the credits and debits as well as adding
  the transaction as pending.

- when your application :ref:`removes the pending transaction
  <2-phase-commits-step-4>` (i.e. step 4) the application would apply
  the transaction on balance, modify the credits and debits as well as
  removing the transaction from the ``pending`` field., all in one update.

Because all of the changes in the above two operations occur within a
single :method:`~db.collection.update()` operation, these
changes are all atomic.

Additionally, for most important transactions, ensure that:

- the database interface (i.e. client library or :term:`driver`) has a
  reasonable :term:`write concern` configured to ensure that
  operations return a response on the success or failure of a write
  operation.

- your :program:`mongod` instance has :term:`journaling <journal>`
  enabled to ensure that your data is always in a recoverable state,
  in the event of an unclean :program:`mongod` shutdown.
.. _read-operations-projection:
.. _projection:

===================================
Limit Fields to Return from a Query
===================================

.. default-domain:: mongodb

The :term:`projection` specification limits the fields to return for
all matching documents. The projection takes the form of a
:term:`document` with a list of fields for inclusion or exclusion from
the result set. You can either specify the fields to include (e.g. ``{
field: 1 }``) or specify the fields to exclude (e.g. ``{ field: 0 }``).

.. important:: The ``_id`` field is, by default, included in the result
   set. To exclude the ``_id`` field from the result set, you need to
   specify in the projection document the exclusion of the ``_id``
   field (i.e. ``{ _id: 0 }``).

You cannot combine inclusion and exclusion semantics in a single
projection with the *exception* of the ``_id`` field.

This tutorial offers various query examples that limit the fields to
return for all matching documents. The examples in this tutorial use a
collection ``inventory`` and use the :method:`db.collection.find()`
method in the :program:`mongo` shell. The
:method:`db.collection.find()` method returns a :doc:`cursor
</core/cursors>` to the retrieved documents. For examples on query
selection criteria, see :doc:`/tutorial/query-documents`.

Return All Fields in Matching Documents
---------------------------------------

If you specify no projection, the :method:`find()
<db.collection.find()>` method returns all fields of all documents that
match the query.

.. code-block:: javascript

   db.inventory.find( { type: 'food' } )

This operation will return all documents in the ``inventory``
collection where the value of the ``type`` field is ``'food'``. The
returned documents contain all its fields.

Return the Specified Fields and the ``_id`` Field Only
------------------------------------------------------

A projection can explicitly include several fields. In the following
operation, :method:`find() <db.collection.find()>` method returns all
documents that match the query. In the result set, only the ``item``
and ``qty`` fields and, by default, the ``_id`` field return in the
matching documents.

.. code-block:: javascript

   db.inventory.find( { type: 'food' }, { item: 1, qty: 1 } )

Return Specified Fields Only
----------------------------

You can remove the ``_id`` field from the results by specifying its
exclusion in the projection, as in the following example:

.. code-block:: javascript

   db.inventory.find( { type: 'food' }, { item: 1, qty: 1, _id:0 } )

This operation returns all documents that match the query. In the
result set, *only* the ``item`` and ``qty`` fields return in the
matching documents.

Return All But the Excluded Field
---------------------------------

To exclude a single field or group of fields you can use a projection
in the following form:

.. code-block:: javascript

   db.inventory.find( { type: 'food' }, { type:0 } )

This operation returns all documents where the value of the ``type``
field is ``food``. In the result set, the ``type`` field does not
return in the matching documents.

With the exception of the ``_id`` field you cannot combine inclusion
and exclusion statements in projection documents.

Projection for Array Fields
---------------------------

The :projection:`$elemMatch` and :projection:`$slice` projection
operators are the *only* way to project *portions* of an array.

.. tip:: MongoDB does not support projections of portions of arrays
   *except* when using the :projection:`$elemMatch` and :projection:`$slice`
   projection operators.
====================
Query a ``2d`` Index
====================

.. default-domain:: mongodb

The following sections describe queries supported by the ``2d`` index.
For an overview of recommended geospatial queries, see
:ref:`geospatial-query-compatibility-chart`.

Points within a Shape Defined on a Flat Surface
-----------------------------------------------

To select all legacy coordinate pairs found within a given shape on a flat
surface, use the :query:`$geoWithin` operator along with a shape
operator. Use the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $geoWithin :
                               { $box|$polygon|$center : <coordinates>
                         } } } )

The following queries for documents within a rectangle defined by ``[ 0
, 0 ]`` at the bottom left corner and by ``[ 100 , 100 ]`` at the top
right corner.

.. code-block:: javascript

   db.places.find( { loc :
                     { $geoWithin :
                        { $box : [ [ 0 , 0 ] ,
                                   [ 100 , 100 ] ]
                    } } } )

The following queries for documents that are within the circle centered
on ``[ -74 , 40.74 ]`` and with a radius of ``10``:

.. code-block:: javascript

   db.places.find( { loc: { $geoWithin :
                             { $center : [ [-74, 40.74 ] , 10 ]
                   } } } )

For syntax and examples for each shape, see the following:

- :query:`$box`

- :query:`$polygon`

- :query:`$center` (defines a circle)

Points within a Circle Defined on a Sphere
------------------------------------------

MongoDB supports rudimentary spherical queries on flat ``2d`` indexes for
legacy reasons. In general, spherical calculations should use a ``2dsphere``
index, as described in :doc:`/core/2dsphere`.

To query for legacy coordinate pairs in a "spherical cap" on a sphere,
use :query:`$geoWithin` with the :query:`$centerSphere` operator.
Specify an array that contains:

- The grid coordinates of the circle's center point

- The circle's radius measured in radians. To calculate radians, see
  :doc:`/tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes`.

Use the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $geoWithin :
                               { $centerSphere : [ [ <x>, <y> ] , <radius> ] }
                         } } )

The following example query returns all documents within a 10-mile
radius of longitude ``88 W`` and latitude ``30 N``. The example converts
distance to radians by dividing distance by the approximate radius of
the earth, 3959 miles:

.. code-block:: javascript

   db.<collection>.find( { loc : { $geoWithin :
                                    { $centerSphere :
                                       [ [ 88 , 30 ] , 10 / 3959 ]
                         } } } )

Proximity to a Point on a Flat Surface
--------------------------------------

Proximity queries return the 100 legacy coordinate pairs closest to the
defined point and sort the results by distance. Use either the
:query:`$near` operator or :dbcommand:`geoNear` command. Both require
a ``2d`` index.

The :query:`$near` operator uses the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $near : [ <x> , <y> ]
                         } } )

For examples, see :query:`$near`.

The :dbcommand:`geoNear` command uses the following syntax:

.. code-block:: javascript

   db.runCommand( { geoNear: <collection>, near: [ <x> , <y> ] } )

The :dbcommand:`geoNear` command offers more options and returns more
information than does the :query:`$near` operator. To run the
command, see :dbcommand:`geoNear`.

.. index:: geospatial queries
.. index:: geospatial queries; exact
.. _geospatial-indexes-exact-match:

Exact Matches on a Flat Surface
-------------------------------

You can use the :method:`db.collection.find()` method to query for an
exact match on a location. These queries use the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field>: [ <x> , <y> ] } )

This query will return any documents with the value of ``[ <x> , <y> ]``.
.. _geospatial-indexes-query-2dsphere:

==========================
Query a ``2dsphere`` Index
==========================

.. default-domain:: mongodb

The following sections describe queries supported by the ``2dsphere`` index.
For an overview of recommended geospatial queries, see
:ref:`geospatial-query-compatibility-chart`.

GeoJSON Objects Bounded by a Polygon
------------------------------------

The :query:`$geoWithin` operator queries for location data found
within a GeoJSON polygon. Your location
data must be stored in GeoJSON format. Use the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $geoWithin :
                              { $geometry :
                                { type : "Polygon" ,
                                  coordinates : [ <coordinates> ]
                         } } } } )

The following example selects all points and shapes that
exist entirely within a GeoJSON polygon:

.. code-block:: javascript

   db.places.find( { loc :
                     { $geoWithin :
                       { $geometry :
                         { type : "Polygon" ,
                           coordinates : [ [
                                             [ 0 , 0 ] ,
                                             [ 3 , 6 ] ,
                                             [ 6 , 1 ] ,
                                             [ 0 , 0 ]
                                           ] ]
                   } } } } )

Intersections of GeoJSON Objects
--------------------------------

.. versionadded:: 2.4

The :query:`$geoIntersects` operator queries for locations that
intersect a specified GeoJSON object. A location intersects the object
if the intersection is non-empty. This includes documents that have a
shared edge.

The :query:`$geoIntersects` operator uses the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $geoIntersects :
                              { $geometry :
                                { type : "<GeoJSON object type>" ,
                                  coordinates : [ <coordinates> ]
                         } } } } )

The following example uses :query:`$geoIntersects` to select all
indexed points and shapes that intersect with the polygon defined by the
``coordinates`` array.

.. code-block:: javascript

   db.places.find( { loc :
                     { $geoIntersects :
                       { $geometry :
                         { type : "Polygon" ,
                           coordinates: [ [
                                            [ 0 , 0 ] ,
                                            [ 3 , 6 ] ,
                                            [ 6 , 1 ] ,
                                            [ 0 , 0 ]
                                          ] ]
                   } } } } )

Proximity to a GeoJSON Point
----------------------------

Proximity queries return the points closest to the defined point and
sorts the results by distance. A proximity query on GeoJSON data
requires a ``2dsphere`` index.

To query for proximity to a GeoJSON point, use either the
:query:`$near` operator or :dbcommand:`geoNear` command. Distance
is in meters.

The :query:`$near` uses the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $near :
                              { $geometry :
                                 { type : "Point" ,
                                   coordinates : [ <longitude> , <latitude> ] } ,
                                $maxDistance : <distance in meters>
                         } } } )

For examples, see :query:`$near`.

The :dbcommand:`geoNear` command uses the following syntax:

.. code-block:: javascript

   db.runCommand( { geoNear : <collection> ,
                 near : { type : "Point" ,
                          coordinates: [ <longitude>, <latitude> ] } ,
                 spherical : true } )

The :dbcommand:`geoNear` command offers more options and returns more
information than does the :query:`$near` operator. To run the
command, see :dbcommand:`geoNear`.

Points within a Circle Defined on a Sphere
------------------------------------------

To select all grid coordinates in a "spherical cap" on a sphere, use
:query:`$geoWithin` with the :query:`$centerSphere` operator.
Specify an array that contains:

- The grid coordinates of the circle's center point

- The circle's radius measured in radians. To calculate radians, see
  :doc:`/tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes`.

Use the following syntax:

.. code-block:: javascript

   db.<collection>.find( { <location field> :
                            { $geoWithin :
                              { $centerSphere :
                                 [ [ <x>, <y> ] , <radius> ] }
                         } } )

The following example queries grid coordinates and returns all
documents within a 10 mile radius of longitude ``88 W`` and latitude
``30 N``. The example converts the distance, 10 miles, to radians by
dividing by the approximate radius of the earth, 3959 miles:

.. code-block:: javascript

   db.places.find( { loc :
                     { $geoWithin :
                       { $centerSphere :
                          [ [ -88 , 30 ] , 10 / 3959 ]
                   } } } )
.. _geospatial-indexes-haystack-queries:

======================
Query a Haystack Index
======================

.. default-domain:: mongodb

A haystack index is a special ``2d`` geospatial index that is optimized
to return results over small areas. To create a haystack index see
:ref:`geospatial-indexes-haystack-index`.

To query a haystack index, use the :dbcommand:`geoSearch` command. You
must specify both the coordinates and the additional
field to :dbcommand:`geoSearch`. For example, to return all documents
with the value ``restaurant`` in the ``type`` field near the example
point, the command would resemble:

.. code-block:: javascript

   db.runCommand( { geoSearch : "places" ,
                    search : { type: "restaurant" } ,
                    near : [-74, 40.74] ,
                    maxDistance : 10 } )

.. note::

   Haystack indexes are not suited to queries for the complete list of
   documents closest to a particular location. The closest documents
   could be more distant compared to the bucket size.

.. note::

   :doc:`Spherical query operations
   </tutorial/calculate-distances-using-spherical-geometry-with-2d-geospatial-indexes>`
   are not currently supported by haystack indexes.

   The :method:`find() <db.collection.find()>` method and
   :dbcommand:`geoNear` command cannot access the haystack index.
.. _read-operations-query-document:
.. _read-operations-query-argument:

===============
Query Documents
===============

.. default-domain:: mongodb

In MongoDB, the :method:`db.collection.find()` method retrieves
documents from a collection. [#findOne]_ The
:method:`db.collection.find()` method returns a :doc:`cursor
</core/cursors>` to the retrieved documents.

This tutorial provides examples of read operations using the
:method:`db.collection.find()` method in the :program:`mongo` shell. In
these examples, the retrieved documents contain all their fields. To
restrict the fields to return in the retrieved documents, see
:doc:`/tutorial/project-fields-from-query-results`.

.. [#findOne]
   The :method:`db.collection.findOne()` method also performs a read
   operation to return a single document. Internally, the
   :method:`db.collection.findOne()` method is the
   :method:`db.collection.find()` method with a limit of 1.

Select All Documents in a Collection
------------------------------------

An empty query document (``{}``) selects all documents in the
collection:

.. code-block:: javascript

   db.inventory.find( {} )

Not specifying a query document to the :method:`~db.collection.find()`
is equivalent to specifying an empty query document. Therefore the
following operation is equivalent to the previous operation:

.. code-block:: javascript

   db.inventory.find()

Specify Equality Condition
--------------------------

To specify equality condition, use the query document ``{ <field>:
<value> }`` to select all documents that contain the ``<field>`` with
the specified ``<value>``.

The following example retrieves from the ``inventory`` collection all
documents where the ``type`` field has the value ``snacks``:

.. code-block:: javascript

   db.inventory.find( { type: "snacks" } )

Specify Conditions Using Query Operators
----------------------------------------

A query document can use the :ref:`query operators <query-selectors>`
to specify conditions in a MongoDB query.

The following example selects all documents in the ``inventory``
collection where the value of the ``type`` field is either ``'food'``
or ``'snacks'``:

.. code-block:: javascript

   db.inventory.find( { type: { $in: [ 'food', 'snacks' ] } } )

Although you can express this query using the :query:`$or` operator,
use the :query:`$in` operator rather than the :query:`$or`
operator when performing equality checks on the same field.

Refer to the :doc:`/reference/operator` document for the complete list
of query operators.

Specify ``AND`` Conditions
--------------------------

A compound query can specify conditions for more than one field in the
collection's documents. Implicitly, a logical ``AND`` conjunction
connects the clauses of a compound query so that the query selects the
documents in the collection that match all the conditions.

In the following example, the query document specifies an equality
match on the field ``food`` **and** a less than (:query:`$lt`)
comparison match on the field ``price``:

.. code-block:: javascript

   db.inventory.find( { type: 'food', price: { $lt: 9.95 } } )

This query selects all documents where the ``type`` field has the value
``'food'`` **and** the value of the ``price`` field is less than
``9.95``. See :ref:`comparison operators <query-selectors-comparison>`
for other comparison operators.

Specify ``OR`` Conditions
-------------------------

Using the :query:`$or` operator, you can specify a compound query
that joins each clause with a logical ``OR`` conjunction so that the
query selects the documents in the collection that match at least one
condition.

In the following example, the query document selects all documents in
the collection where the field ``qty`` has a value greater than
(:query:`$gt`) ``100`` **or** the value of the ``price`` field is
less than (:query:`$lt`) ``9.95``:

.. code-block:: javascript

   db.inventory.find(
                      { $or: [
                               { qty: { $gt: 100 } },
                               { price: { $lt: 9.95 } }
                             ]
                      }
                    )

Specify ``AND`` as well as ``OR`` Conditions
--------------------------------------------

With additional clauses, you can specify precise conditions for
matching documents.

In the following example, the compound query document selects all
documents in the collection where the value of the ``type`` field is
``'food'`` **and** *either* the ``qty`` has a value greater than
(:query:`$gt`) ``100`` *or* the value of the ``price`` field is less
than (:query:`$lt`) ``9.95``:

.. code-block:: javascript

   db.inventory.find( { type: 'food', $or: [ { qty: { $gt: 100 } },
                                               { price: { $lt: 9.95 } } ]
                      } )

.. _read-operations-subdocuments:

Subdocuments
------------

When the field holds an embedded document (i.e. subdocument), you can
either specify the entire subdocument as the value of a field, or
"reach into" the subdocument using :term:`dot notation`, to specify
values for individual fields in the subdocument:

Exact Match on Subdocument
~~~~~~~~~~~~~~~~~~~~~~~~~~

To specify an equality match on the whole subdocument, use the query
document ``{ <field>: <value> }`` where ``<value>`` is the subdocument
to match. Equality matches on a subdocument require that the
subdocument field match *exactly* the specified ``<value>``, including
the field order.

In the following example, the query matches all documents where
the value of the field ``producer`` is a subdocument that contains
*only* the field ``company`` with the value ``'ABC123'`` and the field
``address`` with the value ``'123 Street'``, in the exact order:

.. code-block:: javascript

   db.inventory.find(
                      {
                        producer: {
                                    company: 'ABC123',
                                    address: '123 Street'
                                  }
                      }
                    )

Equality Match on Fields within Subdocument
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Equality matches for specific fields within subdocuments select the
documents in the collection when the field in the subdocument contains
a field that matches the specified value.

In the following example, the query uses the :term:`dot notation` to
match all documents where the value of the field ``producer`` is a
subdocument that contains a field ``company`` with the value
``'ABC123'`` and may contain other fields:

.. code-block:: javascript

   db.inventory.find( { 'producer.company': 'ABC123' } )

.. _read-operations-arrays:

Arrays
------

When the field holds an array, you can query for an exact array match
or for specific values in the array. If the array holds sub-documents,
you can query for specific fields within the sub-documents using
:term:`dot notation`:

Exact Match on an Array
~~~~~~~~~~~~~~~~~~~~~~~

To specify equality match on an array, use the query document ``{
<field>: <value> }`` where ``<value>`` is the array to match. Equality
matches on the array require that the array field match *exactly* the
specified ``<value>``, including the element order.

In the following example, the query matches all documents where the
value of the field ``tags`` is an array that holds exactly three
elements, ``'fruit'``, ``'food'``, and ``'citrus'``, in this order:

.. code-block:: javascript

   db.inventory.find( { tags: [ 'fruit', 'food', 'citrus' ] } )

Match an Array Element
~~~~~~~~~~~~~~~~~~~~~~

Equality matches can specify a single element in the array to match.
These specifications match if the array contains at least *one* element
with the specified value.

In the following example, the query matches all documents where the
value of the field ``tags`` is an array that contains ``'fruit'`` as
one of its elements:

.. code-block:: javascript

   db.inventory.find( { tags: 'fruit' } )

Match a Specific Element of an Array
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Equality matches can specify equality matches for an element at a
particular index or position of the array.

In the following example, the query uses the :term:`dot notation` to
match all documents where the value of the ``tags`` field is an array
whose first element equals ``'fruit'``:

.. code-block:: javascript

   db.inventory.find( { 'tags.0' : 'fruit' } )

Array of Subdocuments
~~~~~~~~~~~~~~~~~~~~~

Match a Field in the Subdocument Using the Array Index
``````````````````````````````````````````````````````

If you know the array index of the subdocument, you can specify the
document using the subdocument's position.

The following example selects all documents where the ``memos``
contains an array whose first element (i.e. index is ``0``) is a
subdocument with the field ``by`` with the value ``'shipping'``:

.. code-block:: javascript

   db.inventory.find( { 'memos.0.by': 'shipping' } )

Match a Field Without Specifying Array Index
````````````````````````````````````````````

If you do not know the index position of the subdocument, concatenate
the name of the field that contains the array, with a dot (``.``) and
the name of the field in the subdocument.

The following example selects all documents where the ``memos`` field
contains an array that contains at least one subdocument with the field
``by`` with the value ``'shipping'``:

.. code-block:: javascript

   db.inventory.find( { 'memos.by': 'shipping' } )

Match Multiple Fields
`````````````````````

To match by multiple fields in the subdocument, you can use either dot
notation or the :query:`$elemMatch` operator:

The following example uses dot notation to query for documents where
the value of the ``memos`` field is an array that has at least one
subdocument that contains the field ``memo`` equal to ``'on time'`` and
the field ``by`` equal to ``'shipping'``:

.. code-block:: javascript

   db.inventory.find(
                      {
                        'memos.memo': 'on time',
                        'memos.by': 'shipping'
                      }
                    )

The following example uses :query:`$elemMatch` to query for
documents where the value of the ``memos`` field is an array that has
at least one subdocument that contains the field ``memo`` equal to
``'on time'`` and the field ``by`` equal to ``'shipping'``:

.. code-block:: javascript

   db.inventory.find( {
                        memos: {
                                   $elemMatch: {
                                                 memo : 'on time',
                                                 by: 'shipping'
                                               }
                               }
                      }
                    )
.. index:: index; rebuild
.. _index-rebuild-index:

===============
Rebuild Indexes
===============

.. default-domain:: mongodb

If you need to rebuild indexes for a collection you can use the
:method:`db.collection.reIndex()` method to rebuild all indexes on a
collection in a single operation.  This operation drops all indexes,
including the :ref:`_id index <index-type-id>`, and then rebuilds all
indexes.

.. seealso:: :doc:`/core/indexes` and :doc:`/administration/indexes`.

Process
-------

The operation takes the following form:

.. code-block:: javascript

   db.accounts.reIndex()

MongoDB will return the following document when the operation
completes:

.. code-block:: javascript

   {
           "nIndexesWas" : 2,
           "msg" : "indexes dropped for collection",
           "nIndexes" : 2,
           "indexes" : [
                   {
                           "key" : {
                                   "_id" : 1,
                                   "tax-id" : 1
                           },
                           "ns" : "records.accounts",
                           "name" : "_id_"
                   }
           ],
           "ok" : 1
   }

This shell helper provides a wrapper around the :dbcommand:`reIndex`
:term:`database command`. Your :doc:`client library </applications/drivers>`
may have a different or additional interface for this operation.

Additional Considerations
-------------------------

.. include:: /includes/note-build-indexes-on-replica-sets.rst
==================================================
Reconfigure a Replica Set with Unavailable Members
==================================================

.. default-domain:: mongodb

To reconfigure a :term:`replica set` when a **minority** of
members are unavailable, use the :method:`rs.reconfig()`
operation on
the current :term:`primary`, following the example in the
:ref:`Replica Set Reconfiguration Procedure
<replica-set-reconfiguration-usage>`.

This document provides the following options for re-configuring a
replica set when a **majority** of members are *not* accessible:

- :ref:`replica-set-force-reconfiguration`
- :ref:`replica-set-reconfigure-by-replacing`

You may need to use one of these procedures, for example, in a
geographically distributed replica set, where *no* local group of
members can reach a majority. See :ref:`replica-set-elections` for more
information on this situation.

.. index:: replica set; reconfiguration
.. _replica-set-force-reconfiguration:

Reconfigure by Forcing the Reconfiguration
------------------------------------------

.. versionchanged:: 2.0

This procedure lets you recover while a majority of :term:`replica set`
members are down or unreachable. You connect to any surviving member and
use the ``force`` option to the :method:`rs.reconfig()`  method.

The ``force`` option forces a new configuration onto the. Use this procedure only to
recover from catastrophic interruptions. Do not use ``force`` every
time you reconfigure. Also, do not use the ``force`` option in any automatic
scripts and do not use ``force`` when there is still a :term:`primary`.

To force reconfiguration:

1. Back up a surviving member.

#. Connect to a surviving member and save the current configuration.
   Consider the following example commands for saving the configuration:

   .. code-block:: javascript

      cfg = rs.conf()

      printjson(cfg)

#. On the same member, remove the down and unreachable members of the
   replica set from the :data:`~local.system.replset.members` array by
   setting the array equal to the surviving members alone. Consider the
   following example, which uses the ``cfg`` variable created in the
   previous step:

   .. code-block:: javascript

      cfg.members = [cfg.members[0] , cfg.members[4] , cfg.members[7]]

#. On the same member, reconfigure the set by using the
   :method:`rs.reconfig()` command with the ``force`` option set to
   ``true``:

   .. code-block:: javascript

      rs.reconfig(cfg, {force : true})

   This operation forces the secondary to use the new configuration. The
   configuration is then propagated to all the surviving members listed
   in the ``members`` array. The replica set then elects a new primary.

   .. note::

      When you use ``force : true``, the version number in the replica
      set configuration increases significantly, by tens or hundreds
      of thousands. This is normal and designed to prevent set version
      collisions if you accidentally force re-configurations on both
      sides of a network partition and then the network partitioning
      ends.

#. If the failure or partition was only temporary, shut down or
   decommission the removed members as soon as possible.

.. _replica-set-reconfigure-by-replacing:

Reconfigure by Replacing the Replica Set
----------------------------------------

Use the following procedure **only** for versions of MongoDB prior to
version 2.0. If you're running MongoDB 2.0 or later, use the above
procedure, :ref:`replica-set-force-reconfiguration`.

These procedures are for situations where a *majority* of the
:term:`replica set` members are down or unreachable. If a majority is
*running*, then skip these procedures and instead use the
:method:`rs.reconfig()` command according to the examples in
:ref:`replica-set-reconfiguration-usage`.

If you run a pre-2.0 version and a majority of your replica set is down,
you have the two options described here. Both involve replacing the
replica set.

Reconfigure by Turning Off Replication
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This option replaces the :term:`replica set` with a :term:`standalone` server.

1. Stop the surviving :program:`mongod` instances. To ensure a clean shutdown, use
   an existing :term:`control script` or use the :method:`db.shutdownServer()` method.

   For example, to use the :method:`db.shutdownServer()` method, connect
   to the server using the :program:`mongo` shell and issue the
   following sequence of commands:

   .. code-block:: javascript

      use admin
      db.shutdownServer()

#. Create a backup of the data directory (i.e. :setting:`~storage.dbPath`) of
   the surviving members of the set.

   .. optional:: If you have a backup of the database you may instead remove
      this data.

#. Restart one of the :program:`mongod` instances *without* the
   :option:`--replSet <mongod --replSet>` parameter.

   The data is now accessible and provided by a single server that is
   not a replica set member. Clients can use this server for both
   reads and writes.

When possible, re-deploy a replica set to provide redundancy and to
protect your deployment from operational interruption.

.. _replica-set-forcing-resync:

Reconfigure by "Breaking the Mirror"
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This option selects a surviving :term:`replica set` member to be the
new :term:`primary` and to "seed" a new replica set. In the following
procedure, the new primary is ``db0.example.net``. MongoDB copies the
data from ``db0.example.net`` to all the other members.

1. Stop the surviving :program:`mongod` instances. To ensure a clean
   shutdown, use an existing :term:`control script` or use the
   :method:`db.shutdownServer()` method.

   For example, to use the :method:`db.shutdownServer()` method, connect
   to the server using the :program:`mongo` shell and issue the
   following sequence of commands:

   .. code-block:: javascript

      use admin
      db.shutdownServer()

#. Move the data directories (i.e. :setting:`~storage.dbPath`)
   for all the members except ``db0.example.net``, so that all the
   members except ``db0.example.net`` have empty data directories. For
   example:

   .. code-block:: sh

      mv /data/db /data/db-old

#. Move the data files for ``local`` database (i.e. ``local.*``) so
   that ``db0.example.net`` has no local database. For example

   .. code-block:: sh

      mkdir /data/local-old
      mv /data/db/local* /data/local-old/

#. Start each member of the replica set normally.

#. Connect to ``db0.example.net`` in a :program:`mongo` shell and run :method:`rs.initiate()`
   to initiate the replica set.

#. Add the other set members using :method:`rs.add()`. For example, to
   add a member running on ``db1.example.net`` at port ``27017``, issue
   the following command:

   .. code-block:: javascript

      rs.add("db1.example.net:27017")

   MongoDB performs an initial sync on the added members by copying all
   data from ``db0.example.net`` to the added members.

.. seealso:: :doc:`/tutorial/resync-replica-set-member`
=========================================
Recover Data after an Unexpected Shutdown
=========================================

.. default-domain:: mongodb

If MongoDB does not shutdown cleanly [#clean-shutdown]_ the on-disk
representation of the data files will likely reflect an inconsistent
state which could lead to data corruption. [#validation]_

To prevent data inconsistency and corruption, always shut down the
database cleanly and use the :ref:`durability journaling
<setting-journal>`. MongoDB writes data to the journal, by default,
every 100 milliseconds, such that MongoDB can always recover to a
consistent state even in the case of an unclean shutdown due to power
loss or other system failure.

If you are *not* running as part of a :term:`replica set` **and** do
*not* have journaling enabled, use the following procedure to recover
data that may be in an inconsistent state. If you are running as part
of a replica set, you should *always* restore from a backup or restart
the :program:`mongod` instance with an empty :setting:`~storage.dbPath` and
allow MongoDB to perform an initial sync to restore the data.

.. seealso:: The :doc:`/administration` documents, including
   :ref:`Replica Set Syncing <replica-set-syncing>`, and the
   documentation on the :setting:`repair`, :setting:`repairpath`, and
   :setting:`storage.journal.enabled` settings.

.. [#clean-shutdown] To ensure a clean shut down, use the
   :method:`db.shutdownServer()` from the :program:`mongo` shell, your
   control script, the :option:`mongod --shutdown` option on Linux
   systems, "Control-C" when running :program:`mongod` in interactive
   mode, or ``kill $(pidof mongod)`` or ``kill -2 $(pidof mongod)``.

.. [#validation] You can also use the :method:`db.collection.validate()`
   method to test the integrity of a single collection. However, this
   process is time consuming, and without journaling you can safely
   assume that the data is in an invalid state and you should either
   run the repair operation or resync from an intact member of the
   replica set.

Process
-------

Indications
~~~~~~~~~~~

When you are aware of a :program:`mongod` instance running without
journaling that stops unexpectedly **and** you're not running with
replication, you should always run the repair operation before
starting MongoDB again. If you're using replication, then restore from
a backup and allow replication to perform an initial :ref:`sync <replica-set-syncing>` to restore data.

If the ``mongod.lock`` file in the data directory specified by
:setting:`~storage.dbPath`, ``/data/db`` by default, is *not* a zero-byte file,
then :program:`mongod` will refuse to start, and you will find a
message that contains the following line in your MongoDB log our
output:

.. code-block:: none

   Unclean shutdown detected.

This indicates that you need to run :program:`mongod` with the
:option:`--repair <mongod --repair>` option. If you run repair when
the ``mongodb.lock`` file exists in your :setting:`~storage.dbPath`, or the
optional :option:`--repairpath <mongod --repairpath>`, you will see a
message that contains the following line:

.. code-block:: none

   old lock file: /data/db/mongod.lock. probably means unclean shutdown

If you see this message, as a last resort you may remove the lockfile
**and** run the repair operation before starting the database
normally, as in the following procedure:

Overview
~~~~~~~~

.. warning:: Recovering a member of a replica set.

   Do not use this procedure to recover a member of a
   :term:`replica set`. Instead you should either restore from
   a :doc:`backup </core/backups>` or perform an initial sync using
   data from an intact member of the set, as described in
   :doc:`/tutorial/resync-replica-set-member`.

There are two processes to repair data files that result from an
unexpected shutdown:

#. Use the :option:`--repair <mongod --repair>` option in
   conjunction with the :option:`--repairpath <mongod --repairpath>`
   option. :program:`mongod` will read the existing data files, and
   write the existing data to new data files. This does not modify or
   alter the existing data files.

   You do not need to remove the ``mongod.lock`` file before using
   this procedure.

#. Use the :option:`--repair <mongod --repair>` option.
   :program:`mongod` will read the existing data files, write the
   existing data to new files and replace the existing, possibly
   corrupt, files with new files.

   You must remove the ``mongod.lock`` file before using this
   procedure.

.. note::

   :option:`--repair <mongod --repair>` functionality is also
   available in the shell with the :method:`db.repairDatabase()`
   helper for the :dbcommand:`repairDatabase` command.

.. _tutorial-repair-procedures:

Procedures
~~~~~~~~~~

To repair your data files using the :option:`--repairpath <mongod --repairpath>`
option to preserve the original data files unmodified:

#. Start :program:`mongod` using :option:`--repair <mongod --repair>`
   to read the existing data files.

   .. code-block:: sh

      mongod --dbpath /data/db --repair --repairpath /data/db0

   When this completes, the new repaired data files will be in the
   ``/data/db0`` directory.

#. Start :program:`mongod` using the following invocation to point the
   :setting:`~storage.dbPath` at ``/data/db0``:

   .. code-block:: sh

      mongod --dbpath /data/db0

   Once you confirm that the data files are operational you may delete
   or archive the data files in the ``/data/db`` directory.

To repair your data files without preserving the original files, do
not use the :option:`--repairpath <mongod --repairpath>` option, as in
the following procedure:

#. Remove the stale lock file:

   .. code-block:: sh

      rm /data/db/mongod.lock

   Replace ``/data/db`` with your :setting:`~storage.dbPath` where your MongoDB
   instance's data files reside.

   .. warning::

      After you remove the ``mongod.lock`` file you *must* run the
      :option:`--repair <mongod --repair>` process before using your
      database.

#. Start :program:`mongod` using :option:`--repair <mongod --repair>`
   to read the existing data files.

   .. code-block:: sh

      mongod --dbpath /data/db --repair

   When this completes, the repaired data files will replace the
   original data files in the ``/data/db`` directory.

#. Start :program:`mongod` using the following invocation to point the
   :setting:`~storage.dbPath` at ``/data/db``:

   .. code-block:: sh

      mongod --dbpath /data/db

``mongod.lock``
---------------

In normal operation, you should **never** remove the ``mongod.lock``
file and start :program:`mongod`. Instead consider the one of the above methods
to recover the database and remove the lock files. In dire
situations you can remove the lockfile, and start the database using the
possibly corrupt files, and attempt to recover data from the database;
however, it's impossible to predict the state of the database in these
situations.

If you are not running with journaling, and your database shuts down
unexpectedly for *any* reason, you should always proceed *as if* your database
is in an inconsistent and likely corrupt state. If at all possible restore
from :doc:`backup </core/backups>` or, if running as a :term:`replica
set`, restore by performing an initial sync using data from an intact
member of the set, as described in :doc:`/tutorial/resync-replica-set-member`.
================
Remove Documents
================

.. default-domain:: mongodb

In MongoDB, the :method:`db.collection.remove()` method removes
documents from a collection. You can remove all documents from a
collection, remove all documents that match a condition, or limit the
operation to remove just a single document.

This tutorial provides examples of remove operations using the
:method:`db.collection.remove()` method in the :program:`mongo` shell.

Remove All Documents
--------------------

To remove all documents from a collection, pass an empty query document
``{}`` to the :method:`~db.collection.remove()` method. The
:method:`~db.collection.remove()` method does not remove the indexes.

The following example removes all documents from the ``inventory``
collection:

.. code-block:: javascript

   db.inventory.remove({})

To remove all documents from a collection, it may be more efficient to
use the :method:`~db.collection.drop()` method to drop the entire
collection, including the indexes, and then recreate the collection and
rebuild the indexes.

Remove Documents that Match a Condition
---------------------------------------

To remove the documents that match a deletion criteria, call the
:method:`~db.collection.remove()` method with the ``<query>``
parameter.

The following example removes all documents from the ``inventory``
collection where the ``type`` field equals ``food``:

.. code-block:: javascript

   db.inventory.remove( { type : "food" } )

For large deletion operations, it may be more efficient to copy the
documents that you want to keep to a new collection and then use
:method:`~db.collection.drop()` on the original collection.

Remove a Single Document that Matches a Condition
-------------------------------------------------

To remove a single document, call the :method:`~db.collection.remove()`
method with the ``justOne`` parameter set to ``true`` or ``1``.

The following example removes one document from the ``inventory``
collection where the ``type`` field equals ``food``:

.. code-block:: javascript

   db.inventory.remove( { type : "food" }, 1 )

To delete a single document sorted by some specified order, use the
:ref:`findAndModify() <findAndModify-wrapper-sorted-remove>` method.
.. index:: index; remove
.. _index-remove-index:

==============
Remove Indexes
==============

.. default-domain:: mongodb

To remove an index from a collection use the
:method:`~db.collection.dropIndex()` method and the following
procedure. If you simply need to rebuild indexes you can use the
process described in the :doc:`/tutorial/rebuild-indexes`
document.

.. seealso:: :doc:`/administration/indexes` and :doc:`/core/indexes`
   for more information about indexes and indexing operations in
   MongoDB.

Operations
----------

To remove an index, use the :method:`db.collection.dropIndex()` method,
as in the following example:

.. code-block:: javascript

   db.accounts.dropIndex( { "tax-id": 1 } )

This will remove the index on the ``"tax-id"`` field in the ``accounts``
collection. The shell provides the following document after completing
the operation:

.. code-block:: javascript

   { "nIndexesWas" : 3, "ok" : 1 }

Where the value of ``nIndexesWas`` reflects the number of indexes
*before* removing this index. You can also use the
:method:`db.collection.dropIndexes()` to remove *all* indexes, except
for the :ref:`_id index <index-type-id>` from a collection.

These shell helpers provide wrappers around the
:dbcommand:`dropIndexes` :term:`database command`. Your :doc:`client
library </applications/drivers>` may have a different or additional
interface for these operations.
===============================
Remove Members from Replica Set
===============================

.. default-domain:: mongodb

To remove a member of a :term:`replica set` use either of the
following procedures.

Remove a Member Using ``rs.remove()``
-------------------------------------

1. Shut down the :program:`mongod` instance for the member you wish to
   remove. To shut down the instance, connect using the
   :program:`mongo` shell and the :method:`db.shutdownServer()`
   method.

#. Connect to the replica set's current :term:`primary`. To determine
   the current primary, use :method:`db.isMaster()` while connected to
   any member of the replica set.

#. Use :method:`rs.remove()` in either of the following forms to
   remove the member:

   .. code-block:: javascript

      rs.remove("mongod3.example.net:27017")
      rs.remove("mongod3.example.net")

   MongoDB disconnects the shell briefly as the replica set elects a
   new primary. The shell then automatically reconnects. The
   shell displays a ``DBClientCursor::init call() failed`` error even
   though the command succeeds.

Remove a Member Using ``rs.reconfig()``
---------------------------------------

To remove a member you can manually edit the :doc:`replica set
configuration document </reference/replica-configuration>`, as described
here.

1. Shut down the :program:`mongod` instance for the member you wish to
   remove. To shut down the instance, connect using the
   :program:`mongo` shell and the :method:`db.shutdownServer()`
   method.

#. Connect to the replica set's current :term:`primary`. To determine
   the current primary, use :method:`db.isMaster()` while connected to
   any member of the replica set.

#. Issue the :method:`rs.conf()` method to view the current
   configuration document and determine the position in the
   ``members`` array of the member to remove:

   .. example::

      ``mongod_C.example.net`` is in position ``2`` of the
      following configuration file:

      .. code-block:: javascript

         {
             "_id" : "rs",
             "version" : 7,
             "members" : [
                 {
                     "_id" : 0,
                     "host" : "mongod_A.example.net:27017"
                 },
                 {
                     "_id" : 1,
                     "host" : "mongod_B.example.net:27017"
                 },
                 {
                     "_id" : 2,
                     "host" : "mongod_C.example.net:27017"
                 }
             ]
         }

#. Assign the current configuration document to the variable ``cfg``:

   .. code-block:: javascript

      cfg = rs.conf()

#. Modify the ``cfg`` object to remove the member.

   .. example::

      To remove ``mongod_C.example.net:27017`` use the following
      JavaScript operation:

      .. code-block:: javascript

         cfg.members.splice(2,1)

#. Overwrite the replica set configuration document with the new
   configuration by issuing the following:

   .. code-block:: javascript

      rs.reconfig(cfg)

   As a result of :method:`rs.reconfig()` the shell will disconnect
   while the replica set renegotiates which member is primary. The
   shell displays a ``DBClientCursor::init call() failed`` error even
   though the command succeeds, and will automatically reconnected.

#. To confirm the new configuration, issue :method:`rs.conf()`.

   For the example above the output would be:

   .. code-block:: javascript

      {
          "_id" : "rs",
          "version" : 8,
          "members" : [
              {
                  "_id" : 0,
                  "host" : "mongod_A.example.net:27017"
              },
              {
                  "_id" : 1,
                  "host" : "mongod_B.example.net:27017"
              }
          ]
      }
==============================================
Remove Shards from an Existing Sharded Cluster
==============================================

.. default-domain:: mongodb

To remove a :term:`shard` you must ensure the shard's data is migrated
to the remaining shards in the cluster. This procedure describes how to
safely migrate data and how to remove a shard.

This procedure describes how to safely remove a *single* shard. *Do not*
use this procedure to migrate an entire cluster to new hardware. To
migrate an entire shard to new hardware, migrate individual shards as if
they were independent replica sets.

.. DOCS-94 will lead to a tutorial about cluster migrations. In the
   mean time the above section will necessarily lack links.

To remove a shard, first connect to one of the cluster's
:program:`mongos` instances using :program:`mongo` shell. Then use the
sequence of tasks in this document to remove a shard from the cluster.

.. _remove-shard-ensure-balancer-is-enabled:
.. _remove-shard-ensure-balancer-is-active:

Ensure the Balancer Process is Enabled
--------------------------------------

To successfully migrate data from a shard, the :term:`balancer` process
**must** be enabled. Check the balancer state using the
:method:`sh.getBalancerState()` helper in the :program:`mongo` shell.
For more information, see the section on :ref:`balancer operations
<sharding-balancing-disable-temporarily>`.

.. _remove-shard-determine-name-shard:

Determine the Name of the Shard to Remove
-----------------------------------------

To determine the name of the shard, connect to a :program:`mongos`
instance with the :program:`mongo` shell and either:

- Use the :dbcommand:`listShards` command, as in the following:

  .. code-block:: javascript

     db.adminCommand( { listShards: 1 } )

- Run either the :method:`sh.status()` or the
  :method:`db.printShardingStatus()` method.

The ``shards._id`` field lists the name of each shard.

.. _remove-shard-remove-chunks:

Remove Chunks from the Shard
----------------------------

From the ``admin`` database, run the :dbcommand:`removeShard` command.
This begins "draining" chunks
from the shard you are removing to other shards in the cluster. For
example, for a shard named ``mongodb0``, run:

.. code-block:: javascript

   use admin
   db.runCommand( { removeShard: "mongodb0" } )

This operation returns immediately, with the following response:

.. code-block:: javascript

   {
       "msg" : "draining started successfully",
       "state" : "started",
       "shard" : "mongodb0",
       "ok" : 1
   }

Depending on your network capacity and the amount of data, this
operation can take from a few minutes to several days to complete.

.. _remove-shard-check-migration-status:

Check the Status of the Migration
---------------------------------

To check the progress of the migration at any stage in the process, run
:dbcommand:`removeShard` from the ``admin`` database again. For example,
for a shard named ``mongodb0``, run:

.. code-block:: javascript

   use admin
   db.runCommand( { removeShard: "mongodb0" } )

The command returns output similar to the following:

.. code-block:: javascript

   {
        "msg" : "draining ongoing",
       "state" : "ongoing",
       "remaining" : {
           "chunks" : 42,
           "dbs" : 1
       },
       "ok" : 1
   }

In the output, the ``remaining`` document displays the remaining number
of chunks that MongoDB must migrate to other shards and the number of
MongoDB databases that have "primary" status on this shard.

Continue checking the status of the `removeShard` command until the
number of chunks remaining is ``0``. Always run the command on the
``admin`` database. If you are on a database other than ``admin``, you can
use :method:`sh._adminCommand` to run the command on ``admin``.

.. _remove-shard-move-unsharded-databases:

Move Unsharded Data
-------------------

If the shard is the :term:`primary shard` for one or more databases in
the cluster, then the shard will have unsharded data. If the shard is
not the primary shard for any databases, skip to the next task,
:ref:`remove-shard-finalize-migration`.

In a cluster, a database with unsharded collections stores those
collections only on a single shard. That shard becomes the primary shard
for that database. (Different databases in a cluster can have different
primary shards.)

.. warning::

   Do not perform this procedure until you have finished draining the
   shard.

1. To determine if the shard you are removing is the primary shard for
   any of the cluster's databases, issue one of the following methods:

   - :method:`sh.status()`

   - :method:`db.printShardingStatus()`

   In the resulting document, the ``databases`` field lists each
   database and its primary shard. For example, the following
   ``database`` field shows that the ``products`` database uses
   ``mongodb0`` as the primary shard:

   .. code-block:: javascript

      {  "_id" : "products",  "partitioned" : true,  "primary" : "mongodb0" }

#. To move a database to another shard, use the :dbcommand:`movePrimary`
   command. For example, to migrate all remaining unsharded data from
   ``mongodb0`` to ``mongodb1``, issue the following command:

   .. code-block:: javascript

      db.runCommand( { movePrimary: "products", to: "mongodb1" })

   This command does not return until MongoDB completes moving all data,
   which may take a long time. The response from this command will
   resemble the following:

   .. code-block:: javascript

      { "primary" : "mongodb1", "ok" : 1 }

.. _remove-shard-finalize-migration:

Finalize the Migration
----------------------

To clean up all metadata information and finalize the removal, run
:dbcommand:`removeShard` again. For example, for a shard named
``mongodb0``, run:

.. code-block:: javascript

   use admin
   db.runCommand( { removeShard: "mongodb0" } )

A success message appears at completion:

.. code-block:: javascript

   {
       "msg" : "removeshard completed successfully",
       "state" : "completed",
       "shard" : "mongodb0",
       "ok" : 1
   }

Once the value of the ``state`` field is "completed", you may safely
stop the processes comprising the ``mongodb0`` shard.
=======================
Replace a Config Server
=======================

.. default-domain:: mongodb

This procedure replaces an inoperable
:ref:`config server <sharding-config-server>` in a
:doc:`sharded cluster </core/sharding>`. Use this procedure only
to replace a config server that has become inoperable (e.g. hardware
failure).

This process assumes that the hostname of the instance will not change.
If you must change the hostname of the instance, use the procedure to
:doc:`migrate a config server and use a new hostname
<migrate-config-servers-with-different-hostnames>`.

#. Disable the cluster balancer process temporarily. See
   :ref:`sharding-balancing-disable-temporarily` for more information.

#. Provision a new system, with the same hostname as the previous
   host.

   You will have to ensure that the new system has the same IP address
   and hostname as the system it's replacing *or* you will need to
   modify the DNS records and wait for them to propagate.

#. Shut down *one* (and only one) of the existing config servers. Copy
   all of this host's :setting:`~storage.dbPath` file system tree from the current system
   to the system that will provide the new config server. This
   command, issued on the system with the data files, may resemble the
   following:

   .. code-block:: sh

      rsync -az /data/configdb mongodb.config2.example.net:/data/configdb

#. Restart the config server process that you used in the previous
   step to copy the data files to the new config server instance.

#. Start the new config server instance. The default invocation is:

   .. code-block:: sh

      mongod --configsvr

#. Re-enable the balancer to allow the cluster to resume normal
   balancing operations. See the
   :ref:`sharding-balancing-disable-temporarily` section for more
   information on managing the balancer process.

.. note::

   In the course of this procedure *never* remove a config server from
   the :setting:`~sharding.configDB` parameter on any of the :program:`mongos`
   instances. If you need to change the name of a config server,
   always make sure that all :program:`mongos` instances have three
   config servers specified in the :setting:`~sharding.configDB` setting at all
   times.
============================
Replace a Replica Set Member
============================

.. default-domain:: mongodb

If you need to change the hostname of a replica set member without
changing the configuration of that member or the set, you can use the
operation outlined in this tutorial. For example if you must
re-provision systems or rename hosts, you can use this pattern to
minimize the scope of that change.

Operation
---------

To change the hostname for a replica set member modify the
:data:`~local.system.replset.members[n].host` field. The value of
:data:`~local.system.replset.members[n]._id` field will not change
when you reconfigure the set.

See :doc:`/reference/replica-configuration` and
:method:`rs.reconfig()` for more information.

.. note::

   Any replica set configuration change can trigger the current
   :term:`primary` to step down, which forces an :ref:`election
   <replica-set-elections>`. During the election, the current shell
   session and clients connected to this replica set disconnect,
   which produces an error even when the operation succeeds.

Example
-------

To change the hostname to ``mongo2.example.net`` for the replica set
member configured at ``members[0]``, issue the following sequence of
commands:

.. code-block:: javascript

   cfg = rs.conf()
   cfg.members[0].host = "mongo2.example.net"
   rs.reconfig(cfg)
==========================================
Restore a Replica Set from MongoDB Backups
==========================================

.. default-domain:: mongodb

This procedure outlines the process for taking MongoDB data and
restoring that data into a new :term:`replica set`. Use this approach
for seeding test deployments from production backups as well as part
of disaster recovery.

You *cannot* restore a single data set to three new
:program:`mongod` instances and *then* create a replica set. In this
situation MongoDB will force the secondaries to perform an initial
sync. The procedures in this document describe the correct and
efficient ways to deploy a replica set.

Restore Database into a Single Node Replica Set
-----------------------------------------------

#. Obtain backup MongoDB Database files. These files may come from a
   :doc:`file system snapshot
   </tutorial/backup-with-filesystem-snapshots>`.  The
   `MongoDB Management Service (MMS)
   <https://mms.mongodb.com/?pk_campaign=mongodb-docs-restore-rs-tutorial>`_
   produces MongoDB database files for :mms:`stored snapshots
   </backup/tutorial/restore-from-snapshot/>` and :mms:`point and time
   snapshots </backup/tutorial/restore-from-point-in-time-snapshot/>`.
   You can also use :program:`mongorestore` to restore database files
   using data created with :program:`mongodump`. See
   :doc:`/tutorial/backup-with-mongodump` for
   more information.


#. Start a :program:`mongod` using data files from the backup as the
   ``dbpath``. In the following example, ``/data/db`` is the ``dbpath`` to
   the data files:

   .. code-block:: sh

      mongod --dbpath /data/db

#. Convert your standalone :program:`mongod` process to a single node
   replica set by shutting down the :program:`mongod` instance, and
   restarting it with the :option:`--replSet <mongod --replSet>`
   option, as in the following example:

   .. code-block:: sh

      mongod --dbpath /data/db --replSet <replName>

   .. optional::

      Consider explicitly setting a :setting:`~replication.oplogSizeMB` to control
      the size of the :term:`oplog` created for this replica set
      member.

#. Connect to the :program:`mongod` instance.

#. Use :method:`rs.initiate()` to initiate the new replica set.

Add Members to the Replica Set
------------------------------

MongoDB provides two options for restoring secondary members of a
replica set:

1. Manually copy the database files to each data directory.

2. Allow :ref:`initial sync <replica-set-initial-sync>` to distribute
   data automatically.

The following sections outlines both approaches.

.. note::

   If your database is large, initial sync can take a long time to
   complete. For large databases, it might be preferable to copy the
   database files onto each host.

Copy Database Files and Restart :program:`mongod` Instance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following sequence of operations to "seed" additional members
of the replica set with the restored data by copying MongoDB data
files directly.

#. Shut down the :program:`mongod` instance that you restored.
   Using :option:`--shutdown <mongod --shutdown>` or
   :method:`db.shutdownServer()` to ensure a clean shut down.

#. Copy the :term:`primary's <primary>` data directory into the
   :setting:`~storage.dbPath` of the other members of the replica set. The
   :setting:`~storage.dbPath` is ``/data/db`` by default.

#. Start the :program:`mongod` instance that you restored.

#. In a :program:`mongo` shell connected to the :term:`primary`, add
   the :term:`secondaries <secondary>` to the replica set using
   :method:`rs.add()`. See :doc:`/tutorial/deploy-replica-set` for
   more information about deploying a replica set.

Update Secondaries using Initial Sync
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Use the following sequence of operations to "seed" additional members
of the replica set with the restored data using the default *initial
sync* operation.

#. Ensure that the data directories on the prospective replica set
   members are empty.

#. Add each prospective member to the replica set. :ref:`Initial Sync
   <replica-set-initial-sync>` will copy the data from the
   :term:`primary` to the other members of the replica set.
=========================
Restore a Sharded Cluster
=========================

.. default-domain:: mongodb

Overview
--------

The procedure outlined in this document addresses how to restore an
entire sharded cluster. For information on related backup procedures
consider the following tutorials which describe backup procedures in
greater detail:

- :doc:`/tutorial/backup-sharded-cluster-with-filesystem-snapshots`
- :doc:`/tutorial/backup-sharded-cluster-with-database-dumps`

The exact procedure used to restore a database depends on the method
used to capture the backup. See the :doc:`/core/backups`
document for an overview of backups with MongoDB and
:doc:`/administration/backup-sharded-clusters` for a complete
information on backups in MongoDB and backups of sharded clusters in
particular.

Procedure
---------

#. Stop all :program:`mongos` and :program:`mongod` processes,
   including all shards *and* all config servers.

#. If shard hostnames have changed, you must manually update the
   ``shards`` collection in the :ref:`config-database` to use the new
   hostnames. Do the following:

   a. Start the three :ref:`config servers <sharding-config-server>` by
      issuing commands similar to the following, using values appropriate
      to your configuration:

      .. code-block:: sh

         mongod --configsvr --dbpath /data/configdb --port 27019

   #. Restore the :ref:`config-database` on each config server.

   #. Start one :program:`mongos` instance.

   #. Update the :ref:`config-database` collection named ``shards`` to reflect the
      new hostnames.

#. Restore the following:

   - Data files for each server in each :term:`shard`. Because replica
     sets provide each production shard, restore all the members of
     the replica set or use the other standard approaches for
     restoring a replica set from backup. See the
     :ref:`backup-restore-snapshot` and :ref:`backup-restore-dump`
     sections for details on these procedures.

   - Data files for each :ref:`config server <sharding-config-server>`,
     if you have not already done so in the previous step.

#. Restart all the :program:`mongos` instances.

#. Restart all the shard :program:`mongod` instances.

#. Restart all the config servers :program:`mongod` instances.

#. Connect to a :program:`mongos` instance from a :program:`mongo` shell
   and use the :method:`db.printShardingStatus()` method to ensure
   that the cluster is operational, as follows:

   .. code-block:: javascript

      db.printShardingStatus()
      show collections
======================
Restore a Single Shard
======================

.. default-domain:: mongodb

Overview
--------

Restoring a single shard from backup with other unaffected shards
requires a number of special considerations and practices. This
document outlines the additional tasks you must perform when restoring
a single shard.

Consider the following resources on backups in general as well as
backup and restoration of sharded clusters specifically:

- :doc:`/administration/backup-sharded-clusters`
- :doc:`/tutorial/restore-sharded-cluster`
- :doc:`/core/backups`

Procedure
---------

Always restore :term:`sharded clusters <sharded cluster>`
as a whole. When you restore a single shard, keep in mind that the
:term:`balancer` process might have moved :term:`chunks <chunk>` to or
from this shard since the last backup. If that's the case, you must
manually move those chunks, as described in this procedure.

1. Restore the shard as you would any other :program:`mongod`
   instance. See :doc:`/core/backups` for overviews of these
   procedures.

#. For all chunks that migrate away from this shard, you do not need
   to do anything at this time. You do not need to delete these
   documents from the shard because the chunks are automatically
   filtered out from queries by :program:`mongos`. You can remove
   these documents from the shard, if you like, at your leisure.

#. For chunks that migrate to this shard after the most recent backup,
   you must manually recover the chunks using backups of other shards,
   or some other source.  To determine what chunks have moved, view the
   ``changelog`` collection in the :ref:`config-database`.
================================
Resync a Member of a Replica Set
================================

.. default-domain:: mongodb

A :term:`replica set` member becomes "stale" when its replication
process falls so far behind that the :term:`primary` overwrites oplog
entries the member has not yet replicated. The member cannot catch up
and becomes "stale." When this occurs, you must completely
resynchronize the member by removing its data and performing an
:ref:`initial sync <replica-set-initial-sync>`.

This tutorial addressed both resyncing a stale member and to creating a
new member using seed data from another member. When syncing a member,
choose a time when the system has the bandwidth to move a large amount
of data. Schedule the synchronization during a time of low usage or
during a maintenance window.

MongoDB provides two options for performing an initial sync:

- Restart the :program:`mongod` with an empty data directory and let
  MongoDB's normal initial syncing feature restore the data. This
  is the more simple option but may take longer to replace the data.

  See :ref:`replica-set-auto-resync-stale-member`.

- Restart the machine with a copy of a recent data directory from
  another member in the replica set. This procedure can replace
  the data more quickly but requires more manual steps.

  See :ref:`replica-set-resync-by-copying`.

.. index:: replica set; sync
.. index:: replica set; resync
.. _replica-set-auto-resync-stale-member:

Procedures
----------

Automatically Sync a Member
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. warning:: During initial sync, :program:`mongod` will remove the
   content of the :setting:`~storage.dbPath`.

This procedure relies on MongoDB's regular process for :ref:`initial
sync <replica-set-initial-sync>`. This will store the current data on
the member. For an overview of MongoDB initial sync process, see the
:ref:`replica-set-syncing` section.

If the instance has no data, you can simply follow the
:doc:`/tutorial/expand-replica-set` or
:doc:`/tutorial/replace-replica-set-member` procedure to add a
new member to a replica set.

You can also force a :program:`mongod` that is already a member of the
set to to perform an initial sync by restarting the instance without
the content of the :setting:`~storage.dbPath` as follows:

a. Stop the member's :program:`mongod` instance.
   To ensure a clean shutdown, use the :method:`db.shutdownServer()`
   method from the :program:`mongo` shell or on Linux systems, the
   :option:`mongod --shutdown` option.

#. Delete all data and sub-directories from the member's data
   directory. By removing the data :setting:`~storage.dbPath`, MongoDB will
   perform a complete resync. Consider making a backup first.

At this point, the :program:`mongod` will perform an initial
sync. The length of the initial sync process depends on the
size of the database and network connection between members of the
replica set.

Initial sync operations can impact the other members of the set and
create additional traffic to the primary and can only occur if
another member of the set is accessible and up to date.

.. index:: replica set; resync
.. _replica-set-resync-by-copying:

Sync by Copying Data Files from Another Member
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This approach "seeds" a new or stale member using the data files from
an existing member of the replica set. The data files **must** be
sufficiently recent to allow the new member to catch up with the
:term:`oplog`. Otherwise the member would need to perform an initial
sync.

Copy the Data Files
```````````````````

You can capture the data files as either a snapshot or a direct copy.
However, in most cases you cannot copy data files from a running
:program:`mongod` instance to another because the data files will change
during the file copy operation.

.. important:: If copying data files, you must copy the content of the ``local``
   database.

You *cannot* use a :program:`mongodump` backup to for the data files,
**only a snapshot backup**. For approaches to capture a consistent
snapshot of a running :program:`mongod` instance, see the
:doc:`/core/backups` documentation.

Sync the Member
```````````````

After you have copied the data files from the "seed" source, start the
:program:`mongod` instance and allow it to apply all operations from
the oplog until it reflects the current state of the replica set.
=======================
Build Old Style Indexes
=======================

.. default-domain:: mongodb

.. important::

   Use this procedure *only* if you **must** have indexes that are compatible
   with a version of MongoDB earlier than 2.0.

MongoDB version 2.0 introduced the ``{v:1}`` index format. MongoDB
versions 2.0 and later support both the ``{v:1}`` format and the
earlier ``{v:0}`` format.

MongoDB versions prior to 2.0, however, support only the ``{v:0}``
format. If you need to roll back MongoDB to a version prior to 2.0,
you must *drop* and *re-create* your indexes.

To build pre-2.0 indexes, use the :method:`dropIndexes()
<db.collection.dropIndexes()>` and :method:`ensureIndex()
<db.collection.ensureIndex()>` methods. You *cannot* simply reindex
the collection. When you reindex on versions that only support
``{v:0}`` indexes, the ``v`` fields in the index definition still hold
values of ``1``, even though the indexes would now use the ``{v:0}``
format. If you were to upgrade again to version 2.0 or later, these
indexes would not work.

.. For reference only, the source material:

   Versions >= 1.8.3 are aware of the index version field, but version
   <= 1.8.2 are not. So if you rollback a [v:1} index to 1.8.2 and
   re-index it its version will still be marked {v: 1} although it
   actual version is {v: 0}. Then if you upgrade again to 2.0 this
   index won't work even though they are marked as {v: 1} in
   db.system.indexes. So if you have to rollback to a version <=
   1.8.2, you must delete the index then create it again (instead of
   simply re-indexing).

.. example::

   Suppose you rolled back from MongoDB 2.0 to MongoDB 1.8, and suppose
   you had the following index on the ``items`` collection:

   .. code-block:: javascript

      { "v" : 1, "key" : { "name" : 1 }, "ns" : "mydb.items", "name" : "name_1" }

   The ``v`` field tells you the index is a ``{v:1}`` index, which
   is incompatible with version 1.8.

   To drop the index, issue the following command:

   .. code-block:: javascript

      db.items.dropIndex( { name : 1 } )

   To recreate the index as a ``{v:0}`` index, issue the following
   command:

   .. code-block:: javascript

      db.foo.ensureIndex( { name : 1 } , { v : 0 } )

.. seealso:: :ref:`2.0-new-index-format`.
================
Rotate Log Files
================

.. default-domain:: mongodb

Overview
--------

Log rotation using MongoDB's standard approach archives the current
log file and starts a new one. To do this, the :program:`mongod` or
:program:`mongos` instance renames the current log file by appending a
UTC (GMT) timestamp to the filename, in :term:`ISODate` format. It then
opens a new log file, closes the old log file, and sends all new log
entries to the new log file.

MongoDB's standard approach to log rotation only rotates logs
in response to the :dbcommand:`logRotate` command, or when the
:program:`mongod` or :program:`mongos` process receives a ``SIGUSR1``
signal from the operating system.

Alternately, you may configure mongod to send log data to ``syslog``. In
this case, you can take advantage of alternate logrotation tools.

.. seealso:: For information on logging, see the
   :ref:`monitoring-standard-loggging` section.

Log Rotation With MongoDB
-------------------------

The following steps create and rotate a log file:

1. Start a :program:`mongod` with verbose logging, with appending
   enabled, and with the following log file:

   .. code-block:: javascript

      mongod -v --logpath /var/log/mongodb/server1.log --logappend

#. In a separate terminal, list the matching files:

   .. code-block:: javascript

      ls /var/log/mongodb/server1.log*

   For results, you get:

   .. code-block:: javascript

      server1.log

#. Rotate the log file using *one* of the following methods.

   - From the :program:`mongo` shell, issue the :dbcommand:`logRotate`
     command from the ``admin`` database:

     .. code-block:: javascript

        use admin
        db.runCommand( { logRotate : 1 } )

     This is the only available method to rotate log files on
     Windows systems.

   - For Linux systems, rotate logs for a single process by issuing
     the following command:

     .. code-block:: javascript

        kill -SIGUSR1 <mongod process id>

#. List the matching files again:

   .. code-block:: javascript

      ls /var/log/mongodb/server1.log*

   For results you get something similar to the following. The
   timestamps will be different.

   .. code-block:: none

      server1.log  server1.log.2011-11-24T23-30-00

   The example results indicate a log rotation performed at exactly
   ``11:30 pm`` on ``November 24th, 2011 UTC``, which is the local time
   offset by the local time zone. The original log file is the one with
   the timestamp. The new log is ``server1.log`` file.

   If you issue a second :dbcommand:`logRotate` command an hour later,
   then an additional file would appear when listing matching files,
   as in the following example:

   .. code-block:: none

      server1.log  server1.log.2011-11-24T23-30-00  server1.log.2011-11-25T00-30-00

   This operation does not modify the
   ``server1.log.2011-11-24T23-30-00`` file created earlier, while
   ``server1.log.2011-11-25T00-30-00`` is the previous ``server1.log``
   file, renamed. ``server1.log`` is a new, empty file that receives
   all new log output.

Syslog Log Rotation
-------------------

.. versionadded:: 2.2

To configure mongod to send log data to syslog rather than writing log
data to a file, use the following procedure.

1. Start a :program:`mongod` with the :setting:`syslog` option.

#. Store and rotate the log output using your system's default log
   rotation mechanism.

.. important:: You cannot use :setting:`syslog` with :setting:`systemLog.path`.
===========================================
Schedule Backup Window for Sharded Clusters
===========================================

.. default-domain:: mongodb

Overview
--------

In a :term:`sharded cluster`, the balancer process is responsible for
distributing sharded data around the cluster, so that each
:term:`shard` has roughly the same amount of data.

However, when creating backups from a sharded cluster it is important
that you disable the balancer while taking backups to ensure that no
chunk migrations affect the content of the backup captured by the
backup procedure. Using the procedure outlined in the section
:ref:`sharding-balancing-disable-temporarily` you can manually stop the
balancer process temporarily. As an alternative you can
use this procedure to define a balancing window so that the balancer
is always disabled during your automated backup operation.

Procedure
---------

If you have an automated backup schedule, you can disable all
balancing operations for a period of time. For instance, consider the
following command:

.. code-block:: javascript

   use config
   db.settings.update( { _id : "balancer" }, { $set : { activeWindow : { start : "6:00", stop : "23:00" } } }, true )

This operation configures the balancer to run between 6:00am and
11:00pm, server time. Schedule your backup operation to run *and
complete* outside of this time. Ensure that the backup can complete
outside the window when the balancer is running *and* that the
balancer can effectively balance the collection among the shards
in the window allotted to each.
===========================================
Shard a Collection Using a Hashed Shard Key
===========================================

.. default-domain:: mongodb

.. versionadded:: 2.4

:ref:`Hashed shard keys <sharding-hashed-sharding>` use a :ref:`hashed
index <index-hashed-index>` of a field as the :term:`shard key` to
partition data across your sharded cluster.

For suggestions on choosing the right field as your hashed shard key, see
:ref:`sharding-hashed-sharding`. For limitations on hashed indexes, see
:ref:`index-hashed-index`.

.. note:: If chunk migrations are in progress while creating a hashed
   shard key collection, the initial chunk distribution may be
   uneven until the balancer automatically balances the
   collection.

Shard the Collection
--------------------

To shard a collection using a hashed shard key, use an operation in
the :program:`mongo` that resembles the following:

.. code-block:: javascript

   sh.shardCollection( "records.active", { a: "hashed" } )

This operation shards the ``active`` collection in the ``records``
database, using a hash of the ``a`` field as the shard key.

Specify the Initial Number of Chunks
------------------------------------

If you shard an empty collection using a hashed shard key, MongoDB
automatically creates and migrates empty chunks so that each shard
has two chunks. To control how many chunks MongoDB creates when
sharding the collection, use :dbcommand:`shardCollection` with the
``numInitialChunks`` parameter.

.. important:: MongoDB 2.4 adds support for hashed shard keys. After
   sharding a collection with a hashed shard key, you must use the
   MongoDB 2.4 or higher :program:`mongos` and :program:`mongod`
   instances in your sharded cluster.

.. include:: /includes/warning-hashed-index-floating-point.rst
=======================
Shard GridFS Data Store
=======================

.. default-domain:: mongodb

When sharding a :term:`GridFS` store, consider the following:

``files`` Collection
--------------------

Most deployments will not need to shard the ``files``
collection. The ``files`` collection is typically small, and only
contains metadata. None of the required keys for GridFS lend
themselves to an even distribution in a sharded situation. If you
*must* shard the ``files`` collection, use the ``_id`` field
possibly in combination with an application field.

Leaving ``files`` unsharded means that all the file metadata
documents live on one shard. For production GridFS stores you *must*
store the ``files`` collection on a replica set.

``chunks`` Collection
---------------------

To shard the ``chunks`` collection by ``{ files_id : 1 , n : 1 }``,
issue commands similar to the following:

.. code-block:: javascript

   db.fs.chunks.ensureIndex( { files_id : 1 , n : 1 } )

   db.runCommand( { shardCollection : "test.fs.chunks" , key : { files_id : 1 , n : 1 } } )

You may also want to shard using just the ``file_id`` field, as in
the following operation:

.. code-block:: javascript

   db.runCommand( { shardCollection : "test.fs.chunks" , key : {  files_id : 1 } } )

.. important:: ``{ files_id : 1 , n : 1 }`` and ``{  files_id : 1 }``
   are the **only** supported shard keys for the ``chunks`` collection
   of a GridFS store.

.. note::

   .. versionchanged:: 2.2

   Before 2.2, you had to create an additional index on ``files_id``
   to shard using *only* this field.

The default ``files_id`` value is an :term:`ObjectId`, as a result
the values of ``files_id`` are always ascending, and applications
will insert all new GridFS data to a single chunk and shard.  If
your write load is too high for a single server to handle, consider
a different shard key or use a different value
for ``_id`` in the ``files`` collection.
.. _index-sort:
.. _sorting-with-indexes:

=================================
Use Indexes to Sort Query Results
=================================

.. default-domain:: mongodb

In MongoDB sort operations that sort documents based on an indexed
field provide the greatest performance. Indexes in MongoDB, as in
other databases, have an order: as a result, using an index to access
documents returns in the same order as the index.

To sort on multiple fields, create a :ref:`compound index
<index-type-compound>`. With compound indexes, the results can be in
the sorted order of either the full index or an index prefix. An index
prefix is a subset of a compound index; the subset consists of one or
more fields at the start of the index, in order. For example, given an
index ``{ a:1, b: 1, c: 1, d: 1 }``, the following subsets are index
prefixes:

.. code-block:: javascript

   { a: 1 }
   { a: 1, b: 1 }
   { a: 1, b: 1, c: 1 }

For more information on sorting by index prefixes, see
:ref:`sort-index-prefix`.

If the query includes **equality** match conditions on an index prefix,
you can sort on a subset of the index that starts after or overlaps with
the prefix. For example, given an index ``{ a: 1, b: 1, c: 1, d: 1 }``,
if the query condition includes equality match conditions on ``a`` and
``b``, you can specify a sort on the subsets ``{ c: 1 }`` or ``{ c: 1,
d: 1 }``:

.. code-block:: javascript

   db.collection.find( { a: 5, b: 3 } ).sort( { c: 1 } )
   db.collection.find( { a: 5, b: 3 } ).sort( { c: 1, d: 1 } )

In these operations, the equality match and the sort documents together
cover the index prefixes ``{ a: 1, b: 1, c: 1 }`` and ``{ a: 1, b: 1,
c: 1, d: 1 }`` respectively.

You can also specify a sort order that includes the prefix; however,
since the query condition specifies equality matches on these fields,
they are constant in the resulting documents and do not contribute to
the sort order:

.. code-block:: javascript

   db.collection.find( { a: 5, b: 3 } ).sort( { a: 1, b: 1, c: 1 } )
   db.collection.find( { a: 5, b: 3 } ).sort( { a: 1, b: 1, c: 1, d: 1 } )

For more information on sorting by index subsets that are not prefixes,
see :ref:`sort-equality-match`.

.. note::

   For in-memory sorts that do not use an index, the :method:`sort()
   <cursor.sort()>` operation is significantly slower. The
   :method:`~cursor.sort()` operation will abort when it uses 32
   megabytes of memory.

Sort With a Subset of Compound Index
------------------------------------

If the sort document contains a subset of the compound index fields,
the subset can determine whether MongoDB can use the index efficiently
to both retrieve and sort the query results. If MongoDB can efficiently
use the index to both retrieve and sort the query results, the output
from the :method:`~cursor.explain()` will display
:data:`~explain.scanAndOrder` as ``false`` or ``0``. If MongoDB can
only use the index for retrieving documents that meet the query
criteria, MongoDB must manually sort the resulting documents without
the use of the index. For in-memory sort operations,
:method:`~cursor.explain()` will display :data:`~explain.scanAndOrder`
as ``true`` or ``1``.

.. _sort-index-prefix:

Sort Subset Starts at the Index Beginning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the sort document is a subset of a compound index and starts from
the beginning of the index, MongoDB can use the index to both retrieve
and sort the query results.

For example, the collection ``collection`` has the following index:

.. code-block:: javascript

   { a: 1, b: 1, c: 1, d: 1 }

The following operations include a sort with a subset of the index.
Because the sort subset starts at beginning of the index, the
operations can use the index for both the query retrieval and sort:

.. code-block:: javascript

   db.collection.find().sort( { a:1 } )
   db.collection.find().sort( { a:1, b:1 } )
   db.collection.find().sort( { a:1, b:1, c:1 } )

   db.collection.find( { a: 4 } ).sort( { a: 1, b: 1 } )
   db.collection.find( { a: { $gt: 4 } } ).sort( { a: 1, b: 1 } )

   db.collection.find( { b: 5 } ).sort( { a: 1, b: 1 } )
   db.collection.find( { b: { $gt:5 }, c: { $gt: 1 } } ).sort( { a: 1, b: 1 } )

The last two operations include query conditions on the field ``b`` but
does not include a query condition on the field ``a``:

.. code-block:: javascript

   db.collection.find( { b: 5 } ).sort( { a: 1, b: 1 } )
   db.collection.find( { b: { $gt:5 }, c: { $gt: 1 } } ).sort( { a: 1, b: 1 } )

Consider the case where the collection has the index ``{ b: 1 }`` in
addition to the ``{ a: 1, b: 1, c: 1, d: 1 }`` index. Because of the
query condition on ``b``, it is not immediately obvious which index
MongoDB may select as the "best" index. To explicitly specify the index
to use, see :method:`~cursor.hint()`.

.. _sort-equality-match:

Sort Subset Does Not Start at the Index Beginning
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The sort document can be a subset of a compound index that does **not**
start from the beginning of the index. For instance, ``{ c: 1 }`` is a
subset of the index ``{ a: 1, b: 1, c: 1, d: 1 }`` that omits the
preceding index fields ``a`` and ``b``. MongoDB can use the index
efficiently **if** the query document includes all the preceding
fields of the index, in this case ``a`` and ``b``, in **equality**
conditions. In other words, the equality conditions in the query
document and the subset in the sort document **contiguously** cover a
prefix of the index.

For example, the collection ``collection`` has the following index:

.. code-block:: javascript

   { a: 1, b: 1, c: 1, d: 1 }

Then following operations can use the index efficiently:

.. code-block:: javascript

   db.collection.find( { a: 5 } ).sort( { b: 1, c: 1 } )
   db.collection.find( { a: 5, c: 4, b: 3 } ).sort( { d: 1 } )

- In the first operation, the query document ``{ a: 5 }`` with the sort
  document ``{ b: 1, c: 1 }`` cover the prefix ``{ a:1 , b: 1, c: 1 }``
  of the index.

- In the second operation, the query document ``{ a: 5, c: 4, b: 3 }``
  with the sort document ``{ d: 1 }`` covers the full index.

Only the index fields preceding the sort subset must have the equality
conditions in the query document. The other index fields may have other
conditions. The following operations can efficiently use the index
since the equality conditions in the query document and the subset in
the sort document **contiguously** cover a prefix of the index:

.. code-block:: javascript

   db.collection.find( { a: 5, b: 3 } ).sort( { c: 1 } )
   db.collection.find( { a: 5, b: 3, c: { $lt: 4 } } ).sort( { c: 1 } )

The following operations specify a sort document of ``{ c: 1 }``, but
the query documents do not contain equality matches on the
**preceding** index fields ``a`` and ``b``:

.. code-block:: javascript

   db.collection.find( { a: { $gt: 2 } } ).sort( { c: 1 } )
   db.collection.find( { c: 5 } ).sort( { c: 1 } )

These operations **will not** efficiently use the index ``{ a: 1, b: 1,
c: 1, d: 1 }`` and may not even use the index to retrieve the documents.
=================================
Specify a Language for Text Index
=================================

.. default-domain:: mongodb

This tutorial describes how to :ref:`specify the default language
associated with the text index <specify-default-language-text-index>`
and also how to :ref:`create text indexes for collections that contain
documents in different languages
<select-from-multiple-languages-for-text-index>`.

.. _specify-default-language-text-index:

Specify the Default Language for a ``text`` Index
-------------------------------------------------

The default language associated with the indexed data determines the
rules to parse word roots (i.e. stemming) and ignore stop words. The
default language for the indexed data is ``english``.

To specify a different language, use the ``default_language`` option
when creating the ``text`` index. See :ref:`text-search-languages` for
the languages available for ``default_language``.

The following example creates for the ``quotes`` collection a ``text``
index on the ``content`` field and sets the ``default_language`` to
``spanish``:

.. code-block:: javascript

   db.quotes.ensureIndex(
      { content : "text" },
      { default_language: "spanish" }
   )

.. _select-from-multiple-languages-for-text-index:

Create a ``text`` Index for a Collection in Multiple Languages
--------------------------------------------------------------

.. versionchanged:: 2.6

   Added support for language overrides within sub-documents.

Specify the Index Language within the Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If a collection contains documents or sub-documents that are in
different languages, include a field named ``language`` in the
documents or sub-documents and specify as its value the language for
that document or sub-document.

MongoDB will use the specified language for that document or
sub-document when building the ``text`` index:

- The specified language in the document overrides the default language
  for the ``text`` index.

- The specified language in a sub-document override the language
  specified in an enclosing document or the default language for the
  index.

See :ref:`text-search-languages` for a list of supported languages.

For example, a collection ``quotes`` contains multi-language documents
that include the ``language`` field in the document and/or the
sub-document as needed:

.. code-block:: javascript

   {
      _id: 1,
      language: "portuguese",
      original: "A sorte protege os audazes.",
      translation:
        [
           {
              language: "english",
              quote: "Fortune favors the bold."
           },
           {
              language: "spanish",
              quote: "Suerte protege a los audaces."
           }
       ]
   }
   {
      _id: 2,
      language: "spanish",
      original: "Nada hay más surreal que la realidad.",
      translation:
         [
           {
             language: "english",
             quote: "There is nothing more surreal than reality."
           },
           {
             language: "french",
             quote: "Il n'ya rien de plus surréaliste que la réalité."
           }
         ]
   }
   {
      _id: 3,
      original: "is this a dagger which I see before me.",
      translation:
      {
         language: "spanish",
         quote: "Es este un puñal que veo delante de mí."
      }
   }

If you create a ``text`` index on the ``quote`` field with the default
language of English.

.. code-block:: javascript

   db.quotes.ensureIndex( { original: "text", "translation.quote": "text" } )

Then, for the documents and subdocuments that contain the ``language``
field, the ``text`` index uses that language to parse word stems and
other linguistic characteristics.

For sub-documents that do not contain the ``language`` field,

- If the enclosing document contains the ``language`` field, then the
  index uses the document's language for the sub-document.

- Otherwise, the index uses the default language for the sub-documents.

For documents that do not contain the ``language`` field, the index
uses the default language, which is English.

Use any Field to Specify the Language for a Document
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To use a field with a name other than ``language``, include
the ``language_override`` option when creating the index.

For example, give the following command to use ``idioma`` as the field
name instead of ``language``:

.. code-block:: javascript

   db.quotes.ensureIndex( { quote : "text" },
                          { language_override: "idioma" } )

The documents of the ``quotes`` collection may specify a language with
the ``idioma`` field:

.. code-block:: javascript

   { _id: 1, idioma: "portuguese", quote: "A sorte protege os audazes" }
   { _id: 2, idioma: "spanish", quote: "Nada hay más surreal que la realidad." }
   { _id: 3, idioma: "english", quote: "is this a dagger which I see before me" }
=================================
Split Chunks in a Sharded Cluster
=================================

.. default-domain:: mongodb

Normally, MongoDB splits a :term:`chunk` after an insert if the chunk
exceeds the maximum :ref:`chunk size <sharding-chunk-size>`. However,
you may want to split chunks manually if:

- you have a large amount of data in your cluster and very few
  :term:`chunks <chunk>`, as is the case after deploying a cluster using
  existing data.

- you expect to add a large amount of data that would initially reside
  in a single chunk or shard. For example, you plan to insert a large
  amount of data with :term:`shard key` values between ``300`` and
  ``400``, *but* all values of your shard keys are between ``250`` and
  ``500`` are in a single chunk.

.. note::

   .. versionadded:: 2.6
      MongoDB provides the :dbcommand:`mergeChunks` command
      to combine contiguous chunk ranges into a single chunk. See
      :doc:`/tutorial/merge-chunks-in-sharded-cluster` for more
      information.

The :term:`balancer` may migrate recently split chunks to a new shard
immediately if :program:`mongos` predicts future insertions will benefit
from the move. The balancer does not distinguish between chunks split
manually and those split automatically by the system.

.. include:: /includes/warning-splitting-chunks.rst

Use :method:`sh.status()` to determine the current chunk ranges across
the cluster.

To split chunks manually, use the :dbcommand:`split` command with either
fields ``middle`` or ``find``. The :program:`mongo` shell provides the
helper methods :method:`sh.splitFind()` and :method:`sh.splitAt()`.

:method:`~sh.splitFind()` splits the chunk that contains the *first*
document returned that matches this query into two equally sized chunks.
You must specify the full namespace (i.e. "``<database>.<collection>``")
of the sharded collection to :method:`~sh.splitFind()`. The query in
:method:`~sh.splitFind()` does not need to use the shard key, though it
nearly always makes sense to do so.

.. example::

   The following command splits the chunk that contains the value of
   ``63109`` for the ``zipcode`` field in the ``people`` collection of
   the ``records`` database:

   .. code-block:: javascript

      sh.splitFind( "records.people", { "zipcode": "63109" } )

Use :method:`~sh.splitAt()` to split a chunk in two, using the queried
document as the lower bound in the new chunk:

.. example::

   The following command splits the chunk that contains the value of
   ``63109`` for the ``zipcode`` field in the ``people`` collection of
   the ``records`` database.

   .. code-block:: javascript

      sh.splitAt( "records.people", { "zipcode": "63109" } )

.. note:: :method:`~sh.splitAt()` does not necessarily split the chunk
   into two equally sized chunks. The split occurs at the location of
   the document matching the query, regardless of where that document is
   in the chunk.
=========================================
Store a JavaScript Function on the Server
=========================================

.. default-domain:: mongodb

.. note::

   We do **not** recommend using server-side stored functions if
   possible.

There is a special system collection named ``system.js`` that can store
JavaScript functions for reuse.

To store a function, you can use the :method:`db.collection.save()`, as
in the following example:

.. code-block:: javascript

   db.system.js.save(
      {
        _id : "myAddFunction" ,
        value : function (x, y){ return x + y; }
      }
   );

- The ``_id`` field holds the name of the function and is unique per
  database.

- The ``value`` field holds the function definition

Once you save a function in the ``system.js`` collection, you can use
the function from any JavaScript context (e.g. :dbcommand:`eval`
command or the :program:`mongo` shell method :method:`db.eval()`,
:query:`$where` operator, :dbcommand:`mapReduce` or :program:`mongo`
shell method :method:`db.collection.mapReduce()`).

Consider the following example from the :program:`mongo` shell that
first saves a function named ``echoFunction`` to the ``system.js``
collection and calls the function using :method:`db.eval()`
method:

.. code-block:: javascript

   db.system.js.save(
                      { _id: "echoFunction",
                        value : function(x) { return x; }
                      }
                    )

   db.eval( "echoFunction( 'test' )" )

See `<http://github.com/mongodb/mongo/tree/master/jstests/storefunc.js>`_ for a full example.

.. versionadded:: 2.1
   In the :program:`mongo` shell, you can use
   :method:`db.loadServerScripts()` to load all the scripts saved in
   the ``system.js`` collection for the current database. Once loaded, you
   can invoke the functions directly in the shell, as in the following
   example:

.. code-block:: javascript

   db.loadServerScripts();

   echoFunction(3);

   myAddFunction(3, 5);
============================
Terminate Running Operations
============================

.. default-domain:: mongodb

Overview
--------

MongoDB provides two facilitates to terminate running operations:
:method:`~cursor.maxTimeMS()` and :method:`db.killOp()`. Use these
operations as needed to control the behavior of operations in a
MongoDB deployment.

Available Procedures
--------------------

``maxTimeMS``
~~~~~~~~~~~~~

.. versionadded:: 2.6

The :method:`~cursor.maxTimeMS()` method sets a time limit for an
operation. When the operation reaches the specified time limit,
MongoDB interrupts the operation at the next :term:`interrupt point`.

Terminate a Query
`````````````````

From the :program:`mongo` shell, use the following method to set a
time limit of 30 milliseconds for this query:

.. code-block:: javascript

   db.location.find( { "town": { "$regex": "(Pine Lumber)",
                                 "$options": 'i' } } ).maxTimeMS(30)

Terminate a Command
```````````````````

Consider a potentially long running operation using
:dbcommand:`distinct` to return each distinct``collection`` field that
has a ``city`` key:

.. code-block:: javascript

   db.runCommand( { distinct: "collection",
                    key: "city" } )

You can add the ``maxTimeMS``  field to the command document to set a
time limit of 30 milliseconds for the operation:

.. code-block:: javascript

   db.runCommand( { distinct: "collection",
                    key: "city",
                    maxTimeMS: 45 } )

:method:`db.getLastError()` and :method:`db.getLastErrorObj()` will return
errors for interrupted options:

.. code-block:: javascript

   { "n" : 0,
     "connectionId" : 1,
     "err" : "operation exceeded time limit",
     "ok" : 1 }

``killOp``
~~~~~~~~~~

The :method:`db.killOp()` method interrupts a running operation at
the next :term:`interrupt point`. :method:`db.killOp()` identifies
the target operation by operation ID.

.. code-block:: javascript

   db.killOp(<opId>)

.. related:: To return a list of running operations see
   :method:`db.currentOp()`.
=======================================
Text Search in the Aggregation Pipeline
=======================================

.. default-domain:: mongodb

.. versionadded:: 2.6

.. _text-agg-expression-behavior:

In the aggregation pipeline, text search is available via the use of
the :query:`$text` query operator in the :pipeline:`$match` stage.

Restrictions
~~~~~~~~~~~~

Text search in the aggregation pipeline has the following restrictions:

- The :pipeline:`$match` stage that includes a :query:`$text` must be
  the **first** stage in the pipeline.

- A :query:`text` operator can only occur once in the stage.

- The :query:`text` operator expression cannot appear in
  :expression:`$or` or :expression:`$not` expressions.

- The text search, by default, does not return the matching documents
  in order of matching scores. Use the :expression:`$meta` aggregation
  expression in the :pipeline:`$sort` stage.

.. Since $geoNear needs to be the first stage in pipeline,
   not documenting that you cannot include the
   $text operation + some other operation that requires a special index.
   Although, if either of the two no longer needs to be the first
   stage, then will need to include.

.. |meta-object| replace:: :expression:`$meta` aggregation
.. |sort-object| replace:: :pipeline:`$sort` pipeline

Text Score
~~~~~~~~~~

.. include:: /includes/fact-text-search-score.rst

The metadata is only available after the :pipeline:`$match` stage that
includes the :query:`$text` operation.

.. _text-search-examples:

Examples
--------

The following examples assume a collection ``articles`` that has a text
index on the field ``subject``:

.. code-block:: javascript

   db.articles.ensureIndex( { subject: "text" } )

Calculate the Total Views for Articles that Contains a Word
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation searches for the term ``cake`` in the
:pipeline:`$match` stage and calculates the total ``views`` for the
matching documents in the :pipeline:`$group` stage.

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "cake" } } },
        { $group: { _id: null, views: { $sum: "$views" } } }
      ]
   )

Return Results Sorted by Text Search Score
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To sort by the text search score, include a :expression:`$meta`
expression in the :pipeline:`$sort` stage. The following example
matches on *either* the term ``cake`` or ``tea``, sorts by the
``textScore`` in descending order, and returns only the ``title`` field
in the results set.

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "cake tea" } } },
        { $sort: { score: { $meta: "textScore" } } },
        { $project: { title: 1, _id: 0 } }
      ]
   )

The specified metadata determines the sort order. For example, the
``"textScore"`` metadata sorts in descending order. See
:expression:`$meta` for more information on metadata as well as an
example of overriding the default sort order of the metadata.

Match on Text Score
~~~~~~~~~~~~~~~~~~~

The ``"textScore"`` metadata is available for projections, sorts, and
conditions subsequent the :pipeline:`$match` stage that includes the
:query:`$text` operation.

The following example matches on *either* the term ``cake`` or ``tea``,
projects the ``title`` and the ``score`` fields, and then returns only
those documents with a ``score`` greater than ``1.0``.

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "cake tea" } } },
        { $project: { title: 1, _id: 0, score: { $meta: "textScore" } } },
        { $match: { score: { $gt: 1.0 } } }
      ]
   )

Specify a Language for Text Search
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following aggregation searches in spanish for documents that
contain the term ``saber`` but not the term ``claro`` in the
:pipeline:`$match` stage and calculates the total ``views`` for the
matching documents in the :pipeline:`$group` stage.

.. code-block:: javascript

   db.articles.aggregate(
      [
        { $match: { $text: { $search: "saber -claro", $language: "es" } } },
        { $group: { _id: null, views: { $sum: "$views" } } }
      ]
   )
=============================================
Troubleshoot Kerberos Authentication on Linux
=============================================

.. default-domain:: mongodb

.. versionadded:: 2.4

.. _kerberos-troubleshooting-checklist:

Kerberos Configuration Checklist
--------------------------------

If you have difficulty starting :program:`mongod` or :program:`mongos`
with :doc:`Kerberos </core/kerberos>` on Linux systems, ensure that:

- The :program:`mongod` and the :program:`mongos` binaries are
  from MongoDB Enterprise.

- You are not using the :ecosystem:`HTTP Console
  </tools/http-interface/#http-console>`. MongoDB Enterprise
  does not support Kerberos authentication over the HTTP Console
  interface.

- Either the service principal name (SPN) in the :ref:`keytab file
  <keytab-files>` matches the SPN for the :program:`mongod` or
  :program:`mongos` instance, or the :program:`mongod` or the
  :program:`mongos` instance use the :parameter:`--setParameter
  saslHostName=\<host name\> <saslHostName>` to match the name in the
  keytab file.

- The canonical system hostname of the system that runs the
  :program:`mongod` or :program:`mongos` instance is a resolvable,
  fully qualified domain for this host. You can test the system
  hostname resolution with the ``hostname -f`` command at the system
  prompt.

- Each host that runs a :program:`mongod` or :program:`mongos` instance
  has both the ``A`` and ``PTR`` DNS records to provide forward and
  reverse lookup. The records allow the host to resolve the components
  of the Kerberos infrastructure.

- Both the Kerberos Key Distribution Center (KDC) and the system
  running :program:`mongod` instance or :program:`mongos` must be able
  to resolve each other using DNS. By default, Kerberos attempts to
  resolve hosts using the content of the ``/etc/kerb5.conf`` before
  using DNS to resolve hosts.

- The time synchronization of the systems running :program:`mongod` or
  the :program:`mongos` instances and the Kerberos infrastructure are
  within the maximum time skew (default is 5 minutes) of each other.
  Time differences greater than the maximum time skew will prevent
  successful authentication.

Debug with More Verbose Logs
----------------------------

If you still encounter problems with Kerberos on Linux, you can start
both :program:`mongod` and :program:`mongo` (or another client) with
the environment variable ``KRB5_TRACE`` set to different files to
produce more verbose logging of the Kerberos process to help further
troubleshooting. For example, the following starts a standalone
:program:`mongod` with ``KRB5_TRACE`` set:

.. code-block:: sh

   env KRB5_KTNAME=/opt/mongodb/mongod.keytab \
       KRB5_TRACE=/opt/mongodb/log/mongodb-kerberos.log \
       /opt/mongodb/bin/mongod --dbpath /opt/mongodb/data \
       --fork --logpath /opt/mongodb/log/mongod.log \
       --auth --setParameter authenticationMechanisms=GSSAPI

Common Error Messages
---------------------

In some situations, MongoDB will return error messages from the GSSAPI
interface if there is a problem with the Kerberos service. Some common
error messages are:

``GSSAPI error in client while negotiating security context.``
  This error occurs on the client and reflects insufficient credentials
  or a malicious attempt to authenticate.

  If you receive this error, ensure that you are using the correct
  credentials and the correct fully qualified domain name when
  connecting to the host.

``GSSAPI error acquiring credentials.``
  This error occurs during the start of the :program:`mongod` or
  :program:`mongos` and reflects improper configuration of the system
  hostname or a missing or incorrectly configured keytab file.

  If you encounter this problem, consider the items in the
  :ref:`kerberos-troubleshooting-checklist`, in particular, whether the
  SPN in the :ref:`keytab file <keytab-files>` matches the SPN for the
  :program:`mongod` or :program:`mongos` instance.

  To determine whether the SPNs match:

  #. Examine the keytab file, with the following command:

     .. code-block:: sh

        klist -k <keytab>

     Replace ``<keytab>`` with the path to your keytab file.

  #. Check the configured hostname for your system, with the following
     command:

     .. code-block:: sh

        hostname -f

     Ensure that this name matches the name in the keytab file, or
     start :program:`mongod` or :program:`mongos` with the
     :parameter:`--setParameter saslHostName=\<hostname\> <saslHostName>`.

.. seealso::

   - :doc:`/core/kerberos`

   - :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`

   - :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
=============================
Troubleshoot the Map Function
=============================

.. default-domain:: mongodb

The ``map`` function is a JavaScript function that associates or “maps”
a value with a key and emits the key and value pair during a
:doc:`map-reduce </core/map-reduce>` operation.

To verify the ``key`` and ``value`` pairs emitted by the ``map``
function, write your own ``emit`` function.

Consider a collection ``orders`` that contains documents of the
following prototype:

.. code-block:: javascript

   {
        _id: ObjectId("50a8240b927d5d8b5891743c"),
        cust_id: "abc123",
        ord_date: new Date("Oct 04, 2012"),
        status: 'A',
        price: 250,
        items: [ { sku: "mmm", qty: 5, price: 2.5 },
                 { sku: "nnn", qty: 5, price: 2.5 } ]
   }

#. Define the ``map`` function that maps the ``price`` to the
   ``cust_id`` for each document and emits the ``cust_id`` and ``price``
   pair:

   .. code-block:: javascript

      var map = function() {
          emit(this.cust_id, this.price);
      };

#. Define the ``emit`` function to print the key and value:

   .. code-block:: javascript

      var emit = function(key, value) {
          print("emit");
          print("key: " + key + "  value: " + tojson(value));
      }

#. Invoke the ``map`` function with a single document from the ``orders``
   collection:

   .. code-block:: javascript

      var myDoc = db.orders.findOne( { _id: ObjectId("50a8240b927d5d8b5891743c") } );
      map.apply(myDoc);

#. Verify the key and value pair is as you expected.

   .. code-block:: javascript

      emit
      key: abc123 value:250

#. Invoke the ``map`` function with multiple documents from the ``orders``
   collection:

   .. code-block:: javascript

      var myCursor = db.orders.find( { cust_id: "abc123" } );

      while (myCursor.hasNext()) {
          var doc = myCursor.next();
          print ("document _id= " + tojson(doc._id));
          map.apply(doc);
          print();
      }

#. Verify the key and value pairs are as you expected.

.. seealso::

   The ``map`` function must meet various requirements. For a list of all
   the requirements for the ``map`` function, see :dbcommand:`mapReduce`,
   or the :program:`mongo` shell helper method
   :method:`db.collection.mapReduce()`.
================================
Troubleshoot the Reduce Function
================================

.. default-domain:: mongodb

The ``reduce`` function is a JavaScript function that “reduces” to a
single object all the values associated with a particular key during a
:doc:`map-reduce </core/map-reduce>` operation. The ``reduce`` function
must meet various requirements. This tutorial helps verify that the
``reduce`` function meets the following criteria:

- The ``reduce`` function must return an object whose *type* must be
  **identical** to the type of the ``value`` emitted by the ``map``
  function.

- The order of the elements in the ``valuesArray`` should not affect
  the output of the ``reduce`` function.

- The ``reduce`` function must be *idempotent*.

For a list of all the requirements for the ``reduce`` function, see
:dbcommand:`mapReduce`, or the :program:`mongo` shell helper method
:method:`db.collection.mapReduce()`.

Confirm Output Type
-------------------

You can test that the ``reduce`` function returns a value that is the
same type as the value emitted from the ``map`` function.

#. Define a ``reduceFunction1`` function that takes the arguments
   ``keyCustId`` and ``valuesPrices``. ``valuesPrices`` is an array of
   integers:

   .. code-block:: javascript

      var reduceFunction1 = function(keyCustId, valuesPrices) {
                                return Array.sum(valuesPrices);
                            };

#. Define a sample array of integers:

   .. code-block:: javascript

      var myTestValues = [ 5, 5, 10 ];

#. Invoke the ``reduceFunction1`` with ``myTestValues``:

   .. code-block:: javascript

      reduceFunction1('myKey', myTestValues);

#. Verify the ``reduceFunction1`` returned an integer:

   .. code-block:: javascript

      20

#. Define a ``reduceFunction2`` function that takes the arguments
   ``keySKU`` and ``valuesCountObjects``. ``valuesCountObjects`` is an array of
   documents that contain two fields ``count`` and ``qty``:

   .. code-block:: javascript

      var reduceFunction2 = function(keySKU, valuesCountObjects) {
                                reducedValue = { count: 0, qty: 0 };

                                for (var idx = 0; idx < valuesCountObjects.length; idx++) {
                                    reducedValue.count += valuesCountObjects[idx].count;
                                    reducedValue.qty += valuesCountObjects[idx].qty;
                                }

                                return reducedValue;
                            };

#. Define a sample array of documents:

   .. code-block:: javascript

      var myTestObjects = [
                            { count: 1, qty: 5 },
                            { count: 2, qty: 10 },
                            { count: 3, qty: 15 }
                          ];

#. Invoke the ``reduceFunction2`` with ``myTestObjects``:

   .. code-block:: javascript

      reduceFunction2('myKey', myTestObjects);

#. Verify the ``reduceFunction2`` returned a document with exactly the
   ``count`` and the ``qty`` field:

   .. code-block:: javascript

      { "count" : 6, "qty" : 30 }

Ensure Insensitivity to the Order of Mapped Values
--------------------------------------------------

The ``reduce`` function takes a ``key`` and a ``values`` array as its
argument. You can test that the result of the ``reduce`` function does
not depend on the order of the elements in the ``values`` array.

#. Define a sample ``values1`` array and a sample ``values2`` array
   that only differ in the order of the array elements:

   .. code-block:: javascript

      var values1 = [
                      { count: 1, qty: 5 },
                      { count: 2, qty: 10 },
                      { count: 3, qty: 15 }
                    ];

      var values2 = [
                      { count: 3, qty: 15 },
                      { count: 1, qty: 5 },
                      { count: 2, qty: 10 }
                    ];

#. Define a ``reduceFunction2`` function that takes the arguments
   ``keySKU`` and ``valuesCountObjects``. ``valuesCountObjects`` is an array of
   documents that contain two fields ``count`` and ``qty``:

   .. code-block:: javascript

      var reduceFunction2 = function(keySKU, valuesCountObjects) {
                                reducedValue = { count: 0, qty: 0 };

                                for (var idx = 0; idx < valuesCountObjects.length; idx++) {
                                    reducedValue.count += valuesCountObjects[idx].count;
                                    reducedValue.qty += valuesCountObjects[idx].qty;
                                }

                                return reducedValue;
                            };

#. Invoke the ``reduceFunction2`` first with ``values1`` and then with
   ``values2``:

   .. code-block:: javascript

      reduceFunction2('myKey', values1);
      reduceFunction2('myKey', values2);

#. Verify the ``reduceFunction2`` returned the same result:

   .. code-block:: javascript

      { "count" : 6, "qty" : 30 }

Ensure Reduce Function Idempotence
----------------------------------

Because the map-reduce operation may call a ``reduce`` multiple times
for the same key, and won't call a ``reduce`` for single instances
of a key in the working set, the ``reduce`` function must return a value of the
same type as the value emitted from the ``map`` function. You can test
that the ``reduce`` function process "reduced" values without
affecting the *final* value.

#. Define a ``reduceFunction2`` function that takes the arguments
   ``keySKU`` and ``valuesCountObjects``. ``valuesCountObjects`` is an array of
   documents that contain two fields ``count`` and ``qty``:

   .. code-block:: javascript

      var reduceFunction2 = function(keySKU, valuesCountObjects) {
                                reducedValue = { count: 0, qty: 0 };

                                for (var idx = 0; idx < valuesCountObjects.length; idx++) {
                                    reducedValue.count += valuesCountObjects[idx].count;
                                    reducedValue.qty += valuesCountObjects[idx].qty;
                                }

                                return reducedValue;
                            };

#. Define a sample key:

   .. code-block:: javascript

      var myKey = 'myKey';

#. Define a sample ``valuesIdempotent`` array that contains an element that is a
   call to the ``reduceFunction2`` function:

   .. code-block:: javascript

      var valuesIdempotent = [
                               { count: 1, qty: 5 },
                               { count: 2, qty: 10 },
                               reduceFunction2(myKey, [ { count:3, qty: 15 } ] )
                             ];

#. Define a sample ``values1`` array that combines the values passed to
   ``reduceFunction2``:

   .. code-block:: javascript

      var values1 = [
                      { count: 1, qty: 5 },
                      { count: 2, qty: 10 },
                      { count: 3, qty: 15 }
                    ];

#. Invoke the ``reduceFunction2`` first with ``myKey`` and
   ``valuesIdempotent`` and then with ``myKey`` and ``values1``:

   .. code-block:: javascript

      reduceFunction2(myKey, valuesIdempotent);
      reduceFunction2(myKey, values1);

#. Verify the ``reduceFunction2`` returned the same result:

   .. code-block:: javascript

      { "count" : 6, "qty" : 30 }
=========================
Troubleshoot Replica Sets
=========================

.. default-domain:: mongodb

This section describes common strategies for troubleshooting
:term:`replica set` deployments.

.. _replica-set-troubleshooting-check-replication-status:

Check Replica Set Status
------------------------

To display the current state of the replica set and current state of
each member, run the :method:`rs.status()` method in a :program:`mongo`
shell connected to the replica set's :term:`primary`. For descriptions
of the information displayed by :method:`rs.status()`, see
:doc:`/reference/command/replSetGetStatus`.

.. note::

   The :method:`rs.status()` method is a wrapper that runs the
   :dbcommand:`replSetGetStatus` database command.

.. _replica-set-replication-lag:

Check the Replication Lag
-------------------------

Replication lag is a delay between an operation on the :term:`primary`
and the application of that operation from the :term:`oplog` to the
:term:`secondary`. Replication lag can be a significant issue and can
seriously affect MongoDB :term:`replica set` deployments. Excessive
replication lag makes "lagged" members ineligible to quickly become
primary and increases the possibility that distributed read operations
will be inconsistent.

To check the current length of replication lag:

- In a :program:`mongo` shell connected to the primary, call the
  :method:`rs.printSlaveReplicationInfo()` method.

  The returned document displays the ``syncedTo`` value for each member,
  which shows you when each member last read from the oplog, as shown in the following
  example:

  .. code-block:: javascript

     source:   m1.example.net:30001
         syncedTo: Tue Oct 02 2012 11:33:40 GMT-0400 (EDT)
             = 7475 secs ago (2.08hrs)
     source:   m2.example.net:30002
         syncedTo: Tue Oct 02 2012 11:33:40 GMT-0400 (EDT)
             = 7475 secs ago (2.08hrs)

  .. note::

     The :method:`rs.status()` method is a wrapper around the
     :dbcommand:`replSetGetStatus` database command.

- Monitor the rate of replication by watching the oplog time in the
  "replica" graph in the `MongoDB Management Service`_. For more
  information see the `documentation for MMS`_.

.. _`MongoDB Management Service`: http://mms.mongodb.com/
.. _`documentation for MMS`: http://mms.mongodb.com/help/

Possible causes of replication lag include:

- **Network Latency**

  Check the network routes between the members of your set to ensure
  that there is no packet loss or network routing issue.

  Use tools including ``ping`` to test latency between set
  members and ``traceroute`` to expose the routing of packets
  network endpoints.

- **Disk Throughput**

  If the file system and disk device on the secondary is
  unable to flush data to disk as quickly as the primary, then
  the secondary will have difficulty keeping state. Disk-related
  issues are incredibly prevalent on multi-tenant systems, including
  virtualized instances, and can be transient if the system accesses
  disk devices over an IP network (as is the case with Amazon's
  EBS system.)

  Use system-level tools to assess disk status, including
  ``iostat`` or ``vmstat``.

- **Concurrency**

  In some cases, long-running operations on the primary can block
  replication on secondaries. For best results, configure :ref:`write
  concern <write-concern>` to require confirmation of replication to
  secondaries, as described in :ref:`replica set write concern
  <replica-set-write-concern>`. This prevents write operations from
  returning if replication cannot keep up with the write load.

  Use the :term:`database profiler` to see if there are slow queries
  or long-running operations that correspond to the incidences of lag.

- **Appropriate Write Concern**

  If you are performing a large data ingestion or bulk load operation
  that requires a large number of writes to the primary, particularly
  with :ref:`unacknowledged write concern
  <write-concern-unacknowledged>`, the secondaries will not be able to
  read the oplog fast enough to keep up with changes.

  To prevent this, require :ref:`write acknowledgment or journaled
  write concern <write-operations-write-concern>` after every 100,
  1,000, or an another interval to provide an opportunity for
  secondaries to catch up with the primary.

  For more information see:

  - :ref:`Replica Acknowledge Write Concern <replica-set-write-concern>`
  - :ref:`Replica Set Write Concern <write-operations-replica-sets>`
  - :ref:`replica-set-oplog-sizing`

.. _replica-set-troubleshooting-check-connection:

Test Connections Between all Members
------------------------------------

All members of a :term:`replica set` must be able to connect to every
other member of the set to support replication. Always verify
connections in both "directions."  Networking topologies and firewall
configurations prevent normal and required connectivity, which can
block replication.

Consider the following example of a bidirectional test of networking:

.. example:: Given a replica set with three members running on three separate
   hosts:

   - ``m1.example.net``
   - ``m2.example.net``
   - ``m3.example.net``

   1. Test the connection from ``m1.example.net`` to the other hosts
      with the following operation set ``m1.example.net``:

      .. code-block:: sh

         mongo --host m2.example.net --port 27017

         mongo --host m3.example.net --port 27017

   #. Test the connection from ``m2.example.net`` to the other two
      hosts with the following operation set from ``m2.example.net``,
      as in:

      .. code-block:: sh

         mongo --host m1.example.net --port 27017

         mongo --host m3.example.net --port 27017

      You have now tested the connection between
      ``m2.example.net`` and ``m1.example.net`` in both directions.

   #. Test the connection from ``m3.example.net`` to the other two
      hosts with the following operation set from the
      ``m3.example.net`` host, as in:

      .. code-block:: sh

         mongo --host m1.example.net --port 27017

         mongo --host m2.example.net --port 27017

   If any connection, in any direction fails, check your networking
   and firewall configuration and reconfigure your environment to
   allow these connections.

Socket Exceptions when Rebooting More than One Secondary
--------------------------------------------------------

When you reboot members of a replica set, ensure that the set is able
to elect a primary during the maintenance. This means ensuring that a majority of
the set's ':data:`~local.system.replset.members[n].votes` are
available.

When a set's active members can no longer form a majority, the set's
:term:`primary` steps down and becomes a :term:`secondary`. The former
primary closes all open connections to client applications. Clients
attempting to write to the former primary receive socket exceptions
and *Connection reset* errors until the set can elect a primary.

.. example:: Given a three-member replica set where every member has
   one vote, the set can elect a primary only as long as two members
   can connect to each other. If two you reboot the two secondaries
   once, the primary steps down and becomes a secondary. Until the at
   least one secondary becomes available, the set has no primary and
   cannot elect a new primary.

For more information on votes, see :doc:`/core/replica-set-elections`. For
related information on connection errors, see :ref:`faq-keepalive`.

.. _replica-set-troubleshooting-check-oplog-size:

Check the Size of the Oplog
---------------------------

A larger :term:`oplog` can give a replica set a greater tolerance for
lag, and make the set more resilient.

To check the size of the oplog for a given :term:`replica set` member,
connect to the member in a :program:`mongo` shell and run the
:method:`rs.printReplicationInfo()` method.

The output displays the size of the oplog and the date ranges of the
operations contained in the oplog. In the following example, the oplog
is about 10MB and is able to fit about 26 hours (94400 seconds) of
operations:

.. code-block:: javascript

   configured oplog size:   10.10546875MB
   log length start to end: 94400 (26.22hrs)
   oplog first event time:  Mon Mar 19 2012 13:50:38 GMT-0400 (EDT)
   oplog last event time:   Wed Oct 03 2012 14:59:10 GMT-0400 (EDT)
   now:                     Wed Oct 03 2012 15:00:21 GMT-0400 (EDT)

The oplog should be long enough to hold all transactions for the
longest downtime you expect on a secondary. At a minimum, an oplog
should be able to hold minimum 24 hours of operations; however, many
users prefer to have 72 hours or even a week's work of operations.

For more information on how oplog size affects operations, see:

- :ref:`replica-set-oplog-sizing`,
- :ref:`replica-set-delayed-members`, and
- :ref:`replica-set-replication-lag`.

.. note:: You normally want the oplog to be the same size on all
   members. If you resize the oplog, resize it on all members.

To change oplog size, see the :doc:`/tutorial/change-oplog-size`
tutorial.


Oplog Entry Timestamp Error
---------------------------

.. todo:: link assertion 13290 here once assertion guide exists.

Consider the following error in :program:`mongod` output and logs:

.. code-block:: javascript

   replSet error fatal couldn't query the local local.oplog.rs collection.  Terminating mongod after 30 seconds.
   <timestamp> [rsStart] bad replSet oplog entry?

Often, an incorrectly typed value in the ``ts`` field in the last
:term:`oplog` entry causes this error. The correct data type is
Timestamp.

Check the type of the ``ts`` value using the following two queries
against the oplog collection:

.. code-block:: javascript

   db = db.getSiblingDB("local")
   db.oplog.rs.find().sort({$natural:-1}).limit(1)
   db.oplog.rs.find({ts:{$type:17}}).sort({$natural:-1}).limit(1)

The first query returns the last document in the oplog, while the
second returns the last document in the oplog where the ``ts`` value
is a Timestamp. The :query:`$type` operator allows you to select
:term:`BSON type <BSON types>` 17, is the Timestamp data type.

If the queries don't return the same document, then the last document in
the oplog has the wrong data type in the ``ts`` field.

.. example::

   If the first query returns this as the last oplog entry:

   .. code-block:: javascript

      { "ts" : {t: 1347982456000, i: 1},
        "h" : NumberLong("8191276672478122996"),
        "op" : "n",
        "ns" : "",
        "o" : { "msg" : "Reconfig set", "version" : 4 } }

   And the second query returns this as the last entry where ``ts``
   has the ``Timestamp`` type:

   .. code-block:: javascript

      { "ts" : Timestamp(1347982454000, 1),
        "h" : NumberLong("6188469075153256465"),
        "op" : "n",
        "ns" : "",
        "o" : { "msg" : "Reconfig set", "version" : 3 } }

   Then the value for the ``ts`` field in the last oplog entry is of the
   wrong data type.

To set the proper type for this value and resolve this issue,
use an update operation that resembles the following:

.. code-block:: javascript

   db.oplog.rs.update( { ts: { t:1347982456000, i:1 } },
                       { $set: { ts: new Timestamp(1347982456000, 1)}})

Modify the timestamp values as needed based on your oplog entry. This
operation may take some period to complete because the update must
scan and pull the entire oplog into memory.

Duplicate Key Error on ``local.slaves``
---------------------------------------

The *duplicate key on local.slaves* error, occurs when a
:term:`secondary` or :term:`slave` changes its hostname and the
:term:`primary` or :term:`master` tries to update its ``local.slaves``
collection with the new name. The update fails because it contains the
same ``_id`` value as the document containing the previous hostname. The
error itself will resemble the following.

.. code-block:: none

   exception 11000 E11000 duplicate key error index: local.slaves.$_id_  dup key: { : ObjectId('<object ID>') } 0ms

This is a benign error and does not affect replication operations on
the :term:`secondary` or :term:`slave`.

To prevent the error from appearing, drop the ``local.slaves``
collection from the :term:`primary` or :term:`master`, with the
following sequence of operations in the :program:`mongo` shell:

.. code-block:: javascript

   use local
   db.slaves.drop()

The next time a :term:`secondary` or :term:`slave` polls the
:term:`primary` or :term:`master`, the :term:`primary` or :term:`master`
recreates the ``local.slaves`` collection.
=============================
Troubleshoot Sharded Clusters
=============================

.. default-domain:: mongodb

This section describes common strategies for troubleshooting
:term:`sharded cluster` deployments.

.. _config-database-string-error:

Config Database String Error
----------------------------

.. DOCS-1143

Start all :program:`mongos` instances in a sharded cluster with an identical
:setting:`~sharding.configDB` string. If a :program:`mongos` instance tries to
connect to the sharded cluster with a :setting:`~sharding.configDB` string that
does not *exactly* match the string used by the other :program:`mongos`
instances, including the order of the hosts, the following errors occur:

.. code-block:: none

   could not initialize sharding on connection

And:

.. code-block:: none

   mongos specified a different config database string

To solve the issue, restart the :program:`mongos` with the correct
string.

Cursor Fails Because of Stale Config Data
-----------------------------------------

.. DOCS-425

A query returns the following warning when one or more of the
:program:`mongos` instances has not yet updated its cache of the
cluster's metadata from the :term:`config database`:

.. code-block:: none

   could not initialize cursor across all shards because : stale config detected

This warning *should* not propagate back to your application. The
warning will repeat until all the :program:`mongos` instances refresh
their caches. To force an instance to refresh its cache, run the
:dbcommand:`flushRouterConfig` command.

Avoid Downtime when Moving Config Servers
-----------------------------------------

.. include:: /includes/fact-use-cnames-for-config-servers.rst
=================
Troubleshoot SNMP
=================

.. default-domain:: mongodb

.. versionadded:: 2.6

.. admonition:: Enterprise Feature

   SNMP is only available in MongoDB Enterprise.

Overview
--------

MongoDB Enterprise can report system information into SNMP traps, to
support centralized data collection and aggregation. This document
identifies common problems you may encounter when deploying MongoDB
Enterprise with SNMP as well as possible solutions for these issues.

See :doc:`/tutorial/monitor-with-snmp`
and :doc:`/tutorial/monitor-with-snmp-on-windows` for
complete installation instructions.

Issues
------

Failed to Connect
~~~~~~~~~~~~~~~~~

The following in the :program:`mongod` logfile:

.. code-block:: none

   Warning: Failed to connect to the agentx master agent

AgentX is the SNMP agent extensibility protocol defined in Internet
`RFC 2741 <http://www.ietf.org/rfc/rfc2741.txt>`_. It explains how
to define additional data to monitor over SNMP. When MongoDB fails
to connect to the agentx master agent, use the following procedure to
ensure that the SNMP subagent can connect properly to the SNMP master.

#. Make sure the master agent is running.

#. Compare the SNMP master's configuration file with the subagent
   configuration file. Ensure that the agentx socket definition is
   the same between the two.

#. Check the SNMP configuration files to see if they specify using UNIX
   Domain Sockets. If so, confirm that the :program:`mongod` has
   appropriate permissions to open a UNIX domain socket.

Error Parsing Command Line
~~~~~~~~~~~~~~~~~~~~~~~~~~

One of the following errors at the command line:

.. code-block:: none

   Error parsing command line: unknown option snmp-master
   try 'mongod --help' for more information

.. code-block:: none

   Error parsing command line: unknown option snmp-subagent
   try 'mongod --help' for more information

:program:`mongod` binaries that are not part of the Enterprise
Edition produce this error. :doc:`Install the Enterprise Edition
</administration/install-enterprise>` and attempt to start
:program:`mongod` again.

Other MongoDB binaries, including :program:`mongos` will produce this
error if you attempt to star them with  :setting:`snmp-master` or
:setting:`snmp-subagent`. Only :program:`mongod` supports SNMP.

Error Starting ``SNMPAgent``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The following line in the log file indicates
that :program:`mongod` cannot read the ``mongod.conf`` file:

.. code-block:: none

   [SNMPAgent] warning: error starting SNMPAgent as master err:1

If running on Linux, ensure ``mongod.conf`` exists in the ``/etc/snmp``
directory, and ensure that the :program:`mongod` UNIX user has
permission to read the ``mongod.conf`` file.

If running on Windows, ensure ``mongod.conf`` exists in
``C:\snmp\etc\config``.
============================
Upgrade a Cluster to Use SSL
============================

.. default-domain:: mongodb

.. note::

   .. include:: /includes/fact-ssl-supported.rst

.. versionchanged:: 2.6

The MongoDB server supports listening for both SSL encrypted and
unencrypted connections on the same TCP port. This allows upgrades of
MongoDB clusters to use SSL encrypted connections. To upgrade from a
MongoDB cluster using no SSL encryption to one using *only* SSL
encryption, use the following rolling upgrade process:

#. For each node of a cluster, start the node with the option
   :option:`--sslMode` set to ``allowSSL``. The :option:`--sslMode
   allowSSL <--sslMode>` setting allows the node to accept both SSL
   and non-SSL incoming connections. Its connections to other servers
   do not use SSL. Include other :doc:`SSL options
   </tutorial/configure-ssl>` as well as any other options that are
   required for your specific configuration. For example:

   .. code-block:: sh

      mongod --replSet <name> --sslMode allowSSL --sslPEMKeyFile <path to SSL Certificate and key PEM file> --sslCAFile <path to root CA PEM file>

   Upgrade all nodes of the cluster to these settings.

   .. note::
      You may also specify these options in the :doc:`configuration file
      </reference/configuration-options>`, as in the following example:

      .. code-block:: ini

         sslMode = <disabled|allowSSL|preferSSL|requireSSL>
         sslPEMKeyFile = <path to SSL certificate and key PEM file>
         sslCAFile = <path to root CA PEM file>

#. Switch all clients to use SSL. See :ref:`ssl-clients`.

#. For each node of a cluster, use the :dbcommand:`setParameter`
   command to update the :parameter:`sslMode` to ``preferSSL``.
   [#update-mode-alternative]_ With ``preferSSL`` as its
   :setting:`net.ssl.mode`, the node accepts both SSL and non-SSL incoming
   connections, and its connections to other servers use SSL. For
   example:

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, sslMode: "preferSSL" } )

   Upgrade all nodes of the cluster to these settings.

   At this point, all connections should be using SSL.

#. For each node of the cluster, use the
   :dbcommand:`setParameter` command to update the :parameter:`sslMode`
   to ``requireSSL``. [#update-mode-alternative]_ With ``requireSSL``
   as its :setting:`net.ssl.mode`, the node will reject any non-SSL
   connections. For example:

   .. code-block:: sh

      db.getSiblingDB('admin').runCommand( { setParameter: 1, sslMode: "requireSSL" } )

#. After the upgrade of all nodes, edit the :doc:`configuration file
   </reference/configuration-options>` with the appropriate SSL
   settings to ensure that upon subsequent restarts, the cluster uses
   SSL.

.. [#update-mode-alternative] As an alternative to using the
   :dbcommand:`setParameter` command, you can also
   restart the nodes with the appropriate SSL options and values.
=========================================
Upgrade to the Latest Revision of MongoDB
=========================================

.. default-domain:: mongodb

Revisions provide security patches, bug fixes, and new or changed
features that do not contain any backward breaking changes. Always
upgrade to the latest revision in your release series. The third number
in the :ref:`MongoDB version number <release-version-numbers>` indicates
the revision.

.. _upgrade-options:

Before Upgrading
----------------

- Ensure you have an up-to-date backup of your data set. See
  :doc:`/core/backups`.

- Consult the following documents for any special considerations or
  compatibility issues specific to your MongoDB release:

  - The release notes, located at :doc:`/release-notes`.

  - The documentation for your driver. See :doc:`/applications/drivers`.

- If your installation includes :term:`replica sets <replica set>`, plan
  the upgrade during a predefined maintenance window.

  .. TODO check: Upgrading a replica set typically involves 10-20 seconds of downtime.

- Before you upgrade a production environment, use the procedures in
  this document to upgrade a *staging* environment that reproduces your
  production environment, to ensure that your production configuration
  is compatible with all changes.

.. _upgrade-procedure:

Upgrade Procedure
-----------------

.. important:: Always backup all of your data before upgrading MongoDB.

Upgrade each :program:`mongod` and :program:`mongos` binary
separately, using the procedure described here. When upgrading a binary,
use the procedure :ref:`upgrade-mongodb-instance`.

Follow this upgrade procedure:

1. For deployments that use authentication, first upgrade all of your
   MongoDB :doc:`drivers </applications/drivers>`. To upgrade, see the
   documentation for your driver.

#. Upgrade sharded clusters, as described in
   :ref:`upgrade-sharded-cluster`.

#. Upgrade any standalone instances. See :ref:`upgrade-mongodb-instance`.

#. Upgrade any replica sets that are not part of a sharded cluster, as
   described in :ref:`upgrade-replica-set`.

.. _upgrade-mongodb-instance:

Upgrade a MongoDB Instance
--------------------------

To upgrade a :program:`mongod` or :program:`mongos` instance, use one
of the following approaches:

- Upgrade the instance using the operating system's package management
  tool and the official MongoDB packages. This is the preferred
  approach. See :doc:`/installation`.

- Upgrade the instance by replacing the existing binaries with new
  binaries. See :ref:`upgrade-replace-binaries`.

.. _upgrade-replace-binaries:

Replace the Existing Binaries
-----------------------------

.. important:: Always backup all of your data before upgrading MongoDB.

This section describes how to upgrade MongoDB by replacing the existing
binaries. The preferred approach to an upgrade is to use the operating
system's package management tool and the official MongoDB packages, as
described in :doc:`/installation`.

To upgrade a :program:`mongod` or :program:`mongos` instance by
replacing the existing binaries:

1. Download the binaries for the latest MongoDB revision from the
   `MongoDB Download Page`_ and store the binaries in a temporary
   location. The binaries download as compressed files that uncompress
   to the directory structure used by the MongoDB installation.

#. Shutdown the instance.

#. Replace the existing MongoDB binaries with the downloaded binaries.

#. Restart the instance.

.. _`MongoDB Download Page`: http://downloads.mongodb.org/

.. _upgrade-sharded-cluster:

Upgrade Sharded Clusters
------------------------

To upgrade a sharded cluster:

1. Disable the cluster's balancer, as described in
   :ref:`sharding-balancing-disable-temporarily`.

#. Upgrade each :program:`mongos` instance by following the instructions
   below in :ref:`upgrade-mongodb-instance`. You can upgrade the
   :program:`mongos` instances in any order.

#. Upgrade each :program:`mongod` :ref:`config server
   <sharding-config-server>` individually starting with the last config
   server listed in your :option:`mongos --configdb` string and working
   backward. To keep the cluster online, make sure at least one config
   server is always running. For each config server upgrade, follow the
   instructions below in :ref:`upgrade-mongodb-instance`

   .. example:: Given the following config string:

      .. code-block:: sh

         mongos --configdb cfg0.example.net:27019,cfg1.example.net:27019,cfg2.example.net:27019

      You would upgrade the config servers in the following order:

      1. cfg2.example.net
      2. cfg1.example.net
      3. cfg0.example.net

#. Upgrade each shard.

   - If a shard is a replica set, upgrade the shard using the
     procedure below titled :ref:`upgrade-replica-set`.

   - If a shard is a standalone instance, upgrade the shard using the
     procedure below titled
     :ref:`upgrade-mongodb-instance`.

#. Re-enable the balancer, as described in :ref:`sharding-balancing-re-enable`.

.. _upgrade-replica-set:

Upgrade Replica Sets
--------------------

To upgrade a replica set, upgrade each member individually, starting with
the :term:`secondaries <secondary>` and finishing with the
:term:`primary`. Plan the upgrade during a predefined maintenance window.

.. TODO check: Upgrading a replica set typically involves 10-20 seconds of downtime.

Upgrade Secondaries
~~~~~~~~~~~~~~~~~~~

Upgrade each secondary separately as follows:

1. Upgrade the secondary's :program:`mongod` binary by following the
   instructions below in :ref:`upgrade-mongodb-instance`.

#. After upgrading a secondary, wait for the secondary to recover to
   the ``SECONDARY`` state before upgrading the next instance. To
   check the member's state, issue :method:`rs.status()` in the
   :program:`mongo` shell.

   The secondary may briefly go into ``STARTUP2`` or ``RECOVERING``.
   This is normal. Make sure to wait for the secondary to fully recover
   to ``SECONDARY`` before you continue the upgrade.

Upgrade the Primary
~~~~~~~~~~~~~~~~~~~

1. Step down the primary to initiate the normal :ref:`failover
   <replica-set-failover>` procedure. Using one of the following:

   - The :method:`rs.stepDown()` helper in the :program:`mongo` shell.

   - The :dbcommand:`replSetStepDown` database command.

   During failover, the set cannot accept writes. Typically this takes
   10-20 seconds. Plan the upgrade during a predefined maintenance
   window.

   .. note:: Stepping down the primary is preferable to directly
      *shutting down* the primary. Stepping down expedites the
      failover procedure.

#. Once the primary has stepped down, call the :method:`rs.status()`
   method from the :program:`mongo` shell until you see that another
   member has assumed the ``PRIMARY`` state.

#. Shut down the original primary and upgrade its instance by
   following the instructions below in :ref:`upgrade-mongodb-instance`.
================================================
Use Capped Collections for Fast Writes and Reads
================================================

.. default-domain:: mongodb

Use Capped Collections for Fast Writes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:doc:`/core/capped-collections` are circular, fixed-size collections
that keep documents well-ordered, even without the use of an
index. This means that capped collections can receive very high-speed
writes and sequential reads.

These collections are particularly useful for keeping log files but are
not limited to that purpose. Use capped collections where appropriate.

Use Natural Order for Fast Reads
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To return documents in the order they exist on disk, return sorted
operations using the :operator:`$natural` operator. On a capped
collection, this also returns the documents in the order in which they
were written.

:term:`Natural
order <natural order>` does not use indexes but can be fast for
operations when you want to select the first or last items on
disk.

.. seealso:: :method:`~cursor.sort()` and :method:`~cursor.limit()`.
=====================
Use Database Commands
=====================

.. default-domain:: mongodb

The MongoDB command interface provides access to all :term:`non CRUD
<crud>` database operations. Fetching server stats, initializing a
replica set, and running a map-reduce job are all accomplished with
commands.

See :doc:`/reference/command` for list of all commands sorted by
function, and :doc:`/reference/command` for a list of all commands
sorted alphabetically.

Database Command Form
---------------------

You specify a command first by constructing a standard :term:`BSON`
document whose first key is the name of the command. For example,
specify the :dbcommand:`isMaster` command using the following
:term:`BSON` document:

.. code-block:: javascript

   { isMaster: 1 }

.. _issue-commands:

Issue Commands
--------------

The :program:`mongo` shell provides a helper method for running
commands called :method:`db.runCommand()`. The following operation in
:program:`mongo` runs the above command:

.. code-block:: javascript

   db.runCommand( { isMaster: 1 } )

Many :doc:`drivers </applications/drivers>` provide an equivalent for
the :method:`db.runCommand()` method. Internally, running commands
with :method:`db.runCommand()` is equivalent to a special query
against the :term:`$cmd` collection.

Many common commands have their own shell helpers or wrappers in the
:program:`mongo` shell and drivers, such as the
:method:`db.isMaster()` method in the :program:`mongo` JavaScript
shell.

.. _admin-command:

``admin`` Database Commands
---------------------------

You must run some commands on the :term:`admin database`. Normally,
these operations resemble the followings:

.. code-block:: javascript

   use admin
   db.runCommand( {buildInfo: 1} )

However, there's also a command helper that automatically runs the
command in the context of the ``admin`` database:

.. code-block:: javascript

   db._adminCommand( {buildInfo: 1} )

Command Responses
-----------------

All commands return, at minimum, a document with an ``ok`` field
indicating whether the command has succeeded:

.. code-block:: javascript

   { 'ok': 1 }

Failed commands return the ``ok`` field with a value of ``0``.
======================
Verify User Privileges
======================

.. default-domain:: mongodb

Overview
--------

A user's privileges determine the access the user has to MongoDB
:ref:`resources <resource-document>` and the :ref:`actions
<security-user-actions>` that user can perform. Users receive privileges
through role assignments. A user can have multiple roles, and each role
can have multiple privileges.

For an overview of roles and privileges, see :ref:`authorization`.

Prerequisites
-------------

.. include:: /includes/access-roles-info.rst

Procedure
---------

.. include:: /includes/steps/verify-user-privileges.rst
==========
View Roles
==========

.. default-domain:: mongodb

Overview
--------

A :ref:`role <roles>` grants privileges to the users who are assigned the
role. Each role is scoped to a particular database, but MongoDB stores all
role information in the :data:`admin.system.roles` collection in the
``admin`` database.

Prerequisites
-------------

.. include:: /includes/access-roles-info.rst

Procedures
----------

The following procedures use the :dbcommand:`rolesInfo` command. You also
can use the methods :method:`db.getRole()` (singular) and
:method:`db.getRoles()`.

View a Role in the Current Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the role is in the current database, you can refer to the role
by name, as for the role ``dataEntry`` on the current database:

.. code-block:: javascript

   db.runCommand({ rolesInfo: "dataEntry" })

View a Role in a Different Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the role is in a different database, specify the role as a document.
Use the following form:

.. code-block:: javascript

   { role: "<role name>", db: "<role db>" }

To view the custom ``appWriter`` role in the ``orders``
database, issue the following command from the :program:`mongo` shell:

.. code-block:: javascript

   db.runCommand({ rolesInfo: { role: "appWriter", db: "orders" } })

View Multiple Roles
~~~~~~~~~~~~~~~~~~~

To view information for multiple roles, specify each role as a document or
string in an array.

To view the custom ``appWriter`` and ``clientWriter`` roles
in the ``orders`` database, as well as the ``dataEntry`` role on the
current database, use the following command from the :program:`mongo`
shell:

.. code-block:: javascript

   db.runCommand( { rolesInfo: [ { role: "appWriter", db: "orders" },
                                 { role: "clientWriter", db: "orders" },
                                 "dataEntry" ]
                  } )

View All Custom Roles
~~~~~~~~~~~~~~~~~~~~~

To view the all custom roles, query :ref:`admin.system.roles
<admin-system-roles-collection>` collection directly, for example:

.. code-block:: javascript

   db = db.getSiblingDB('admin')
   db.system.roles.find()
.. _sharding-manage-shards:

==========================
View Cluster Configuration
==========================

.. default-domain:: mongodb

.. _sharding-procedure-list-databases:

List Databases with Sharding Enabled
------------------------------------

To list the databases that have sharding enabled, query the
``databases`` collection in the :ref:`config-database`.
A database has sharding enabled if the value of the ``partitioned``
field is ``true``. Connect to a :program:`mongos` instance with a
:program:`mongo` shell, and run the following operation to get a full
list of databases with sharding enabled:

.. code-block:: javascript

   use config
   db.databases.find( { "partitioned": true } )

.. example:: You can use the following sequence of commands when to
   return a list of all databases in the cluster:

   .. code-block:: javascript

      use config
      db.databases.find()

   If this returns the following result set:

   .. code-block:: javascript

      { "_id" : "admin", "partitioned" : false, "primary" : "config" }
      { "_id" : "animals", "partitioned" : true, "primary" : "m0.example.net:30001" }
      { "_id" : "farms", "partitioned" : false, "primary" : "m1.example2.net:27017" }

   Then sharding is only enabled for the ``animals`` database.

.. _sharding-procedure-list-shards:

List Shards
-----------

To list the current set of configured shards, use the :dbcommand:`listShards`
command, as follows:

.. code-block:: javascript

   use admin
   db.runCommand( { listShards : 1 } )

.. _sharding-procedure-view-clusters:

View Cluster Details
--------------------

To view cluster details, issue :method:`db.printShardingStatus()` or
:method:`sh.status()`. Both methods return the same output.

.. example:: In the following example output from :method:`sh.status()`

   - ``sharding version`` displays the version number of the shard
     metadata.

   - ``shards`` displays a list of the :program:`mongod` instances
     used as shards in the cluster.

   - ``databases`` displays all databases in the cluster,
     including database that do not have sharding enabled.

   - The ``chunks`` information for the ``foo`` database displays how
     many chunks are on each shard and displays the range of each chunk.

   .. code-block:: javascript

      --- Sharding Status ---
        sharding version: { "_id" : 1, "version" : 3 }
        shards:
          {  "_id" : "shard0000",  "host" : "m0.example.net:30001" }
          {  "_id" : "shard0001",  "host" : "m3.example2.net:50000" }
        databases:
          {  "_id" : "admin",  "partitioned" : false,  "primary" : "config" }
          {  "_id" : "contacts",  "partitioned" : true,  "primary" : "shard0000" }
              foo.contacts
                  shard key: { "zip" : 1 }
                  chunks:
                      shard0001    2
                      shard0002    3
                      shard0000    2
                  { "zip" : { "$minKey" : 1 } } -->> { "zip" : "56000" } on : shard0001 { "t" : 2, "i" : 0 }
                  { "zip" : 56000 } -->> { "zip" : "56800" } on : shard0002 { "t" : 3, "i" : 4 }
                  { "zip" : 56800 } -->> { "zip" : "57088" } on : shard0002 { "t" : 4, "i" : 2 }
                  { "zip" : 57088 } -->> { "zip" : "57500" } on : shard0002 { "t" : 4, "i" : 3 }
                  { "zip" : 57500 } -->> { "zip" : "58140" } on : shard0001 { "t" : 4, "i" : 0 }
                  { "zip" : 58140 } -->> { "zip" : "59000" } on : shard0000 { "t" : 4, "i" : 1 }
                  { "zip" : 59000 } -->> { "zip" : { "$maxKey" : 1 } } on : shard0000 { "t" : 3, "i" : 3 }
          {  "_id" : "test",  "partitioned" : false,  "primary" : "shard0000" }
=====================================
Write Scripts for the ``mongo`` Shell
=====================================

.. default-domain:: mongodb

You can write scripts for the :program:`mongo` shell in JavaScript
that manipulate data in MongoDB or perform administrative
operation. For more information about the :program:`mongo` shell see
:doc:`/administration/scripting`, and see the :ref:`running-js-scripts-in-mongo-on-mongod-host`
section for more information about using these :program:`mongo`
script.

This tutorial provides an introduction to writing JavaScript that uses
the :program:`mongo` shell to access MongoDB.

.. _mongo-shell-new-connections:

Opening New Connections
-----------------------

From the :program:`mongo` shell or from a JavaScript file, you can
instantiate database connections using the :method:`Mongo()`
constructor:

.. code-block:: javascript

   new Mongo()
   new Mongo(<host>)
   new Mongo(<host:port>)

Consider the following example that instantiates a new connection to
the MongoDB instance running on localhost on the default port and sets
the global ``db`` variable to ``myDatabase`` using the
:method:`~Mongo.getDB()` method:

.. code-block:: javascript

   conn = new Mongo();
   db = conn.getDB("myDatabase");

Additionally, you can use the :method:`connect()` method
to connect to the MongoDB instance. The following example connects to
the MongoDB instance that is running on ``localhost`` with the
non-default port ``27020`` and set the global ``db`` variable:

.. code-block:: javascript

   db = connect("localhost:27020/myDatabase");

Differences Between Interactive and Scripted ``mongo``
------------------------------------------------------

When writing scripts for the :program:`mongo` shell, consider the
following:

- To set the ``db`` global variable, use the :method:`~Mongo.getDB()`
  method or the :method:`connect()` method. You can assign the database
  reference to a variable other than ``db``.

- Inside the script, call :method:`db.getLastError()` explicitly to
  wait for the result of :doc:`write operations
  </core/write-operations>`.

- You **cannot** use any shell helper (e.g. ``use <dbname>``, ``show
  dbs``, etc.) inside the JavaScript file because they are not valid
  JavaScript.

  The following table maps the most common :program:`mongo` shell
  helpers to their JavaScript equivalents.

  .. include:: /includes/table/helpers-to-javascript.rst

- In interactive mode, :program:`mongo` prints the results of
  operations including the content of all cursors. In scripts, either
  use the JavaScript ``print()`` function or the :program:`mongo`
  specific ``printjson()`` function which returns formatted JSON.

  .. example::

     To print all items in a result cursor in :program:`mongo` shell
     scripts, use the following idiom:

     .. code-block:: javascript

        cursor = db.collection.find();
        while ( cursor.hasNext() ) {
           printjson( cursor.next() );
        }

.. _mongo-shell-scripting:

Scripting
---------

From the system prompt, use :program:`mongo` to evaluate JavaScript.

``--eval`` option
~~~~~~~~~~~~~~~~~

Use the :option:`--eval <mongo --eval>` option to :program:`mongo` to
pass the shell a JavaScript fragment, as in the following:

.. code-block:: sh

   mongo test --eval "printjson(db.getCollectionNames())"

This returns the output of :method:`db.getCollectionNames()` using the
:program:`mongo` shell connected to the :program:`mongod` or
:program:`mongos` instance running on port ``27017`` on the
``localhost`` interface.

.. _mongo-shell-javascript-file:

Execute a JavaScript file
~~~~~~~~~~~~~~~~~~~~~~~~~

You can specify a ``.js`` file to the :program:`mongo` shell, and
:program:`mongo` will execute the JavaScript directly. Consider the
following example:

.. code-block:: sh

   mongo localhost:27017/test myjsfile.js

This operation executes the ``myjsfile.js`` script in a
:program:`mongo` shell that connects to the ``test`` :term:`database`
on the :program:`mongod` instance accessible via the ``localhost``
interface on port ``27017``.

Alternately, you can specify the mongodb connection parameters inside
of the javascript file using the ``Mongo()`` constructor. See
:ref:`mongo-shell-new-connections` for more information.

.. include:: /includes/fact-execute-javascript-from-shell.rst
.. index:: tutorials
.. _tutorial:
.. _tutorials:

=================
MongoDB Tutorials
=================

This page lists the tutorials available
as part of the :doc:`MongoDB Manual <contents>`. In addition to these
documents, you can refer to the introductory :doc:`MongoDB Tutorial
</tutorial/getting-started>`. If there is a process or pattern that you
would like to see included here, please open a :issue:`Jira Case <DOCS>`.

Getting Started
---------------

- :doc:`/tutorial/install-mongodb-on-linux`
- :doc:`/tutorial/install-mongodb-on-red-hat-centos-or-fedora-linux`
- :doc:`/tutorial/install-mongodb-on-debian`
- :doc:`/tutorial/install-mongodb-on-ubuntu`
- :doc:`/tutorial/install-mongodb-on-os-x`
- :doc:`/tutorial/install-mongodb-on-windows`
- :doc:`/tutorial/getting-started`
- :doc:`/tutorial/generate-test-data`

.. index:: tutorials; administration
.. index:: administration tutorials
.. _administration-tutorials:
.. _tutorials-administration:
.. _tutorial-administration:

Administration
--------------

Replica Sets
~~~~~~~~~~~~

- :doc:`/tutorial/deploy-replica-set`
- :doc:`/tutorial/deploy-replica-set-with-auth`
- :doc:`/tutorial/convert-standalone-to-replica-set`
- :doc:`/tutorial/expand-replica-set`
- :doc:`/tutorial/remove-replica-set-member`
- :doc:`/tutorial/replace-replica-set-member`
- :doc:`/tutorial/adjust-replica-set-member-priority`
- :doc:`/tutorial/resync-replica-set-member`
- :doc:`/tutorial/deploy-geographically-distributed-replica-set`
- :doc:`/tutorial/change-oplog-size`
- :doc:`/tutorial/force-member-to-be-primary`
- :doc:`/tutorial/change-hostnames-in-a-replica-set`
- :doc:`/tutorial/add-replica-set-arbiter`
- :doc:`/tutorial/convert-secondary-into-arbiter`
- :doc:`/tutorial/configure-replica-set-secondary-sync-target`
- :doc:`/tutorial/configure-a-delayed-replica-set-member`
- :doc:`/tutorial/configure-a-hidden-replica-set-member`
- :doc:`/tutorial/configure-a-non-voting-replica-set-member`
- :doc:`/tutorial/configure-secondary-only-replica-set-member`
- :doc:`/tutorial/configure-replica-set-tag-sets`
- :doc:`/tutorial/manage-chained-replication`
- :doc:`/tutorial/reconfigure-replica-set-with-unavailable-members`
- :doc:`/tutorial/recover-data-following-unexpected-shutdown`
- :doc:`/tutorial/troubleshoot-replica-sets`

Sharding
~~~~~~~~

- :doc:`/tutorial/deploy-shard-cluster`
- :doc:`/tutorial/convert-replica-set-to-replicated-shard-cluster`
- :doc:`/tutorial/add-shards-to-shard-cluster`
- :doc:`/tutorial/remove-shards-from-cluster`
- :doc:`/tutorial/deploy-config-servers`
- :doc:`/tutorial/migrate-config-servers-with-same-hostname`
- :doc:`/tutorial/migrate-config-servers-with-different-hostnames`
- :doc:`/tutorial/replace-config-server`
- :doc:`/tutorial/migrate-sharded-cluster-to-new-hardware`
- :doc:`/tutorial/backup-sharded-cluster-metadata`
- :doc:`/tutorial/backup-small-sharded-cluster-with-mongodump`
- :doc:`/tutorial/backup-sharded-cluster-with-filesystem-snapshots`
- :doc:`/tutorial/backup-sharded-cluster-with-database-dumps`
- :doc:`/tutorial/restore-single-shard`
- :doc:`/tutorial/restore-sharded-cluster`
- :doc:`/tutorial/schedule-backup-window-for-sharded-clusters`
- :doc:`/tutorial/administer-shard-tags`

Basic Operations
~~~~~~~~~~~~~~~~

- :doc:`/tutorial/use-database-commands`
- :doc:`/tutorial/recover-data-following-unexpected-shutdown`
- :doc:`/tutorial/expire-data`
- :doc:`/tutorial/manage-the-database-profiler`
- :doc:`/tutorial/rotate-log-files`
- :doc:`/tutorial/roll-back-to-v1.8-index`
- :doc:`/tutorial/manage-mongodb-processes`
- :doc:`/tutorial/backup-with-mongodump`
- :doc:`/tutorial/backup-with-filesystem-snapshots`

Security
~~~~~~~~

- :doc:`/tutorial/configure-linux-iptables-firewall`
- :doc:`/tutorial/configure-windows-netsh-firewall`
- :doc:`/tutorial/enable-authentication`
- :doc:`/tutorial/add-user-administrator`
- :doc:`/tutorial/add-user-to-database`
- :doc:`/tutorial/define-roles`
- :doc:`/tutorial/change-user-privileges`
- :doc:`/tutorial/view-roles`
- :doc:`/tutorial/generate-key-file`
- :doc:`/tutorial/control-access-to-mongodb-with-kerberos-authentication`
- :doc:`/tutorial/create-a-vulnerability-report`

.. index:: tutorials; development patterns
.. index:: development tutorials
.. _tutorials-development-patterns:
.. _tutorial-development-patterns:

Development Patterns
--------------------

- :doc:`/tutorial/perform-two-phase-commits`
- :doc:`/tutorial/isolate-sequence-of-operations`
- :doc:`/tutorial/create-an-auto-incrementing-field`
- :doc:`/tutorial/enforce-unique-keys-for-sharded-collections`
- :doc:`/applications/aggregation`
- :doc:`/tutorial/model-data-for-keyword-search`
- :doc:`/tutorial/limit-number-of-elements-in-updated-array`
- :doc:`/tutorial/perform-incremental-map-reduce`
- :doc:`/tutorial/troubleshoot-map-function`
- :doc:`/tutorial/troubleshoot-reduce-function`
- :doc:`/tutorial/store-javascript-function-on-server`

.. index:: tutorials; text search
.. index::  text search tutorials
.. _tutorials-text-search:
.. _tutorial-text-search:

Text Search Patterns
--------------------

- :doc:`/tutorial/enable-text-search`
- :doc:`/tutorial/create-text-index-on-multiple-fields`
- :doc:`/tutorial/specify-language-for-text-index`
- :doc:`/tutorial/avoid-text-index-name-limit`
- :doc:`/tutorial/control-results-of-text-search`
- :doc:`/tutorial/limit-number-of-items-scanned-for-text-search`

Data Modeling Patterns
----------------------

- :doc:`/tutorial/model-embedded-one-to-one-relationships-between-documents`
- :doc:`/tutorial/model-embedded-one-to-many-relationships-between-documents`
- :doc:`/tutorial/model-referenced-one-to-many-relationships-between-documents`
- :doc:`/tutorial/model-data-for-atomic-operations`
- :doc:`/tutorial/model-tree-structures-with-parent-references`
- :doc:`/tutorial/model-tree-structures-with-child-references`
- :doc:`/tutorial/model-tree-structures-with-materialized-paths`
- :doc:`/tutorial/model-tree-structures-with-nested-sets`
